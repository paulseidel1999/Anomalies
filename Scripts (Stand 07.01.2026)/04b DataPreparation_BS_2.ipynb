{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctXZHVs-zurM"
   },
   "source": [
    "### Mount Google Drive, Import Libraries and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16507,
     "status": "ok",
     "timestamp": 1765204621256,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "DVbbtM4ztajO",
    "outputId": "dad71183-8a46-4a74-8e18-c362f660fd97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: False\n",
      "BASE_PATH: /home/jovyan/work/hpool1/pseidel/test\n",
      "Importing numpy ...\n",
      "numpy OK\n",
      "Importing scipy ...\n",
      "scipy OK\n",
      "Importing pandas ...\n",
      "pandas OK\n",
      "Importing linearmodels ...\n",
      "linearmodels OK\n",
      "Importing xlsxwriter ...\n",
      "xlsxwriter OK\n",
      "Paths configured. Temp outputs -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview\n",
      "Example input path -> /home/jovyan/work/hpool1/pseidel/test/Input/WSFV_f_20250131.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT SETUP + PATH CONFIGURATION (SERVER / COLAB COMPATIBLE)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) HARD SAFETY: cap native thread usage (prevents pthread_create EAGAIN)\n",
    "#    MUST be set before importing numpy / scipy / pandas\n",
    "# -----------------------------------------------------------------------------\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"BLIS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Detect environment\n",
    "# -----------------------------------------------------------------------------\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) (Colab only) Mount Google Drive\n",
    "# -----------------------------------------------------------------------------\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "else:\n",
    "    # Server base path (your target)\n",
    "    BASE_PATH = \"/home/jovyan/work/hpool1/pseidel/test\"\n",
    "\n",
    "print(\"IN_COLAB:\", IN_COLAB)\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Sanity checks: path exists + write permission\n",
    "# -----------------------------------------------------------------------------\n",
    "BASE = Path(BASE_PATH)\n",
    "if not BASE.exists():\n",
    "    raise FileNotFoundError(f\"BASE_PATH does not exist: {BASE}\")\n",
    "\n",
    "# quick write test (fails fast if you don't have permissions)\n",
    "test_file = BASE / \".write_test_tmp\"\n",
    "try:\n",
    "    test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "    test_file.unlink()\n",
    "except Exception as e:\n",
    "    raise PermissionError(f\"No write permission in {BASE}. Error: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Environment check: ensure required packages import cleanly\n",
    "# -----------------------------------------------------------------------------\n",
    "required_packages = [\"numpy\", \"scipy\", \"pandas\", \"linearmodels\", \"xlsxwriter\"]\n",
    "\n",
    "for pkg in required_packages:\n",
    "    print(f\"Importing {pkg} ...\")\n",
    "    importlib.import_module(pkg)\n",
    "    print(f\"{pkg} OK\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Base paths and input/output locations\n",
    "# -----------------------------------------------------------------------------\n",
    "Input_file_path   = str(BASE / \"Input\")\n",
    "Temp_file_path    = str(BASE / \"Temp\")\n",
    "Output_file_path  = str(BASE / \"Output\")\n",
    "\n",
    "Fundamentals_file_path = f\"{Input_file_path}/WSFV_f_20250131.txt\"\n",
    "Current_file_path      = f\"{Input_file_path}/WSCurrent_f_20250131.txt\"\n",
    "Calendar_file_path     = f\"{Input_file_path}/WSCalendarPrd_f_20250131.txt\"\n",
    "Meta_file_path         = f\"{Input_file_path}/WSMetaData_f_20250131.txt\"\n",
    "Excel_file_path        = f\"{Input_file_path}/WS PIT Table Definitions V5 with start dates.xls\"\n",
    "\n",
    "MarketValues_file_path          = f\"{Input_file_path}/Daily MV USD\"\n",
    "MarketValues_file_path_LC       = f\"{Input_file_path}/Daily MV LC\"\n",
    "DailyTotalReturns_file_path     = f\"{Input_file_path}/Daily Returns USD\"\n",
    "DailyIndexReturns_file_path     = f\"{Input_file_path}/Daily Index Returns USD\"\n",
    "Constituents_file_path          = f\"{Input_file_path}/Constituents.01.csv\"\n",
    "UniversalMatching_file_path     = f\"{Input_file_path}/Universal Matching File\"\n",
    "\n",
    "Temp_file_path_GO  = f\"{Temp_file_path}/TempGeneralOverview\"\n",
    "Temp_file_path_EoC = f\"{Temp_file_path}/TempExtractionofCharacteristics\"\n",
    "Temp_file_path_DP  = f\"{Temp_file_path}/TempDataPreparation\"\n",
    "Temp_file_path_A   = f\"{Temp_file_path}/TempAnomalies\"\n",
    "Temp_file_path_R   = f\"{Temp_file_path}/TempRegressionModel\"\n",
    "\n",
    "Relevant_items_path   = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_A = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_B = f\"{Input_file_path}/RelevantItemsB.txt\"\n",
    "Relevant_items_path_C = f\"{Input_file_path}/RelevantItemsC.txt\"\n",
    "Relevant_items_path_D = f\"{Input_file_path}/RelevantItemsD.txt\"\n",
    "\n",
    "Subset_file_path = f\"{Temp_file_path_GO}/Subsets\"\n",
    "Fundamentals_clean_file_path = f\"{Temp_file_path_GO}/Fundamentals_clean.txt\"\n",
    "Current_clean_file_path      = f\"{Temp_file_path_GO}/Current_clean.txt\"\n",
    "Calendar_clean_file_path     = f\"{Temp_file_path_GO}/Input/Calendar_clean.txt\"\n",
    "Meta_clean_file_path         = f\"{Temp_file_path_GO}/Input/Meta_clean.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Ensure required directories exist\n",
    "# -----------------------------------------------------------------------------\n",
    "Path(Output_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_GO).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_EoC).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_A).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(Subset_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Path(Calendar_clean_file_path).parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Streaming / deduplication settings\n",
    "# -----------------------------------------------------------------------------\n",
    "CHUNK_SIZE = 2_000_000\n",
    "DATE_COL = \"PIT Date\"\n",
    "DEDUP_KEYS = [\"ID\", \"ItemCode\", DATE_COL]\n",
    "\n",
    "print(\"Paths configured. Temp outputs ->\", Temp_file_path_GO)\n",
    "print(\"Example input path ->\", Fundamentals_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           754Gi       241Gi       167Gi        55Mi       354Gi       512Gi\n",
      "Swap:             0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQMCPR6XzurR"
   },
   "source": [
    "### Import Data Files to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3548,
     "status": "ok",
     "timestamp": 1765204624809,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "zW-FBOO8zurR",
    "outputId": "cfc22188-b89d-4e3e-aa9c-bfb7ce05221d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imported RelevantItems.txt as DataFrame 'RelevantItems'\n",
      "Preview of 'RelevantItems':\n",
      "  ItemCode\n",
      "0    01001\n",
      "1    01051\n",
      "2    01075\n",
      "3    01101\n",
      "4    01151 \n",
      "\n",
      "\n",
      "Imported CountryCodes.txt as DataFrame 'CountryCodes'\n",
      "Preview of 'CountryCodes':\n",
      "  NatCo ImplCountry\n",
      "0   012     Algeria\n",
      "1   440   Lithuania\n",
      "2   025   Argentina\n",
      "3   442  Luxembourg\n",
      "4   036   Australia \n",
      "\n",
      "\n",
      "Imported ADR_clean.txt as DataFrame 'ADR_clean'\n",
      "Preview of 'ADR_clean':\n",
      "          ID ADRIndicator\n",
      "0  C036F63D0            N\n",
      "1  C056879S0            X\n",
      "2  C2461T100            N\n",
      "3  C2504O500            N\n",
      "4  C250C9180            N \n",
      "\n",
      "\n",
      "Imported CompanyName_clean.txt as DataFrame 'CompanyName_clean'\n",
      "Preview of 'CompanyName_clean':\n",
      "          ID                               CompanyName\n",
      "0  C00948205             AGRIFORCE GROWING SYSTEMS LTD\n",
      "1  C02500770            PEUGEOT CITROEN ARGENTINA S.A.\n",
      "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA\n",
      "3  C02520220                       ALPARGATAS S.A.I.C.\n",
      "4  C02520230               ALUAR ALUMINIO ARGENTINO SA \n",
      "\n",
      "\n",
      "Imported CurrencyCodes_clean.txt as DataFrame 'CurrencyCodes_clean'\n",
      "Preview of 'CurrencyCodes_clean':\n",
      "          ID CurrencyCode1 SwitchDate1 CurrencyCode2 SwitchDate2  \\\n",
      "0  C00948205           Usd  2021-07-09           NaN         NaN   \n",
      "1  C02500770           Ars  1995-12-29           NaN         NaN   \n",
      "2  C0250077A           Ars  1999-10-01           NaN         NaN   \n",
      "3  C0250077B           Ars  1999-10-01           NaN         NaN   \n",
      "4  C0250077C           Ars  1999-10-01           NaN         NaN   \n",
      "\n",
      "  CurrencyCode3 SwitchDate3 CurrentCurrency  \n",
      "0           NaN         NaN             Usd  \n",
      "1           NaN         NaN             Ars  \n",
      "2           NaN         NaN             Ars  \n",
      "3           NaN         NaN             Ars  \n",
      "4           NaN         NaN             Ars   \n",
      "\n",
      "\n",
      "Imported FYE_clean.txt as DataFrame 'FYE_clean'\n",
      "Preview of 'FYE_clean':\n",
      "          ID    FY FYE Month\n",
      "0  C00948205  2018  December\n",
      "1  C00948205  2019  December\n",
      "2  C00948205  2020     March\n",
      "3  C00948205  2021  December\n",
      "4  C00948205  2022  December \n",
      "\n",
      "\n",
      "Imported ID_clean.txt as DataFrame 'ID_clean'\n",
      "Preview of 'ID_clean':\n",
      "          ID\n",
      "0  C02500770\n",
      "1  C02520200\n",
      "2  C02520220\n",
      "3  C02520230\n",
      "4  C02520240 \n",
      "\n",
      "\n",
      "Imported UpdateCodes_clean.txt as DataFrame 'UpdateCodes_clean'\n",
      "Preview of 'UpdateCodes_clean':\n",
      "          ID    PIT Date Frequency FiscalPeriod UpdateCode\n",
      "0  C02500770  1995-12-29         A         1985          3\n",
      "1  C02500770  1995-12-29         A         1986          3\n",
      "2  C02500770  1995-12-29         A         1987          3\n",
      "3  C02500770  1995-12-29         A         1988          3\n",
      "4  C02500770  1995-12-29         A         1989          3 \n",
      "\n",
      "\n",
      "Imported ValueCoding.txt as DataFrame 'ValueCoding'\n",
      "Preview of 'ValueCoding':\n",
      "  ItemCode                           ItemName  Source\n",
      "0    05006               Market Price Current  Market\n",
      "1    05007      Market Price YTD High Current  Market\n",
      "2    05008       Market Price YTD Low Current  Market\n",
      "3    05009              Date of Current Price  Market\n",
      "4    05091  Market Price 52 Week High Current  Market \n",
      "\n",
      "\n",
      "Identifying subset files to process...\n",
      "  Found subset_01001.txt\n",
      "  Found subset_01051.txt\n",
      "  Found subset_01075.txt\n",
      "  Found subset_01101.txt\n",
      "  Found subset_01151.txt\n",
      "  Found subset_01250.txt\n",
      "  Found subset_01451.txt\n",
      "  Found subset_01551.txt\n",
      "  Found subset_01706.txt\n",
      "  Found subset_02001.txt\n",
      "  Found subset_02051.txt\n",
      "  Found subset_02101.txt\n",
      "  Found subset_02149.txt\n",
      "  Found subset_02201.txt\n",
      "  Found subset_02250.txt\n",
      "  Found subset_02256.txt\n",
      "  Found subset_02257.txt\n",
      "  Found subset_02258.txt\n",
      "  Found subset_02263.txt\n",
      "  Found subset_02501.txt\n",
      "  Found subset_02652.txt\n",
      "  Found subset_02999.txt\n",
      "  Found subset_03040.txt\n",
      "  Found subset_03051.txt\n",
      "  Found subset_03063.txt\n",
      "  Found subset_03066.txt\n",
      "  Found subset_03101.txt\n",
      "  Found subset_03251.txt\n",
      "  Found subset_03263.txt\n",
      "  Found subset_03273.txt\n",
      "  Found subset_03351.txt\n",
      "  Found subset_03426.txt\n",
      "  Found subset_03451.txt\n",
      "  Found subset_03501.txt\n",
      "  Found subset_04251.txt\n",
      "  Found subset_04401.txt\n",
      "  Found subset_04551.txt\n",
      "  Found subset_04601.txt\n",
      "  Found subset_04701.txt\n",
      "  Found subset_04751.txt\n",
      "  Found subset_04860.txt\n",
      "  Found subset_04870.txt\n",
      "  Found subset_04890.txt\n",
      "  Found subset_05202.txt\n",
      "  Found subset_04201.txt\n",
      "  Found subset_04225.txt\n",
      "  Found subset_04831.txt\n",
      "  Found subset_04351.txt\n",
      "  Found subset_05508.txt\n",
      "\n",
      "Identified 49 subset files for processing.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell:\n",
    "#\n",
    "#   1. Defines a helper function `import_file_to_dataframe` that reads a pipe-delimited\n",
    "#      text file into a pandas DataFrame (all columns as string; returns None on error).\n",
    "#   2. Imports a list of \"input\" files from Input_file_path into DataFrames\n",
    "#      (RelevantItems, CountryCodes, ...), storing them in globals() by filename.\n",
    "#   3. Imports a list of \"temp\" files from Temp_file_path_EoC into DataFrames\n",
    "#      (ADR_clean, CompanyName_clean, CurrencyCodes_clean, FYE_clean, ID_clean,\n",
    "#       UpdateCodes_clean, ValueCoding), also stored in globals().\n",
    "#   4. Identifies which subset_*.txt files exist in Subset_file_path based on the IDs\n",
    "#      listed in RelevantItems.txt, and records their names (without .txt) in\n",
    "#      `successful_subset_names`.\n",
    "#\n",
    "# No actual subset data is loaded here; that is deferred to later steps to keep\n",
    "# memory usage under control.\n",
    "\n",
    "\n",
    "# Function to import a file and return a pandas DataFrame\n",
    "def import_file_to_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Import a pipe-separated text file as a pandas DataFrame.\n",
    "\n",
    "    - Uses sep='|' to read pipe-delimited files.\n",
    "    - Reads all columns as strings (dtype=str), which helps preserve things like\n",
    "      leading zeros in codes (e.g., NatCo, ItemCode).\n",
    "    - Returns None on failure and prints a brief error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='|', dtype=str)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error importing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Import files from Input directory\n",
    "# -------------------------------------------------------------------------\n",
    "input_files_to_import = [\"RelevantItems.txt\", \"CountryCodes.txt\"]\n",
    "\n",
    "for file_name in input_files_to_import:\n",
    "    file_path = os.path.join(Input_file_path, file_name)\n",
    "    var_name = file_name.replace(\".txt\", \"\")  # e.g. \"RelevantItems\"\n",
    "    globals()[var_name] = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if globals()[var_name] is not None:\n",
    "        print(f\"\\nImported {file_name} as DataFrame '{var_name}'\")\n",
    "        print(f\"Preview of '{var_name}':\")\n",
    "        print(globals()[var_name].head(), \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Import files from Temp directory (end-of-cleaning stage)\n",
    "# -------------------------------------------------------------------------\n",
    "temp_files_to_import = [\n",
    "    \"ADR_clean.txt\",\n",
    "    \"CompanyName_clean.txt\",\n",
    "    \"CurrencyCodes_clean.txt\",\n",
    "    \"FYE_clean.txt\",\n",
    "    \"ID_clean.txt\",\n",
    "    \"UpdateCodes_clean.txt\",\n",
    "    \"ValueCoding.txt\"\n",
    "]\n",
    "\n",
    "for file_name in temp_files_to_import:\n",
    "    file_path = os.path.join(Temp_file_path_EoC, file_name)\n",
    "    var_name = file_name.replace(\".txt\", \"\")  # e.g. \"ADR_clean\"\n",
    "    globals()[var_name] = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if globals()[var_name] is not None:\n",
    "        print(f\"\\nImported {file_name} as DataFrame '{var_name}'\")\n",
    "        print(f\"Preview of '{var_name}':\")\n",
    "        print(globals()[var_name].head(), \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Identify subset files that exist for the relevant items\n",
    "# -------------------------------------------------------------------------\n",
    "successful_subset_names = []\n",
    "\n",
    "if 'RelevantItems' in globals() and RelevantItems is not None:\n",
    "    # Assume first column of RelevantItems holds the item IDs used in subset filenames\n",
    "    relevant_ids = RelevantItems.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "    print(\"\\nIdentifying subset files to process...\")\n",
    "    for item_id in relevant_ids:\n",
    "        file_name = f\"subset_{item_id}.txt\"\n",
    "        file_path = os.path.join(Subset_file_path, file_name)\n",
    "\n",
    "        # Check the existence of each candidate subset file\n",
    "        if os.path.exists(file_path):\n",
    "            successful_subset_names.append(f\"subset_{item_id}\")\n",
    "            print(f\"  Found {file_name}\")\n",
    "        else:\n",
    "            print(f\"  File not found: {file_name}. Skipping.\")\n",
    "\n",
    "    print(f\"\\nIdentified {len(successful_subset_names)} subset files for processing.\")\n",
    "else:\n",
    "    print(\"RelevantItems DataFrame not found or is empty. Cannot identify subset files.\")\n",
    "\n",
    "# Note: actual loading and processing of subset files happens later, in\n",
    "# batch-based steps, to manage memory usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmcsWZB0oDMl"
   },
   "source": [
    "# 4.0. Extracting the most recent, annualized values per PIT Date (incl. Plausibility checks for the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcll5N-9z4yZ"
   },
   "source": [
    "## 4.1. Split according to source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1765204624974,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "DWNOVA9Iz4ya",
    "outputId": "87d5cc07-acea-4da5-90c0-b08d927071da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ValueCoding DataFrame...\n",
      "\n",
      "Processed ValueCoding DataFrame (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>ItemName</th>\n",
       "      <th>Source</th>\n",
       "      <th>ItemName_Sanitized</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05006</td>\n",
       "      <td>Market Price Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05007</td>\n",
       "      <td>Market Price YTD High Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_YTD_High_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05008</td>\n",
       "      <td>Market Price YTD Low Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_YTD_Low_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05009</td>\n",
       "      <td>Date of Current Price</td>\n",
       "      <td>Market</td>\n",
       "      <td>Date_of_Current_Price</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05091</td>\n",
       "      <td>Market Price 52 Week High Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_52_Week_High_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ItemCode                           ItemName  Source  \\\n",
       "0    05006               Market Price Current  Market   \n",
       "1    05007      Market Price YTD High Current  Market   \n",
       "2    05008       Market Price YTD Low Current  Market   \n",
       "3    05009              Date of Current Price  Market   \n",
       "4    05091  Market Price 52 Week High Current  Market   \n",
       "\n",
       "                  ItemName_Sanitized Category  \n",
       "0               Market_Price_Current    Mixed  \n",
       "1      Market_Price_YTD_High_Current    Mixed  \n",
       "2       Market_Price_YTD_Low_Current    Mixed  \n",
       "3              Date_of_Current_Price    Mixed  \n",
       "4  Market_Price_52_Week_High_Current    Mixed  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Annualized items: 148\n",
      "Number of Mixed items: 141\n",
      "Number of Special items: 62\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell processes a ValueCoding DataFrame and assigns a Category to each item\n",
    "# (per sanitized item name), based on its data sources:\n",
    "#\n",
    "#   1. Validates that `ValueCoding` exists and is non-empty.\n",
    "#   2. Sanitizes `ItemName` to a filesystem-safe `ItemName_Sanitized` (same rules as\n",
    "#      used for filenames).\n",
    "#   3. Normalizes the `Source` column (string type, trimmed).\n",
    "#   4. Groups all distinct sources per `ItemName_Sanitized`.\n",
    "#   5. Uses `decide_category` to map each sanitized name to a Category:\n",
    "#        - Hardcoded overrides for certain items.\n",
    "#        - Generic rules:\n",
    "#             * presence of IS / Other  -> \"Mixed\"\n",
    "#             * presence of Market / BS -> \"Annualized\"\n",
    "#             * presence of CFS         -> \"Special\"\n",
    "#        - otherwise                    -> None\n",
    "#   6. Attaches the Category back to each row based on `ItemName_Sanitized`.\n",
    "#   7. Creates three unique-item DataFrames:\n",
    "#        - `annualized_items`\n",
    "#        - `mixed_items`\n",
    "#        - `special_items`\n",
    "#   8. Exposes the processed objects in `globals()` for use in later cells.\n",
    "#   9. Shows a sample and prints counts of each category.\n",
    "#\n",
    "# If `ValueCoding` is not present or is empty, processing is skipped.\n",
    "\n",
    "# CELL 1 — Process ValueCoding and assign Category per ItemName_Sanitized\n",
    "\n",
    "if 'ValueCoding' in globals() and ValueCoding is not None and not ValueCoding.empty:\n",
    "    # Inform that processing of ValueCoding is starting\n",
    "    print(\"Processing ValueCoding DataFrame...\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original ValueCoding\n",
    "    value_coding_processed = ValueCoding.copy()\n",
    "\n",
    "    # --- Sanitize ItemName ---\n",
    "    # Ensure ItemName is treated as string (avoid issues with numbers / NaNs)\n",
    "    value_coding_processed['ItemName'] = value_coding_processed['ItemName'].astype(str)\n",
    "\n",
    "    # First pass: replace spaces and certain filesystem-unsafe characters with underscores\n",
    "    # Same rule set as used for building filenames elsewhere\n",
    "    value_coding_processed['ItemName_Sanitized'] = value_coding_processed['ItemName'].str.replace(\n",
    "        r'[ \\-/\\:\\\\*\\?\"<>|]', '_', regex=True\n",
    "    )\n",
    "    # Second pass: strip any remaining characters not in [word chars, dot, hyphen]\n",
    "    value_coding_processed['ItemName_Sanitized'] = value_coding_processed['ItemName_Sanitized'].str.replace(\n",
    "        r'[^\\w.-]', '', regex=True\n",
    "    )\n",
    "\n",
    "    # --- Normalize Source ---\n",
    "    # Convert Source to string and strip leading/trailing whitespace\n",
    "    value_coding_processed['Source'] = (\n",
    "        value_coding_processed['Source']\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Group all sources per sanitized name\n",
    "    # ------------------------------------------------------------------\n",
    "    # For each ItemName_Sanitized, collect the set of all non-null sources\n",
    "    sources_per_name = (\n",
    "        value_coding_processed\n",
    "        .groupby('ItemName_Sanitized')['Source']\n",
    "        .apply(lambda s: set(s.dropna()))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Helper to decide category per sanitized name\n",
    "    # ------------------------------------------------------------------\n",
    "    def decide_category(name, sources: set):\n",
    "        \"\"\"\n",
    "        Decide a category string (\"Mixed\", \"Annualized\", \"Special\", or None)\n",
    "        for a given sanitized item name based on its set of sources.\n",
    "        \"\"\"\n",
    "        # Item-specific overrides (these take precedence over generic rules)\n",
    "        if name == 'Depreciation_Depletion__Amortization':\n",
    "            # Prefer 'IS' interpretation -> treat as Mixed\n",
    "            return 'Mixed'\n",
    "        if name == 'Minority_Interest':\n",
    "            # Prefer 'BS' interpretation -> treat as Annualized\n",
    "            return 'Annualized'\n",
    "\n",
    "        # Generic rules:\n",
    "\n",
    "        # If any of the sources is Income Statement or \"Other\", classify as Mixed\n",
    "        if any(src in ['IS', 'Market'] for src in sources):\n",
    "            return 'Mixed'\n",
    "\n",
    "        # If any of the sources is Market or Balance Sheet, classify as Annualized\n",
    "        if any(src in ['BS'] for src in sources):\n",
    "            return 'Annualized'\n",
    "\n",
    "        # If any of the sources is Cash Flow Statement, classify as Special\n",
    "        if any(src in ['CFS'] for src in sources):\n",
    "            return 'Special'\n",
    "\n",
    "        # If none of the rules matched, leave as None (no clear mapping)\n",
    "        return None\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Build category_map for all sanitized names\n",
    "    # ------------------------------------------------------------------\n",
    "    # Map each sanitized item name to a Category by applying decide_category\n",
    "    category_map = {\n",
    "        name: decide_category(name, srcs)\n",
    "        for name, srcs in sources_per_name.items()\n",
    "    }\n",
    "\n",
    "    # Attach final Category back to each row, via ItemName_Sanitized\n",
    "    value_coding_processed['Category'] = (\n",
    "        value_coding_processed['ItemName_Sanitized'].map(category_map)\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Derive annualized_items / mixed_items / special_items\n",
    "    # as unique per sanitized name\n",
    "    # ------------------------------------------------------------------\n",
    "    annualized_items = (\n",
    "        value_coding_processed[value_coding_processed['Category'] == 'Annualized']\n",
    "        .drop_duplicates(subset=['ItemName_Sanitized'])\n",
    "        .copy()\n",
    "    )\n",
    "    mixed_items = (\n",
    "        value_coding_processed[value_coding_processed['Category'] == 'Mixed']\n",
    "        .drop_duplicates(subset=['ItemName_Sanitized'])\n",
    "        .copy()\n",
    "    )\n",
    "    special_items = (\n",
    "        value_coding_processed[value_coding_processed['Category'] == 'Special']\n",
    "        .drop_duplicates(subset=['ItemName_Sanitized'])\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Export to globals for use in later cells\n",
    "    # ------------------------------------------------------------------\n",
    "    globals()['value_coding_processed'] = value_coding_processed\n",
    "    globals()['annualized_items'] = annualized_items\n",
    "    globals()['mixed_items'] = mixed_items\n",
    "    globals()['special_items'] = special_items\n",
    "    globals()['category_map'] = category_map\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Display sample and counts\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\nProcessed ValueCoding DataFrame (sample):\")\n",
    "    display(value_coding_processed.head())\n",
    "\n",
    "    print(f\"\\nNumber of Annualized items: {len(annualized_items)}\")\n",
    "    print(f\"Number of Mixed items: {len(mixed_items)}\")\n",
    "    print(f\"Number of Special items: {len(special_items)}\")\n",
    "\n",
    "else:\n",
    "    # If ValueCoding is not available or has no rows, skip processing\n",
    "    print(\"ValueCoding DataFrame not found or is empty. Skipping processing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d59odhYbz4yc"
   },
   "source": [
    "### Sort into correct bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1765204625163,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "UqDUV12jz4yc",
    "outputId": "5ed83138-1c26-4baf-e71c-db46471f2ff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying work_subset files and creating variables based on categories...\n",
      "\n",
      "Found 49 work_subset files in Temp directory.\n",
      "  'work_subset_Accounts_Payable.txt' -> Annualized (variable 'Annualized_1').\n",
      "  'work_subset_Capital_Expenditures_Addtns_to_Fixed_Assets.txt' -> Special (variable 'Special_1').\n",
      "  'work_subset_Cash_Dividends_Paid___Total.txt' -> Special (variable 'Special_2').\n",
      "  'work_subset_Cash__Short_Term_Investments.txt' -> Annualized (variable 'Annualized_2').\n",
      "  'work_subset_Com_Pfd_Redeemed_Retired_Converted_Etc..txt' -> Special (variable 'Special_3').\n",
      "  'work_subset_Common_Equity.txt' -> Annualized (variable 'Annualized_3').\n",
      "  'work_subset_Cost_of_Goods_Sold_Excl_Depreciation.txt' -> Mixed (variable 'Mixed_1').\n",
      "  'work_subset_Current_Assets___Total.txt' -> Annualized (variable 'Annualized_4').\n",
      "  'work_subset_Current_Liabilities___Total.txt' -> Annualized (variable 'Annualized_5').\n",
      "  'work_subset_Deferred_Taxes.txt' -> Annualized (variable 'Annualized_6').\n",
      "  'work_subset_Depreciation_Depletion__Amortization.txt' -> Mixed (variable 'Mixed_2').\n",
      "  'work_subset_Disposal_of_Fixed_Assets.txt' -> Special (variable 'Special_4').\n",
      "  'work_subset_Earnings_Per_Share_Fiscal_Year_End.txt' -> Mixed (variable 'Mixed_3').\n",
      "  'work_subset_Extraordinary_Items.txt' -> Special (variable 'Special_5').\n",
      "  'work_subset_Funds_From_For_Other_Operating_Activities.txt' -> Special (variable 'Special_6').\n",
      "  'work_subset_Funds_From_Operations.txt' -> Special (variable 'Special_7').\n",
      "  'work_subset_Income_Taxes.txt' -> Mixed (variable 'Mixed_4').\n",
      "  'work_subset_Income_Taxes_Payable.txt' -> Annualized (variable 'Annualized_7').\n",
      "  'work_subset_Interest_Expense___Total.txt' -> Mixed (variable 'Mixed_5').\n",
      "  'work_subset_Inventories___Total.txt' -> Annualized (variable 'Annualized_8').\n",
      "  'work_subset_Investments_in_Associated_Companies.txt' -> Annualized (variable 'Annualized_9').\n",
      "  'work_subset_Investments_in_Sales__Direct_Financing_Leases.txt' -> Annualized (variable 'Annualized_10').\n",
      "  'work_subset_Long_Term_Borrowings.txt' -> Special (variable 'Special_8').\n",
      "  'work_subset_Long_Term_Debt.txt' -> Annualized (variable 'Annualized_11').\n",
      "  'work_subset_Long_Term_Receivables.txt' -> Annualized (variable 'Annualized_12').\n",
      "  'work_subset_Minority_Interest.txt' -> Annualized (variable 'Annualized_13').\n",
      "  'work_subset_Net_Cash_Flow___Financing.txt' -> Special (variable 'Special_9').\n",
      "  'work_subset_Net_Cash_Flow___Investing.txt' -> Special (variable 'Special_10').\n",
      "  'work_subset_Net_Cash_Flow___Operating_Activities.txt' -> Special (variable 'Special_11').\n",
      "  'work_subset_Net_Income_Before_Extra_Items_Preferred_Divs.txt' -> Mixed (variable 'Mixed_6').\n",
      "  'work_subset_Net_Income_Used_to_Calculate_Basic_EPS.txt' -> Mixed (variable 'Mixed_7').\n",
      "  'work_subset_Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt' -> Special (variable 'Special_12').\n",
      "  'work_subset_Net_Sales_or_Revenues.txt' -> Mixed (variable 'Mixed_8').\n",
      "  'work_subset_Operating_Income.txt' -> Mixed (variable 'Mixed_9').\n",
      "  'work_subset_Other_Assets___Total.txt' -> Annualized (variable 'Annualized_14').\n",
      "  'work_subset_Other_Current_Assets.txt' -> Annualized (variable 'Annualized_15').\n",
      "  'work_subset_Other_Current_Liabilities.txt' -> Annualized (variable 'Annualized_16').\n",
      "  'work_subset_Other_Investments.txt' -> Annualized (variable 'Annualized_17').\n",
      "  'work_subset_Other_Liabilities.txt' -> Annualized (variable 'Annualized_18').\n",
      "  'work_subset_Preferred_Stock.txt' -> Annualized (variable 'Annualized_19').\n",
      "  'work_subset_Property_Plant__Equipment___Net.txt' -> Annualized (variable 'Annualized_20').\n",
      "  'work_subset_ReceivablesNet.txt' -> Annualized (variable 'Annualized_21').\n",
      "  'work_subset_Reduction_in_Long_Term_Debt.txt' -> Special (variable 'Special_13').\n",
      "  'work_subset_Sales_Per_Share.txt' -> Mixed (variable 'Mixed_10').\n",
      "  'work_subset_Selling_General__Administrative_Expenses.txt' -> Mixed (variable 'Mixed_11').\n",
      "  'work_subset_Short_Term_Debt__Current_Portion_of_LT_Debt.txt' -> Annualized (variable 'Annualized_22').\n",
      "  'work_subset_Total_Assets.txt' -> Annualized (variable 'Annualized_23').\n",
      "  'work_subset_Total_Liabilities.txt' -> Annualized (variable 'Annualized_24').\n",
      "  'work_subset_Unspecified_Other_Loans.txt' -> Annualized (variable 'Annualized_25').\n",
      "\n",
      "Variable creation complete.\n",
      "Created 25 Annualized variables.\n",
      "Created 11 Mixed variables.\n",
      "Created 13 Special variables.\n",
      "\n",
      "Annualized Variables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Annualized_1': 'Accounts_Payable',\n",
       " 'Annualized_2': 'Cash__Short_Term_Investments',\n",
       " 'Annualized_3': 'Common_Equity',\n",
       " 'Annualized_4': 'Current_Assets___Total',\n",
       " 'Annualized_5': 'Current_Liabilities___Total',\n",
       " 'Annualized_6': 'Deferred_Taxes',\n",
       " 'Annualized_7': 'Income_Taxes_Payable',\n",
       " 'Annualized_8': 'Inventories___Total',\n",
       " 'Annualized_9': 'Investments_in_Associated_Companies',\n",
       " 'Annualized_10': 'Investments_in_Sales__Direct_Financing_Leases',\n",
       " 'Annualized_11': 'Long_Term_Debt',\n",
       " 'Annualized_12': 'Long_Term_Receivables',\n",
       " 'Annualized_13': 'Minority_Interest',\n",
       " 'Annualized_14': 'Other_Assets___Total',\n",
       " 'Annualized_15': 'Other_Current_Assets',\n",
       " 'Annualized_16': 'Other_Current_Liabilities',\n",
       " 'Annualized_17': 'Other_Investments',\n",
       " 'Annualized_18': 'Other_Liabilities',\n",
       " 'Annualized_19': 'Preferred_Stock',\n",
       " 'Annualized_20': 'Property_Plant__Equipment___Net',\n",
       " 'Annualized_21': 'ReceivablesNet',\n",
       " 'Annualized_22': 'Short_Term_Debt__Current_Portion_of_LT_Debt',\n",
       " 'Annualized_23': 'Total_Assets',\n",
       " 'Annualized_24': 'Total_Liabilities',\n",
       " 'Annualized_25': 'Unspecified_Other_Loans'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixed Variables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Mixed_1': 'Cost_of_Goods_Sold_Excl_Depreciation',\n",
       " 'Mixed_2': 'Depreciation_Depletion__Amortization',\n",
       " 'Mixed_3': 'Earnings_Per_Share_Fiscal_Year_End',\n",
       " 'Mixed_4': 'Income_Taxes',\n",
       " 'Mixed_5': 'Interest_Expense___Total',\n",
       " 'Mixed_6': 'Net_Income_Before_Extra_Items_Preferred_Divs',\n",
       " 'Mixed_7': 'Net_Income_Used_to_Calculate_Basic_EPS',\n",
       " 'Mixed_8': 'Net_Sales_or_Revenues',\n",
       " 'Mixed_9': 'Operating_Income',\n",
       " 'Mixed_10': 'Sales_Per_Share',\n",
       " 'Mixed_11': 'Selling_General__Administrative_Expenses'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Special Variables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Special_1': 'Capital_Expenditures_Addtns_to_Fixed_Assets',\n",
       " 'Special_2': 'Cash_Dividends_Paid___Total',\n",
       " 'Special_3': 'Com_Pfd_Redeemed_Retired_Converted_Etc.',\n",
       " 'Special_4': 'Disposal_of_Fixed_Assets',\n",
       " 'Special_5': 'Extraordinary_Items',\n",
       " 'Special_6': 'Funds_From_For_Other_Operating_Activities',\n",
       " 'Special_7': 'Funds_From_Operations',\n",
       " 'Special_8': 'Long_Term_Borrowings',\n",
       " 'Special_9': 'Net_Cash_Flow___Financing',\n",
       " 'Special_10': 'Net_Cash_Flow___Investing',\n",
       " 'Special_11': 'Net_Cash_Flow___Operating_Activities',\n",
       " 'Special_12': 'Net_Proceeds_From_Sale_Issue_of_Com__Pfd',\n",
       " 'Special_13': 'Reduction_in_Long_Term_Debt'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# CELL 2 maps work_subset_*.txt files to categories (\"Annualized\", \"Mixed\", \"Special\")\n",
    "# based on the ItemName_Sanitized that was derived in the previous cell.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Check that the required categorized DataFrames (annualized_items, mixed_items,\n",
    "#      special_items) and the temporary directory path Temp_file_path_DP exist.\n",
    "#   2. Build three sets of sanitized item names (annualized_names, mixed_names,\n",
    "#      special_names) from those DataFrames.\n",
    "#   3. List all files in Temp_file_path_DP and filter for those matching\n",
    "#      \"work_subset_*.txt\".\n",
    "#   4. For each work_subset file:\n",
    "#        - Extract the sanitized item name from the filename.\n",
    "#        - Determine whether it belongs to Mixed, Annualized, or Special based on\n",
    "#          the sets created in step 2.\n",
    "#        - Assign it a variable name (Mixed_n, Annualized_n, Special_n) and store\n",
    "#          that mapping in dicts mixed_vars, annualized_vars, special_vars.\n",
    "#   5. Store these dicts in globals() for use in later cells.\n",
    "#   6. Print summary information and display the created dictionaries.\n",
    "#   7. Perform garbage collection at the end.\n",
    "#\n",
    "# If any of the prerequisites are missing, it prints a message and skips the mapping.\n",
    "\n",
    "\n",
    "# CELL 2 — Map work_subset files to categories using ItemName_Sanitized\n",
    "\n",
    "if ('annualized_items' not in globals() or annualized_items is None or\n",
    "    'mixed_items' not in globals() or mixed_items is None or\n",
    "    'special_items' not in globals() or special_items is None or\n",
    "    'Temp_file_path_DP' not in globals()):\n",
    "    # If required DataFrames or directory path are missing, do not proceed\n",
    "    print(\"Required DataFrames (annualized_items, mixed_items, special_items) or Temp_file_path_DP not found. Please run the categorization cell.\")\n",
    "else:\n",
    "    print(\"Identifying work_subset files and creating variables based on categories...\")\n",
    "\n",
    "    # Sets of sanitized names that are final Annualized/Mixed/Special\n",
    "    annualized_names = set(annualized_items['ItemName_Sanitized'].dropna())\n",
    "    mixed_names      = set(mixed_items['ItemName_Sanitized'].dropna())\n",
    "    special_names    = set(special_items['ItemName_Sanitized'].dropna())\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Get a list of work_subset files from the temp directory\n",
    "    # ------------------------------------------------------------------\n",
    "    temp_files = os.listdir(Temp_file_path_DP)\n",
    "    work_subset_files = [\n",
    "        f for f in temp_files\n",
    "        if f.startswith('work_subset_') and f.endswith('.txt')\n",
    "    ]\n",
    "\n",
    "    # Dictionaries to hold mappings:\n",
    "    #   \"Annualized_1\" -> \"SomeItemName\"\n",
    "    #   \"Mixed_1\"      -> \"AnotherItemName\"\n",
    "    #   \"Special_1\"    -> \"SpecialItemName\"\n",
    "    annualized_vars = {}\n",
    "    mixed_vars = {}\n",
    "    special_vars = {}\n",
    "\n",
    "    print(f\"\\nFound {len(work_subset_files)} work_subset files in Temp directory.\")\n",
    "\n",
    "    # Sort files to have deterministic order when assigning variable names\n",
    "    work_subset_files.sort()\n",
    "\n",
    "    # Counters for how many items fall into each category; used for variable suffixes\n",
    "    annualized_count = 0\n",
    "    mixed_count = 0\n",
    "    special_count = 0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Iterate over each work_subset file and map it to a category\n",
    "    # ------------------------------------------------------------------\n",
    "    for file_name in work_subset_files:\n",
    "        # Extract sanitized item name from filename, expecting \"work_subset_<name>.txt\"\n",
    "        match = re.match(r'work_subset_(.+)\\.txt$', file_name)\n",
    "        if not match:\n",
    "            print(f\"  Filename format not as expected for '{file_name}'. Skipping processing.\")\n",
    "            continue\n",
    "\n",
    "        sanitized_item_name = match.group(1)\n",
    "\n",
    "        # Use the resolved sets. No more ambiguous precedence:\n",
    "        # priority Mixed -> Annualized -> Special, in this order of checks.\n",
    "        if sanitized_item_name in mixed_names:\n",
    "            mixed_count += 1\n",
    "            var_name = f\"Mixed_{mixed_count}\"\n",
    "            mixed_vars[var_name] = sanitized_item_name\n",
    "            print(f\"  '{file_name}' -> Mixed (variable '{var_name}').\")\n",
    "\n",
    "        elif sanitized_item_name in annualized_names:\n",
    "            annualized_count += 1\n",
    "            var_name = f\"Annualized_{annualized_count}\"\n",
    "            annualized_vars[var_name] = sanitized_item_name\n",
    "            print(f\"  '{file_name}' -> Annualized (variable '{var_name}').\")\n",
    "\n",
    "        elif sanitized_item_name in special_names:\n",
    "            special_count += 1\n",
    "            var_name = f\"Special_{special_count}\"\n",
    "            special_vars[var_name] = sanitized_item_name\n",
    "            print(f\"  '{file_name}' -> Special (variable '{var_name}').\")\n",
    "\n",
    "        else:\n",
    "            # No category mapping found for this sanitized name\n",
    "            print(f\"  '{file_name}' -> No matching Category (might be unmapped or ambiguous). Skipping.\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Expose the mapping dictionaries globally for later use\n",
    "    # ------------------------------------------------------------------\n",
    "    globals()['annualized_vars'] = annualized_vars\n",
    "    globals()['mixed_vars'] = mixed_vars\n",
    "    globals()['special_vars'] = special_vars\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Summary output and inspection\n",
    "    # ------------------------------------------------------------------\n",
    "    print(f\"\\nVariable creation complete.\")\n",
    "    print(f\"Created {len(annualized_vars)} Annualized variables.\")\n",
    "    print(f\"Created {len(mixed_vars)} Mixed variables.\")\n",
    "    print(f\"Created {len(special_vars)} Special variables.\")\n",
    "\n",
    "    print(\"\\nAnnualized Variables:\")\n",
    "    display(annualized_vars)\n",
    "\n",
    "    print(\"\\nMixed Variables:\")\n",
    "    display(mixed_vars)\n",
    "\n",
    "    print(\"\\nSpecial Variables:\")\n",
    "    display(special_vars)\n",
    "\n",
    "    # Run garbage collection after building mappings\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh7zifBYNg5C"
   },
   "source": [
    "## 4.4. Balance Sheet (10-18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXSsUb80SBPa"
   },
   "source": [
    "### Annualized 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVaGRdVY3Y39"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1765204625413,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "-paeYHwe3Y3-",
    "outputId": "1481fb88-7305-4448-9529-c48d22670d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_10  ->  ItemName: 'Investments_in_Sales__Direct_Financing_Leases'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 10  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sgkpW_73Y3_"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1765204625601,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "J5Y_qkPS3Y3_",
    "outputId": "682c9348-6cb7-48a5-8b3e-85f222a98e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Investments_in_Sales__Direct_Financing_Leases' ...\n",
      "Full annualized dataset loaded successfully: 473 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C03697590</td>\n",
       "      <td>NYLEX LIMITED</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Aud</td>\n",
       "      <td>Aud</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995</td>\n",
       "      <td>June</td>\n",
       "      <td>2257</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C03697590</td>\n",
       "      <td>NYLEX LIMITED</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Aud</td>\n",
       "      <td>Aud</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996</td>\n",
       "      <td>June</td>\n",
       "      <td>2257</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C036AJ830</td>\n",
       "      <td>CREDIT CORP GROUP LTD</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Aud</td>\n",
       "      <td>Aud</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>December</td>\n",
       "      <td>2257</td>\n",
       "      <td>0.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C036AJ830</td>\n",
       "      <td>CREDIT CORP GROUP LTD</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Aud</td>\n",
       "      <td>Aud</td>\n",
       "      <td>2021-08-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2021</td>\n",
       "      <td>December</td>\n",
       "      <td>2257</td>\n",
       "      <td>1.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C036AJ830</td>\n",
       "      <td>CREDIT CORP GROUP LTD</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Aud</td>\n",
       "      <td>Aud</td>\n",
       "      <td>2022-08-02</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2022</td>\n",
       "      <td>December</td>\n",
       "      <td>2257</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID            CompanyName ImplCountry CurrentCurrency HistCurrency  \\\n",
       "0  C03697590          NYLEX LIMITED   Australia             Aud          Aud   \n",
       "1  C03697590          NYLEX LIMITED   Australia             Aud          Aud   \n",
       "2  C036AJ830  CREDIT CORP GROUP LTD   Australia             Aud          Aud   \n",
       "3  C036AJ830  CREDIT CORP GROUP LTD   Australia             Aud          Aud   \n",
       "4  C036AJ830  CREDIT CORP GROUP LTD   Australia             Aud          Aud   \n",
       "\n",
       "     PIT Date Frequency UpdateCode FiscalPeriod FYE Month ItemCode  Value  \n",
       "0  1999-10-01         A        NaN         1995      June     2257    0.0  \n",
       "1  1999-10-01         A        NaN         1996      June     2257    0.0  \n",
       "2  2020-07-28         A          3         2020  December     2257  0.404  \n",
       "3  2021-08-03         A          3         2021  December     2257  1.059  \n",
       "4  2022-08-02         A          3         2022  December     2257  0.325  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7qE3h2C3Y4A"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1765204626068,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "1Voujf2F3Y4A",
    "outputId": "02f882a4-d38d-4253-c2e5-70c482e7d501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Investments_in_Sales__Direct_Financing_Leases' ...\n",
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_655400/200153074.py:98: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Y95' 'Y96' 'Y20' 'Y21' 'Y22' 'Y23' 'Y22' 'Y23' 'Y22' 'Y23' 'Y22' 'Y22'\n",
      " 'Y23' 'Y04' 'Y08' 'Y01' 'Y99' 'Y00' 'Y02' 'Y03' 'Y04' 'Y05' 'Y02' 'Y03'\n",
      " 'Y04' 'Y84' 'Y85' 'Y86' 'Y87' 'Y88' 'Y89' 'Y90' 'Y91' 'Y89' 'Y90' 'Y91'\n",
      " 'Y92' 'Y93' 'Y94' 'Y95' 'Y96' 'Y97' 'Y15' 'Y16' 'Y17' 'Y16' 'Y18' 'Y13'\n",
      " 'Y14' 'Y15' 'Y16' 'Y14' 'Y15' 'Y17' 'Y18' 'Y19' 'Y18' 'Y20' 'Y19' 'Y20'\n",
      " 'Y21' 'Y22' 'Y23' 'Y87' 'Y84' 'Y85' 'Y86' 'Y84' 'Y85' 'Y86' 'Y87' 'Y88'\n",
      " 'Y89' 'Y87' 'Y88' 'Y89' 'Y00' 'Y13' 'Y14' 'Y13' 'Y14' 'Y15' 'Y16' 'Y17'\n",
      " 'Y18' 'Y17' 'Y19' 'Y18' 'Y20' 'Y19' 'Y21' 'Y22' 'Y22' 'Y23' 'Y91' 'Y92'\n",
      " 'Y93' 'Y14' 'Y87' 'Y87' 'Y93' 'Y94' 'Y95' 'Y96' 'Y97' 'Y98' 'Y99' 'Y17'\n",
      " 'Y18' 'Y18' 'Y17' 'Y18' 'Y19' 'Y18' 'Y20' 'Y17' 'Y18' 'Y19' 'Y20' 'Y18'\n",
      " 'Y22' 'Y23' 'Y13' 'Y14' 'Y13' 'Y15' 'Y15' 'Y16' 'Y15' 'Y17' 'Y18' 'Y19'\n",
      " 'Y20' 'Y21' 'Y21' 'Y22' 'Y23' 'Y05' 'Y08' 'Y09' 'Y10' 'Y11' 'Y10' 'Y12'\n",
      " 'Y93' 'Y94' 'Y95' 'Y96' 'Y97' 'Y98' 'Y99' 'Y00' 'Y01' 'Y93' 'Y94' 'Y95'\n",
      " 'Y96' 'Y97' 'Y98' 'Y99' 'Y00' 'Y01' 'Y02' 'Y03' 'Y04' 'Y00' 'Y01' 'Y02'\n",
      " 'Y03' 'Y11' 'Y12' 'Y13' 'Y14' 'Y15' 'Y23' 'Y17' 'Y17' 'Y90' 'Y19' 'Y20'\n",
      " 'Y21' 'Y22' 'Y23' 'Y22' 'Y19' 'Y20' 'Y21' 'Y22' 'Y23' 'Y22' 'Y24' 'Y14'\n",
      " 'Y15' 'Y89' 'Y17' 'Y18' 'Y19' 'Y20' 'Y21' 'Y20' 'Y20' 'Y21' 'Y22' 'Y20'\n",
      " 'Y23' 'Y13' 'Y14' 'Y15' 'Y16' 'Y15' 'Y16' 'Y17' 'Y16' 'Y18' 'Y19' 'Y18'\n",
      " 'Y19' 'Y18' 'Y20' 'Y21' 'Y22' 'Y21' 'Y23' 'Y22' 'Y23' 'Y13' 'Y13' 'Y14'\n",
      " 'Y15' 'Y16' 'Y17' 'Y18' 'Y19' 'Y19' 'Y20' 'Y21' 'Y22' 'Y22' 'Y23' 'Y14'\n",
      " 'Y13' 'Y15' 'Y16' 'Y16' 'Y17' 'Y18' 'Y19' 'Y13' 'Y14' 'Y15' 'Y16' 'Y17'\n",
      " 'Y20' 'Y21' 'Y22' 'Y23' 'Y13' 'Y14' 'Y13' 'Y15' 'Y14' 'Y16' 'Y12' 'Y17'\n",
      " 'Y18' 'Y19' 'Y16' 'Y20' 'Y21' 'Y22' 'Y23' 'Y11' 'Y12' 'Y13' 'Y14' 'Y15'\n",
      " 'Y16' 'Y17' 'Y12' 'Y13' 'Y18' 'Y19' 'Y20' 'Y21' 'Y23' 'Y13' 'Y14' 'Y15'\n",
      " 'Y16' 'Y17' 'Y18' 'Y19' 'Y20' 'Y21' 'Y22' 'Y23' 'Y13' 'Y88' 'Y88' 'Y89'\n",
      " 'Y90' 'Y91' 'Y95' 'Y88' 'Y89' 'Y90' 'Y91' 'Y92' 'Y93' 'Y94' 'Y95' 'Y96'\n",
      " 'Y97' 'Y98' 'Y99' 'Y00' 'Y01' 'Y02' 'Y03' 'Y04' 'Y05' 'Y05' 'Y88' 'Y89'\n",
      " 'Y90' 'Y91' 'Y92' 'Y87' 'Y89' 'Y94' 'Y95' 'Y96' 'Y81' 'Y82' 'Y83' 'Y84'\n",
      " 'Y85' 'Y86' 'Y87' 'Y88' 'Y89' 'Y90' 'Y91' 'Y92' 'Y93' 'Y94' 'Y95' 'Y96'\n",
      " 'Y97' 'Y98' 'Y99' 'Y00' 'Y01' 'Y02' 'Y03' 'Y03' 'Y04' 'Y04' 'Y97' 'Y98'\n",
      " 'Y99' 'Y00' 'Y01' 'Y02' 'Y03' 'Y04' 'Y03' 'Y04' 'Y96' 'Y97' 'Y98' 'Y99'\n",
      " 'Y97' 'Y98' 'Y99' 'Y00' 'Y01' 'Y02' 'Y03' 'Y04' 'Y80' 'Y81' 'Y82' 'Y83'\n",
      " 'Y84' 'Y85' 'Y86' 'Y87' 'Y88' 'Y89' 'Y90' 'Y91' 'Y98' 'Y99' 'Y00' 'Y80'\n",
      " 'Y81' 'Y82' 'Y83' 'Y84' 'Y85' 'Y86' 'Y87' 'Y88' 'Y89' 'Y90' 'Y91' 'Y98'\n",
      " 'Y99' 'Y00' 'Y01' 'Y02' 'Y03' 'Y97' 'Y98' 'Y99' 'Y00' 'Y01' 'Y97' 'Y98'\n",
      " 'Y99' 'Y97' 'Y98' 'Y99' 'Y95' 'Y96' 'Y97' 'Y98' 'Y99' 'Y00' 'Y01' 'Y02'\n",
      " 'Y03' 'Y92' 'Y93' 'Y94' 'Y95' 'Y96' 'Y97' 'Y98' 'Y99' 'Y91' 'Y96' 'Y97'\n",
      " 'Y98' 'Y99' 'Y00' 'Y01' 'Y02' 'Y02' 'Y03' 'Y88' 'Y89' 'Y90' 'Y91' 'Y92'\n",
      " 'Y93' 'Y94' 'Y95' 'Y96' 'Y97' 'Y97' 'Y98' 'Y97' 'Y98' 'Y95' 'Y97' 'Y98'\n",
      " 'Y99' 'Y94' 'Y96' 'Y97' 'Y99']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C03697590</td>\n",
       "      <td>NYLEX LIMITED</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Aud</td>\n",
       "      <td>Aud</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995</td>\n",
       "      <td>June</td>\n",
       "      <td>2257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C03697590</td>\n",
       "      <td>NYLEX LIMITED</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Aud</td>\n",
       "      <td>Aud</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996</td>\n",
       "      <td>June</td>\n",
       "      <td>2257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C036AJ830</td>\n",
       "      <td>CREDIT CORP GROUP LTD</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Aud</td>\n",
       "      <td>Aud</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>December</td>\n",
       "      <td>2257</td>\n",
       "      <td>0.404</td>\n",
       "      <td>Y20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C036AJ830</td>\n",
       "      <td>CREDIT CORP GROUP LTD</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Aud</td>\n",
       "      <td>Aud</td>\n",
       "      <td>2021-08-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2021</td>\n",
       "      <td>December</td>\n",
       "      <td>2257</td>\n",
       "      <td>1.059</td>\n",
       "      <td>Y21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C036AJ830</td>\n",
       "      <td>CREDIT CORP GROUP LTD</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Aud</td>\n",
       "      <td>Aud</td>\n",
       "      <td>2022-08-02</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>2022</td>\n",
       "      <td>December</td>\n",
       "      <td>2257</td>\n",
       "      <td>0.325</td>\n",
       "      <td>Y22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID            CompanyName ImplCountry CurrentCurrency HistCurrency  \\\n",
       "0  C03697590          NYLEX LIMITED   Australia             Aud          Aud   \n",
       "1  C03697590          NYLEX LIMITED   Australia             Aud          Aud   \n",
       "2  C036AJ830  CREDIT CORP GROUP LTD   Australia             Aud          Aud   \n",
       "3  C036AJ830  CREDIT CORP GROUP LTD   Australia             Aud          Aud   \n",
       "4  C036AJ830  CREDIT CORP GROUP LTD   Australia             Aud          Aud   \n",
       "\n",
       "     PIT Date Frequency UpdateCode  FiscalPeriod FYE Month ItemCode  Value  \\\n",
       "0  1999-10-01         A        NaN          1995      June     2257    0.0   \n",
       "1  1999-10-01         A        NaN          1996      June     2257    0.0   \n",
       "2  2020-07-28         A          3          2020  December     2257  0.404   \n",
       "3  2021-08-03         A          3          2021  December     2257  1.059   \n",
       "4  2022-08-02         A          3          2022  December     2257  0.325   \n",
       "\n",
       "  Str_FiscalPrd  \n",
       "0           Y95  \n",
       "1           Y96  \n",
       "2           Y20  \n",
       "3           Y21  \n",
       "4           Y22  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBoK14aS3Y4B"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1765204626703,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "2hbPfTq03Y4B",
    "outputId": "02cdf35d-8fab-4834-e1f8-80a13d730203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 473 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 308\n",
      "                mean: 97.26625919983455\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 96.13788846589779\n",
      "                 p10: 99.99999985693175\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 15\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 293\n",
      "                mean: 99.17607122734715\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.16737338026707\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Investments_in_Sales__Direct_Financing_Leases_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Investments_in_Sales__Direct_Financing_Leases_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     473\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 15\n",
      "Output rows (final):            458\n",
      "Check: excluded + dropped + output = 473\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nB2IiDmSCJS"
   },
   "source": [
    "### Annualized 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USi9UxAA3fSB"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1765204626887,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "2PanQrzu3fSD",
    "outputId": "0fa88c83-d518-4741-9de4-dd6598ad4787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_11  ->  ItemName: 'Long_Term_Debt'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 11  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9OQS6dk3fSE"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1765204627738,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "a79_edrj3fSF",
    "outputId": "84739f45-de2a-4b09-93ff-d2763fae3658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Long_Term_Debt' ...\n",
      "Full annualized dataset loaded successfully: 3,469,033 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3251</td>\n",
       "      <td>10.096319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3251</td>\n",
       "      <td>106.116741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3251</td>\n",
       "      <td>130.770231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3251</td>\n",
       "      <td>32.376442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3251</td>\n",
       "      <td>0.124015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     3251   10.096319  \n",
       "1     3251  106.116741  \n",
       "2     3251  130.770231  \n",
       "3     3251   32.376442  \n",
       "4     3251    0.124015  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjWQogb63fSF"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1765204628034,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "36kZk6dL3fSF",
    "outputId": "23b22301-8df3-4fc9-a3e5-dc22afd27f18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Long_Term_Debt' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_655400/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q4Y01' 'Q4Y01' 'Q1Y02' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3251</td>\n",
       "      <td>10.096319</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3251</td>\n",
       "      <td>106.116741</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3251</td>\n",
       "      <td>130.770231</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3251</td>\n",
       "      <td>32.376442</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3251</td>\n",
       "      <td>0.124015</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     3251   10.096319           Y92  \n",
       "1     3251  106.116741           Y93  \n",
       "2     3251  130.770231           Y94  \n",
       "3     3251   32.376442           Y95  \n",
       "4     3251    0.124015           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUO9SEcr3fSG"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18807,
     "status": "ok",
     "timestamp": 1765204646844,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "6F50dYby3fSG",
    "outputId": "117acf2e-ba17-4699-8b92-33a113fde9b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,469,033 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 2667229\n",
      "                mean: 19667.03479696375\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 107.88714081535998\n",
      "                 p10: 77.48229735483598\n",
      "                 p20: 99.26138926477222\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 102.03104843117963\n",
      "                 p90: 120.22417143348679\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 245,732\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 2421497\n",
      "                mean: 101.86663943886437\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 101.7733582082207\n",
      "                 p10: 92.73722276475225\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.86015425977703\n",
      "                 p90: 113.48700997680706\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Long_Term_Debt_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Long_Term_Debt_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,469,033\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 245,732\n",
      "Output rows (final):            3,223,301\n",
      "Check: excluded + dropped + output = 3,469,033\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1JakGszSDEm"
   },
   "source": [
    "### Annualized 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0j7HjRx3gHC"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1765204647056,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "AGa8fP7Z3gHD",
    "outputId": "0b27925d-3be1-4053-a71b-c5f9f1b89a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_12  ->  ItemName: 'Long_Term_Receivables'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 12  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jHCUIwZ3gHE"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1765204647239,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "kwQRG0mA3gHE",
    "outputId": "ec526b39-e68b-4ecc-92a0-957867c140b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Long_Term_Receivables' ...\n",
      "Full annualized dataset loaded successfully: 838,675 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2258</td>\n",
       "      <td>2.382643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2258</td>\n",
       "      <td>10.452608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2258</td>\n",
       "      <td>19.387744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2258</td>\n",
       "      <td>33.879816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2258</td>\n",
       "      <td>50.371469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode      Value  \n",
       "0     2258   2.382643  \n",
       "1     2258  10.452608  \n",
       "2     2258  19.387744  \n",
       "3     2258  33.879816  \n",
       "4     2258  50.371469  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "td7OkRT53gHF"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1765204647573,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "8GgZVLgS3gHF",
    "outputId": "d53da7b2-3962-4f19-e89b-bbab29d9a596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Long_Term_Receivables' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_655400/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y11' 'Q3Y07' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13'\n",
      " 'Q1Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y12' 'Q3Y12' 'Q1Y13'\n",
      " 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q2Y13' 'Q3Y13'\n",
      " 'Q1Y13' 'Q3Y13' 'Q1Y14' 'Q1Y13' 'Q2Y13' 'Q3Y12' 'Q1Y13' 'Q2Y13' 'Q3Y13'\n",
      " 'Q2Y14' 'Q2Y14' 'Q1Y13' 'Q3Y13' 'Q1Y14' 'Q3Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14'\n",
      " 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q1Y14' 'Q2Y14' 'Q1Y14'\n",
      " 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q2Y14'\n",
      " 'Q3Y13' 'Q1Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14'\n",
      " 'Q2Y14' 'Q1Y14' 'Q2Y14' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q1Y14' 'Q1Y14' 'Q1Y13'\n",
      " 'Q2Y13' 'Q3Y13' 'Q2Y14' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q3Y13' 'Q1Y14' 'Q2Y14'\n",
      " 'Q1Y14' 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q3Y13' 'Q1Y14'\n",
      " 'Q1Y14' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q2Y14' 'Q3Y13'\n",
      " 'Q1Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q1Y14' 'Q1Y14' 'Q1Y13' 'Q2Y13'\n",
      " 'Q3Y13' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q3Y13' 'Q1Y14' 'Q1Y13' 'Q2Y13' 'Q2Y13'\n",
      " 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q1Y13' 'Q2Y13'\n",
      " 'Q3Y13' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q3Y13' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q1Y14'\n",
      " 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q1Y14' 'Q2Y14'\n",
      " 'Q3Y13' 'Q1Y13' 'Q2Y14' 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q1Y14'\n",
      " 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14'\n",
      " 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14'\n",
      " 'Q1Y13' 'Q1Y14' 'Q2Y14' 'Q2Y14' 'Q1Y14' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14'\n",
      " 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q1Y14' 'Q1Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14'\n",
      " 'Q2Y14' 'Q1Y14' 'Q2Y14' 'Q2Y14' 'Q1Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14'\n",
      " 'Q2Y14' 'Q3Y13' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14'\n",
      " 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q1Y14' 'Q1Y14' 'Q1Y14' 'Q2Y14'\n",
      " 'Q1Y14' 'Q1Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q3Y13' 'Q1Y14' 'Q2Y14'\n",
      " 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q1Y14' 'Q1Y13' 'Q2Y13' 'Q1Y13'\n",
      " 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q2Y14' 'Q3Y13' 'Q1Y14'\n",
      " 'Q2Y14' 'Q2Y14' 'Q3Y13' 'Q1Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q1Y13' 'Q2Y13'\n",
      " 'Q3Y13' 'Q3Y13' 'Q2Y14' 'Q2Y14' 'Q2Y14' 'Q1Y14' 'Q2Y14' 'Q1Y14' 'Q2Y14'\n",
      " 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q1Y14' 'Q2Y14'\n",
      " 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q3Y13' 'Q3Y13'\n",
      " 'Q1Y14' 'Q3Y13' 'Q3Y13' 'Q1Y14' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q1Y14' 'Q2Y14'\n",
      " 'Q1Y14' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q3Y14' 'Q1Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13'\n",
      " 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13'\n",
      " 'Q1Y14' 'Q2Y14' 'Q2Y13' 'Q1Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14'\n",
      " 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q2Y14'\n",
      " 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13'\n",
      " 'Q1Y14' 'Q2Y14' 'Q2Y14' 'Q2Y13' 'Q3Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13'\n",
      " 'Q1Y14' 'Q3Y13' 'Q1Y14' 'Q2Y14' 'Q2Y14' 'Q1Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13'\n",
      " 'Q1Y14' 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y14' 'Q1Y13'\n",
      " 'Q2Y13' 'Q3Y13' 'Q1Y14' 'Q1Y14' 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y14'\n",
      " 'Q2Y14' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13'\n",
      " 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13'\n",
      " 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13'\n",
      " 'Q2Y13' 'Q1Y13' 'Q1Y13' 'Q2Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13'\n",
      " 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13'\n",
      " 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13'\n",
      " 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13'\n",
      " 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13'\n",
      " 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13'\n",
      " 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13'\n",
      " 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13'\n",
      " 'Q2Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q1Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13'\n",
      " 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q3Y12'\n",
      " 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13'\n",
      " 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13'\n",
      " 'Q3Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q1Y13'\n",
      " 'Q2Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q1Y13' 'Q1Y13' 'Q1Y13' 'Q1Y13' 'Q2Y13'\n",
      " 'Q1Y13' 'Q2Y13' 'Q1Y07' 'Q2Y07' 'Q1Y13' 'Q3Y13' 'Q2Y13' 'Q1Y13' 'Q2Y13'\n",
      " 'Q1Y13' 'Q1Y13' 'Q1Y13' 'Q2Y13' 'Q3Y13' 'Q3Y13' 'Q1Y07' 'Q2Y07' 'Q1Y09'\n",
      " 'Q2Y12' 'Q1Y13' 'Q2Y13' 'Q1Y08' 'Q3Y12' 'Q3Y13' 'Q1Y13' 'Q3Y13' 'Q1Y13'\n",
      " 'Q2Y13' 'Q2Y13' 'Q3Y13' 'Q2Y13' 'Q1Y13' 'Q1Y13' 'Q2Y13' 'Q1Y13' 'Q3Y13'\n",
      " 'Q1Y13' 'Q1Y13' 'Q3Y13' 'Q1Y01' 'Q3Y13' 'Q3Y12' 'Q1Y02' 'Q1Y99' 'Q1Y11'\n",
      " 'Q1Y01' 'Q1Y04']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2258</td>\n",
       "      <td>2.382643</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2258</td>\n",
       "      <td>10.452608</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2258</td>\n",
       "      <td>19.387744</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2258</td>\n",
       "      <td>33.879816</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2258</td>\n",
       "      <td>50.371469</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode      Value Str_FiscalPrd  \n",
       "0     2258   2.382643           Y92  \n",
       "1     2258  10.452608           Y93  \n",
       "2     2258  19.387744           Y94  \n",
       "3     2258  33.879816           Y95  \n",
       "4     2258  50.371469           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjhFGPz53gHG"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2974,
     "status": "ok",
     "timestamp": 1765204650551,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "UHCj7s-f3gHG",
    "outputId": "92ee33f4-8e3e-4981-f765-14a317446602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 838,675 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 368966\n",
      "                mean: -702030.5116342524\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.19285095947802\n",
      "                 p10: 86.86939681303633\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 36,464\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 332502\n",
      "                mean: 100.20783155044795\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.03781905335858\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Long_Term_Receivables_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Long_Term_Receivables_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     838,675\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 36,464\n",
      "Output rows (final):            802,211\n",
      "Check: excluded + dropped + output = 838,675\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9vM4WYsSEAP"
   },
   "source": [
    "### Annualized 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJ5SSTUy3g39"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 161,
     "status": "ok",
     "timestamp": 1765204650736,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "IEwK6edz3g3-",
    "outputId": "3f88edc2-ba37-48b1-ac07-927f3e076e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_13  ->  ItemName: 'Minority_Interest'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 13  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IP9tL-A33g3_"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 179,
     "status": "ok",
     "timestamp": 1765204650925,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "e1QdCH923g4A",
    "outputId": "1225aa7a-f67c-4b6c-8da0-6c51c2d0ad17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Minority_Interest' ...\n",
      "Full annualized dataset loaded successfully: 3,264,381 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3426</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3426</td>\n",
       "      <td>0.117886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3426</td>\n",
       "      <td>0.007074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3426</td>\n",
       "      <td>0.005706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3426</td>\n",
       "      <td>0.00466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode     Value  \n",
       "0     3426       0.0  \n",
       "1     3426  0.117886  \n",
       "2     3426  0.007074  \n",
       "3     3426  0.005706  \n",
       "4     3426   0.00466  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcXDq9mO3g4B"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1765204651130,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "CbRuHZqJ3g4B",
    "outputId": "d3c9ed05-82ef-4b1a-c144-0424aa03d47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Minority_Interest' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_655400/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y00' 'Q4Y01' ... 'Q3Y18' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3426</td>\n",
       "      <td>0.117886</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3426</td>\n",
       "      <td>0.007074</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3426</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3426</td>\n",
       "      <td>0.00466</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode     Value Str_FiscalPrd  \n",
       "0     3426       0.0           Y92  \n",
       "1     3426  0.117886           Y93  \n",
       "2     3426  0.007074           Y94  \n",
       "3     3426  0.005706           Y95  \n",
       "4     3426   0.00466           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfI_YRR43g4C"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18013,
     "status": "ok",
     "timestamp": 1765204669145,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "TJ3Ziol63g4C",
    "outputId": "166d49b0-f9e2-452c-89b2-476fe94f3015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,264,381 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 1485366\n",
      "                mean: 10024.986743134714\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.40353301602599\n",
      "                 p10: 77.59314660769272\n",
      "                 p20: 95.10725476018318\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 106.51979710680635\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 127,732\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 1357634\n",
      "                mean: 99.82596471692966\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.6900880297001\n",
      "                 p10: 90.51236966571659\n",
      "                 p20: 98.34459341686458\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 104.45422850168977\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Minority_Interest_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Minority_Interest_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,264,381\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 127,732\n",
      "Output rows (final):            3,136,649\n",
      "Check: excluded + dropped + output = 3,264,381\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hu8pT-JASFPb"
   },
   "source": [
    "### Annualized 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyEGQfNY3hiZ"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1765204669368,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "PMhOSBG83hia",
    "outputId": "e1f1ebff-9555-4116-bd1f-7399ff8a8af4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_14  ->  ItemName: 'Other_Assets___Total'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 14  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztoWmv3b3hia"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1765204669589,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ysYvbJlo3hib",
    "outputId": "623b14c4-51a4-467e-e00a-d46067eaa64e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Other_Assets___Total' ...\n",
      "Full annualized dataset loaded successfully: 3,983,619 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2652</td>\n",
       "      <td>34.493544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2652</td>\n",
       "      <td>45.444753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2652</td>\n",
       "      <td>59.092809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2652</td>\n",
       "      <td>27.082113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2652</td>\n",
       "      <td>57.544877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode      Value  \n",
       "0     2652  34.493544  \n",
       "1     2652  45.444753  \n",
       "2     2652  59.092809  \n",
       "3     2652  27.082113  \n",
       "4     2652  57.544877  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJBlaooh3hib"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 168,
     "status": "ok",
     "timestamp": 1765204669760,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "5AecOPWM3hic",
    "outputId": "3da089c4-0194-4831-8157-0c79be1a7b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Other_Assets___Total' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_655400/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q4Y01' 'Q4Y01' 'Q1Y02' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2652</td>\n",
       "      <td>34.493544</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2652</td>\n",
       "      <td>45.444753</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2652</td>\n",
       "      <td>59.092809</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2652</td>\n",
       "      <td>27.082113</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2652</td>\n",
       "      <td>57.544877</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode      Value Str_FiscalPrd  \n",
       "0     2652  34.493544           Y92  \n",
       "1     2652  45.444753           Y93  \n",
       "2     2652  59.092809           Y94  \n",
       "3     2652  27.082113           Y95  \n",
       "4     2652  57.544877           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa2mMajE3hic"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21965,
     "status": "ok",
     "timestamp": 1765204691727,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "kDS0HW1s3hic",
    "outputId": "3d04f863-de75-4b86-8af6-6e8cb4fec9d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,983,619 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 3560543\n",
      "                mean: 518184103924.95514\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 326.06763800983396\n",
      "                 p10: 96.28357472782227\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.49270835317832\n",
      "                 p70: 112.44827691731732\n",
      "                 p80: 161.71540506506227\n",
      "                 p90: 380.67150716526874\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 718,023\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 2842520\n",
      "                mean: 106.26657553261518\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 106.27184136363914\n",
      "                 p10: 98.66431818324229\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.22626839379811\n",
      "                 p80: 107.39793470840608\n",
      "                 p90: 130.4434113646116\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Other_Assets___Total_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Other_Assets___Total_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,983,619\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 718,023\n",
      "Output rows (final):            3,265,596\n",
      "Check: excluded + dropped + output = 3,983,619\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5DWUCxkSHOO"
   },
   "source": [
    "### Annualized 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-2vHQEo3ihA"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1765204692076,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "kugNkZks3ihA",
    "outputId": "c91ac699-874d-42e0-b351-fc2e2c959ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_15  ->  ItemName: 'Other_Current_Assets'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 15  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YegAzUaN3ihB"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1765204692288,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "jMeS_Keq3ihC",
    "outputId": "3ae13fb6-a973-44e1-c61b-ed613fcaff04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Other_Current_Assets' ...\n",
      "Full annualized dataset loaded successfully: 4,257,428 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2149</td>\n",
       "      <td>1.852706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2149</td>\n",
       "      <td>13.658856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2149</td>\n",
       "      <td>27.954926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2149</td>\n",
       "      <td>37.516906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2149</td>\n",
       "      <td>40.665488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode      Value  \n",
       "0     2149   1.852706  \n",
       "1     2149  13.658856  \n",
       "2     2149  27.954926  \n",
       "3     2149  37.516906  \n",
       "4     2149  40.665488  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3St4ALy33ihC"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1765204692508,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "lgiD-bqk3ihD",
    "outputId": "b397af48-fab5-4998-dc47-e5ec14b2998a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Other_Current_Assets' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_655400/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q4Y01' 'Q4Y01' 'Q4Y01' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2149</td>\n",
       "      <td>1.852706</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2149</td>\n",
       "      <td>13.658856</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2149</td>\n",
       "      <td>27.954926</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2149</td>\n",
       "      <td>37.516906</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2149</td>\n",
       "      <td>40.665488</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode      Value Str_FiscalPrd  \n",
       "0     2149   1.852706           Y92  \n",
       "1     2149  13.658856           Y93  \n",
       "2     2149  27.954926           Y94  \n",
       "3     2149  37.516906           Y95  \n",
       "4     2149  40.665488           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ybc8ATmN3ihD"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23200,
     "status": "ok",
     "timestamp": 1765204715712,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Fd2u2Kr93ihE",
    "outputId": "2351d580-7a20-43b7-dbf9-7e120b2baf4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 4,257,428 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 3097043\n",
      "                mean: -10779386063059.982\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 1001.0026968088858\n",
      "                 p10: 37.24582590240025\n",
      "                 p20: 98.66666666666667\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 116.71493017206345\n",
      "                 p80: 246.52467883586598\n",
      "                 p90: 1042.8576800608985\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 1,023,531\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 2073512\n",
      "                mean: 103.6461066584614\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 103.61523201733455\n",
      "                 p10: 91.25073907818965\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 124.45965601380266\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Other_Current_Assets_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Other_Current_Assets_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     4,257,428\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 1,023,531\n",
      "Output rows (final):            3,233,897\n",
      "Check: excluded + dropped + output = 4,257,428\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlJBz31ZSIVo"
   },
   "source": [
    "### Annualized 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX36VG3b3jUU"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1765204715986,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "YYFlkKiB3jUU",
    "outputId": "db3f2ee9-e57d-4a22-925d-cd98dd376797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_16  ->  ItemName: 'Other_Current_Liabilities'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 16  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cNFK-8Q3jUV"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1765204716235,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ux_Y8MYw3jUW",
    "outputId": "b9fe559a-728f-4534-b2ec-233a4012a59c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Other_Current_Liabilities' ...\n",
      "Full annualized dataset loaded successfully: 4,271,683 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3066</td>\n",
       "      <td>192.428709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3066</td>\n",
       "      <td>249.834966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3066</td>\n",
       "      <td>240.967714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3066</td>\n",
       "      <td>236.361064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3066</td>\n",
       "      <td>263.192178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     3066  192.428709  \n",
       "1     3066  249.834966  \n",
       "2     3066  240.967714  \n",
       "3     3066  236.361064  \n",
       "4     3066  263.192178  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nE4bBHNL3jUW"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1765204716575,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "TpU9DK643jUW",
    "outputId": "23964faf-9088-404c-b05f-7fef51aae02f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Other_Current_Liabilities' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_655400/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q4Y01' 'Q4Y01' 'Q1Y02' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3066</td>\n",
       "      <td>192.428709</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3066</td>\n",
       "      <td>249.834966</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3066</td>\n",
       "      <td>240.967714</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3066</td>\n",
       "      <td>236.361064</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3066</td>\n",
       "      <td>263.192178</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     3066  192.428709           Y92  \n",
       "1     3066  249.834966           Y93  \n",
       "2     3066  240.967714           Y94  \n",
       "3     3066  236.361064           Y95  \n",
       "4     3066  263.192178           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UC-YbdQq3jUX"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23230,
     "status": "ok",
     "timestamp": 1765204739873,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "yUlb5Cvs3jUX",
    "outputId": "22b62db9-6a19-4d33-8d59-cfea1b96454c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 4,271,683 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 4029270\n",
      "                mean: -11431936541.178967\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 149.08865369803627\n",
      "                 p10: 68.22444865903131\n",
      "                 p20: 99.75964384492157\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 113.70718403147684\n",
      "                 p80: 151.182870227118\n",
      "                 p90: 256.0173016034437\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 854,815\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 3174455\n",
      "                mean: 107.1542778287721\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 107.14390155620016\n",
      "                 p10: 89.82737418805186\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.04119555120218\n",
      "                 p80: 114.86884160211926\n",
      "                 p90: 142.55302170342955\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Other_Current_Liabilities_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Other_Current_Liabilities_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     4,271,683\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 854,815\n",
      "Output rows (final):            3,416,868\n",
      "Check: excluded + dropped + output = 4,271,683\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dym4HXYfSJgN"
   },
   "source": [
    "### Annualized 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT9RxiKx3XnC"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1765204740212,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "cxEACmek3XnG",
    "outputId": "123eb2b0-e157-48e7-e527-6601db599cad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_17  ->  ItemName: 'Other_Investments'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 17  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX6tCx5t3XnI"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1765204740460,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "horyPBIr3XnJ",
    "outputId": "c88c8002-d157-4502-cd70-a166750278e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Other_Investments' ...\n",
      "Full annualized dataset loaded successfully: 1,143,062 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1987</td>\n",
       "      <td>June</td>\n",
       "      <td>2250</td>\n",
       "      <td>0.0001383667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1988</td>\n",
       "      <td>June</td>\n",
       "      <td>2250</td>\n",
       "      <td>0.0261401144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1989</td>\n",
       "      <td>June</td>\n",
       "      <td>2250</td>\n",
       "      <td>0.8265783741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1990</td>\n",
       "      <td>June</td>\n",
       "      <td>2250</td>\n",
       "      <td>38.5098743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1991</td>\n",
       "      <td>June</td>\n",
       "      <td>2250</td>\n",
       "      <td>0.530348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName ImplCountry  \\\n",
       "0  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "1  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "3  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "4  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "\n",
       "  CurrentCurrency HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod  \\\n",
       "0             Ars          Ars  1996-05-03         A          3         1987   \n",
       "1             Ars          Ars  1996-05-03         A          3         1988   \n",
       "2             Ars          Ars  1996-05-03         A          3         1989   \n",
       "3             Ars          Ars  1996-05-03         A          3         1990   \n",
       "4             Ars          Ars  1996-05-03         A          3         1991   \n",
       "\n",
       "  FYE Month ItemCode         Value  \n",
       "0      June     2250  0.0001383667  \n",
       "1      June     2250  0.0261401144  \n",
       "2      June     2250  0.8265783741  \n",
       "3      June     2250    38.5098743  \n",
       "4      June     2250      0.530348  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DReyFe3d3XnK"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1765204740855,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "7_exn72V3XnL",
    "outputId": "63312295-6bed-4b46-91fd-c89bfe5b470d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Other_Investments' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_655400/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y08' 'Q4Y08' 'Q1Y07' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1987</td>\n",
       "      <td>June</td>\n",
       "      <td>2250</td>\n",
       "      <td>0.0001383667</td>\n",
       "      <td>Y87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1988</td>\n",
       "      <td>June</td>\n",
       "      <td>2250</td>\n",
       "      <td>0.0261401144</td>\n",
       "      <td>Y88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1989</td>\n",
       "      <td>June</td>\n",
       "      <td>2250</td>\n",
       "      <td>0.8265783741</td>\n",
       "      <td>Y89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1990</td>\n",
       "      <td>June</td>\n",
       "      <td>2250</td>\n",
       "      <td>38.5098743</td>\n",
       "      <td>Y90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1991</td>\n",
       "      <td>June</td>\n",
       "      <td>2250</td>\n",
       "      <td>0.530348</td>\n",
       "      <td>Y91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName ImplCountry  \\\n",
       "0  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "1  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "3  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "4  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "\n",
       "  CurrentCurrency HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod  \\\n",
       "0             Ars          Ars  1996-05-03         A          3          1987   \n",
       "1             Ars          Ars  1996-05-03         A          3          1988   \n",
       "2             Ars          Ars  1996-05-03         A          3          1989   \n",
       "3             Ars          Ars  1996-05-03         A          3          1990   \n",
       "4             Ars          Ars  1996-05-03         A          3          1991   \n",
       "\n",
       "  FYE Month ItemCode         Value Str_FiscalPrd  \n",
       "0      June     2250  0.0001383667           Y87  \n",
       "1      June     2250  0.0261401144           Y88  \n",
       "2      June     2250  0.8265783741           Y89  \n",
       "3      June     2250    38.5098743           Y90  \n",
       "4      June     2250      0.530348           Y91  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmKVoqSb3XnM"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4655,
     "status": "ok",
     "timestamp": 1765204745514,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "SwN3dczs3XnM",
    "outputId": "7d6f6380-474b-4d76-98ea-02b9d898bb62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,143,062 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 747661\n",
      "                mean: 10915.950774330988\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 114.24131878688344\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 59,023\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 688638\n",
      "                mean: 100.7803979425712\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.67427485328744\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Other_Investments_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Other_Investments_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,143,062\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 59,023\n",
      "Output rows (final):            1,084,039\n",
      "Check: excluded + dropped + output = 1,143,062\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7teK3AFSKh5"
   },
   "source": [
    "### Annualized 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htl5urSISKh6"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1765204745882,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "_5zsemDfSKh6",
    "outputId": "9d8605af-2d8a-4c5f-dc91-04bcb189f215"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_18  ->  ItemName: 'Other_Liabilities'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 18  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSEQKgsNSKh7"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1765204746186,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "bT056HIcSKh7",
    "outputId": "8378fe9c-853a-4c29-9dd0-803427151eaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Other_Liabilities' ...\n",
      "Full annualized dataset loaded successfully: 3,750,638 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3273</td>\n",
       "      <td>1.735377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3273</td>\n",
       "      <td>3.187839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode     Value  \n",
       "0     3273     0.018  \n",
       "1     3273     0.018  \n",
       "2     3273     0.018  \n",
       "3     3273  1.735377  \n",
       "4     3273  3.187839  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHqjDUatSKh8"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1765204746670,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "-LXiCedGSKh8",
    "outputId": "020f7916-66ae-4eac-f12e-1a308d61830a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Other_Liabilities' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_655400/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q4Y01' 'Q4Y01' 'Q1Y02' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.018</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.018</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.018</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3273</td>\n",
       "      <td>1.735377</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3273</td>\n",
       "      <td>3.187839</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode     Value Str_FiscalPrd  \n",
       "0     3273     0.018           Y92  \n",
       "1     3273     0.018           Y93  \n",
       "2     3273     0.018           Y94  \n",
       "3     3273  1.735377           Y95  \n",
       "4     3273  3.187839           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNO2PV90SKh8"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19442,
     "status": "ok",
     "timestamp": 1765204766123,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "MzZjhrcRSKh8",
    "outputId": "090f8b95-3967-4e31-d8d6-1a8378e4ae12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,750,638 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 2352504\n",
      "                mean: 10417898034072.594\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 3254.8560137419076\n",
      "                 p10: 88.50836601947523\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.02669229248302\n",
      "                 p70: 140.90479202040407\n",
      "                 p80: 317.8899250834953\n",
      "                 p90: 1189.2703862660944\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 739,246\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 1613258\n",
      "                mean: 105.28857690299857\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 105.28951993686752\n",
      "                 p10: 99.38299671882992\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 101.50224780971735\n",
      "                 p90: 127.02390163206823\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Other_Liabilities_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Other_Liabilities_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,750,638\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 739,246\n",
      "Output rows (final):            3,011,392\n",
      "Check: excluded + dropped + output = 3,750,638\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOud+irk+ziJQVrnuT5rRMK",
   "collapsed_sections": [
    "ctXZHVs-zurM",
    "uQMCPR6XzurR",
    "FmcsWZB0oDMl",
    "fcll5N-9z4yZ",
    "d59odhYbz4yc",
    "wh7zifBYNg5C",
    "nXSsUb80SBPa",
    "mVaGRdVY3Y39",
    "1sgkpW_73Y3_",
    "o7qE3h2C3Y4A",
    "WBoK14aS3Y4B",
    "7nB2IiDmSCJS",
    "O9OQS6dk3fSE",
    "AjWQogb63fSF",
    "FUO9SEcr3fSG",
    "A1JakGszSDEm",
    "x0j7HjRx3gHC",
    "0jHCUIwZ3gHE",
    "td7OkRT53gHF",
    "YjhFGPz53gHG",
    "z9vM4WYsSEAP",
    "KJ5SSTUy3g39",
    "IP9tL-A33g3_",
    "rcXDq9mO3g4B",
    "NfI_YRR43g4C",
    "Hu8pT-JASFPb",
    "dyEGQfNY3hiZ",
    "ztoWmv3b3hia",
    "eJBlaooh3hib",
    "oa2mMajE3hic",
    "e5DWUCxkSHOO",
    "f-2vHQEo3ihA",
    "YegAzUaN3ihB",
    "3St4ALy33ihC",
    "Ybc8ATmN3ihD",
    "NlJBz31ZSIVo",
    "IX36VG3b3jUU",
    "5cNFK-8Q3jUV",
    "nE4bBHNL3jUW",
    "UC-YbdQq3jUX",
    "dym4HXYfSJgN",
    "cX6tCx5t3XnI",
    "DReyFe3d3XnK",
    "hmKVoqSb3XnM",
    "Y7teK3AFSKh5",
    "htl5urSISKh6",
    "eSEQKgsNSKh7",
    "KHqjDUatSKh8"
   ],
   "history_visible": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
