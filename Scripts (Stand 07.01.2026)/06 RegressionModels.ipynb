{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctXZHVs-zurM"
   },
   "source": [
    "### Mount Google Drive, Import Libraries and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1668,
     "status": "ok",
     "timestamp": 1765534038453,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "DVbbtM4ztajO",
    "outputId": "06e9a88d-dc89-479f-9d48-79c44da67fdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: False\n",
      "BASE_PATH: /home/jovyan/work/hpool1/pseidel/test\n",
      "Importing numpy ...\n",
      "numpy OK\n",
      "Importing scipy ...\n",
      "scipy OK\n",
      "Importing pandas ...\n",
      "pandas OK\n",
      "Importing linearmodels ...\n",
      "linearmodels OK\n",
      "Importing xlsxwriter ...\n",
      "xlsxwriter OK\n",
      "Paths configured. Temp outputs -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview\n",
      "Example input path -> /home/jovyan/work/hpool1/pseidel/test/Input/WSFV_f_20250131.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT SETUP + PATH CONFIGURATION (SERVER / COLAB COMPATIBLE)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) HARD SAFETY: cap native thread usage (prevents pthread_create EAGAIN)\n",
    "#    MUST be set before importing numpy / scipy / pandas\n",
    "# -----------------------------------------------------------------------------\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"BLIS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Detect environment\n",
    "# -----------------------------------------------------------------------------\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) (Colab only) Mount Google Drive\n",
    "# -----------------------------------------------------------------------------\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "else:\n",
    "    # Server base path (your target)\n",
    "    BASE_PATH = \"/home/jovyan/work/hpool1/pseidel/test\"\n",
    "\n",
    "print(\"IN_COLAB:\", IN_COLAB)\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Sanity checks: path exists + write permission\n",
    "# -----------------------------------------------------------------------------\n",
    "BASE = Path(BASE_PATH)\n",
    "if not BASE.exists():\n",
    "    raise FileNotFoundError(f\"BASE_PATH does not exist: {BASE}\")\n",
    "\n",
    "# quick write test (fails fast if you don't have permissions)\n",
    "test_file = BASE / \".write_test_tmp\"\n",
    "try:\n",
    "    test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "    test_file.unlink()\n",
    "except Exception as e:\n",
    "    raise PermissionError(f\"No write permission in {BASE}. Error: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Environment check: ensure required packages import cleanly\n",
    "# -----------------------------------------------------------------------------\n",
    "required_packages = [\"numpy\", \"scipy\", \"pandas\", \"linearmodels\", \"xlsxwriter\"]\n",
    "\n",
    "for pkg in required_packages:\n",
    "    print(f\"Importing {pkg} ...\")\n",
    "    importlib.import_module(pkg)\n",
    "    print(f\"{pkg} OK\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Base paths and input/output locations\n",
    "# -----------------------------------------------------------------------------\n",
    "Input_file_path   = str(BASE / \"Input\")\n",
    "Temp_file_path    = str(BASE / \"Temp\")\n",
    "Output_file_path  = str(BASE / \"Output\")\n",
    "\n",
    "Fundamentals_file_path = f\"{Input_file_path}/WSFV_f_20250131.txt\"\n",
    "Current_file_path      = f\"{Input_file_path}/WSCurrent_f_20250131.txt\"\n",
    "Calendar_file_path     = f\"{Input_file_path}/WSCalendarPrd_f_20250131.txt\"\n",
    "Meta_file_path         = f\"{Input_file_path}/WSMetaData_f_20250131.txt\"\n",
    "Excel_file_path        = f\"{Input_file_path}/WS PIT Table Definitions V5 with start dates.xls\"\n",
    "\n",
    "MarketValues_file_path          = f\"{Input_file_path}/Daily MV USD\"\n",
    "MarketValues_file_path_LC       = f\"{Input_file_path}/Daily MV LC\"\n",
    "DailyTotalReturns_file_path     = f\"{Input_file_path}/Daily Returns USD\"\n",
    "DailyIndexReturns_file_path     = f\"{Input_file_path}/Daily Index Returns USD\"\n",
    "Constituents_file_path          = f\"{Input_file_path}/Constituents.01.csv\"\n",
    "UniversalMatching_file_path     = f\"{Input_file_path}/Universal Matching File\"\n",
    "\n",
    "Temp_file_path_GO  = f\"{Temp_file_path}/TempGeneralOverview\"\n",
    "Temp_file_path_EoC = f\"{Temp_file_path}/TempExtractionofCharacteristics\"\n",
    "Temp_file_path_DP  = f\"{Temp_file_path}/TempDataPreparation\"\n",
    "Temp_file_path_A   = f\"{Temp_file_path}/TempAnomalies\"\n",
    "Temp_file_path_R   = f\"{Temp_file_path}/TempRegressionModel\"\n",
    "\n",
    "Relevant_items_path   = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_A = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_B = f\"{Input_file_path}/RelevantItemsB.txt\"\n",
    "Relevant_items_path_C = f\"{Input_file_path}/RelevantItemsC.txt\"\n",
    "Relevant_items_path_D = f\"{Input_file_path}/RelevantItemsD.txt\"\n",
    "\n",
    "Subset_file_path = f\"{Temp_file_path_GO}/Subsets\"\n",
    "Fundamentals_clean_file_path = f\"{Temp_file_path_GO}/Fundamentals_clean.txt\"\n",
    "Current_clean_file_path      = f\"{Temp_file_path_GO}/Current_clean.txt\"\n",
    "Calendar_clean_file_path     = f\"{Temp_file_path_GO}/Input/Calendar_clean.txt\"\n",
    "Meta_clean_file_path         = f\"{Temp_file_path_GO}/Input/Meta_clean.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Ensure required directories exist\n",
    "# -----------------------------------------------------------------------------\n",
    "Path(Output_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_GO).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_EoC).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_A).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(Subset_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Path(Calendar_clean_file_path).parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Streaming / deduplication settings\n",
    "# -----------------------------------------------------------------------------\n",
    "CHUNK_SIZE = 2_000_000\n",
    "DATE_COL = \"PIT Date\"\n",
    "DEDUP_KEYS = [\"ID\", \"ItemCode\", DATE_COL]\n",
    "\n",
    "print(\"Paths configured. Temp outputs ->\", Temp_file_path_GO)\n",
    "print(\"Example input path ->\", Fundamentals_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           754Gi       188Gi       398Gi        55Mi       175Gi       565Gi\n",
      "Swap:             0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-JLzgUW9gDm"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9SDg4A4CoTL"
   },
   "source": [
    "### Merge Fundamentals & Market Data + Roll Forward (Max 2 Calendar Years) + Annual/Daily Deciles or Total/Country/Replication\n",
    "\n",
    "=> Comment: Previously separate cells but I/O was so intense that I combined it to only load the files once and also switched to .parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Benchmark: FF_Benchmark_data_clean.txt...\n",
      "Filtering Benchmark for range: 1994-01-01 to 2024-12-31\n",
      "Benchmark Ready: 143,802,215 rows.\n",
      "Loading Factors...\n",
      "Loaded 6 factor files.\n",
      "\n",
      "Processing 27 anomalies (Full Production Run).\n",
      "Workers: 6\n",
      "Output Format: Parquet\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed: 41.0min\n",
      "[Parallel(n_jobs=6)]: Done  19 out of  27 | elapsed: 55.8min remaining: 23.5min\n",
      "[Parallel(n_jobs=6)]: Done  22 out of  27 | elapsed: 60.2min remaining: 13.7min\n",
      "[Parallel(n_jobs=6)]: Done  25 out of  27 | elapsed: 67.9min remaining:  5.4min\n",
      "[Parallel(n_jobs=6)]: Done  27 out of  27 | elapsed: 74.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DONE: acc -> 6 files. Time: 11.6 min\n",
      "DONE: ag -> 6 files. Time: 16.9 min\n",
      "DONE: at -> 6 files. Time: 14.7 min\n",
      "DONE: cat -> 6 files. Time: 14.0 min\n",
      "DONE: cpm -> 6 files. Time: 15.0 min\n",
      "DONE: ec -> 6 files. Time: 8.9 min\n",
      "DONE: es -> 6 files. Time: 13.2 min\n",
      "DONE: gp -> 6 files. Time: 15.2 min\n",
      "DONE: ig -> 6 files. Time: 16.1 min\n",
      "DONE: inv -> 6 files. Time: 13.9 min\n",
      "DONE: ltg -> 6 files. Time: 9.7 min\n",
      "DONE: nca -> 6 files. Time: 15.7 min\n",
      "DONE: noa -> 6 files. Time: 16.1 min\n",
      "DONE: nwc -> 6 files. Time: 16.0 min\n",
      "DONE: ol -> 6 files. Time: 12.8 min\n",
      "DONE: osc -> 6 files. Time: 15.5 min\n",
      "DONE: pm -> 6 files. Time: 15.6 min\n",
      "DONE: poa -> 6 files. Time: 15.3 min\n",
      "DONE: pro -> 6 files. Time: 15.3 min\n",
      "DONE: pta -> 6 files. Time: 14.2 min\n",
      "DONE: roe -> 6 files. Time: 14.5 min\n",
      "DONE: rs -> 6 files. Time: 13.8 min\n",
      "DONE: sg -> 6 files. Time: 15.4 min\n",
      "DONE: sli -> 6 files. Time: 13.5 min\n",
      "DONE: slx -> 6 files. Time: 13.0 min\n",
      "DONE: txf -> 6 files. Time: 11.1 min\n",
      "DONE: tx -> 6 files. Time: 15.5 min\n",
      "============================================================\n",
      "Total Pipeline Time: 77.6 minutes.\n",
      "\n",
      "************************************************************\n",
      "5. VALIDATION CHECK FOR ID: C12486370\n",
      "   Target Window: 2000-06-20 to 2000-07-10\n",
      "************************************************************\n",
      "\n",
      "A. RAW INPUT (acc)\n",
      "   (No data in window. Checking fill-forward source...)\n",
      "       ID    DayDate     acc\n",
      "C12486370 1999-04-02 0.11415\n",
      "----------------------------------------\n",
      "C. FINAL OUTPUTS (Checking ALL 6 Scenarios for acc)\n",
      "\n",
      "   >>> Checking: acc_Global_Daily_clean.parquet\n",
      "       ID    DayDate     acc  Decile       ret    Mkt-RF       SMB       RMW       CMA       Mom     RF\n",
      "C12486370 2000-06-20 0.11415    10.0  0.000996  0.004234 -0.007490 -0.006942  0.004060  0.011568 0.0002\n",
      "C12486370 2000-06-21 0.11415    10.0 -0.012129  0.001674 -0.002756 -0.012594  0.004227  0.007244 0.0002\n",
      "C12486370 2000-06-22 0.11415    10.0 -0.000197 -0.011643  0.006379 -0.004623  0.009223 -0.012259 0.0002\n",
      "C12486370 2000-06-23 0.11415    10.0 -0.017141 -0.007516  0.005847  0.006244  0.009682 -0.005051 0.0002\n",
      "C12486370 2000-06-26 0.11415    10.0 -0.002116  0.002691 -0.003967  0.001586 -0.006034  0.003223 0.0002\n",
      "C12486370 2000-06-27 0.11415    10.0  0.014486 -0.000241  0.002879  0.001969  0.010593 -0.006759 0.0002\n",
      "C12486370 2000-06-28 0.11415    10.0  0.010935  0.005876  0.000802  0.011553 -0.001534  0.007248 0.0002\n",
      "C12486370 2000-06-29 0.11415    10.0  0.002546 -0.007126  0.012409  0.012194  0.018311 -0.008824 0.0002\n",
      "C12486370 2000-06-30 0.11415    10.0 -0.000803  0.009028 -0.004428 -0.004543 -0.008043  0.006607 0.0002\n",
      "C12486370 2000-07-03 0.11415    10.0  0.001738  0.008298 -0.002594 -0.018571  0.004650 -0.007825 0.0002\n",
      "C12486370 2000-07-04 0.11415    10.0 -0.016439 -0.000415  0.000138  0.001119 -0.003450  0.001274 0.0002\n",
      "C12486370 2000-07-05 0.11415    10.0  0.016185 -0.007911 -0.000482 -0.011365  0.000618 -0.010079 0.0002\n",
      "C12486370 2000-07-06 0.11415    10.0 -0.017055  0.003000 -0.007507  0.000628 -0.010123  0.005859 0.0002\n",
      "C12486370 2000-07-07 0.11415    10.0  0.002517  0.010382 -0.006854 -0.004185 -0.006513  0.010314 0.0002\n",
      "C12486370 2000-07-10 0.11415    10.0 -0.012177  0.003204  0.006569  0.002657  0.008451 -0.002312 0.0002\n",
      "\n",
      "   >>> Checking: acc_Country_Daily_clean.parquet\n",
      "       (No rows. Decile likely not 1 or 10, or filtered by gap rules.)\n",
      "\n",
      "   >>> Checking: acc_Replication_Daily_clean.parquet\n",
      "       ID    DayDate     acc  Decile       ret    Mkt-RF       SMB       RMW       CMA       Mom     RF\n",
      "C12486370 2000-06-20 0.11415    10.0  0.000996 -0.001950 -0.008541 -0.001424 -0.000329  0.012223 0.0002\n",
      "C12486370 2000-06-21 0.11415    10.0 -0.012129  0.007389 -0.003553 -0.007734 -0.019259  0.009720 0.0002\n",
      "C12486370 2000-06-22 0.11415    10.0 -0.000197 -0.019818  0.016126  0.005443  0.010339 -0.018607 0.0002\n",
      "C12486370 2000-06-23 0.11415    10.0 -0.017141 -0.009056  0.000549  0.006103  0.017622 -0.005109 0.0002\n",
      "C12486370 2000-06-26 0.11415    10.0 -0.002116  0.011156 -0.005759 -0.021415 -0.009985  0.015438 0.0002\n",
      "C12486370 2000-06-27 0.11415    10.0  0.014486 -0.007274  0.006575  0.002890  0.012408 -0.012605 0.0002\n",
      "C12486370 2000-06-28 0.11415    10.0  0.010935  0.007620  0.003374  0.003864 -0.013244  0.016011 0.0002\n",
      "C12486370 2000-06-29 0.11415    10.0  0.002546 -0.008161  0.011280  0.011780  0.011255 -0.007662 0.0002\n",
      "C12486370 2000-06-30 0.11415    10.0 -0.000803  0.011917 -0.021047 -0.008547 -0.015369  0.021077 0.0002\n",
      "C12486370 2000-07-03 0.11415    10.0  0.001738  0.008088 -0.001098 -0.003960  0.007872 -0.006145 0.0002\n",
      "C12486370 2000-07-04 0.11415    10.0 -0.016439  0.000138 -0.003403 -0.000520  0.005057  0.000378 0.0002\n",
      "C12486370 2000-07-05 0.11415    10.0  0.016185 -0.015460 -0.006905 -0.033314  0.030161 -0.028344 0.0002\n",
      "C12486370 2000-07-06 0.11415    10.0 -0.017055  0.008775 -0.008831  0.036896 -0.027717  0.016584 0.0002\n",
      "C12486370 2000-07-07 0.11415    10.0  0.002517  0.014238 -0.001909 -0.004260 -0.001098  0.014870 0.0002\n",
      "C12486370 2000-07-10 0.11415    10.0 -0.012177 -0.001748  0.013098  0.002079  0.004678 -0.007467 0.0002\n",
      "\n",
      "   >>> Checking: acc_Global_Annual_clean.parquet\n",
      "       ID    DayDate     acc  Decile       ret    Mkt-RF       SMB       RMW       CMA       Mom     RF\n",
      "C12486370 2000-06-20 0.11415    10.0  0.000996  0.002641  0.001316 -0.011538  0.003251  0.006976 0.0002\n",
      "C12486370 2000-06-21 0.11415    10.0 -0.012129  0.001111 -0.001402 -0.005513  0.000288  0.003423 0.0002\n",
      "C12486370 2000-06-22 0.11415    10.0 -0.000197 -0.011640  0.010450 -0.004112  0.006875 -0.006282 0.0002\n",
      "C12486370 2000-06-23 0.11415    10.0 -0.017141 -0.007271  0.002548  0.000305  0.007190 -0.003995 0.0002\n",
      "C12486370 2000-06-26 0.11415    10.0 -0.002116  0.002586 -0.000333  0.006183 -0.007674 -0.001968 0.0002\n",
      "C12486370 2000-06-27 0.11415    10.0  0.014486 -0.000786  0.003687  0.000611  0.005358 -0.003578 0.0002\n",
      "C12486370 2000-06-28 0.11415    10.0  0.010935  0.005542  0.003556 -0.001733 -0.002018 -0.001395 0.0002\n",
      "C12486370 2000-06-29 0.11415    10.0  0.002546 -0.007887  0.010168 -0.003196  0.011943 -0.006294 0.0002\n",
      "C12486370 2000-06-30 0.11415    10.0 -0.000803  0.009436 -0.002594 -0.001538 -0.012934  0.005056 0.0002\n",
      "C12486370 2000-07-03 0.11415    10.0  0.001738  0.008298  0.001533 -0.013290  0.004153 -0.007024 0.0002\n",
      "C12486370 2000-07-04 0.11415    10.0 -0.016439 -0.000415  0.001226  0.002701 -0.002394  0.000800 0.0002\n",
      "C12486370 2000-07-05 0.11415    10.0  0.016185 -0.007911  0.002126 -0.000719 -0.000147 -0.010079 0.0002\n",
      "C12486370 2000-07-06 0.11415    10.0 -0.017055  0.003000 -0.006502  0.000844 -0.011765  0.004739 0.0002\n",
      "C12486370 2000-07-07 0.11415    10.0  0.002517  0.010382 -0.009701 -0.001780 -0.007941  0.010956 0.0002\n",
      "C12486370 2000-07-10 0.11415    10.0 -0.012177  0.003204  0.004371  0.000190  0.007326 -0.001186 0.0002\n",
      "\n",
      "   >>> Checking: acc_Country_Annual_clean.parquet\n",
      "       ID    DayDate     acc  Decile       ret    Mkt-RF       SMB       RMW       CMA       Mom     RF\n",
      "C12486370 2000-06-30 0.11415    10.0 -0.000803  0.009703  0.000716 -0.024921 -0.015636  0.002994 0.0002\n",
      "C12486370 2000-07-03 0.11415    10.0  0.001738  0.001759 -0.001195  0.000082 -0.002193  0.000577 0.0002\n",
      "C12486370 2000-07-04 0.11415    10.0 -0.016439  0.006748  0.001905 -0.012016  0.004364  0.002846 0.0002\n",
      "C12486370 2000-07-05 0.11415    10.0  0.016185 -0.000832 -0.007983  0.000279  0.008493 -0.009040 0.0002\n",
      "C12486370 2000-07-06 0.11415    10.0 -0.017055 -0.003617  0.000390  0.008968 -0.008307  0.003395 0.0002\n",
      "C12486370 2000-07-07 0.11415    10.0  0.002517  0.017159 -0.016726 -0.004011 -0.010179  0.013952 0.0002\n",
      "C12486370 2000-07-10 0.11415    10.0 -0.012177 -0.010692  0.010933  0.002034  0.003682 -0.010314 0.0002\n",
      "\n",
      "   >>> Checking: acc_Replication_Annual_clean.parquet\n",
      "       ID    DayDate     acc  Decile       ret    Mkt-RF       SMB       RMW       CMA       Mom     RF\n",
      "C12486370 2000-06-20 0.11415    10.0  0.000996 -0.004398  0.007417 -0.017096 -0.000325  0.015909 0.0002\n",
      "C12486370 2000-06-21 0.11415    10.0 -0.012129  0.006362 -0.000264 -0.001288 -0.005335  0.005174 0.0002\n",
      "C12486370 2000-06-22 0.11415    10.0 -0.000197 -0.019859  0.005257  0.007101  0.005743 -0.015249 0.0002\n",
      "C12486370 2000-06-23 0.11415    10.0 -0.017141 -0.008387  0.001493  0.005945  0.010192 -0.014701 0.0002\n",
      "C12486370 2000-06-26 0.11415    10.0 -0.002116  0.010195 -0.001606  0.000062 -0.011156  0.005332 0.0002\n",
      "C12486370 2000-06-27 0.11415    10.0  0.014486 -0.007265 -0.004386  0.009658  0.000189 -0.013552 0.0002\n",
      "C12486370 2000-06-28 0.11415    10.0  0.010935  0.005976  0.013293 -0.004787 -0.004777  0.011575 0.0002\n",
      "C12486370 2000-06-29 0.11415    10.0  0.002546 -0.009354  0.005149  0.000898  0.013729 -0.009558 0.0002\n",
      "C12486370 2000-06-30 0.11415    10.0 -0.000803  0.011215 -0.007207 -0.001738 -0.023136  0.021542 0.0002\n",
      "C12486370 2000-07-03 0.11415    10.0  0.001738  0.008088  0.008130 -0.006554  0.002776 -0.007503 0.0002\n",
      "C12486370 2000-07-04 0.11415    10.0 -0.016439  0.000138 -0.001728  0.000363  0.003079  0.000285 0.0002\n",
      "C12486370 2000-07-05 0.11415    10.0  0.016185 -0.015460 -0.007391 -0.007577  0.023085 -0.026852 0.0002\n",
      "C12486370 2000-07-06 0.11415    10.0 -0.017055  0.008775 -0.006522  0.010449 -0.024175  0.013090 0.0002\n",
      "C12486370 2000-07-07 0.11415    10.0  0.002517  0.014238 -0.011287  0.009117 -0.004938  0.013109 0.0002\n",
      "C12486370 2000-07-10 0.11415    10.0 -0.012177 -0.001748  0.010000  0.006529  0.005639 -0.003985 0.0002\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MASTER PRODUCTION CELL: UNIFIED ANOMALY PROCESSING PIPELINE\n",
    "# =============================================================================\n",
    "#\n",
    "# SYSTEM ARCHITECTURE:\n",
    "# --------------------\n",
    "# This script is a \"One-Shot\" pipeline. Instead of saving intermediate files to disk\n",
    "# (which caused the I/O bottleneck), it holds the data for a single anomaly in RAM\n",
    "# from start to finish.\n",
    "#\n",
    "# HARDWARE CONFIGURATION:\n",
    "# - Workers: 10 (Parallel Processes)\n",
    "#   * Each worker needs ~30-40GB RAM at peak.\n",
    "#   * Total Peak RAM: ~400GB (Safe on your 754GB server).\n",
    "# - Output Format: Apache Parquet (Binary, Compressed, Fast).\n",
    "#\n",
    "# DETAILED LOGIC FLOW (Per Anomaly):\n",
    "# ----------------------------------\n",
    "# 1. SETUP:\n",
    "#    - The Main Process loads the \"Benchmark\" (Returns/Price) and \"Factors\" (Fama-French)\n",
    "#      into shared memory so workers don't have to reload them 50 times.\n",
    "#\n",
    "# 2. MERGE (The Data Enrichment):\n",
    "#    - Worker loads ONE anomaly file (e.g., \"ACC_clean.txt\").\n",
    "#    - It filters the Benchmark to keep only IDs present in that anomaly file.\n",
    "#    - It merges them: Market Data + Accounting Signal.\n",
    "#\n",
    "# 3. FILL-FORWARD (The Signal Propagation):\n",
    "#    - Accounting data comes once a year; Returns come daily.\n",
    "#    - The script \"fills forward\" the accounting signal from the announcement date\n",
    "#      to subsequent trading days.\n",
    "#    - SAFETY RULES:\n",
    "#      a. Stop filling if the signal is > 2 Years old (730 days).\n",
    "#      b. Stop filling if there is a trading gap > 14 days (stock suspended/delisted).\n",
    "#\n",
    "# 4. RANKING (The Decile Calculation):\n",
    "#    - DAILY Mode: Ranks are calculated every single day where 'EventDate == 1' (New News).\n",
    "#    - ANNUAL Mode: Ranks are calculated once per year (June 30th) and held constant.\n",
    "#    - SCOPES: Ranks are calculated 3 ways: Global, Country-Neutral, and Replication Subset.\n",
    "#\n",
    "# 5. FILTER & SAVE (The Output Generation):\n",
    "#    - The script drops all rows EXCEPT Decile 1 (Top) and Decile 10 (Bottom).\n",
    "#    - It merges the relevant Fama-French Factors (Global/Country specific).\n",
    "#    - It saves 6 separate Parquet files per anomaly.\n",
    "#\n",
    "# FINAL OUTPUTS:\n",
    "# --------------\n",
    "# Total Files = (Num Anomalies) * 6.\n",
    "# Examples:\n",
    "# - acc_Global_Daily_clean.parquet\n",
    "# - acc_Country_Annual_clean.parquet\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "# --- SAFETY FIX: PREVENT THREAD CRASHES ---\n",
    "if \"NUMEXPR_MAX_THREADS\" in os.environ:\n",
    "    del os.environ[\"NUMEXPR_MAX_THREADS\"]\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# PATHS\n",
    "BENCHMARK_FILE = Path(Temp_file_path_DP) / \"FF_Benchmark_data_clean.txt\"\n",
    "INPUT_DIR      = Path(Temp_file_path_A) # Folder containing ACC_clean.txt, etc.\n",
    "FACTOR_DIR     = Path(Temp_file_path_A) # Folder containing Factor files\n",
    "OUTPUT_DIR     = Path(Temp_file_path_R) # Folder for final .parquet output\n",
    "\n",
    "# SETTINGS\n",
    "INPUT_PATTERN  = \"*_clean.txt\"      # Matches ACC_clean.txt, AG_clean.txt...\n",
    "SEP            = \"|\"                # Separator for input text files\n",
    "TWO_YEARS_DAYS = 730                # Limit for carrying data forward\n",
    "N_JOBS         = 6                 # Optimized for 754GB RAM\n",
    "REPLICATION_COUNTRIES = [124, 840]  # Country Codes for Replication subset\n",
    "\n",
    "# VALIDATION SETTINGS (Sanity Check at the end)\n",
    "VAL_ID         = \"C12486370\"\n",
    "VAL_START      = \"2000-06-20\"\n",
    "VAL_END        = \"2000-07-10\"\n",
    "VAL_ANOMALY    = \"acc\"\n",
    "\n",
    "# COLUMNS TO FORCE AS TEXT (Preserves \"0124\" vs \"124\")\n",
    "STRING_COLS = [\"ID\", \"FiscalPeriod\", \"HistCurrency\", \"PCUR\", \"Country\"]\n",
    "\n",
    "# DATE LIMITS\n",
    "START_DATE     = \"1994-01-01\"\n",
    "END_DATE       = \"2024-12-31\"\n",
    "\n",
    "# =============================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def load_benchmark():\n",
    "    \"\"\"Loads the massive Benchmark file. Forces 'Country' to string. Filters Dates.\"\"\"\n",
    "    print(f\"Loading Benchmark: {BENCHMARK_FILE.name}...\")\n",
    "    dtype_map = {\"ID\": str, \"Country\": str} \n",
    "    try:\n",
    "        df = pd.read_csv(BENCHMARK_FILE, sep=SEP, dtype=dtype_map, low_memory=False)\n",
    "    except:\n",
    "        df = pd.read_csv(BENCHMARK_FILE, sep=SEP, low_memory=False)\n",
    "        df[\"ID\"] = df[\"ID\"].astype(str)\n",
    "        if \"Country\" in df.columns: df[\"Country\"] = df[\"Country\"].astype(str)\n",
    "\n",
    "    df[\"DayDate\"] = pd.to_datetime(df[\"DayDate\"], errors=\"coerce\")\n",
    "    \n",
    "    # --- NEW FILTERING STEP HERE ---\n",
    "    print(f\"Filtering Benchmark for range: {START_DATE} to {END_DATE}\")\n",
    "    df = df[(df[\"DayDate\"] >= START_DATE) & (df[\"DayDate\"] <= END_DATE)]\n",
    "    # -------------------------------\n",
    "    \n",
    "    if \"rf\" in df.columns: df.drop(columns=[\"rf\"], inplace=True)\n",
    "    if \"ret_bps\" in df.columns:\n",
    "        df[\"ret_bps\"] = pd.to_numeric(df[\"ret_bps\"], errors=\"coerce\")\n",
    "        df[\"ret\"] = df[\"ret_bps\"] / 10000.0\n",
    "        \n",
    "    print(f\"Benchmark Ready: {len(df):,} rows.\")\n",
    "    return df\n",
    "\n",
    "def load_factors():\n",
    "    \"\"\"Loads all 6 Fama-French Factor files into a dictionary.\"\"\"\n",
    "    print(\"Loading Factors...\")\n",
    "    factors = {}\n",
    "    files_map = {\n",
    "        (\"Daily\", \"Global\"):      \"Factors_Daily_Global.txt\",\n",
    "        (\"Daily\", \"Country\"):     \"Factors_Daily_Country.txt\",\n",
    "        (\"Daily\", \"Replication\"): \"Factors_Daily_Replication.txt\",\n",
    "        (\"Annual\", \"Global\"):     \"Factors_Annual_Global.txt\",\n",
    "        (\"Annual\", \"Country\"):    \"Factors_Annual_Country.txt\",\n",
    "        (\"Annual\", \"Replication\"): \"Factors_Annual_Replication.txt\",\n",
    "    }\n",
    "    \n",
    "    for (rebal, scope), fname in files_map.items():\n",
    "        fpath = FACTOR_DIR / fname\n",
    "        if fpath.exists():\n",
    "            dtype_map = {\"Country\": str} if scope == \"Country\" else {}\n",
    "            df = pd.read_csv(fpath, sep=SEP, dtype=dtype_map, low_memory=False)\n",
    "            df[\"DayDate\"] = pd.to_datetime(df[\"DayDate\"], errors=\"coerce\")\n",
    "            factors[(rebal, scope)] = df\n",
    "    print(f\"Loaded {len(factors)} factor files.\")\n",
    "    return factors\n",
    "\n",
    "def calc_deciles_vectorized(series):\n",
    "    \"\"\"\n",
    "    Computes Deciles 1-10 using pd.qcut (True Statistical Method).\n",
    "    \"\"\"\n",
    "    valid = series.dropna()\n",
    "    if len(valid) < 2: \n",
    "        return pd.Series(index=series.index, dtype=float)\n",
    "\n",
    "    try:\n",
    "        # TRUE METHOD: Quantile Cut\n",
    "        # duplicates='drop' merges bins if data is too clustered or sparse\n",
    "        deciles = pd.qcut(valid, 10, labels=False, duplicates='drop') + 1\n",
    "        \n",
    "    except ValueError:\n",
    "        # Fallback for extreme edge cases\n",
    "        ranks = valid.rank(method='first')\n",
    "        deciles = pd.qcut(ranks, 10, labels=False) + 1\n",
    "\n",
    "    return deciles.astype(float)\n",
    "\n",
    "def track_ram_usage(df):\n",
    "    \"\"\"Returns the deep memory usage of a DataFrame in GB.\"\"\"\n",
    "    mem_bytes = df.memory_usage(deep=True).sum()\n",
    "    return mem_bytes / (1024**3)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. WORKER FUNCTION (THE PIPELINE)\n",
    "# =============================================================================\n",
    "\n",
    "def process_pipeline_worker(input_path, df_bench_ref, factors_dict):\n",
    "    \"\"\"\n",
    "    The Worker Logic. Runs entire lifecycle for one anomaly file in RAM.\n",
    "    \"\"\"\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "    try:\n",
    "        import numexpr\n",
    "        numexpr.set_num_threads(1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    stem = input_path.stem.split(\"_\")[0].lower()\n",
    "    start_t = time.time()\n",
    "    \n",
    "    try:\n",
    "        # --- STEP 1: LOAD & MERGE ---\n",
    "        dtype_map = {col: str for col in STRING_COLS if col in STRING_COLS}\n",
    "        try:\n",
    "            df_anom = pd.read_csv(input_path, sep=SEP, dtype=dtype_map, low_memory=False)\n",
    "        except:\n",
    "            df_anom = pd.read_csv(input_path, sep=SEP, low_memory=False)\n",
    "\n",
    "        if \"PIT Date\" in df_anom.columns:\n",
    "            df_anom[\"PIT Date\"] = pd.to_datetime(df_anom[\"PIT Date\"], errors=\"coerce\")\n",
    "            df_anom = df_anom.rename(columns={\"PIT Date\": \"DayDate\"})\n",
    "        \n",
    "        valid_ids = df_anom[\"ID\"].unique()\n",
    "        df_merged = df_bench_ref[df_bench_ref[\"ID\"].isin(valid_ids)].copy()\n",
    "        \n",
    "        df_merged = pd.merge(df_merged, df_anom, on=[\"ID\", \"DayDate\"], how=\"left\", suffixes=(\"\", \"_new\"))\n",
    "        \n",
    "        # --- STEP 2: FILL FORWARD ---\n",
    "        df_merged = df_merged.sort_values(by=[\"ID\", \"DayDate\"])\n",
    "        df_merged[\"UpdateDate\"] = np.where(df_merged[stem].notna(), 1, 0)\n",
    "\n",
    "        # =========================================================================\n",
    "        # SAVE INTERMEDIATE \"INFORMATION ARRIVAL\" DATASET (PARQUET)\n",
    "        # =========================================================================\n",
    "        # Goal: Save only the rows where new information arrived (UpdateDate == 1) \n",
    "        # for use in the \"Information Lag\" analysis (Table II).\n",
    "        # =========================================================================\n",
    "        \n",
    "        mask_new_info = df_merged[\"UpdateDate\"] == 1\n",
    "        \n",
    "        if mask_new_info.any():\n",
    "            # Select only the columns required for the next script\n",
    "            cols_to_save = [\"ID\", \"DayDate\", \"UpdateDate\", \"FiscalPeriod\"]\n",
    "            valid_cols = [c for c in cols_to_save if c in df_merged.columns]\n",
    "            \n",
    "            df_intermediate = df_merged.loc[mask_new_info, valid_cols].copy()\n",
    "            \n",
    "            # Save as Parquet with a clear, descriptive name\n",
    "            inter_fname = f\"{stem}_Information_Arrival.parquet\"\n",
    "            inter_path = OUTPUT_DIR / inter_fname  # Or INTERMEDIATE_DIR if you defined one\n",
    "            \n",
    "            df_intermediate.to_parquet(inter_path, index=False)\n",
    "            \n",
    "            # Clean up memory\n",
    "            del df_intermediate\n",
    "\n",
    "        df_merged[\"_spell_id\"] = df_merged.groupby(\"ID\")[\"UpdateDate\"].cumsum()\n",
    "        \n",
    "        cols_to_fill = [stem, \"FiscalPeriod\", \"HistCurrency\"]\n",
    "        df_merged[cols_to_fill] = df_merged.groupby([\"ID\", \"_spell_id\"])[cols_to_fill].ffill()\n",
    "        \n",
    "        df_merged[\"_anchor_date\"] = np.where(df_merged[\"UpdateDate\"] == 1, df_merged[\"DayDate\"], pd.NaT)\n",
    "        df_merged[\"_anchor_date\"] = pd.to_datetime(df_merged[\"_anchor_date\"])\n",
    "        df_merged[\"_anchor_date\"] = df_merged.groupby([\"ID\", \"_spell_id\"])[\"_anchor_date\"].ffill()\n",
    "        df_merged[\"EventDate\"] = df_merged.groupby([\"ID\", \"_spell_id\"]).cumcount()\n",
    "\n",
    "        # --- STEP 3: CONSTRAINTS ---\n",
    "        days_diff = (df_merged[\"DayDate\"] - df_merged[\"_anchor_date\"]).dt.days\n",
    "        mask_too_old = (days_diff > TWO_YEARS_DAYS) | days_diff.isna()\n",
    "        \n",
    "        prev_dates = df_merged[\"DayDate\"].shift(1)\n",
    "        prev_ids = df_merged[\"ID\"].shift(1)\n",
    "        gap_days = (df_merged[\"DayDate\"] - prev_dates).dt.days\n",
    "        gap_days = np.where(df_merged[\"ID\"] == prev_ids, gap_days, 0)\n",
    "        mask_gap_error = gap_days > 14\n",
    "        \n",
    "        mask_invalid = mask_too_old | mask_gap_error\n",
    "        df_merged.loc[mask_invalid, cols_to_fill] = np.nan\n",
    "        df_merged = df_merged[df_merged[stem].notna()]\n",
    "        \n",
    "        df_merged.drop(columns=[\"_spell_id\", \"_anchor_date\", \"UpdateDate\"], inplace=True)\n",
    "        \n",
    "        # RAM Safety Check\n",
    "        peak_ram = track_ram_usage(df_merged)\n",
    "        if peak_ram > 50: print(f\"[{stem}] HIGH RAM USE: {peak_ram:.2f} GB\")\n",
    "        \n",
    "        if \"Country\" in df_merged.columns:\n",
    "            df_merged[\"_country_num\"] = pd.to_numeric(df_merged[\"Country\"], errors=\"coerce\")\n",
    "        df_merged[\"_rank_val\"] = pd.to_numeric(df_merged[stem], errors=\"coerce\")\n",
    "\n",
    "        # --- STEP 4: DAILY DECILES ---\n",
    "        df_merged[\"Decile_Total_Daily\"] = np.nan\n",
    "        df_merged[\"Decile_Country_Daily\"] = np.nan\n",
    "        df_merged[\"Decile_Rep_Daily\"] = np.nan\n",
    "\n",
    "        mask_rebal_daily = (df_merged[\"EventDate\"] == 1) & (df_merged[\"_rank_val\"].notna())\n",
    "        \n",
    "        if mask_rebal_daily.any():\n",
    "            df_merged.loc[mask_rebal_daily, \"Decile_Total_Daily\"] = (\n",
    "                df_merged[mask_rebal_daily].groupby(\"DayDate\")[\"_rank_val\"]\n",
    "                .transform(calc_deciles_vectorized)\n",
    "            )\n",
    "            df_merged.loc[mask_rebal_daily, \"Decile_Country_Daily\"] = (\n",
    "                df_merged[mask_rebal_daily].groupby([\"DayDate\", \"Country\"])[\"_rank_val\"]\n",
    "                .transform(calc_deciles_vectorized)\n",
    "            )\n",
    "            mask_rep = mask_rebal_daily & (df_merged[\"_country_num\"].isin(REPLICATION_COUNTRIES))\n",
    "            if mask_rep.any():\n",
    "                df_merged.loc[mask_rep, \"Decile_Rep_Daily\"] = (\n",
    "                    df_merged[mask_rep].groupby(\"DayDate\")[\"_rank_val\"]\n",
    "                    .transform(calc_deciles_vectorized)\n",
    "                )\n",
    "        \n",
    "        daily_cols = [\"Decile_Total_Daily\", \"Decile_Country_Daily\", \"Decile_Rep_Daily\"]\n",
    "        df_merged[daily_cols] = df_merged.groupby(\"ID\")[daily_cols].ffill()\n",
    "\n",
    "        # --- STEP 5: ANNUAL DECILES ---\n",
    "        df_merged[\"Decile_Total_Annual\"] = np.nan\n",
    "        df_merged[\"Decile_Country_Annual\"] = np.nan\n",
    "        df_merged[\"Decile_Rep_Annual\"] = np.nan\n",
    "\n",
    "        df_merged[\"_rebal_cohort\"] = df_merged[\"DayDate\"].dt.year.astype(\"int64\")\n",
    "        mask_prev = (df_merged[\"DayDate\"].dt.month < 6) | ((df_merged[\"DayDate\"].dt.month == 6) & (df_merged[\"DayDate\"].dt.day < 30))\n",
    "        df_merged.loc[mask_prev, \"_rebal_cohort\"] -= 1\n",
    "        \n",
    "        df_merged[\"_is_anchor\"] = ~df_merged.duplicated(subset=[\"ID\", \"_rebal_cohort\"], keep=\"first\")\n",
    "        mask_calc_annual = df_merged[\"_is_anchor\"] & df_merged[\"_rank_val\"].notna()\n",
    "        \n",
    "        if mask_calc_annual.any():\n",
    "            df_merged.loc[mask_calc_annual, \"Decile_Total_Annual\"] = (\n",
    "                df_merged[mask_calc_annual].groupby(\"DayDate\")[\"_rank_val\"]\n",
    "                .transform(calc_deciles_vectorized)\n",
    "            )\n",
    "            df_merged.loc[mask_calc_annual, \"Decile_Country_Annual\"] = (\n",
    "                df_merged[mask_calc_annual].groupby([\"DayDate\", \"Country\"])[\"_rank_val\"]\n",
    "                .transform(calc_deciles_vectorized)\n",
    "            )\n",
    "            mask_rep = mask_calc_annual & (df_merged[\"_country_num\"].isin(REPLICATION_COUNTRIES))\n",
    "            if mask_rep.any():\n",
    "                df_merged.loc[mask_rep, \"Decile_Rep_Annual\"] = (\n",
    "                    df_merged[mask_rep].groupby(\"DayDate\")[\"_rank_val\"]\n",
    "                    .transform(calc_deciles_vectorized)\n",
    "                )\n",
    "\n",
    "        ann_cols = [\"Decile_Total_Annual\", \"Decile_Country_Annual\", \"Decile_Rep_Annual\"]\n",
    "        df_merged[ann_cols] = df_merged.groupby([\"ID\", \"_rebal_cohort\"])[ann_cols].ffill()\n",
    "\n",
    "        # --- STEP 6: SAVE LOOP ---\n",
    "        files_created = []\n",
    "        scenarios = [\n",
    "            (\"Daily\", \"Decile_Total_Daily\",   \"Global\",      [\"DayDate\"]),\n",
    "            (\"Daily\", \"Decile_Country_Daily\", \"Country\",     [\"DayDate\", \"Country\"]),\n",
    "            (\"Daily\", \"Decile_Rep_Daily\",     \"Replication\", [\"DayDate\"]),\n",
    "            (\"Annual\", \"Decile_Total_Annual\",   \"Global\",      [\"DayDate\"]),\n",
    "            (\"Annual\", \"Decile_Country_Annual\", \"Country\",     [\"DayDate\", \"Country\"]),\n",
    "            (\"Annual\", \"Decile_Rep_Annual\",     \"Replication\", [\"DayDate\"]),\n",
    "        ]\n",
    "\n",
    "        for rebal_mode, decile_col, scope, merge_keys in scenarios:\n",
    "            mask_keep = df_merged[decile_col].isin([1, 10])\n",
    "            if not mask_keep.any(): continue\n",
    "                \n",
    "            df_sub = df_merged[mask_keep].copy()\n",
    "            df_sub.rename(columns={decile_col: \"Decile\"}, inplace=True)\n",
    "            \n",
    "            factor_key = (rebal_mode, scope)\n",
    "            if factor_key not in factors_dict: continue\n",
    "            df_factors = factors_dict[factor_key]\n",
    "            \n",
    "            # Type safety\n",
    "            if \"Country\" in merge_keys:\n",
    "                df_sub[\"Country\"] = df_sub[\"Country\"].astype(str)\n",
    "                df_factors[\"Country\"] = df_factors[\"Country\"].astype(str)\n",
    "            \n",
    "            df_final = pd.merge(df_sub, df_factors, on=merge_keys, how=\"inner\")\n",
    "            \n",
    "            if df_final.empty: continue\n",
    "            \n",
    "            # Clean columns\n",
    "            cols_to_drop = [\n",
    "                \"_rank_val\", \"_country_num\", \"_rebal_cohort\", \"_is_anchor\",\n",
    "                \"Decile_Total_Daily\", \"Decile_Country_Daily\", \"Decile_Rep_Daily\",\n",
    "                \"Decile_Total_Annual\", \"Decile_Country_Annual\", \"Decile_Rep_Annual\",\n",
    "                \"PCUR\", \"MV_LC\", \"ret_bps\", \"FiscalPeriod\", \"HistCurrency\"\n",
    "            ]\n",
    "            df_final.drop(columns=[c for c in cols_to_drop if c in df_final.columns], inplace=True)\n",
    "            \n",
    "            out_name = f\"{stem}_{scope}_{rebal_mode}_clean.parquet\"\n",
    "            df_final.to_parquet(OUTPUT_DIR / out_name, index=False)\n",
    "            files_created.append(out_name)\n",
    "            \n",
    "            del df_sub, df_final\n",
    "            \n",
    "        elapsed = time.time() - start_t\n",
    "        return f\"DONE: {stem} -> {len(files_created)} files. Time: {elapsed/60:.1f} min\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {stem} - {str(e)}\"\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MAIN EXECUTION & VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_start = time.time()\n",
    "    \n",
    "    # --- PART A: LOAD DATA ---\n",
    "    df_bench = load_benchmark()\n",
    "    factors_dict = load_factors()\n",
    "    \n",
    "    if df_bench.empty or not factors_dict:\n",
    "        print(\"Critical Error: Missing Inputs. Stopping.\")\n",
    "        sys.exit()\n",
    "\n",
    "    # --- PART B: PROCESSING ---\n",
    "    all_files = sorted(list(INPUT_DIR.glob(INPUT_PATTERN)))\n",
    "    print(f\"\\nProcessing {len(all_files)} anomalies (Full Production Run).\")\n",
    "    print(f\"Workers: {N_JOBS}\")\n",
    "    print(\"Output Format: Parquet\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = Parallel(n_jobs=N_JOBS, backend=\"loky\", verbose=10)(\n",
    "        delayed(process_pipeline_worker)(f, df_bench, factors_dict) for f in all_files\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    for res in results:\n",
    "        print(res)\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Pipeline Time: {(time.time() - total_start)/60:.1f} minutes.\")\n",
    "    \n",
    "    del results\n",
    "    gc.collect()\n",
    "\n",
    "    # --- PART C: FINAL VALIDATION ---\n",
    "    print(\"\\n\" + \"*\"*60)\n",
    "    print(f\"5. VALIDATION CHECK FOR ID: {VAL_ID}\")\n",
    "    print(f\"   Target Window: {VAL_START} to {VAL_END}\")\n",
    "    print(\"*\"*60 + \"\\n\")\n",
    "\n",
    "    # 1. INPUT CHECK\n",
    "    print(f\"A. RAW INPUT ({VAL_ANOMALY})\")\n",
    "    # Smart file search (Case Insensitive)\n",
    "    input_file_path = next((f for f in all_files if f.name.lower().startswith(VAL_ANOMALY.lower() + \"_\")), None)\n",
    "    \n",
    "    if input_file_path:\n",
    "        try:\n",
    "            df_in = pd.read_csv(input_file_path, sep=SEP, dtype={\"ID\": str}, low_memory=False)\n",
    "            if \"PIT Date\" in df_in.columns: df_in.rename(columns={\"PIT Date\": \"DayDate\"}, inplace=True)\n",
    "            df_in[\"DayDate\"] = pd.to_datetime(df_in[\"DayDate\"])\n",
    "            \n",
    "            mask_win = (df_in[\"ID\"] == VAL_ID) & (df_in[\"DayDate\"].between(VAL_START, VAL_END))\n",
    "            view = df_in[mask_win]\n",
    "            \n",
    "            if not view.empty:\n",
    "                print(view[[\"ID\", \"DayDate\", VAL_ANOMALY]].to_string(index=False))\n",
    "            else:\n",
    "                print(\"   (No data in window. Checking fill-forward source...)\")\n",
    "                mask_prior = (df_in[\"ID\"] == VAL_ID) & (df_in[\"DayDate\"] < VAL_START)\n",
    "                prior = df_in[mask_prior].sort_values(\"DayDate\").tail(1)\n",
    "                if not prior.empty:\n",
    "                    print(prior[[\"ID\", \"DayDate\", VAL_ANOMALY]].to_string(index=False))\n",
    "                else:\n",
    "                    print(\"   (No data found for this ID at all.)\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error: {e}\")\n",
    "    else:\n",
    "        print(\"   (Input file not found)\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # 2. OUTPUT CHECK (ALL 6 SCENARIOS)\n",
    "    print(f\"C. FINAL OUTPUTS (Checking ALL 6 Scenarios for {VAL_ANOMALY})\")\n",
    "    \n",
    "    val_scenarios = [\n",
    "        (\"Daily\", \"Global\"), (\"Daily\", \"Country\"), (\"Daily\", \"Replication\"),\n",
    "        (\"Annual\", \"Global\"), (\"Annual\", \"Country\"), (\"Annual\", \"Replication\")\n",
    "    ]\n",
    "\n",
    "    for rebal, scope in val_scenarios:\n",
    "        out_fname = f\"{VAL_ANOMALY}_{scope}_{rebal}_clean.parquet\"\n",
    "        out_path = OUTPUT_DIR / out_fname\n",
    "        print(f\"\\n   >>> Checking: {out_fname}\")\n",
    "        \n",
    "        if out_path.exists():\n",
    "            try:\n",
    "                df_out = pd.read_parquet(out_path)\n",
    "                df_out[\"DayDate\"] = pd.to_datetime(df_out[\"DayDate\"])\n",
    "                \n",
    "                mask_out = (df_out[\"ID\"] == VAL_ID) & (df_out[\"DayDate\"].between(VAL_START, VAL_END))\n",
    "                view_out = df_out[mask_out]\n",
    "                \n",
    "                if not view_out.empty:\n",
    "                    # Dynamically find columns to show (Fixes duplicate column bugs)\n",
    "                    possible_cols = [\"ID\", \"DayDate\", VAL_ANOMALY, \"acc\", \"ACC\", \"Decile\", \"ret\", \n",
    "                                     \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"Mom\", \"RF\"]\n",
    "                    cols = [c for c in possible_cols if c in df_out.columns]\n",
    "                    # Remove duplicates from list if any exist\n",
    "                    cols = list(dict.fromkeys(cols)) \n",
    "                    print(view_out[cols].to_string(index=False))\n",
    "                else:\n",
    "                    print(\"       (No rows. Decile likely not 1 or 10, or filtered by gap rules.)\")\n",
    "            except Exception as e:\n",
    "                print(f\"       Error reading file: {e}\")\n",
    "        else:\n",
    "            print(\"       (File not found. Pipeline may have skipped it if empty.)\")\n",
    "\n",
    "    # Cleanup\n",
    "    del df_bench\n",
    "    del factors_dict\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJ-nLTuvzzWr"
   },
   "source": [
    "# Event Time Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bxdfge8Mnv46"
   },
   "source": [
    "### Event Time Regression TBD\n",
    "\n",
    "=> Comment: RunTime Error Warning is not a problem as it is handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 170602,
     "status": "ok",
     "timestamp": 1765532641870,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "UDBfFTldrX_Z",
    "outputId": "8cfbbe44-a140-4523-ff2a-33731832cd62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING SEQUENTIAL REGRESSION RUN\n",
      "Anomalies: 27\n",
      "Output: /home/jovyan/work/hpool1/pseidel/test/Temp/TempRegressionModel/EventTimeRegression_Daily_Final_FULL.xlsx\n",
      "================================================================================\n",
      "\n",
      "[1/27] Processing Anomaly: acc ...\n",
      "   [AUDIT] Country File: Loaded 27,766,736 rows covering 66 countries.\n",
      "   [FILTER] Kept 51 countries. Dropped 11 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n",
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/27] Processing Anomaly: ag ...\n",
      "   [AUDIT] Country File: Loaded 35,645,205 rows covering 66 countries.\n",
      "   [FILTER] Kept 58 countries. Dropped 5 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n",
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n",
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n",
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/27] Processing Anomaly: at ...\n",
      "   [AUDIT] Country File: Loaded 36,466,055 rows covering 66 countries.\n",
      "   [FILTER] Kept 55 countries. Dropped 8 (ID < 30).\n",
      "\n",
      "[4/27] Processing Anomaly: cat ...\n",
      "   [AUDIT] Country File: Loaded 33,613,889 rows covering 66 countries.\n",
      "   [FILTER] Kept 55 countries. Dropped 8 (ID < 30).\n",
      "\n",
      "[5/27] Processing Anomaly: cpm ...\n",
      "   [AUDIT] Country File: Loaded 35,110,426 rows covering 66 countries.\n",
      "   [FILTER] Kept 57 countries. Dropped 6 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/27] Processing Anomaly: ec ...\n",
      "   [AUDIT] Country File: Loaded 19,225,459 rows covering 65 countries.\n",
      "   [FILTER] Kept 54 countries. Dropped 8 (ID < 30).\n",
      "\n",
      "[7/27] Processing Anomaly: es ...\n",
      "   [AUDIT] Country File: Loaded 30,451,491 rows covering 66 countries.\n",
      "   [FILTER] Kept 57 countries. Dropped 6 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8/27] Processing Anomaly: gp ...\n",
      "   [AUDIT] Country File: Loaded 37,589,186 rows covering 66 countries.\n",
      "   [FILTER] Kept 56 countries. Dropped 7 (ID < 30).\n",
      "\n",
      "[9/27] Processing Anomaly: ig ...\n",
      "   [AUDIT] Country File: Loaded 33,965,657 rows covering 66 countries.\n",
      "   [FILTER] Kept 57 countries. Dropped 6 (ID < 30).\n",
      "\n",
      "[10/27] Processing Anomaly: inv ...\n",
      "   [AUDIT] Country File: Loaded 33,806,914 rows covering 66 countries.\n",
      "   [FILTER] Kept 55 countries. Dropped 8 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n",
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11/27] Processing Anomaly: ltg ...\n",
      "   [AUDIT] Country File: Loaded 22,671,159 rows covering 65 countries.\n",
      "   [FILTER] Kept 45 countries. Dropped 17 (ID < 30).\n",
      "\n",
      "[12/27] Processing Anomaly: nca ...\n",
      "   [AUDIT] Country File: Loaded 35,967,590 rows covering 66 countries.\n",
      "   [FILTER] Kept 57 countries. Dropped 6 (ID < 30).\n",
      "\n",
      "[13/27] Processing Anomaly: noa ...\n",
      "   [AUDIT] Country File: Loaded 36,558,145 rows covering 66 countries.\n",
      "   [FILTER] Kept 57 countries. Dropped 6 (ID < 30).\n",
      "\n",
      "[14/27] Processing Anomaly: nwc ...\n",
      "   [AUDIT] Country File: Loaded 37,243,933 rows covering 66 countries.\n",
      "   [FILTER] Kept 57 countries. Dropped 6 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n",
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[15/27] Processing Anomaly: ol ...\n",
      "   [AUDIT] Country File: Loaded 30,471,907 rows covering 66 countries.\n",
      "   [FILTER] Kept 55 countries. Dropped 8 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[16/27] Processing Anomaly: osc ...\n",
      "   [AUDIT] Country File: Loaded 38,199,626 rows covering 66 countries.\n",
      "   [FILTER] Kept 58 countries. Dropped 5 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[17/27] Processing Anomaly: pm ...\n",
      "   [AUDIT] Country File: Loaded 36,986,298 rows covering 66 countries.\n",
      "   [FILTER] Kept 58 countries. Dropped 5 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[18/27] Processing Anomaly: poa ...\n",
      "   [AUDIT] Country File: Loaded 37,580,861 rows covering 66 countries.\n",
      "   [FILTER] Kept 58 countries. Dropped 5 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n",
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[19/27] Processing Anomaly: pro ...\n",
      "   [AUDIT] Country File: Loaded 37,882,690 rows covering 66 countries.\n",
      "   [FILTER] Kept 58 countries. Dropped 5 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20/27] Processing Anomaly: pta ...\n",
      "   [AUDIT] Country File: Loaded 34,521,518 rows covering 66 countries.\n",
      "   [FILTER] Kept 56 countries. Dropped 7 (ID < 30).\n",
      "\n",
      "[21/27] Processing Anomaly: roe ...\n",
      "   [AUDIT] Country File: Loaded 35,573,536 rows covering 66 countries.\n",
      "   [FILTER] Kept 52 countries. Dropped 11 (ID < 30).\n",
      "\n",
      "[22/27] Processing Anomaly: rs ...\n",
      "   [AUDIT] Country File: Loaded 33,196,412 rows covering 66 countries.\n",
      "   [FILTER] Kept 57 countries. Dropped 6 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n",
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[23/27] Processing Anomaly: sg ...\n",
      "   [AUDIT] Country File: Loaded 35,211,537 rows covering 66 countries.\n",
      "   [FILTER] Kept 53 countries. Dropped 10 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[24/27] Processing Anomaly: sli ...\n",
      "   [AUDIT] Country File: Loaded 33,901,518 rows covering 66 countries.\n",
      "   [FILTER] Kept 56 countries. Dropped 7 (ID < 30).\n",
      "\n",
      "[25/27] Processing Anomaly: slx ...\n",
      "   [AUDIT] Country File: Loaded 30,602,398 rows covering 66 countries.\n",
      "   [FILTER] Kept 55 countries. Dropped 8 (ID < 30).\n",
      "\n",
      "[26/27] Processing Anomaly: tx ...\n",
      "   [AUDIT] Country File: Loaded 38,213,750 rows covering 66 countries.\n",
      "   [FILTER] Kept 57 countries. Dropped 6 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[27/27] Processing Anomaly: txf ...\n",
      "   [AUDIT] Country File: Loaded 25,076,477 rows covering 66 countries.\n",
      "   [FILTER] Kept 52 countries. Dropped 11 (ID < 30).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.12/site-packages/linearmodels/panel/results.py:88: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return Series(np.sqrt(np.diag(self.cov)), self._var_names, name=\"std_error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "AGGREGATING DATA & CALCULATING AVERAGES\n",
      "================================================================================\n",
      "\n",
      "Writing results to /home/jovyan/work/hpool1/pseidel/test/Temp/TempRegressionModel/EventTimeRegression_Daily_Final_FULL.xlsx...\n",
      "Final Report contains data for 58 unique countries.\n",
      "Success! File saved.\n",
      "\n",
      "Completed in 537.3 minutes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from pathlib import Path\n",
    "from linearmodels.panel import PanelOLS\n",
    "import gc\n",
    "\n",
    "# =================================================================================================\n",
    "# MASTER SCRIPT: SEQUENTIAL EVENT-TIME ANOMALY REGRESSION (FULL PRODUCTION)\n",
    "# =================================================================================================\n",
    "#\n",
    "# 1. EXECUTIVE SUMMARY\n",
    "# --------------------\n",
    "# This script performs a robust financial analysis of 27 equity market anomalies. It calculates\n",
    "# \"Event-Time Alphas\" by running Long-Short regressions on daily data. The analysis is performed\n",
    "# across three distinct scopes:\n",
    "#    a) Total (Global Universe)\n",
    "#    b) Replication (USA & Canada / ID 124 & 840)\n",
    "#    c) Individual Countries (Iterative loop)\n",
    "#\n",
    "# 2. OUTPUT FORMATTING (UPDATED)\n",
    "# ------------------------------\n",
    "#    * \"Average_Overview\" Tab:\n",
    "#      - Now provides a detailed granular view for the \"Average Anomaly\" across ALL time windows.\n",
    "#      - Rows: Country + Time Window (All, 20, 40, 80, 81-240).\n",
    "#      - Metrics: Annualized Alpha, SE, T-Stat, and Significance Stars.\n",
    "#\n",
    "#    * Summary Tabs (Total, Rep, Country):\n",
    "#      - Features \"Interleaved Columns\" for easier reading of statistical significance.\n",
    "#      - Format: [Window Alpha] | [Window T-Stat] | [Next Window Alpha] | [Next Window T-Stat] ...\n",
    "#      - Rows: Each individual anomaly + the \"AVERAGE\" anomaly at the top.\n",
    "#\n",
    "# 3. SYSTEM ARCHITECTURE (STABILITY FOCUSED)\n",
    "# ------------------------------------------\n",
    "#    * SEQUENTIAL EXECUTION: The script runs on a SINGLE WORKER (Main Process).\n",
    "#      - Reason: To strictly avoid \"Resource temporarily unavailable\" / \"pthread_create\" errors \n",
    "#        caused by OpenBLAS thread explosion on restricted servers (e.g., Binder/JupyterHub).\n",
    "#    * THREAD LOCKING: Environment variables (OMP_NUM_THREADS, etc.) are forced to '1' before\n",
    "#      any math libraries load.\n",
    "#    * MEMORY MANAGEMENT: Garbage collection (gc.collect) is triggered after every anomaly file \n",
    "#      to prevent RAM bloat during the long overnight run.\n",
    "#\n",
    "# 4. ECONOMETRIC SPECIFICATION\n",
    "# ----------------------------\n",
    "#    * MODEL: 5-Factor Fama-French + Momentum (6 Factors total).\n",
    "#      Formula: eret ~ 1 + hedge * (mktrf + smb + rmw + cma + umd)\n",
    "#    * ESTIMATOR (PANEL): Linearmodels 'PanelOLS' used for individual anomalies.\n",
    "#      - Covariance: Clustered by Entity (Firm) and Time (Date).\n",
    "#    * ESTIMATOR (AVERAGE): Statsmodels 'OLS' used for the \"Average Anomaly\".\n",
    "#      - Covariance: Newey-West HAC (Heteroskedasticity and Autocorrelation Consistent).\n",
    "#      - Lags: 6 Trading Days (approx. 1 week lag).\n",
    "#    * ANNUALIZATION:\n",
    "#      - Coefficients: Geometric Compounding -> ((1 + daily_alpha) ^ 252) - 1\n",
    "#      - Standard Errors: Delta Method Approximation -> SE * 252 * (1 + daily_alpha)^251\n",
    "#\n",
    "# 5. DATA PIPELINE & FILTERS\n",
    "# --------------------------\n",
    "#    * INPUT: Parquet files named by anomaly stem (e.g., \"acc_Country_Daily_clean.parquet\").\n",
    "#    * SCOPE HANDLING:\n",
    "#      - \"Total\": Runs regression on the full dataset.\n",
    "#      - \"Replication\": automatically filters for USA (840) and Canada (124).\n",
    "#      - \"Country\": Loops through every country code found in the file.\n",
    "#    * SAFETY FILTER (CRITICAL):\n",
    "#      - Countries with fewer than 30 unique Firm IDs are DROPPED automatically.\n",
    "#      - This prevents Singular Matrix errors and statistical noise from tiny markets.\n",
    "#    * EVENT WINDOWS:\n",
    "#      - All (Full history), 20 days, 40 days, 80 days, and 81-240 days.\n",
    "#\n",
    "# 6. AUDITING & LOGGING\n",
    "# ---------------------\n",
    "\n",
    "# --- THREAD SAFETY (CRITICAL) ---\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "try:\n",
    "    import numexpr\n",
    "    numexpr.set_num_threads(1)\n",
    "except: pass\n",
    "\n",
    "# =================================================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =================================================================================================\n",
    "\n",
    "# --- PATHS ---\n",
    "DATA_FOLDER = Path(Temp_file_path_R) \n",
    "OUTPUT_EXCEL = os.path.join(DATA_FOLDER, \"EventTimeRegression_Daily_Final_FULL.xlsx\")\n",
    "\n",
    "# --- ANOMALY CONFIGURATION (ALL 27 ITEMS) ---\n",
    "ANOMALY_CONFIG = {\n",
    "    \"acc\": (1, 10), \"ag\": (1, 10), \"at\": (10, 1), \"cat\": (10, 1),\n",
    "    \"cpm\": (10, 1), \"ec\": (10, 1), \"es\": (10, 1), \"gp\": (10, 1),\n",
    "    \"ig\": (1, 10), \"inv\": (1, 10), \"ltg\": (10, 1), \"nca\": (1, 10),\n",
    "    \"noa\": (1, 10), \"nwc\": (1, 10), \"ol\": (10, 1), \"osc\": (1, 10),\n",
    "    \"pm\": (10, 1), \"poa\": (1, 10), \"pro\": (10, 1), \"pta\": (1, 10),\n",
    "    \"roe\": (10, 1), \"rs\": (10, 1), \"sg\": (1, 10), \"sli\": (10, 1),\n",
    "    \"slx\": (10, 1), \"tx\": (10, 1), \"txf\": (1, 10),\n",
    "}\n",
    "\n",
    "# --- FILE PATTERNS ---\n",
    "FILE_PATTERNS = {\n",
    "    \"Total\":       \"{stem}_Global_Daily_clean.parquet\",       \n",
    "    \"Replication\": \"{stem}_Replication_Daily_clean.parquet\", \n",
    "    \"Country\":     \"{stem}_Country_Daily_clean.parquet\"      \n",
    "}\n",
    "\n",
    "# --- REGRESSION PARAMETERS ---\n",
    "TRADING_DAYS = 252\n",
    "MIN_OBS = 252\n",
    "NEWEY_WEST_LAGS = 6\n",
    "MIN_COUNTRY_IDS = 30     # Filter for small countries\n",
    "\n",
    "# --- EVENT WINDOWS (Excludes Day 0) ---\n",
    "WINDOW_SPECS = {\n",
    "    # \"All\"\n",
    "    \"All\":     lambda s: s >= 1,  \n",
    "    \n",
    "    # Strict 1 to X windows\n",
    "    \"20\":      lambda s: (s >= 1) & (s <= 20),\n",
    "    \"40\":      lambda s: (s >= 1) & (s <= 40),\n",
    "    \"80\":      lambda s: (s >= 1) & (s <= 80),\n",
    "    \n",
    "    # Distinct bucket for the tail\n",
    "    \"81_240\":  lambda s: (s >= 81) & (s <= 240),\n",
    "}\n",
    "\n",
    "# --- MODEL SPECIFICATION ---\n",
    "FORMULA = \"eret ~ 1 + hedge * (mktrf + smb + rmw + cma + umd)\"\n",
    "FACTOR_COLS = [\"mktrf\", \"smb\", \"rmw\", \"cma\", \"umd\"]\n",
    "\n",
    "# --- DTYPES ---\n",
    "DTYPE_DICT = {\n",
    "    \"ret\": \"float32\", \"rf\": \"float32\", \n",
    "    \"mktrf\": \"float32\", \"smb\": \"float32\", \"rmw\": \"float32\", \n",
    "    \"cma\": \"float32\", \"umd\": \"float32\", \n",
    "}\n",
    "\n",
    "# =================================================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# =================================================================================================\n",
    "\n",
    "def add_stars(t):\n",
    "    at = abs(t)\n",
    "    if at >= 2.58: return \"***\"\n",
    "    elif at >= 1.96: return \"**\"\n",
    "    elif at >= 1.65: return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "def extract_stats(result, label, window, scope, country_lbl, model_type=\"Panel\"):\n",
    "    rows = []\n",
    "    if model_type == \"Panel\":\n",
    "        params = result.params\n",
    "        std = result.std_errors\n",
    "    else: # Statsmodels\n",
    "        params = result.params\n",
    "        std = result.bse\n",
    "        \n",
    "    for pname in params.index:\n",
    "        if \"hedge\" in pname and \":\" not in pname: \n",
    "            coef = params[pname]\n",
    "            se = std[pname]\n",
    "            \n",
    "            ann_coef = (1.0 + coef) ** TRADING_DAYS - 1.0\n",
    "            derivative = TRADING_DAYS * (1.0 + coef) ** (TRADING_DAYS - 1)\n",
    "            ann_se = se * derivative\n",
    "            \n",
    "            if model_type == \"Panel\": tstat = coef / se if se != 0 else 0\n",
    "            else: tstat = result.tvalues[pname]\n",
    "\n",
    "            rows.append({\n",
    "                \"file\": label, \"window\": window, \"scope\": scope, \"country\": country_lbl,\n",
    "                \"param\": \"hedge\",\n",
    "                \"annualized_alpha\": ann_coef, \"annualized_se\": ann_se,\n",
    "                \"daily_alpha\": coef, \"daily_se\": se,\n",
    "                \"tstat\": tstat, \"stars\": add_stars(tstat)\n",
    "            })\n",
    "            break \n",
    "    return rows\n",
    "\n",
    "def calc_daily_metrics(df, label):\n",
    "    if df.empty: return None\n",
    "    ret_grp = df.groupby([\"DayDate\", \"hedge\"])[\"eret\"].mean().unstack()\n",
    "    if 1.0 in ret_grp.columns: ret_grp.rename(columns={1.0: \"long_ret\"}, inplace=True)\n",
    "    if 0.0 in ret_grp.columns: ret_grp.rename(columns={0.0: \"short_ret\"}, inplace=True)\n",
    "    \n",
    "    fact_grp = df.groupby(\"DayDate\")[FACTOR_COLS].mean()\n",
    "    daily_data = ret_grp.join(fact_grp, how=\"inner\")\n",
    "    \n",
    "    if \"long_ret\" not in daily_data.columns and \"short_ret\" not in daily_data.columns: return None\n",
    "    return daily_data\n",
    "\n",
    "# =================================================================================================\n",
    "# 3. PROCESSING LOGIC\n",
    "# =================================================================================================\n",
    "\n",
    "def process_anomaly_stem(stem, long_dec, short_dec, current_idx, total_count):\n",
    "    \"\"\"Main Logic Function with Auditing.\"\"\"\n",
    "    print(f\"\\n[{current_idx}/{total_count}] Processing Anomaly: {stem} ...\")\n",
    "    results_rows = []\n",
    "    return_series_dict = {}\n",
    "\n",
    "    def process_one_file(file_type, file_pattern, scope_name_func):\n",
    "        fpath = DATA_FOLDER / file_pattern.format(stem=stem)\n",
    "        \n",
    "        # --- AUDIT: CHECK FILE ---\n",
    "        if not fpath.exists(): \n",
    "            print(f\"   [Skip] File missing: {fpath.name}\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_parquet(fpath)\n",
    "            \n",
    "            # --- AUDIT: CHECK CONTENT ---\n",
    "            if file_type == \"Country\":\n",
    "                if \"Country\" in df.columns:\n",
    "                    raw_ctry_count = df[\"Country\"].astype(str).nunique()\n",
    "                    print(f\"   [AUDIT] {file_type} File: Loaded {len(df):,} rows covering {raw_ctry_count} countries.\")\n",
    "            \n",
    "            # RENAME & CAST\n",
    "            rename_map = {\n",
    "                \"Mkt-RF\": \"mktrf\", \"SMB\": \"smb\", \"HML\": \"hml\", \n",
    "                \"RMW\": \"rmw\", \"CMA\": \"cma\", \"Mom\": \"umd\", \"RF\": \"rf\", \"Rf\": \"rf\" \n",
    "            }\n",
    "            df.rename(columns=rename_map, inplace=True)\n",
    "            for col, dtype in DTYPE_DICT.items():\n",
    "                if col in df.columns: df[col] = df[col].astype(dtype)\n",
    "\n",
    "            # RETURNS & HEDGE\n",
    "            if \"Decile\" in df.columns: dcol = \"Decile\"\n",
    "            else: dcol = f\"Decile_{file_type}\" \n",
    "            \n",
    "            if \"ret\" not in df.columns and \"ret_bps\" in df.columns:\n",
    "                 df[\"ret\"] = df[\"ret_bps\"] / 10000.0\n",
    "            if \"rf\" not in df.columns: df[\"rf\"] = 0.0\n",
    "            df[\"eret\"] = df[\"ret\"] - df[\"rf\"]\n",
    "            \n",
    "            df[\"hedge\"] = np.nan\n",
    "            df.loc[df[dcol] == long_dec, \"hedge\"] = 1\n",
    "            df.loc[df[dcol] == short_dec, \"hedge\"] = 0\n",
    "            \n",
    "            # FILTER\n",
    "            valid_factors = [f for f in FACTOR_COLS if f in df.columns]\n",
    "            df = df.dropna(subset=[\"hedge\", \"eret\"] + valid_factors).copy()\n",
    "            df = df.set_index([\"ID\", \"DayDate\"]).sort_index()\n",
    "\n",
    "            # SCOPES & COUNTRY FILTER\n",
    "            iter_items = [(scope_name_func(None), df)]\n",
    "            \n",
    "            if file_type == \"Country\":\n",
    "                iter_items = []\n",
    "                dropped_list = []\n",
    "                keep_count = 0\n",
    "                unique_countries = df[\"Country\"].unique()\n",
    "                \n",
    "                for c in unique_countries:\n",
    "                    sub_c = df[df[\"Country\"]==c]\n",
    "                    # Check Index Level 0 (\"ID\") for unique counts\n",
    "                    unique_ids = sub_c.index.get_level_values(\"ID\").nunique()\n",
    "                    \n",
    "                    if unique_ids >= MIN_COUNTRY_IDS:\n",
    "                        iter_items.append((str(c), sub_c))\n",
    "                        keep_count += 1\n",
    "                    else:\n",
    "                        dropped_list.append(f\"{c}({unique_ids})\")\n",
    "                \n",
    "                print(f\"   [FILTER] Kept {keep_count} countries. Dropped {len(dropped_list)} (ID < {MIN_COUNTRY_IDS}).\")\n",
    "            \n",
    "            # REGRESSIONS\n",
    "            for country_lbl, sub_df in iter_items:\n",
    "                if sub_df.empty: continue\n",
    "                scope_key = country_lbl if file_type == \"Country\" else file_type\n",
    "                if scope_key not in return_series_dict: return_series_dict[scope_key] = {}\n",
    "\n",
    "                for w_name, w_cond in WINDOW_SPECS.items():\n",
    "                    mask = w_cond(sub_df[\"EventDate\"])\n",
    "                    sub_w = sub_df[mask]\n",
    "                    \n",
    "                    if not sub_w.empty:\n",
    "                        metrics = calc_daily_metrics(sub_w, stem)\n",
    "                        if metrics is not None:\n",
    "                            if w_name not in return_series_dict[scope_key]:\n",
    "                                return_series_dict[scope_key][w_name] = []\n",
    "                            return_series_dict[scope_key][w_name].append(metrics)\n",
    "\n",
    "                    if len(sub_w) > MIN_OBS and sub_w[\"hedge\"].nunique() > 1:\n",
    "                        avail_formula = \"eret ~ 1 + hedge * (\" + \" + \".join(valid_factors) + \")\"\n",
    "                        try:\n",
    "                            res = PanelOLS.from_formula(avail_formula, sub_w).fit(cov_type=\"clustered\", cluster_entity=True, cluster_time=True)\n",
    "                            results_rows.extend(extract_stats(res, stem, w_name, file_type, country_lbl, \"Panel\"))\n",
    "                        except: pass\n",
    "\n",
    "            del df; gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   [Error] Processing {file_type} for {stem}: {e}\")\n",
    "\n",
    "    process_one_file(\"Total\", FILE_PATTERNS[\"Total\"], lambda x: \"Total\")\n",
    "    process_one_file(\"Replication\", FILE_PATTERNS[\"Replication\"], lambda x: \"124_840\")\n",
    "    process_one_file(\"Country\", FILE_PATTERNS[\"Country\"], lambda x: \"Country\")\n",
    "\n",
    "    return {\"rows\": results_rows, \"series\": return_series_dict}\n",
    "\n",
    "# =================================================================================================\n",
    "# 4. MAIN EXECUTION\n",
    "# =================================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    total_anomalies = len(ANOMALY_CONFIG)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"STARTING SEQUENTIAL REGRESSION RUN\")\n",
    "    print(f\"Anomalies: {total_anomalies}\")\n",
    "    print(f\"Output: {OUTPUT_EXCEL}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # --- STEP 1: SEQUENTIAL LOOP ---\n",
    "    results = []\n",
    "    for i, (stem, ls) in enumerate(ANOMALY_CONFIG.items(), 1):\n",
    "        res = process_anomaly_stem(stem, ls[0], ls[1], i, total_anomalies)\n",
    "        results.append(res)\n",
    "    \n",
    "    # --- STEP 2: AGGREGATE ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AGGREGATING DATA & CALCULATING AVERAGES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_rows = []\n",
    "    avg_inputs = {}\n",
    "\n",
    "    for r in results:\n",
    "        all_rows.extend(r[\"rows\"])\n",
    "        for scope_key, window_dict in r[\"series\"].items():\n",
    "            if scope_key not in avg_inputs: avg_inputs[scope_key] = {}\n",
    "            for w_name, df_list in window_dict.items():\n",
    "                if w_name not in avg_inputs[scope_key]: avg_inputs[scope_key][w_name] = []\n",
    "                avg_inputs[scope_key][w_name].extend(df_list)\n",
    "\n",
    "    # --- STEP 3: CALCULATE \"AVERAGE ANOMALY\" ---\n",
    "    avg_rows = []\n",
    "\n",
    "    def solve_average_unified(scope_name, window_name, input_dfs, country_lbl):\n",
    "        if not input_dfs: return []\n",
    "        combined = pd.concat(input_dfs)\n",
    "        daily_avgs = combined.groupby(\"DayDate\").mean()\n",
    "        \n",
    "        if \"long_ret\" not in daily_avgs.columns or \"short_ret\" not in daily_avgs.columns: return []\n",
    "        daily_avgs = daily_avgs.dropna(subset=[\"long_ret\", \"short_ret\"])\n",
    "        if daily_avgs.empty: return []\n",
    "\n",
    "        longs = daily_avgs[[\"long_ret\"] + [c for c in FACTOR_COLS if c in daily_avgs.columns]].copy()\n",
    "        longs = longs.rename(columns={\"long_ret\": \"eret\"}); longs[\"hedge\"] = 1\n",
    "        shorts = daily_avgs[[\"short_ret\"] + [c for c in FACTOR_COLS if c in daily_avgs.columns]].copy()\n",
    "        shorts = shorts.rename(columns={\"short_ret\": \"eret\"}); shorts[\"hedge\"] = 0\n",
    "        \n",
    "        reg_df = pd.concat([longs, shorts]).sort_index()\n",
    "        \n",
    "        try:\n",
    "            avail_factors = [c for c in FACTOR_COLS if c in reg_df.columns]\n",
    "            dyn_formula = \"eret ~ 1 + hedge * (\" + \" + \".join(avail_factors) + \")\"\n",
    "            mod = smf.ols(dyn_formula, data=reg_df)\n",
    "            res = mod.fit(cov_type='HAC', cov_kwds={'maxlags': NEWEY_WEST_LAGS})\n",
    "            return extract_stats(res, \"AVERAGE\", window_name, scope_name, country_lbl, \"Statsmodels\")\n",
    "        except: return []\n",
    "\n",
    "    for scope_key, window_dict in avg_inputs.items():\n",
    "        if scope_key == \"Total\": final_scope, final_country = \"Total\", \"Total\"\n",
    "        elif scope_key == \"Replication\": final_scope, final_country = \"Replication\", \"124_840\"\n",
    "        else: final_scope, final_country = \"Country\", scope_key\n",
    "\n",
    "        for w_name, dfs in window_dict.items():\n",
    "            avg_rows.extend(solve_average_unified(final_scope, w_name, dfs, final_country))\n",
    "\n",
    "    final_df = pd.DataFrame(all_rows + avg_rows)\n",
    "\n",
    "    # --- STEP 4: EXCEL REPORTING (FORMATTING UPDATES) ---\n",
    "    print(f\"\\nWriting results to {OUTPUT_EXCEL}...\")\n",
    "    \n",
    "    if not final_df.empty:\n",
    "        found_countries = final_df[final_df[\"scope\"]==\"Country\"][\"country\"].unique()\n",
    "        print(f\"Final Report contains data for {len(found_countries)} unique countries.\")\n",
    "        \n",
    "        with pd.ExcelWriter(OUTPUT_EXCEL) as writer:\n",
    "            \n",
    "            # --- TAB 1: Average Overview (DETAILED: ALL WINDOWS) ---\n",
    "            # Now includes ALL windows (All, 20, 40...) for every country\n",
    "            avg_mask = (final_df[\"file\"] == \"AVERAGE\") & (final_df[\"window\"].isin([\"All\", \"20\", \"40\", \"80\", \"81_240\"]))\n",
    "            overview = final_df[avg_mask].copy()\n",
    "            if not overview.empty:\n",
    "                overview[\"Alpha %\"] = overview[\"annualized_alpha\"] * 100\n",
    "                overview[\"SE %\"] = overview[\"annualized_se\"] * 100\n",
    "                \n",
    "                # Sort logic to keep windows ordered\n",
    "                win_order = {\"All\": 0, \"20\": 1, \"40\": 2, \"80\": 3, \"81_240\": 4}\n",
    "                overview[\"w_rank\"] = overview[\"window\"].map(win_order)\n",
    "                overview = overview.sort_values([\"scope\", \"country\", \"w_rank\"])\n",
    "                \n",
    "                # Columns to display\n",
    "                cols = [\"scope\", \"country\", \"window\", \"Alpha %\", \"SE %\", \"tstat\", \"stars\"]\n",
    "                overview[cols].to_excel(writer, sheet_name=\"Average_Overview\", index=False)\n",
    "            \n",
    "            # --- HELPER: Interleaved Columns for Summary Tabs ---\n",
    "            def write_summary_tab(df_in, sheet_name):\n",
    "                if df_in.empty: return\n",
    "                \n",
    "                # Filter relevant windows\n",
    "                windows = [\"All\", \"20\", \"40\", \"80\", \"81_240\"]\n",
    "                df_in = df_in[df_in[\"window\"].isin(windows)].copy()\n",
    "                \n",
    "                # Create Pre-Formatted Columns\n",
    "                df_in[\"Alpha\"] = (df_in[\"annualized_alpha\"]*100).map('{:,.2f}'.format) + df_in[\"stars\"]\n",
    "                df_in[\"T-Stat\"] = df_in[\"tstat\"].map('{:,.2f}'.format)\n",
    "                \n",
    "                # Construct Interleaved Table Manually for Control\n",
    "                # Row Index = File (Anomaly), Columns = Interleaved Windows\n",
    "                unique_files = sorted(df_in[\"file\"].unique())\n",
    "                # Ensure AVERAGE is at top\n",
    "                if \"AVERAGE\" in unique_files:\n",
    "                    unique_files.remove(\"AVERAGE\")\n",
    "                    unique_files = [\"AVERAGE\"] + unique_files\n",
    "                \n",
    "                final_piv = pd.DataFrame(index=unique_files)\n",
    "                \n",
    "                for w in windows:\n",
    "                    # Filter for specific window\n",
    "                    sub = df_in[df_in[\"window\"] == w]\n",
    "                    if sub.empty: continue\n",
    "                    \n",
    "                    # Set index to file to match main DataFrame\n",
    "                    sub = sub.set_index(\"file\")\n",
    "                    \n",
    "                    # Add Columns\n",
    "                    if \"Alpha\" in sub.columns:\n",
    "                        final_piv[w] = sub[\"Alpha\"]\n",
    "                    if \"T-Stat\" in sub.columns:\n",
    "                        final_piv[f\"{w} (t-stat)\"] = sub[\"T-Stat\"]\n",
    "                \n",
    "                final_piv.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "            # --- GENERATE TABS ---\n",
    "            write_summary_tab(final_df[final_df[\"scope\"]==\"Total\"], \"Summary_Total\")\n",
    "            write_summary_tab(final_df[final_df[\"scope\"]==\"Replication\"], \"Summary_Rep\")\n",
    "            \n",
    "            countries = final_df[final_df[\"scope\"]==\"Country\"][\"country\"].unique()\n",
    "            for c in sorted(countries):\n",
    "                sheet_name = f\"Summary_C{c}\"[:31]\n",
    "                sub_c = final_df[(final_df[\"scope\"]==\"Country\") & (final_df[\"country\"]==c)]\n",
    "                write_summary_tab(sub_c, sheet_name)\n",
    "            \n",
    "            # Raw Data\n",
    "            final_df.to_excel(writer, sheet_name=\"Raw_Results\", index=False)\n",
    "            \n",
    "        print(\"Success! File saved.\")\n",
    "    else:\n",
    "        print(\"Warning: No results generated.\")\n",
    "\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"\\nCompleted in {elapsed:.1f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqzwM-GH0Aq4"
   },
   "source": [
    "# Calendar Event Time Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM0fiGlXhW6H"
   },
   "source": [
    "### Calendar Time (Not Rolling, Code Replication, Date Clustering)\n",
    "\n",
    "=> Comment: RunTime Error Warning is not a problem as it is handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "L_SWGVL3hpvm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING SCOPE: Total\n",
      "============================================================\n",
      "[1/27] Processing Anomaly: ACC ...\n",
      "[2/27] Processing Anomaly: AG ...\n",
      "[3/27] Processing Anomaly: AT ...\n",
      "[4/27] Processing Anomaly: CAT ...\n",
      "[5/27] Processing Anomaly: CPM ...\n",
      "[6/27] Processing Anomaly: EC ...\n",
      "[7/27] Processing Anomaly: ES ...\n",
      "[8/27] Processing Anomaly: GP ...\n",
      "[9/27] Processing Anomaly: IG ...\n",
      "[10/27] Processing Anomaly: INV ...\n",
      "[11/27] Processing Anomaly: LTG ...\n",
      "[12/27] Processing Anomaly: NCA ...\n",
      "[13/27] Processing Anomaly: NOA ...\n",
      "[14/27] Processing Anomaly: NWC ...\n",
      "[15/27] Processing Anomaly: OL ...\n",
      "[16/27] Processing Anomaly: OSC ...\n",
      "[17/27] Processing Anomaly: PM ...\n",
      "[18/27] Processing Anomaly: POA ...\n",
      "[19/27] Processing Anomaly: PRO ...\n",
      "[20/27] Processing Anomaly: PTA ...\n",
      "[21/27] Processing Anomaly: ROE ...\n",
      "[22/27] Processing Anomaly: RS ...\n",
      "[23/27] Processing Anomaly: SG ...\n",
      "[24/27] Processing Anomaly: SLI ...\n",
      "[25/27] Processing Anomaly: SLX ...\n",
      "[26/27] Processing Anomaly: TX ...\n",
      "[27/27] Processing Anomaly: TXF ...\n",
      "\n",
      ">>> Calculating AVERAGE ANOMALY for Total ...\n",
      "\n",
      "============================================================\n",
      "STARTING SCOPE: Replication\n",
      "============================================================\n",
      "[1/27] Processing Anomaly: ACC ...\n",
      "[2/27] Processing Anomaly: AG ...\n",
      "[3/27] Processing Anomaly: AT ...\n",
      "[4/27] Processing Anomaly: CAT ...\n",
      "[5/27] Processing Anomaly: CPM ...\n",
      "[6/27] Processing Anomaly: EC ...\n",
      "[7/27] Processing Anomaly: ES ...\n",
      "[8/27] Processing Anomaly: GP ...\n",
      "[9/27] Processing Anomaly: IG ...\n",
      "[10/27] Processing Anomaly: INV ...\n",
      "[11/27] Processing Anomaly: LTG ...\n",
      "[12/27] Processing Anomaly: NCA ...\n",
      "[13/27] Processing Anomaly: NOA ...\n",
      "[14/27] Processing Anomaly: NWC ...\n",
      "[15/27] Processing Anomaly: OL ...\n",
      "[16/27] Processing Anomaly: OSC ...\n",
      "[17/27] Processing Anomaly: PM ...\n",
      "[18/27] Processing Anomaly: POA ...\n",
      "[19/27] Processing Anomaly: PRO ...\n",
      "[20/27] Processing Anomaly: PTA ...\n",
      "[21/27] Processing Anomaly: ROE ...\n",
      "[22/27] Processing Anomaly: RS ...\n",
      "[23/27] Processing Anomaly: SG ...\n",
      "[24/27] Processing Anomaly: SLI ...\n",
      "[25/27] Processing Anomaly: SLX ...\n",
      "[26/27] Processing Anomaly: TX ...\n",
      "[27/27] Processing Anomaly: TXF ...\n",
      "\n",
      ">>> Calculating AVERAGE ANOMALY for Replication ...\n",
      "\n",
      "============================================================\n",
      "STARTING SCOPE: Country\n",
      "============================================================\n",
      "[1/27] Processing Anomaly: ACC ...\n",
      "[2/27] Processing Anomaly: AG ...\n",
      "[3/27] Processing Anomaly: AT ...\n",
      "[4/27] Processing Anomaly: CAT ...\n",
      "[5/27] Processing Anomaly: CPM ...\n",
      "[6/27] Processing Anomaly: EC ...\n",
      "[7/27] Processing Anomaly: ES ...\n",
      "[8/27] Processing Anomaly: GP ...\n",
      "[9/27] Processing Anomaly: IG ...\n",
      "[10/27] Processing Anomaly: INV ...\n",
      "[11/27] Processing Anomaly: LTG ...\n",
      "[12/27] Processing Anomaly: NCA ...\n",
      "[13/27] Processing Anomaly: NOA ...\n",
      "[14/27] Processing Anomaly: NWC ...\n",
      "[15/27] Processing Anomaly: OL ...\n",
      "[16/27] Processing Anomaly: OSC ...\n",
      "[17/27] Processing Anomaly: PM ...\n",
      "[18/27] Processing Anomaly: POA ...\n",
      "[19/27] Processing Anomaly: PRO ...\n",
      "[20/27] Processing Anomaly: PTA ...\n",
      "[21/27] Processing Anomaly: ROE ...\n",
      "[22/27] Processing Anomaly: RS ...\n",
      "[23/27] Processing Anomaly: SG ...\n",
      "[24/27] Processing Anomaly: SLI ...\n",
      "[25/27] Processing Anomaly: SLX ...\n",
      "[26/27] Processing Anomaly: TX ...\n",
      "[27/27] Processing Anomaly: TXF ...\n",
      "\n",
      ">>> Calculating AVERAGE ANOMALY for Country ...\n",
      "\n",
      "Writing results to /home/jovyan/work/hpool1/pseidel/test/Temp/TempRegressionModel/CalendarTime_Model1_Clustered.xlsx...\n",
      "\n",
      "SUCCESS. Model 1 (Clustered Errors) Complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# =================================================================================================\n",
    "# MASTER SCRIPT: CALENDAR-TIME COMPARISON (MODEL 1: CLUSTERED ERRORS - FIXED)\n",
    "# =================================================================================================\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "try:\n",
    "    DATA_FOLDER = Path(Temp_file_path_R)\n",
    "except NameError:\n",
    "    print(\"Error: 'Temp_file_path_R' variable is not defined.\")\n",
    "    sys.exit(\"Stopping execution because fallback path is disabled.\")\n",
    "\n",
    "OUTPUT_EXCEL = DATA_FOLDER / \"CalendarTime_Model1_Clustered.xlsx\"\n",
    "\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# ANOMALY LIST\n",
    "ANOMALY_CONFIG = {\n",
    "    \"acc\": (1, 10), \"ag\": (1, 10), \"at\": (10, 1), \"cat\": (10, 1),\n",
    "    \"cpm\": (10, 1), \"ec\": (10, 1), \"es\": (10, 1), \"gp\": (10, 1),\n",
    "    \"ig\": (1, 10), \"inv\": (1, 10), \"ltg\": (10, 1), \"nca\": (1, 10),\n",
    "    \"noa\": (1, 10), \"nwc\": (1, 10), \"ol\": (10, 1), \"osc\": (1, 10),\n",
    "    \"pm\": (10, 1), \"poa\": (1, 10), \"pro\": (10, 1), \"pta\": (1, 10),\n",
    "    \"roe\": (10, 1), \"rs\": (10, 1), \"sg\": (1, 10), \"sli\": (10, 1),\n",
    "    \"slx\": (10, 1), \"tx\": (10, 1), \"txf\": (1, 10),\n",
    "}\n",
    "\n",
    "# PATTERNS\n",
    "PATTERNS = {\n",
    "    \"Total\":       \"{stem}_Global_{freq}_clean.parquet\",\n",
    "    \"Replication\": \"{stem}_Replication_{freq}_clean.parquet\",\n",
    "    \"Country\":     \"{stem}_Country_{freq}_clean.parquet\"\n",
    "}\n",
    "\n",
    "# SETTINGS\n",
    "INTERNAL_FACTORS = [\"mktrf\", \"smb\", \"rmw\", \"cma\", \"umd\"] \n",
    "TRADING_DAYS = 252\n",
    "MIN_COUNTRY_IDS = 30\n",
    "\n",
    "# =================================================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# =================================================================================================\n",
    "\n",
    "def annualized_stats(params, bse, term_name):\n",
    "    \"\"\"Calculates Annualized Alpha and SE.\"\"\"\n",
    "    if term_name not in params: return np.nan, np.nan, \"\"\n",
    "    \n",
    "    coef = params[term_name]\n",
    "    se = bse[term_name]\n",
    "    \n",
    "    if abs(coef) > 0.5: return np.nan, np.nan, \" (Outlier)\"\n",
    "\n",
    "    try:\n",
    "        ann_coef = (1 + coef)**TRADING_DAYS - 1\n",
    "        derivative = TRADING_DAYS * (1 + coef)**(TRADING_DAYS - 1)\n",
    "        ann_se = se * derivative\n",
    "        \n",
    "        tstat = coef / se if se != 0 else 0\n",
    "        stars = \"\"\n",
    "        if abs(tstat) >= 2.58: stars = \"***\"\n",
    "        elif abs(tstat) >= 1.96: stars = \"**\"\n",
    "        elif abs(tstat) >= 1.65: stars = \"*\"\n",
    "        \n",
    "        return ann_coef * 100, ann_se * 100, stars\n",
    "        \n",
    "    except (OverflowError, ValueError, RuntimeWarning):\n",
    "        return np.nan, np.nan, \" (Error)\"\n",
    "\n",
    "def run_clustered_regression(df, formula, cluster_col=\"DayDate\"):\n",
    "    \"\"\"\n",
    "    Runs OLS with Clustered SE. \n",
    "    Explicitly drops NaNs to align the cluster array with the regression data.\n",
    "    \"\"\"\n",
    "    if df.empty or df['eret'].nunique() <= 1: return None\n",
    "    \n",
    "    # 1. Identify columns used in the formula (plus the cluster column)\n",
    "    # We cheat slightly and just drop NaNs from ALL factors + eret + hedge + cluster_col\n",
    "    # This prevents the 'shape mismatch' crash in statsmodels.\n",
    "    cols_to_check = ['eret', 'hedge', cluster_col] + INTERNAL_FACTORS\n",
    "    \n",
    "    # Only check columns that actually exist in the DF (e.g. 'is_daily' might be in formula)\n",
    "    cols_present = [c for c in cols_to_check if c in df.columns]\n",
    "    \n",
    "    # 2. Drop NaNs explicitly\n",
    "    df_clean = df.dropna(subset=cols_present).copy()\n",
    "    \n",
    "    if df_clean.empty: return None\n",
    "\n",
    "    try:\n",
    "        mod = smf.ols(formula, data=df_clean)\n",
    "        # 3. Pass the CLEAN dataframe column to cov_kwds\n",
    "        res = mod.fit(cov_type='cluster', cov_kwds={'groups': df_clean[cluster_col]})\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        # print(f\"Reg Error: {e}\") # Uncomment for debug\n",
    "        return None\n",
    "\n",
    "# =================================================================================================\n",
    "# 3. DATA LOADER\n",
    "# =================================================================================================\n",
    "\n",
    "def get_portfolio_data_optimized(stem, scope, freq, long_dec, short_dec):\n",
    "    \"\"\"Loads ONE file, aggregates to portfolio level.\"\"\"\n",
    "    fname = PATTERNS[scope].format(stem=stem, freq=freq)\n",
    "    fpath = DATA_FOLDER / fname\n",
    "    \n",
    "    if not fpath.exists(): \n",
    "        print(f\"      [Warning] File not found: {fname}\")\n",
    "        return None\n",
    "    \n",
    "    col_map = {\"rf\": \"RF\", \"mktrf\": \"Mkt-RF\", \"smb\": \"SMB\", \"rmw\": \"RMW\", \"cma\": \"CMA\", \"umd\": \"Mom\"}\n",
    "    rename_map = {v: k for k, v in col_map.items()}\n",
    "    file_cols_needed = [\"DayDate\", \"Decile\", \"ret\", col_map[\"rf\"]] + [col_map[f] for f in INTERNAL_FACTORS]\n",
    "    if scope == \"Country\": file_cols_needed += [\"Country\", \"ID\"]\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_parquet(fpath, columns=file_cols_needed)\n",
    "        df.rename(columns=rename_map, inplace=True)\n",
    "        \n",
    "        df[\"hedge\"] = np.nan\n",
    "        df.loc[df[\"Decile\"] == long_dec, \"hedge\"] = 1\n",
    "        df.loc[df[\"Decile\"] == short_dec, \"hedge\"] = 0\n",
    "        df.dropna(subset=[\"hedge\"], inplace=True)\n",
    "        df[\"eret\"] = df[\"ret\"] - df[\"rf\"]\n",
    "        \n",
    "        if scope == \"Country\":\n",
    "            df[\"id_count\"] = df.groupby(\"Country\")[\"ID\"].transform(\"nunique\")\n",
    "            df = df[df[\"id_count\"] >= MIN_COUNTRY_IDS].drop(columns=[\"id_count\", \"ID\"])\n",
    "            \n",
    "        grp_cols = [\"DayDate\", \"hedge\"]\n",
    "        if \"Country\" in df.columns: grp_cols.append(\"Country\")\n",
    "        \n",
    "        port_df = df.groupby(grp_cols)[[\"eret\"] + INTERNAL_FACTORS].mean().reset_index()\n",
    "        return port_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      [Error] Failed reading {fname}: {e}\")\n",
    "        return None\n",
    "\n",
    "# =================================================================================================\n",
    "# 4. MAIN LOGIC\n",
    "# =================================================================================================\n",
    "\n",
    "def analyze_scope_safe(scope_name):\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"STARTING SCOPE: {scope_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    final_results = []\n",
    "    avg_cache_daily = {}\n",
    "    avg_cache_annual = {}\n",
    "    \n",
    "    for i, (anom, (l_dec, s_dec)) in enumerate(ANOMALY_CONFIG.items(), 1):\n",
    "        print(f\"[{i}/{len(ANOMALY_CONFIG)}] Processing Anomaly: {anom.upper()} ...\")\n",
    "        \n",
    "        df_d = get_portfolio_data_optimized(anom, scope_name, \"Daily\", l_dec, s_dec)\n",
    "        df_a = get_portfolio_data_optimized(anom, scope_name, \"Annual\", l_dec, s_dec)\n",
    "        gc.collect() \n",
    "        \n",
    "        if df_d is None or df_a is None: continue\n",
    "\n",
    "        iter_items = []\n",
    "        if scope_name == \"Country\":\n",
    "            common_ctry = set(df_d[\"Country\"].unique()) & set(df_a[\"Country\"].unique())\n",
    "            for c in common_ctry:\n",
    "                iter_items.append((c, df_d[df_d[\"Country\"]==c], df_a[df_a[\"Country\"]==c]))\n",
    "        else:\n",
    "            iter_items.append((scope_name, df_d, df_a))\n",
    "            \n",
    "        # PROCESS EACH UNIT\n",
    "        for c_lbl, sub_d, sub_a in iter_items:\n",
    "            \n",
    "            \n",
    "            # 1. Identify days that have BOTH legs (Long & Short) in Daily data\n",
    "            # We group by Date and count the 'hedge' rows. Must be 2 (0.0 and 1.0).\n",
    "            counts_d = sub_d.groupby(\"DayDate\")[\"hedge\"].count()\n",
    "            valid_dates_d = counts_d[counts_d == 2].index\n",
    "            \n",
    "            # 2. Identify days that have BOTH legs in Annual data\n",
    "            counts_a = sub_a.groupby(\"DayDate\")[\"hedge\"].count()\n",
    "            valid_dates_a = counts_a[counts_a == 2].index\n",
    "            \n",
    "            # 3. Find the Intersection of these \"Complete\" days\n",
    "            common_dates = set(valid_dates_d) & set(valid_dates_a)\n",
    "            \n",
    "            # 4. Filter the dataframes\n",
    "            sub_d = sub_d[sub_d[\"DayDate\"].isin(common_dates)].copy()\n",
    "            sub_a = sub_a[sub_a[\"DayDate\"].isin(common_dates)].copy()\n",
    "            \n",
    "            if sub_d.empty or sub_a.empty: continue\n",
    "\n",
    "            # Cache for Average Anomaly calculation later\n",
    "            if c_lbl not in avg_cache_daily: \n",
    "                avg_cache_daily[c_lbl] = []\n",
    "                avg_cache_annual[c_lbl] = []\n",
    "            \n",
    "            keep = [\"DayDate\", \"hedge\", \"eret\"] + INTERNAL_FACTORS\n",
    "            avg_cache_daily[c_lbl].append(sub_d[keep].copy())\n",
    "            avg_cache_annual[c_lbl].append(sub_a[keep].copy())\n",
    "\n",
    "            res_row = {\"Anomaly\": anom, \"Scope\": scope_name, \"Country\": c_lbl}\n",
    "            \n",
    "            base_formula = \"eret ~ hedge * (\" + \" + \".join(INTERNAL_FACTORS) + \")\"\n",
    "            diff_formula = \"eret ~ hedge * is_daily * (\" + \" + \".join(INTERNAL_FACTORS) + \")\"\n",
    "            \n",
    "            # (I) INFO / DAILY\n",
    "            m = run_clustered_regression(sub_d, base_formula)\n",
    "            if m:\n",
    "                a, se, st = annualized_stats(m.params, m.bse, \"hedge\")\n",
    "                res_row.update({\"Info_Ret\": a, \"Info_SE\": se, \"Info_Star\": st})\n",
    "                \n",
    "            # (II) FF92 / ANNUAL\n",
    "            m = run_clustered_regression(sub_a, base_formula)\n",
    "            if m:\n",
    "                a, se, st = annualized_stats(m.params, m.bse, \"hedge\")\n",
    "                res_row.update({\"FF92_Ret\": a, \"FF92_SE\": se, \"FF92_Star\": st})\n",
    "                \n",
    "            # (III) DIFFERENCE (Stacked)\n",
    "            sub_d_tagged = sub_d.copy(); sub_d_tagged[\"is_daily\"] = 1\n",
    "            sub_a_tagged = sub_a.copy(); sub_a_tagged[\"is_daily\"] = 0\n",
    "            stacked = pd.concat([sub_d_tagged, sub_a_tagged], ignore_index=True)\n",
    "            \n",
    "            m = run_clustered_regression(stacked, diff_formula)\n",
    "            if m:\n",
    "                a, se, st = annualized_stats(m.params, m.bse, \"hedge:is_daily\")\n",
    "                res_row.update({\"Diff_Ret\": a, \"Diff_SE\": se, \"Diff_Star\": st})\n",
    "            \n",
    "            final_results.append(res_row)\n",
    "        \n",
    "        del df_d, df_a, iter_items\n",
    "        gc.collect()\n",
    "\n",
    "    # --- AVERAGE ANOMALY ---\n",
    "    print(f\"\\n>>> Calculating AVERAGE ANOMALY for {scope_name} ...\")\n",
    "    \n",
    "    for c_lbl in avg_cache_daily.keys():\n",
    "        list_d = avg_cache_daily[c_lbl]\n",
    "        list_a = avg_cache_annual[c_lbl]\n",
    "        \n",
    "        if not list_d: continue\n",
    "        \n",
    "        res_row = {\"Anomaly\": \"AVERAGE\", \"Scope\": scope_name, \"Country\": c_lbl}\n",
    "        \n",
    "        # Aggregate across all anomalies first\n",
    "        avg_d = pd.concat(list_d).groupby([\"DayDate\", \"hedge\"])[[\"eret\"] + INTERNAL_FACTORS].mean().reset_index()\n",
    "        avg_a = pd.concat(list_a).groupby([\"DayDate\", \"hedge\"])[[\"eret\"] + INTERNAL_FACTORS].mean().reset_index()\n",
    "        \n",
    "        \n",
    "        # 1. Ensure Average Daily has both legs\n",
    "        counts_d = avg_d.groupby(\"DayDate\")[\"hedge\"].count()\n",
    "        valid_dates_d = counts_d[counts_d == 2].index\n",
    "        \n",
    "        # 2. Ensure Average Annual has both legs\n",
    "        counts_a = avg_a.groupby(\"DayDate\")[\"hedge\"].count()\n",
    "        valid_dates_a = counts_a[counts_a == 2].index\n",
    "        \n",
    "        # 3. Intersection\n",
    "        common_avg = set(valid_dates_d) & set(valid_dates_a)\n",
    "        \n",
    "        # 4. Filter\n",
    "        avg_d = avg_d[avg_d[\"DayDate\"].isin(common_avg)].copy()\n",
    "        avg_a = avg_a[avg_a[\"DayDate\"].isin(common_avg)].copy()\n",
    "        \n",
    "        if avg_d.empty or avg_a.empty: continue\n",
    "\n",
    "        # ... (rest of the average logic: formulas, regressions, etc.)\n",
    "\n",
    "        base_formula = \"eret ~ hedge * (\" + \" + \".join(INTERNAL_FACTORS) + \")\"\n",
    "        \n",
    "        # (I) INFO\n",
    "        m = run_clustered_regression(avg_d, base_formula)\n",
    "        if m:\n",
    "            a, se, st = annualized_stats(m.params, m.bse, \"hedge\")\n",
    "            res_row.update({\"Info_Ret\": a, \"Info_SE\": se, \"Info_Star\": st})\n",
    "            \n",
    "        # (II) FF92\n",
    "        m = run_clustered_regression(avg_a, base_formula)\n",
    "        if m:\n",
    "            a, se, st = annualized_stats(m.params, m.bse, \"hedge\")\n",
    "            res_row.update({\"FF92_Ret\": a, \"FF92_SE\": se, \"FF92_Star\": st})\n",
    "            \n",
    "        # (III) DIFF\n",
    "        avg_d[\"is_daily\"] = 1\n",
    "        avg_a[\"is_daily\"] = 0\n",
    "        stacked = pd.concat([avg_d, avg_a], ignore_index=True)\n",
    "        \n",
    "        diff_formula = \"eret ~ hedge * is_daily * (\" + \" + \".join(INTERNAL_FACTORS) + \")\"\n",
    "        \n",
    "        m = run_clustered_regression(stacked, diff_formula)\n",
    "        if m:\n",
    "            a, se, st = annualized_stats(m.params, m.bse, \"hedge:is_daily\")\n",
    "            res_row.update({\"Diff_Ret\": a, \"Diff_SE\": se, \"Diff_Star\": st})\n",
    "        \n",
    "        final_results.append(res_row)\n",
    "\n",
    "    return pd.DataFrame(final_results)\n",
    "\n",
    "# =================================================================================================\n",
    "# 5. EXECUTION BLOCK\n",
    "# =================================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    all_res = []\n",
    "    \n",
    "    for sc in [\"Total\", \"Replication\", \"Country\"]:\n",
    "        try:\n",
    "            df_res = analyze_scope_safe(sc)\n",
    "            if not df_res.empty:\n",
    "                all_res.append(df_res)\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR in scope {sc}: {e}\")\n",
    "\n",
    "    if not all_res:\n",
    "        print(\"No results generated.\")\n",
    "    else:\n",
    "        master_df = pd.concat(all_res, ignore_index=True)\n",
    "        \n",
    "        print(f\"\\nWriting results to {OUTPUT_EXCEL}...\")\n",
    "        with pd.ExcelWriter(OUTPUT_EXCEL) as writer:\n",
    "            \n",
    "            def write_sheet(df_sub, sheet_name):\n",
    "                if df_sub.empty: return\n",
    "                out = df_sub.copy()\n",
    "                \n",
    "                for prefix in [\"FF92\", \"Info\", \"Diff\"]:\n",
    "                    if f\"{prefix}_Ret\" in out.columns:\n",
    "                        out[f\"{prefix}_Ret\"] = out[f\"{prefix}_Ret\"].fillna(0)\n",
    "                        out[f\"{prefix}_SE\"] = out[f\"{prefix}_SE\"].fillna(0)\n",
    "                        out[f\"{prefix} Return\"] = out[f\"{prefix}_Ret\"].map('{:.2f}'.format) + out[f\"{prefix}_Star\"].fillna(\"\")\n",
    "                        out[f\"{prefix} StdErr\"] = \"(\" + out[f\"{prefix}_SE\"].map('{:.2f}'.format) + \")\"\n",
    "                \n",
    "                cols = [\"Anomaly\"]\n",
    "                for prefix in [\"FF92\", \"Info\", \"Diff\"]:\n",
    "                    cols.extend([f\"{prefix} Return\", f\"{prefix} StdErr\"])\n",
    "                \n",
    "                avg_row = out[out[\"Anomaly\"] == \"AVERAGE\"]\n",
    "                others = out[out[\"Anomaly\"] != \"AVERAGE\"].sort_values(\"Anomaly\")\n",
    "                final_view = pd.concat([avg_row, others])\n",
    "                final_cols = [c for c in cols if c in final_view.columns]\n",
    "                final_view[final_cols].to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "            avg_view = master_df[master_df[\"Anomaly\"] == \"AVERAGE\"].copy()\n",
    "            if not avg_view.empty:\n",
    "                avg_view.to_excel(writer, sheet_name=\"Average_Overview\", index=False)\n",
    "            \n",
    "            write_sheet(master_df[master_df[\"Scope\"] == \"Total\"], \"Summary_Total\")\n",
    "            write_sheet(master_df[master_df[\"Scope\"] == \"Replication\"], \"Summary_Rep\")\n",
    "            \n",
    "            country_data = master_df[master_df[\"Scope\"] == \"Country\"]\n",
    "            if not country_data.empty:\n",
    "                for c in sorted(country_data[\"Country\"].unique()):\n",
    "                    sub = country_data[country_data[\"Country\"] == c]\n",
    "                    write_sheet(sub, f\"Summary_C{c}\"[:31])\n",
    "        \n",
    "        print(\"\\nSUCCESS. Model 1 (Clustered Errors) Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spread Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING SCOPE: Total\n",
      "============================================================\n",
      "[1/27] Processing Anomaly: ACC ...\n",
      "[2/27] Processing Anomaly: AG ...\n",
      "[3/27] Processing Anomaly: AT ...\n",
      "[4/27] Processing Anomaly: CAT ...\n",
      "[5/27] Processing Anomaly: CPM ...\n",
      "[6/27] Processing Anomaly: EC ...\n",
      "[7/27] Processing Anomaly: ES ...\n",
      "[8/27] Processing Anomaly: GP ...\n",
      "[9/27] Processing Anomaly: IG ...\n",
      "[10/27] Processing Anomaly: INV ...\n",
      "[11/27] Processing Anomaly: LTG ...\n",
      "[12/27] Processing Anomaly: NCA ...\n",
      "[13/27] Processing Anomaly: NOA ...\n",
      "[14/27] Processing Anomaly: NWC ...\n",
      "[15/27] Processing Anomaly: OL ...\n",
      "[16/27] Processing Anomaly: OSC ...\n",
      "[17/27] Processing Anomaly: PM ...\n",
      "[18/27] Processing Anomaly: POA ...\n",
      "[19/27] Processing Anomaly: PRO ...\n",
      "[20/27] Processing Anomaly: PTA ...\n",
      "[21/27] Processing Anomaly: ROE ...\n",
      "[22/27] Processing Anomaly: RS ...\n",
      "[23/27] Processing Anomaly: SG ...\n",
      "[24/27] Processing Anomaly: SLI ...\n",
      "[25/27] Processing Anomaly: SLX ...\n",
      "[26/27] Processing Anomaly: TX ...\n",
      "[27/27] Processing Anomaly: TXF ...\n",
      "\n",
      ">>> Calculating AVERAGE ANOMALY for Total ...\n",
      "\n",
      "============================================================\n",
      "STARTING SCOPE: Replication\n",
      "============================================================\n",
      "[1/27] Processing Anomaly: ACC ...\n",
      "[2/27] Processing Anomaly: AG ...\n",
      "[3/27] Processing Anomaly: AT ...\n",
      "[4/27] Processing Anomaly: CAT ...\n",
      "[5/27] Processing Anomaly: CPM ...\n",
      "[6/27] Processing Anomaly: EC ...\n",
      "[7/27] Processing Anomaly: ES ...\n",
      "[8/27] Processing Anomaly: GP ...\n",
      "[9/27] Processing Anomaly: IG ...\n",
      "[10/27] Processing Anomaly: INV ...\n",
      "[11/27] Processing Anomaly: LTG ...\n",
      "[12/27] Processing Anomaly: NCA ...\n",
      "[13/27] Processing Anomaly: NOA ...\n",
      "[14/27] Processing Anomaly: NWC ...\n",
      "[15/27] Processing Anomaly: OL ...\n",
      "[16/27] Processing Anomaly: OSC ...\n",
      "[17/27] Processing Anomaly: PM ...\n",
      "[18/27] Processing Anomaly: POA ...\n",
      "[19/27] Processing Anomaly: PRO ...\n",
      "[20/27] Processing Anomaly: PTA ...\n",
      "[21/27] Processing Anomaly: ROE ...\n",
      "[22/27] Processing Anomaly: RS ...\n",
      "[23/27] Processing Anomaly: SG ...\n",
      "[24/27] Processing Anomaly: SLI ...\n",
      "[25/27] Processing Anomaly: SLX ...\n",
      "[26/27] Processing Anomaly: TX ...\n",
      "[27/27] Processing Anomaly: TXF ...\n",
      "\n",
      ">>> Calculating AVERAGE ANOMALY for Replication ...\n",
      "\n",
      "============================================================\n",
      "STARTING SCOPE: Country\n",
      "============================================================\n",
      "[1/27] Processing Anomaly: ACC ...\n",
      "[2/27] Processing Anomaly: AG ...\n",
      "[3/27] Processing Anomaly: AT ...\n",
      "[4/27] Processing Anomaly: CAT ...\n",
      "[5/27] Processing Anomaly: CPM ...\n",
      "[6/27] Processing Anomaly: EC ...\n",
      "[7/27] Processing Anomaly: ES ...\n",
      "[8/27] Processing Anomaly: GP ...\n",
      "[9/27] Processing Anomaly: IG ...\n",
      "[10/27] Processing Anomaly: INV ...\n",
      "[11/27] Processing Anomaly: LTG ...\n",
      "[12/27] Processing Anomaly: NCA ...\n",
      "[13/27] Processing Anomaly: NOA ...\n",
      "[14/27] Processing Anomaly: NWC ...\n",
      "[15/27] Processing Anomaly: OL ...\n",
      "[16/27] Processing Anomaly: OSC ...\n",
      "[17/27] Processing Anomaly: PM ...\n",
      "[18/27] Processing Anomaly: POA ...\n",
      "[19/27] Processing Anomaly: PRO ...\n",
      "[20/27] Processing Anomaly: PTA ...\n",
      "[21/27] Processing Anomaly: ROE ...\n",
      "[22/27] Processing Anomaly: RS ...\n",
      "[23/27] Processing Anomaly: SG ...\n",
      "[24/27] Processing Anomaly: SLI ...\n",
      "[25/27] Processing Anomaly: SLX ...\n",
      "[26/27] Processing Anomaly: TX ...\n",
      "[27/27] Processing Anomaly: TXF ...\n",
      "\n",
      ">>> Calculating AVERAGE ANOMALY for Country ...\n",
      "\n",
      "Writing results to /home/jovyan/work/hpool1/pseidel/test/Temp/TempRegressionModel/CalendarTime_Model2_Spread_Consistent.xlsx...\n",
      "\n",
      "SUCCESS. Model 2 (Spread + Consistent Sample) Complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# =================================================================================================\n",
    "# MASTER SCRIPT: CALENDAR-TIME COMPARISON (MODEL 2: SPREAD APPROACH + CONSISTENT SAMPLE)\n",
    "# =================================================================================================\n",
    "#\n",
    "# 1. EXECUTIVE SUMMARY\n",
    "# --------------------\n",
    "# This script performs the \"Calendar-Time Portfolio\" regression analysis using the \"Spread\" methodology.\n",
    "# Unlike Model 1 (which stacks Long/Short legs), this model calculates the performance of the \n",
    "# Long-Short strategy *before* the regression.\n",
    "#\n",
    "#    a) \"Info\" (Daily Rebalancing): Returns of (Daily Long - Daily Short).\n",
    "#    b) \"FF92\" (Annual Rebalancing): Returns of (Annual Long - Annual Short).\n",
    "#    c) \"Difference\": The time-series difference of (Daily Spread - Annual Spread).\n",
    "#\n",
    "# 2. ECONOMETRIC SPECIFICATION (MODEL 2)\n",
    "# --------------------------------------\n",
    "#    * DATA STRUCTURE: Single Time Series.\n",
    "#      - The data is collapsed to one observation per day (the Strategy Spread).\n",
    "#      - This isolates the volatility of the \"Difference\" itself, often providing more precise\n",
    "#        inference for the gap between strategies than the stacked interaction model.\n",
    "#\n",
    "#    * CONSISTENT SAMPLE FIX (CRITICAL):\n",
    "#      - The script enforces an INNER JOIN between the Daily and Annual datasets.\n",
    "#      - Regressions for Info, FF92, and Difference are run on the EXACT SAME days.\n",
    "#      - This ensures logical consistency: Alpha(Info) - Alpha(FF92) ≈ Alpha(Diff).\n",
    "#      - Without this, the \"Info\" column might include days where \"FF92\" is missing, skewing the comparison.\n",
    "#\n",
    "#    * REGRESSION FORMULA:\n",
    "#      Spread ~ 1 + Factors\n",
    "#      - The Intercept (Alpha) captures the abnormal return of the strategy (or the difference).\n",
    "#\n",
    "#    * INFERENCE (NEWEY-WEST HAC):\n",
    "#      - Standard Errors are estimated using Newey-West (HAC) with 6 Lags.\n",
    "#      - RATIONALE: Since the data is collapsed to a single time series (one row per date),\n",
    "#        \"Clustering by Date\" is mathematically redundant (only 1 item per cluster).\n",
    "#        The primary risk to inference is Autocorrelation (correlation across time).\n",
    "#        Newey-West explicitly corrects for this serial correlation.\n",
    "#\n",
    "# 3. ANALYSIS SCOPE\n",
    "# -----------------\n",
    "#    * Universes: Total (Global), Replication (USA/Can), and Country-Specific.\n",
    "#    * Factors: Fama-French 5-Factor + Momentum (HML excluded per user config).\n",
    "#\n",
    "# 4. SYSTEM ARCHITECTURE\n",
    "# ----------------------\n",
    "#    * Memory-safe \"Load -> Aggregate -> Discard\" cycle.\n",
    "#    * Strict pathing using 'Temp_file_path_R'.\n",
    "#    * Single-threaded execution for server stability.\n",
    "#\n",
    "# 5. OUTPUT\n",
    "# ---------\n",
    "#    * File: CalendarTime_Model2_Spread_Consistent.xlsx\n",
    "#    * Metrics: Annualized Alpha, Newey-West SE, T-Stats for Info, FF92, and Difference.\n",
    "# =================================================================================================\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "try:\n",
    "    DATA_FOLDER = Path(Temp_file_path_R)\n",
    "except NameError:\n",
    "    print(\"Error: 'Temp_file_path_R' variable is not defined.\")\n",
    "    sys.exit(\"Stopping execution because fallback path is disabled.\")\n",
    "\n",
    "OUTPUT_EXCEL = DATA_FOLDER / \"CalendarTime_Model2_Spread_Consistent.xlsx\"\n",
    "\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# ANOMALY LIST\n",
    "ANOMALY_CONFIG = {\n",
    "    \"acc\": (1, 10), \"ag\": (1, 10), \"at\": (10, 1), \"cat\": (10, 1),\n",
    "    \"cpm\": (10, 1), \"ec\": (10, 1), \"es\": (10, 1), \"gp\": (10, 1),\n",
    "    \"ig\": (1, 10), \"inv\": (1, 10), \"ltg\": (10, 1), \"nca\": (1, 10),\n",
    "    \"noa\": (1, 10), \"nwc\": (1, 10), \"ol\": (10, 1), \"osc\": (1, 10),\n",
    "    \"pm\": (10, 1), \"poa\": (1, 10), \"pro\": (10, 1), \"pta\": (1, 10),\n",
    "    \"roe\": (10, 1), \"rs\": (10, 1), \"sg\": (1, 10), \"sli\": (10, 1),\n",
    "    \"slx\": (10, 1), \"tx\": (10, 1), \"txf\": (1, 10),\n",
    "}\n",
    "\n",
    "# PATTERNS\n",
    "PATTERNS = {\n",
    "    \"Total\":       \"{stem}_Global_{freq}_clean.parquet\",\n",
    "    \"Replication\": \"{stem}_Replication_{freq}_clean.parquet\",\n",
    "    \"Country\":     \"{stem}_Country_{freq}_clean.parquet\"\n",
    "}\n",
    "\n",
    "# SETTINGS\n",
    "INTERNAL_FACTORS = [\"mktrf\", \"smb\", \"rmw\", \"cma\", \"umd\"] \n",
    "TRADING_DAYS = 252\n",
    "MIN_COUNTRY_IDS = 30\n",
    "NW_LAGS = 6\n",
    "\n",
    "# =================================================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# =================================================================================================\n",
    "\n",
    "def annualized_stats(params, bse, term_name=\"Intercept\"):\n",
    "    \"\"\"Calculates Annualized Alpha and SE.\"\"\"\n",
    "    if term_name not in params: return np.nan, np.nan, \"\"\n",
    "    \n",
    "    coef = params[term_name]\n",
    "    se = bse[term_name]\n",
    "    \n",
    "    if abs(coef) > 0.5: return np.nan, np.nan, \" (Outlier)\"\n",
    "\n",
    "    try:\n",
    "        ann_coef = (1 + coef)**TRADING_DAYS - 1\n",
    "        derivative = TRADING_DAYS * (1 + coef)**(TRADING_DAYS - 1)\n",
    "        ann_se = se * derivative\n",
    "        \n",
    "        tstat = coef / se if se != 0 else 0\n",
    "        stars = \"\"\n",
    "        if abs(tstat) >= 2.58: stars = \"***\"\n",
    "        elif abs(tstat) >= 1.96: stars = \"**\"\n",
    "        elif abs(tstat) >= 1.65: stars = \"*\"\n",
    "        \n",
    "        return ann_coef * 100, ann_se * 100, stars\n",
    "        \n",
    "    except (OverflowError, ValueError, RuntimeWarning):\n",
    "        return np.nan, np.nan, \" (Error)\"\n",
    "\n",
    "def make_spread_portfolio(df):\n",
    "    \"\"\"\n",
    "    Converts Stacked DF (Long/Short rows) into Spread DF (Long - Short).\n",
    "    Returns a DF with ['spread'] and factor columns.\n",
    "    \"\"\"\n",
    "    if df.empty: return None\n",
    "    # Pivot to separate Long (1.0) and Short (0.0)\n",
    "    pivoted = df.pivot_table(index=\"DayDate\", columns=\"hedge\", values=\"eret\", aggfunc=\"mean\")\n",
    "    \n",
    "    if 1.0 not in pivoted.columns or 0.0 not in pivoted.columns:\n",
    "        return None\n",
    "        \n",
    "    spread_series = pivoted[1.0] - pivoted[0.0]\n",
    "    \n",
    "    # Get factors (constant per date)\n",
    "    factors = df.groupby(\"DayDate\")[INTERNAL_FACTORS].first()\n",
    "    \n",
    "    res = pd.DataFrame(spread_series, columns=[\"spread\"])\n",
    "    res = res.join(factors, how=\"inner\")\n",
    "    return res\n",
    "\n",
    "def run_spread_regression(df):\n",
    "    \"\"\"\n",
    "    Regress Spread ~ 1 + Factors.\n",
    "    Uses Newey-West HAC (Lag 6) because data is a single time series.\n",
    "    \"\"\"\n",
    "    if df.empty or len(df) < 20: return None\n",
    "    formula = \"spread ~ \" + \" + \".join(INTERNAL_FACTORS)\n",
    "    try:\n",
    "        mod = smf.ols(formula, data=df)\n",
    "        res = mod.fit(cov_type='HAC', cov_kwds={'maxlags': NW_LAGS})\n",
    "        return res\n",
    "    except: return None\n",
    "\n",
    "# =================================================================================================\n",
    "# 3. DATA LOADER\n",
    "# =================================================================================================\n",
    "\n",
    "def get_portfolio_data_optimized(stem, scope, freq, long_dec, short_dec):\n",
    "    \"\"\"Loads ONE file, aggregates to portfolio level.\"\"\"\n",
    "    fname = PATTERNS[scope].format(stem=stem, freq=freq)\n",
    "    fpath = DATA_FOLDER / fname\n",
    "    \n",
    "    if not fpath.exists(): \n",
    "        print(f\"      [Warning] File not found: {fname}\")\n",
    "        return None\n",
    "    \n",
    "    col_map = {\"rf\": \"RF\", \"mktrf\": \"Mkt-RF\", \"smb\": \"SMB\", \"rmw\": \"RMW\", \"cma\": \"CMA\", \"umd\": \"Mom\"}\n",
    "    rename_map = {v: k for k, v in col_map.items()}\n",
    "    file_cols_needed = [\"DayDate\", \"Decile\", \"ret\", col_map[\"rf\"]] + [col_map[f] for f in INTERNAL_FACTORS]\n",
    "    if scope == \"Country\": file_cols_needed += [\"Country\", \"ID\"]\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_parquet(fpath, columns=file_cols_needed)\n",
    "        df.rename(columns=rename_map, inplace=True)\n",
    "        \n",
    "        df[\"hedge\"] = np.nan\n",
    "        df.loc[df[\"Decile\"] == long_dec, \"hedge\"] = 1\n",
    "        df.loc[df[\"Decile\"] == short_dec, \"hedge\"] = 0\n",
    "        df.dropna(subset=[\"hedge\"], inplace=True)\n",
    "        df[\"eret\"] = df[\"ret\"] - df[\"rf\"]\n",
    "        \n",
    "        if scope == \"Country\":\n",
    "            df[\"id_count\"] = df.groupby(\"Country\")[\"ID\"].transform(\"nunique\")\n",
    "            df = df[df[\"id_count\"] >= MIN_COUNTRY_IDS].drop(columns=[\"id_count\", \"ID\"])\n",
    "            \n",
    "        grp_cols = [\"DayDate\", \"hedge\"]\n",
    "        if \"Country\" in df.columns: grp_cols.append(\"Country\")\n",
    "        \n",
    "        port_df = df.groupby(grp_cols)[[\"eret\"] + INTERNAL_FACTORS].mean().reset_index()\n",
    "        return port_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      [Error] Failed reading {fname}: {e}\")\n",
    "        return None\n",
    "\n",
    "# =================================================================================================\n",
    "# 4. MAIN LOGIC (CONSISTENT SAMPLE + SPREAD + NEWEY WEST)\n",
    "# =================================================================================================\n",
    "\n",
    "def analyze_scope_safe(scope_name):\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"STARTING SCOPE: {scope_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    final_results = []\n",
    "    \n",
    "    # Storage for \"Average Anomaly\" calculation\n",
    "    # We store dictionaries containing the 3 aligned DataFrames: {info, ff92, diff}\n",
    "    avg_cache_diff = [] \n",
    "    \n",
    "    for i, (anom, (l_dec, s_dec)) in enumerate(ANOMALY_CONFIG.items(), 1):\n",
    "        print(f\"[{i}/{len(ANOMALY_CONFIG)}] Processing Anomaly: {anom.upper()} ...\")\n",
    "        \n",
    "        # A. LOAD\n",
    "        df_d = get_portfolio_data_optimized(anom, scope_name, \"Daily\", l_dec, s_dec)\n",
    "        df_a = get_portfolio_data_optimized(anom, scope_name, \"Annual\", l_dec, s_dec)\n",
    "        gc.collect() \n",
    "        \n",
    "        if df_d is None or df_a is None: continue\n",
    "\n",
    "        # B. DEFINE ITERATION UNITS\n",
    "        iter_items = []\n",
    "        if scope_name == \"Country\":\n",
    "            common_ctry = set(df_d[\"Country\"].unique()) & set(df_a[\"Country\"].unique())\n",
    "            for c in common_ctry:\n",
    "                iter_items.append((c, df_d[df_d[\"Country\"]==c], df_a[df_a[\"Country\"]==c]))\n",
    "        else:\n",
    "            iter_items.append((scope_name, df_d, df_a))\n",
    "            \n",
    "        # C. PROCESS EACH UNIT\n",
    "        for c_lbl, sub_d, sub_a in iter_items:\n",
    "            \n",
    "            # 1. MAKE SPREADS (Long - Short)\n",
    "            spr_d = make_spread_portfolio(sub_d)\n",
    "            spr_a = make_spread_portfolio(sub_a)\n",
    "            if spr_d is None or spr_a is None: continue\n",
    "\n",
    "            # --- START CHANGE: KEEP BOTH FACTOR SETS ---\n",
    "\n",
    "            # 2. CONSISTENT SAMPLE FIX (INNER JOIN)\n",
    "            # Use suffixes to keep BOTH Daily (_d) and Annual (_a) factors\n",
    "            combined = spr_d.join(spr_a, lsuffix=\"_d\", rsuffix=\"_a\", how=\"inner\")\n",
    "            \n",
    "            if combined.empty: continue\n",
    "            \n",
    "            # Define dictionaries to rename columns back to standard names (mktrf, smb, etc.)\n",
    "            rename_daily  = {f\"{f}_d\": f for f in INTERNAL_FACTORS}\n",
    "            rename_annual = {f\"{f}_a\": f for f in INTERNAL_FACTORS}\n",
    "            \n",
    "            # 3. CONSTRUCT THE 3 REGRESSION DATASETS\n",
    "            \n",
    "            # (I) INFO: Daily Spread ~ Daily Factors\n",
    "            df_info = combined.copy()\n",
    "            df_info.rename(columns=rename_daily, inplace=True)  # Activate Daily Factors\n",
    "            df_info[\"spread\"] = df_info[\"spread_d\"]\n",
    "            \n",
    "            # (II) FF92: Annual Spread ~ ANNUAL Factors\n",
    "            df_ff92 = combined.copy()\n",
    "            df_ff92.rename(columns=rename_annual, inplace=True) # Activate Annual Factors\n",
    "            df_ff92[\"spread\"] = df_ff92[\"spread_a\"]\n",
    "            \n",
    "            # (III) DIFF: (Daily - Annual) ~ DAILY Factors\n",
    "            # We use Daily Factors as the benchmark for the difference\n",
    "            df_diff = combined.copy()\n",
    "            df_diff.rename(columns=rename_daily, inplace=True)  # Activate Daily Factors\n",
    "            df_diff[\"spread\"] = df_diff[\"spread_d\"] - df_diff[\"spread_a\"]\n",
    "            \n",
    "            # 4. CACHE FOR AVERAGE (Store these aligned datasets)\n",
    "            avg_cache_diff.append({\n",
    "                \"country\": c_lbl, \n",
    "                \"info\": df_info[[\"spread\"] + INTERNAL_FACTORS],\n",
    "                \"ff92\": df_ff92[[\"spread\"] + INTERNAL_FACTORS],\n",
    "                \"diff\": df_diff[[\"spread\"] + INTERNAL_FACTORS]\n",
    "            })\n",
    "\n",
    "            # 5. RUN REGRESSIONS (NEWEY WEST)\n",
    "            res_row = {\"Anomaly\": anom, \"Scope\": scope_name, \"Country\": c_lbl}\n",
    "            \n",
    "            # Info Regression\n",
    "            m = run_spread_regression(df_info)\n",
    "            if m:\n",
    "                a, se, st = annualized_stats(m.params, m.bse)\n",
    "                res_row.update({\"Info_Ret\": a, \"Info_SE\": se, \"Info_Star\": st})\n",
    "                \n",
    "            # FF92 Regression\n",
    "            m = run_spread_regression(df_ff92)\n",
    "            if m:\n",
    "                a, se, st = annualized_stats(m.params, m.bse)\n",
    "                res_row.update({\"FF92_Ret\": a, \"FF92_SE\": se, \"FF92_Star\": st})\n",
    "                \n",
    "            # Diff Regression\n",
    "            m = run_spread_regression(df_diff)\n",
    "            if m:\n",
    "                a, se, st = annualized_stats(m.params, m.bse)\n",
    "                res_row.update({\"Diff_Ret\": a, \"Diff_SE\": se, \"Diff_Star\": st})\n",
    "            \n",
    "            final_results.append(res_row)\n",
    "        \n",
    "        del df_d, df_a, iter_items\n",
    "        gc.collect()\n",
    "\n",
    "    # --- AVERAGE ANOMALY (CONSISTENT) ---\n",
    "    print(f\"\\n>>> Calculating AVERAGE ANOMALY for {scope_name} ...\")\n",
    "    \n",
    "    # Re-organize cache by Country\n",
    "    country_caches = {}\n",
    "    for item in avg_cache_diff:\n",
    "        c = item[\"country\"]\n",
    "        if c not in country_caches: country_caches[c] = {\"info\": [], \"ff92\": [], \"diff\": []}\n",
    "        country_caches[c][\"info\"].append(item[\"info\"])\n",
    "        country_caches[c][\"ff92\"].append(item[\"ff92\"])\n",
    "        country_caches[c][\"diff\"].append(item[\"diff\"])\n",
    "        \n",
    "    for c_lbl, dfs in country_caches.items():\n",
    "        if not dfs[\"info\"]: continue\n",
    "        \n",
    "        # Calculate Average Spread across anomalies (on aligned dates)\n",
    "        # We concat and groupby index (Date) to get the mean\n",
    "        avg_info = pd.concat(dfs[\"info\"]).groupby(level=0).mean()\n",
    "        avg_ff92 = pd.concat(dfs[\"ff92\"]).groupby(level=0).mean()\n",
    "        avg_diff = pd.concat(dfs[\"diff\"]).groupby(level=0).mean()\n",
    "        \n",
    "        res_row = {\"Anomaly\": \"AVERAGE\", \"Scope\": scope_name, \"Country\": c_lbl}\n",
    "        \n",
    "        # (I) INFO\n",
    "        m = run_spread_regression(avg_info)\n",
    "        if m:\n",
    "            a, se, st = annualized_stats(m.params, m.bse)\n",
    "            res_row.update({\"Info_Ret\": a, \"Info_SE\": se, \"Info_Star\": st})\n",
    "            \n",
    "        # (II) FF92\n",
    "        m = run_spread_regression(avg_ff92)\n",
    "        if m:\n",
    "            a, se, st = annualized_stats(m.params, m.bse)\n",
    "            res_row.update({\"FF92_Ret\": a, \"FF92_SE\": se, \"FF92_Star\": st})\n",
    "            \n",
    "        # (III) DIFF\n",
    "        m = run_spread_regression(avg_diff)\n",
    "        if m:\n",
    "            a, se, st = annualized_stats(m.params, m.bse)\n",
    "            res_row.update({\"Diff_Ret\": a, \"Diff_SE\": se, \"Diff_Star\": st})\n",
    "            \n",
    "        final_results.append(res_row)\n",
    "\n",
    "    return pd.DataFrame(final_results)\n",
    "\n",
    "# =================================================================================================\n",
    "# 5. EXECUTION BLOCK\n",
    "# =================================================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    all_res = []\n",
    "    \n",
    "    for sc in [\"Total\", \"Replication\", \"Country\"]:\n",
    "        try:\n",
    "            df_res = analyze_scope_safe(sc)\n",
    "            if not df_res.empty:\n",
    "                all_res.append(df_res)\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR in scope {sc}: {e}\")\n",
    "\n",
    "    if not all_res:\n",
    "        print(\"No results generated.\")\n",
    "    else:\n",
    "        master_df = pd.concat(all_res, ignore_index=True)\n",
    "        \n",
    "        print(f\"\\nWriting results to {OUTPUT_EXCEL}...\")\n",
    "        with pd.ExcelWriter(OUTPUT_EXCEL) as writer:\n",
    "            \n",
    "            def write_sheet(df_sub, sheet_name):\n",
    "                if df_sub.empty: return\n",
    "                out = df_sub.copy()\n",
    "                \n",
    "                for prefix in [\"FF92\", \"Info\", \"Diff\"]:\n",
    "                    if f\"{prefix}_Ret\" in out.columns:\n",
    "                        out[f\"{prefix}_Ret\"] = out[f\"{prefix}_Ret\"].fillna(0)\n",
    "                        out[f\"{prefix}_SE\"] = out[f\"{prefix}_SE\"].fillna(0)\n",
    "                        out[f\"{prefix} Return\"] = out[f\"{prefix}_Ret\"].map('{:.2f}'.format) + out[f\"{prefix}_Star\"].fillna(\"\")\n",
    "                        out[f\"{prefix} StdErr\"] = \"(\" + out[f\"{prefix}_SE\"].map('{:.2f}'.format) + \")\"\n",
    "                \n",
    "                cols = [\"Anomaly\"]\n",
    "                for prefix in [\"FF92\", \"Info\", \"Diff\"]:\n",
    "                    cols.extend([f\"{prefix} Return\", f\"{prefix} StdErr\"])\n",
    "                \n",
    "                avg_row = out[out[\"Anomaly\"] == \"AVERAGE\"]\n",
    "                others = out[out[\"Anomaly\"] != \"AVERAGE\"].sort_values(\"Anomaly\")\n",
    "                final_view = pd.concat([avg_row, others])\n",
    "                final_cols = [c for c in cols if c in final_view.columns]\n",
    "                final_view[final_cols].to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "            avg_view = master_df[master_df[\"Anomaly\"] == \"AVERAGE\"].copy()\n",
    "            if not avg_view.empty:\n",
    "                avg_view.to_excel(writer, sheet_name=\"Average_Overview\", index=False)\n",
    "            \n",
    "            write_sheet(master_df[master_df[\"Scope\"] == \"Total\"], \"Summary_Total\")\n",
    "            write_sheet(master_df[master_df[\"Scope\"] == \"Replication\"], \"Summary_Rep\")\n",
    "            \n",
    "            country_data = master_df[master_df[\"Scope\"] == \"Country\"]\n",
    "            if not country_data.empty:\n",
    "                for c in sorted(country_data[\"Country\"].unique()):\n",
    "                    sub = country_data[country_data[\"Country\"] == c]\n",
    "                    write_sheet(sub, f\"Summary_C{c}\"[:31])\n",
    "        \n",
    "        print(\"\\nSUCCESS. Model 2 (Spread + Consistent Sample) Complete.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM1cZ22dNqnYLv/sKWJ1tSV",
   "collapsed_sections": [
    "ctXZHVs-zurM",
    "E9SDg4A4CoTL",
    "r28A6_YrGkuA",
    "6KX3-EZwBPtc",
    "H-C5yNobzr-_",
    "rqzwM-GH0Aq4",
    "CtB80vvq0Hpb",
    "kM0fiGlXhW6H",
    "1lpqlf0Phe-R",
    "s5ru2rBliEgL"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
