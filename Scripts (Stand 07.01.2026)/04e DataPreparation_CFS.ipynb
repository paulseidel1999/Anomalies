{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b06f6df4"
   },
   "source": [
    "### Mount Google Drive, Import Libraries and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18791,
     "status": "ok",
     "timestamp": 1764881554836,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "DVbbtM4ztajO",
    "outputId": "fa90dfe7-e702-4d88-a7dc-c37f9d540bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: False\n",
      "BASE_PATH: /home/jovyan/work/hpool1/pseidel/test\n",
      "Importing numpy ...\n",
      "numpy OK\n",
      "Importing scipy ...\n",
      "scipy OK\n",
      "Importing pandas ...\n",
      "pandas OK\n",
      "Importing linearmodels ...\n",
      "linearmodels OK\n",
      "Importing xlsxwriter ...\n",
      "xlsxwriter OK\n",
      "Paths configured. Temp outputs -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview\n",
      "Example input path -> /home/jovyan/work/hpool1/pseidel/test/Input/WSFV_f_20250131.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT SETUP + PATH CONFIGURATION (SERVER / COLAB COMPATIBLE)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) HARD SAFETY: cap native thread usage (prevents pthread_create EAGAIN)\n",
    "#    MUST be set before importing numpy / scipy / pandas\n",
    "# -----------------------------------------------------------------------------\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"BLIS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Detect environment\n",
    "# -----------------------------------------------------------------------------\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) (Colab only) Mount Google Drive\n",
    "# -----------------------------------------------------------------------------\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "else:\n",
    "    # Server base path (your target)\n",
    "    BASE_PATH = \"/home/jovyan/work/hpool1/pseidel/test\"\n",
    "\n",
    "print(\"IN_COLAB:\", IN_COLAB)\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Sanity checks: path exists + write permission\n",
    "# -----------------------------------------------------------------------------\n",
    "BASE = Path(BASE_PATH)\n",
    "if not BASE.exists():\n",
    "    raise FileNotFoundError(f\"BASE_PATH does not exist: {BASE}\")\n",
    "\n",
    "# quick write test (fails fast if you don't have permissions)\n",
    "test_file = BASE / \".write_test_tmp\"\n",
    "try:\n",
    "    test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "    test_file.unlink()\n",
    "except Exception as e:\n",
    "    raise PermissionError(f\"No write permission in {BASE}. Error: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Environment check: ensure required packages import cleanly\n",
    "# -----------------------------------------------------------------------------\n",
    "required_packages = [\"numpy\", \"scipy\", \"pandas\", \"linearmodels\", \"xlsxwriter\"]\n",
    "\n",
    "for pkg in required_packages:\n",
    "    print(f\"Importing {pkg} ...\")\n",
    "    importlib.import_module(pkg)\n",
    "    print(f\"{pkg} OK\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Base paths and input/output locations\n",
    "# -----------------------------------------------------------------------------\n",
    "Input_file_path   = str(BASE / \"Input\")\n",
    "Temp_file_path    = str(BASE / \"Temp\")\n",
    "Output_file_path  = str(BASE / \"Output\")\n",
    "\n",
    "Fundamentals_file_path = f\"{Input_file_path}/WSFV_f_20250131.txt\"\n",
    "Current_file_path      = f\"{Input_file_path}/WSCurrent_f_20250131.txt\"\n",
    "Calendar_file_path     = f\"{Input_file_path}/WSCalendarPrd_f_20250131.txt\"\n",
    "Meta_file_path         = f\"{Input_file_path}/WSMetaData_f_20250131.txt\"\n",
    "Excel_file_path        = f\"{Input_file_path}/WS PIT Table Definitions V5 with start dates.xls\"\n",
    "\n",
    "MarketValues_file_path          = f\"{Input_file_path}/Daily MV USD\"\n",
    "MarketValues_file_path_LC       = f\"{Input_file_path}/Daily MV LC\"\n",
    "DailyTotalReturns_file_path     = f\"{Input_file_path}/Daily Returns USD\"\n",
    "DailyIndexReturns_file_path     = f\"{Input_file_path}/Daily Index Returns USD\"\n",
    "Constituents_file_path          = f\"{Input_file_path}/Constituents.01.csv\"\n",
    "UniversalMatching_file_path     = f\"{Input_file_path}/Universal Matching File\"\n",
    "\n",
    "Temp_file_path_GO  = f\"{Temp_file_path}/TempGeneralOverview\"\n",
    "Temp_file_path_EoC = f\"{Temp_file_path}/TempExtractionofCharacteristics\"\n",
    "Temp_file_path_DP  = f\"{Temp_file_path}/TempDataPreparation\"\n",
    "Temp_file_path_A   = f\"{Temp_file_path}/TempAnomalies\"\n",
    "Temp_file_path_R   = f\"{Temp_file_path}/TempRegressionModel\"\n",
    "\n",
    "Relevant_items_path   = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_A = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_B = f\"{Input_file_path}/RelevantItemsB.txt\"\n",
    "Relevant_items_path_C = f\"{Input_file_path}/RelevantItemsC.txt\"\n",
    "Relevant_items_path_D = f\"{Input_file_path}/RelevantItemsD.txt\"\n",
    "\n",
    "Subset_file_path = f\"{Temp_file_path_GO}/Subsets\"\n",
    "Fundamentals_clean_file_path = f\"{Temp_file_path_GO}/Fundamentals_clean.txt\"\n",
    "Current_clean_file_path      = f\"{Temp_file_path_GO}/Current_clean.txt\"\n",
    "Calendar_clean_file_path     = f\"{Temp_file_path_GO}/Input/Calendar_clean.txt\"\n",
    "Meta_clean_file_path         = f\"{Temp_file_path_GO}/Input/Meta_clean.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Ensure required directories exist\n",
    "# -----------------------------------------------------------------------------\n",
    "Path(Output_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_GO).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_EoC).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_A).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(Subset_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Path(Calendar_clean_file_path).parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Streaming / deduplication settings\n",
    "# -----------------------------------------------------------------------------\n",
    "CHUNK_SIZE = 2_000_000\n",
    "DATE_COL = \"PIT Date\"\n",
    "DEDUP_KEYS = [\"ID\", \"ItemCode\", DATE_COL]\n",
    "\n",
    "print(\"Paths configured. Temp outputs ->\", Temp_file_path_GO)\n",
    "print(\"Example input path ->\", Fundamentals_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87ba0a55"
   },
   "source": [
    "### Import Data Files to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5437,
     "status": "ok",
     "timestamp": 1764881570926,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "94c3b5e3",
    "outputId": "7f8678e1-05ea-4104-918f-ff54d1e639be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imported RelevantItems.txt as DataFrame 'RelevantItems'\n",
      "Preview of 'RelevantItems':\n",
      "  ItemCode\n",
      "0    01001\n",
      "1    01051\n",
      "2    01075\n",
      "3    01101\n",
      "4    01151 \n",
      "\n",
      "\n",
      "Imported CountryCodes.txt as DataFrame 'CountryCodes'\n",
      "Preview of 'CountryCodes':\n",
      "  NatCo ImplCountry\n",
      "0   012     Algeria\n",
      "1   440   Lithuania\n",
      "2   025   Argentina\n",
      "3   442  Luxembourg\n",
      "4   036   Australia \n",
      "\n",
      "\n",
      "Imported ADR_clean.txt as DataFrame 'ADR_clean'\n",
      "Preview of 'ADR_clean':\n",
      "          ID ADRIndicator\n",
      "0  C036F63D0            N\n",
      "1  C056879S0            X\n",
      "2  C2461T100            N\n",
      "3  C2504O500            N\n",
      "4  C250C9180            N \n",
      "\n",
      "\n",
      "Imported CompanyName_clean.txt as DataFrame 'CompanyName_clean'\n",
      "Preview of 'CompanyName_clean':\n",
      "          ID                               CompanyName\n",
      "0  C00948205             AGRIFORCE GROWING SYSTEMS LTD\n",
      "1  C02500770            PEUGEOT CITROEN ARGENTINA S.A.\n",
      "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA\n",
      "3  C02520220                       ALPARGATAS S.A.I.C.\n",
      "4  C02520230               ALUAR ALUMINIO ARGENTINO SA \n",
      "\n",
      "\n",
      "Imported CurrencyCodes_clean.txt as DataFrame 'CurrencyCodes_clean'\n",
      "Preview of 'CurrencyCodes_clean':\n",
      "          ID CurrencyCode1 SwitchDate1 CurrencyCode2 SwitchDate2  \\\n",
      "0  C00948205           Usd  2021-07-09           NaN         NaN   \n",
      "1  C02500770           Ars  1995-12-29           NaN         NaN   \n",
      "2  C0250077A           Ars  1999-10-01           NaN         NaN   \n",
      "3  C0250077B           Ars  1999-10-01           NaN         NaN   \n",
      "4  C0250077C           Ars  1999-10-01           NaN         NaN   \n",
      "\n",
      "  CurrencyCode3 SwitchDate3 CurrentCurrency  \n",
      "0           NaN         NaN             Usd  \n",
      "1           NaN         NaN             Ars  \n",
      "2           NaN         NaN             Ars  \n",
      "3           NaN         NaN             Ars  \n",
      "4           NaN         NaN             Ars   \n",
      "\n",
      "\n",
      "Imported FYE_clean.txt as DataFrame 'FYE_clean'\n",
      "Preview of 'FYE_clean':\n",
      "          ID    FY FYE Month\n",
      "0  C00948205  2018  December\n",
      "1  C00948205  2019  December\n",
      "2  C00948205  2020     March\n",
      "3  C00948205  2021  December\n",
      "4  C00948205  2022  December \n",
      "\n",
      "\n",
      "Imported ID_clean.txt as DataFrame 'ID_clean'\n",
      "Preview of 'ID_clean':\n",
      "          ID\n",
      "0  C02500770\n",
      "1  C02520200\n",
      "2  C02520220\n",
      "3  C02520230\n",
      "4  C02520240 \n",
      "\n",
      "\n",
      "Imported UpdateCodes_clean.txt as DataFrame 'UpdateCodes_clean'\n",
      "Preview of 'UpdateCodes_clean':\n",
      "          ID    PIT Date Frequency FiscalPeriod UpdateCode\n",
      "0  C02500770  1995-12-29         A         1985          3\n",
      "1  C02500770  1995-12-29         A         1986          3\n",
      "2  C02500770  1995-12-29         A         1987          3\n",
      "3  C02500770  1995-12-29         A         1988          3\n",
      "4  C02500770  1995-12-29         A         1989          3 \n",
      "\n",
      "\n",
      "Imported ValueCoding.txt as DataFrame 'ValueCoding'\n",
      "Preview of 'ValueCoding':\n",
      "  ItemCode                           ItemName  Source\n",
      "0    05006               Market Price Current  Market\n",
      "1    05007      Market Price YTD High Current  Market\n",
      "2    05008       Market Price YTD Low Current  Market\n",
      "3    05009              Date of Current Price  Market\n",
      "4    05091  Market Price 52 Week High Current  Market \n",
      "\n",
      "\n",
      "Identifying subset files to process...\n",
      "  Found subset_01001.txt\n",
      "  Found subset_01051.txt\n",
      "  Found subset_01075.txt\n",
      "  Found subset_01101.txt\n",
      "  Found subset_01151.txt\n",
      "  Found subset_01250.txt\n",
      "  Found subset_01451.txt\n",
      "  Found subset_01551.txt\n",
      "  Found subset_01706.txt\n",
      "  Found subset_02001.txt\n",
      "  Found subset_02051.txt\n",
      "  Found subset_02101.txt\n",
      "  Found subset_02149.txt\n",
      "  Found subset_02201.txt\n",
      "  Found subset_02250.txt\n",
      "  Found subset_02256.txt\n",
      "  Found subset_02257.txt\n",
      "  Found subset_02258.txt\n",
      "  Found subset_02263.txt\n",
      "  Found subset_02501.txt\n",
      "  Found subset_02652.txt\n",
      "  Found subset_02999.txt\n",
      "  Found subset_03040.txt\n",
      "  Found subset_03051.txt\n",
      "  Found subset_03063.txt\n",
      "  Found subset_03066.txt\n",
      "  Found subset_03101.txt\n",
      "  Found subset_03251.txt\n",
      "  Found subset_03263.txt\n",
      "  Found subset_03273.txt\n",
      "  Found subset_03351.txt\n",
      "  Found subset_03426.txt\n",
      "  Found subset_03451.txt\n",
      "  Found subset_03501.txt\n",
      "  Found subset_04251.txt\n",
      "  Found subset_04401.txt\n",
      "  Found subset_04551.txt\n",
      "  Found subset_04601.txt\n",
      "  Found subset_04701.txt\n",
      "  Found subset_04751.txt\n",
      "  Found subset_04860.txt\n",
      "  Found subset_04870.txt\n",
      "  Found subset_04890.txt\n",
      "  Found subset_05202.txt\n",
      "  Found subset_04201.txt\n",
      "  Found subset_04225.txt\n",
      "  Found subset_04831.txt\n",
      "  Found subset_04351.txt\n",
      "  Found subset_05508.txt\n",
      "\n",
      "Identified 49 subset files for processing.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell:\n",
    "#\n",
    "#   1. Defines a helper function `import_file_to_dataframe` that reads a pipe-delimited\n",
    "#      text file into a pandas DataFrame (all columns as string; returns None on error).\n",
    "#   2. Imports a list of \"input\" files from Input_file_path into DataFrames\n",
    "#      (RelevantItems, CountryCodes, ...), storing them in globals() by filename.\n",
    "#   3. Imports a list of \"temp\" files from Temp_file_path_EoC into DataFrames\n",
    "#      (ADR_clean, CompanyName_clean, CurrencyCodes_clean, FYE_clean, ID_clean,\n",
    "#       UpdateCodes_clean, ValueCoding), also stored in globals().\n",
    "#   4. Identifies which subset_*.txt files exist in Subset_file_path based on the IDs\n",
    "#      listed in RelevantItems.txt, and records their names (without .txt) in\n",
    "#      `successful_subset_names`.\n",
    "#\n",
    "# No actual subset data is loaded here; that is deferred to later steps to keep\n",
    "# memory usage under control.\n",
    "\n",
    "\n",
    "# Function to import a file and return a pandas DataFrame\n",
    "def import_file_to_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Import a pipe-separated text file as a pandas DataFrame.\n",
    "\n",
    "    - Uses sep='|' to read pipe-delimited files.\n",
    "    - Reads all columns as strings (dtype=str), which helps preserve things like\n",
    "      leading zeros in codes (e.g., NatCo, ItemCode).\n",
    "    - Returns None on failure and prints a brief error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='|', dtype=str)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error importing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Import files from Input directory\n",
    "# -------------------------------------------------------------------------\n",
    "input_files_to_import = [\"RelevantItems.txt\", \"CountryCodes.txt\"]\n",
    "\n",
    "for file_name in input_files_to_import:\n",
    "    file_path = os.path.join(Input_file_path, file_name)\n",
    "    var_name = file_name.replace(\".txt\", \"\")  # e.g. \"RelevantItems\"\n",
    "    globals()[var_name] = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if globals()[var_name] is not None:\n",
    "        print(f\"\\nImported {file_name} as DataFrame '{var_name}'\")\n",
    "        print(f\"Preview of '{var_name}':\")\n",
    "        print(globals()[var_name].head(), \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Import files from Temp directory (end-of-cleaning stage)\n",
    "# -------------------------------------------------------------------------\n",
    "temp_files_to_import = [\n",
    "    \"ADR_clean.txt\",\n",
    "    \"CompanyName_clean.txt\",\n",
    "    \"CurrencyCodes_clean.txt\",\n",
    "    \"FYE_clean.txt\",\n",
    "    \"ID_clean.txt\",\n",
    "    \"UpdateCodes_clean.txt\",\n",
    "    \"ValueCoding.txt\"\n",
    "]\n",
    "\n",
    "for file_name in temp_files_to_import:\n",
    "    file_path = os.path.join(Temp_file_path_EoC, file_name)\n",
    "    var_name = file_name.replace(\".txt\", \"\")  # e.g. \"ADR_clean\"\n",
    "    globals()[var_name] = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if globals()[var_name] is not None:\n",
    "        print(f\"\\nImported {file_name} as DataFrame '{var_name}'\")\n",
    "        print(f\"Preview of '{var_name}':\")\n",
    "        print(globals()[var_name].head(), \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Identify subset files that exist for the relevant items\n",
    "# -------------------------------------------------------------------------\n",
    "successful_subset_names = []\n",
    "\n",
    "if 'RelevantItems' in globals() and RelevantItems is not None:\n",
    "    # Assume first column of RelevantItems holds the item IDs used in subset filenames\n",
    "    relevant_ids = RelevantItems.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "    print(\"\\nIdentifying subset files to process...\")\n",
    "    for item_id in relevant_ids:\n",
    "        file_name = f\"subset_{item_id}.txt\"\n",
    "        file_path = os.path.join(Subset_file_path, file_name)\n",
    "\n",
    "        # Check the existence of each candidate subset file\n",
    "        if os.path.exists(file_path):\n",
    "            successful_subset_names.append(f\"subset_{item_id}\")\n",
    "            print(f\"  Found {file_name}\")\n",
    "        else:\n",
    "            print(f\"  File not found: {file_name}. Skipping.\")\n",
    "\n",
    "    print(f\"\\nIdentified {len(successful_subset_names)} subset files for processing.\")\n",
    "else:\n",
    "    print(\"RelevantItems DataFrame not found or is empty. Cannot identify subset files.\")\n",
    "\n",
    "# Note: actual loading and processing of subset files happens later, in\n",
    "# batch-based steps, to manage memory usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmcsWZB0oDMl"
   },
   "source": [
    "# 4.0. Extracting the most recent, annualized values per PIT Date (incl. Plausibility checks for the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WkpqfMTNN5n"
   },
   "source": [
    "## 4.1. Split according to source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1764881571481,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "moZxdHacNpxi",
    "outputId": "db1a196d-4499-4555-9803-744c38de1ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ValueCoding DataFrame...\n",
      "\n",
      "Processed ValueCoding DataFrame (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>ItemName</th>\n",
       "      <th>Source</th>\n",
       "      <th>ItemName_Sanitized</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05006</td>\n",
       "      <td>Market Price Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05007</td>\n",
       "      <td>Market Price YTD High Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_YTD_High_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05008</td>\n",
       "      <td>Market Price YTD Low Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_YTD_Low_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05009</td>\n",
       "      <td>Date of Current Price</td>\n",
       "      <td>Market</td>\n",
       "      <td>Date_of_Current_Price</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05091</td>\n",
       "      <td>Market Price 52 Week High Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_52_Week_High_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ItemCode                           ItemName  Source  \\\n",
       "0    05006               Market Price Current  Market   \n",
       "1    05007      Market Price YTD High Current  Market   \n",
       "2    05008       Market Price YTD Low Current  Market   \n",
       "3    05009              Date of Current Price  Market   \n",
       "4    05091  Market Price 52 Week High Current  Market   \n",
       "\n",
       "                  ItemName_Sanitized Category  \n",
       "0               Market_Price_Current    Mixed  \n",
       "1      Market_Price_YTD_High_Current    Mixed  \n",
       "2       Market_Price_YTD_Low_Current    Mixed  \n",
       "3              Date_of_Current_Price    Mixed  \n",
       "4  Market_Price_52_Week_High_Current    Mixed  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Annualized items: 148\n",
      "Number of Mixed items: 141\n",
      "Number of Special items: 62\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell processes a ValueCoding DataFrame and assigns a Category to each item\n",
    "# (per sanitized item name), based on its data sources:\n",
    "#\n",
    "#   1. Validates that `ValueCoding` exists and is non-empty.\n",
    "#   2. Sanitizes `ItemName` to a filesystem-safe `ItemName_Sanitized` (same rules as\n",
    "#      used for filenames).\n",
    "#   3. Normalizes the `Source` column (string type, trimmed).\n",
    "#   4. Groups all distinct sources per `ItemName_Sanitized`.\n",
    "#   5. Uses `decide_category` to map each sanitized name to a Category:\n",
    "#        - Hardcoded overrides for certain items.\n",
    "#        - Generic rules:\n",
    "#             * presence of IS / Other  -> \"Mixed\"\n",
    "#             * presence of Market / BS -> \"Annualized\"\n",
    "#             * presence of CFS         -> \"Special\"\n",
    "#        - otherwise                    -> None\n",
    "#   6. Attaches the Category back to each row based on `ItemName_Sanitized`.\n",
    "#   7. Creates three unique-item DataFrames:\n",
    "#        - `annualized_items`\n",
    "#        - `mixed_items`\n",
    "#        - `special_items`\n",
    "#   8. Exposes the processed objects in `globals()` for use in later cells.\n",
    "#   9. Shows a sample and prints counts of each category.\n",
    "#\n",
    "# If `ValueCoding` is not present or is empty, processing is skipped.\n",
    "\n",
    "# CELL 1 — Process ValueCoding and assign Category per ItemName_Sanitized\n",
    "\n",
    "if 'ValueCoding' in globals() and ValueCoding is not None and not ValueCoding.empty:\n",
    "    # Inform that processing of ValueCoding is starting\n",
    "    print(\"Processing ValueCoding DataFrame...\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original ValueCoding\n",
    "    value_coding_processed = ValueCoding.copy()\n",
    "\n",
    "    # --- Sanitize ItemName ---\n",
    "    # Ensure ItemName is treated as string (avoid issues with numbers / NaNs)\n",
    "    value_coding_processed['ItemName'] = value_coding_processed['ItemName'].astype(str)\n",
    "\n",
    "    # First pass: replace spaces and certain filesystem-unsafe characters with underscores\n",
    "    # Same rule set as used for building filenames elsewhere\n",
    "    value_coding_processed['ItemName_Sanitized'] = value_coding_processed['ItemName'].str.replace(\n",
    "        r'[ \\-/\\:\\\\*\\?\"<>|]', '_', regex=True\n",
    "    )\n",
    "    # Second pass: strip any remaining characters not in [word chars, dot, hyphen]\n",
    "    value_coding_processed['ItemName_Sanitized'] = value_coding_processed['ItemName_Sanitized'].str.replace(\n",
    "        r'[^\\w.-]', '', regex=True\n",
    "    )\n",
    "\n",
    "    # --- Normalize Source ---\n",
    "    # Convert Source to string and strip leading/trailing whitespace\n",
    "    value_coding_processed['Source'] = (\n",
    "        value_coding_processed['Source']\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Group all sources per sanitized name\n",
    "    # ------------------------------------------------------------------\n",
    "    # For each ItemName_Sanitized, collect the set of all non-null sources\n",
    "    sources_per_name = (\n",
    "        value_coding_processed\n",
    "        .groupby('ItemName_Sanitized')['Source']\n",
    "        .apply(lambda s: set(s.dropna()))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Helper to decide category per sanitized name\n",
    "    # ------------------------------------------------------------------\n",
    "    def decide_category(name, sources: set):\n",
    "        \"\"\"\n",
    "        Decide a category string (\"Mixed\", \"Annualized\", \"Special\", or None)\n",
    "        for a given sanitized item name based on its set of sources.\n",
    "        \"\"\"\n",
    "        # Item-specific overrides (these take precedence over generic rules)\n",
    "        if name == 'Depreciation_Depletion__Amortization':\n",
    "            # Prefer 'IS' interpretation -> treat as Mixed\n",
    "            return 'Mixed'\n",
    "        if name == 'Minority_Interest':\n",
    "            # Prefer 'BS' interpretation -> treat as Annualized\n",
    "            return 'Annualized'\n",
    "\n",
    "        # Generic rules:\n",
    "\n",
    "        # If any of the sources is Income Statement or \"Other\", classify as Mixed\n",
    "        if any(src in ['IS', 'Market'] for src in sources):\n",
    "            return 'Mixed'\n",
    "\n",
    "        # If any of the sources is Market or Balance Sheet, classify as Annualized\n",
    "        if any(src in ['BS'] for src in sources):\n",
    "            return 'Annualized'\n",
    "\n",
    "        # If any of the sources is Cash Flow Statement, classify as Special\n",
    "        if any(src in ['CFS'] for src in sources):\n",
    "            return 'Special'\n",
    "\n",
    "        # If none of the rules matched, leave as None (no clear mapping)\n",
    "        return None\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Build category_map for all sanitized names\n",
    "    # ------------------------------------------------------------------\n",
    "    # Map each sanitized item name to a Category by applying decide_category\n",
    "    category_map = {\n",
    "        name: decide_category(name, srcs)\n",
    "        for name, srcs in sources_per_name.items()\n",
    "    }\n",
    "\n",
    "    # Attach final Category back to each row, via ItemName_Sanitized\n",
    "    value_coding_processed['Category'] = (\n",
    "        value_coding_processed['ItemName_Sanitized'].map(category_map)\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Derive annualized_items / mixed_items / special_items\n",
    "    # as unique per sanitized name\n",
    "    # ------------------------------------------------------------------\n",
    "    annualized_items = (\n",
    "        value_coding_processed[value_coding_processed['Category'] == 'Annualized']\n",
    "        .drop_duplicates(subset=['ItemName_Sanitized'])\n",
    "        .copy()\n",
    "    )\n",
    "    mixed_items = (\n",
    "        value_coding_processed[value_coding_processed['Category'] == 'Mixed']\n",
    "        .drop_duplicates(subset=['ItemName_Sanitized'])\n",
    "        .copy()\n",
    "    )\n",
    "    special_items = (\n",
    "        value_coding_processed[value_coding_processed['Category'] == 'Special']\n",
    "        .drop_duplicates(subset=['ItemName_Sanitized'])\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Export to globals for use in later cells\n",
    "    # ------------------------------------------------------------------\n",
    "    globals()['value_coding_processed'] = value_coding_processed\n",
    "    globals()['annualized_items'] = annualized_items\n",
    "    globals()['mixed_items'] = mixed_items\n",
    "    globals()['special_items'] = special_items\n",
    "    globals()['category_map'] = category_map\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Display sample and counts\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\nProcessed ValueCoding DataFrame (sample):\")\n",
    "    display(value_coding_processed.head())\n",
    "\n",
    "    print(f\"\\nNumber of Annualized items: {len(annualized_items)}\")\n",
    "    print(f\"Number of Mixed items: {len(mixed_items)}\")\n",
    "    print(f\"Number of Special items: {len(special_items)}\")\n",
    "\n",
    "else:\n",
    "    # If ValueCoding is not available or has no rows, skip processing\n",
    "    print(\"ValueCoding DataFrame not found or is empty. Skipping processing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Nb7Pqn0eFOw"
   },
   "source": [
    "### Sort into correct bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1764881571546,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "735bc0be",
    "outputId": "cdff64b4-677f-4798-a056-566a3e5cc05d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying work_subset files and creating variables based on categories...\n",
      "\n",
      "Found 49 work_subset files in Temp directory.\n",
      "  'work_subset_Accounts_Payable.txt' -> Annualized (variable 'Annualized_1').\n",
      "  'work_subset_Capital_Expenditures_Addtns_to_Fixed_Assets.txt' -> Special (variable 'Special_1').\n",
      "  'work_subset_Cash_Dividends_Paid___Total.txt' -> Special (variable 'Special_2').\n",
      "  'work_subset_Cash__Short_Term_Investments.txt' -> Annualized (variable 'Annualized_2').\n",
      "  'work_subset_Com_Pfd_Redeemed_Retired_Converted_Etc..txt' -> Special (variable 'Special_3').\n",
      "  'work_subset_Common_Equity.txt' -> Annualized (variable 'Annualized_3').\n",
      "  'work_subset_Cost_of_Goods_Sold_Excl_Depreciation.txt' -> Mixed (variable 'Mixed_1').\n",
      "  'work_subset_Current_Assets___Total.txt' -> Annualized (variable 'Annualized_4').\n",
      "  'work_subset_Current_Liabilities___Total.txt' -> Annualized (variable 'Annualized_5').\n",
      "  'work_subset_Deferred_Taxes.txt' -> Annualized (variable 'Annualized_6').\n",
      "  'work_subset_Depreciation_Depletion__Amortization.txt' -> Mixed (variable 'Mixed_2').\n",
      "  'work_subset_Disposal_of_Fixed_Assets.txt' -> Special (variable 'Special_4').\n",
      "  'work_subset_Earnings_Per_Share_Fiscal_Year_End.txt' -> Mixed (variable 'Mixed_3').\n",
      "  'work_subset_Extraordinary_Items.txt' -> Special (variable 'Special_5').\n",
      "  'work_subset_Funds_From_For_Other_Operating_Activities.txt' -> Special (variable 'Special_6').\n",
      "  'work_subset_Funds_From_Operations.txt' -> Special (variable 'Special_7').\n",
      "  'work_subset_Income_Taxes.txt' -> Mixed (variable 'Mixed_4').\n",
      "  'work_subset_Income_Taxes_Payable.txt' -> Annualized (variable 'Annualized_7').\n",
      "  'work_subset_Interest_Expense___Total.txt' -> Mixed (variable 'Mixed_5').\n",
      "  'work_subset_Inventories___Total.txt' -> Annualized (variable 'Annualized_8').\n",
      "  'work_subset_Investments_in_Associated_Companies.txt' -> Annualized (variable 'Annualized_9').\n",
      "  'work_subset_Investments_in_Sales__Direct_Financing_Leases.txt' -> Annualized (variable 'Annualized_10').\n",
      "  'work_subset_Long_Term_Borrowings.txt' -> Special (variable 'Special_8').\n",
      "  'work_subset_Long_Term_Debt.txt' -> Annualized (variable 'Annualized_11').\n",
      "  'work_subset_Long_Term_Receivables.txt' -> Annualized (variable 'Annualized_12').\n",
      "  'work_subset_Minority_Interest.txt' -> Annualized (variable 'Annualized_13').\n",
      "  'work_subset_Net_Cash_Flow___Financing.txt' -> Special (variable 'Special_9').\n",
      "  'work_subset_Net_Cash_Flow___Investing.txt' -> Special (variable 'Special_10').\n",
      "  'work_subset_Net_Cash_Flow___Operating_Activities.txt' -> Special (variable 'Special_11').\n",
      "  'work_subset_Net_Income_Before_Extra_Items_Preferred_Divs.txt' -> Mixed (variable 'Mixed_6').\n",
      "  'work_subset_Net_Income_Used_to_Calculate_Basic_EPS.txt' -> Mixed (variable 'Mixed_7').\n",
      "  'work_subset_Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt' -> Special (variable 'Special_12').\n",
      "  'work_subset_Net_Sales_or_Revenues.txt' -> Mixed (variable 'Mixed_8').\n",
      "  'work_subset_Operating_Income.txt' -> Mixed (variable 'Mixed_9').\n",
      "  'work_subset_Other_Assets___Total.txt' -> Annualized (variable 'Annualized_14').\n",
      "  'work_subset_Other_Current_Assets.txt' -> Annualized (variable 'Annualized_15').\n",
      "  'work_subset_Other_Current_Liabilities.txt' -> Annualized (variable 'Annualized_16').\n",
      "  'work_subset_Other_Investments.txt' -> Annualized (variable 'Annualized_17').\n",
      "  'work_subset_Other_Liabilities.txt' -> Annualized (variable 'Annualized_18').\n",
      "  'work_subset_Preferred_Stock.txt' -> Annualized (variable 'Annualized_19').\n",
      "  'work_subset_Property_Plant__Equipment___Net.txt' -> Annualized (variable 'Annualized_20').\n",
      "  'work_subset_ReceivablesNet.txt' -> Annualized (variable 'Annualized_21').\n",
      "  'work_subset_Reduction_in_Long_Term_Debt.txt' -> Special (variable 'Special_13').\n",
      "  'work_subset_Sales_Per_Share.txt' -> Mixed (variable 'Mixed_10').\n",
      "  'work_subset_Selling_General__Administrative_Expenses.txt' -> Mixed (variable 'Mixed_11').\n",
      "  'work_subset_Short_Term_Debt__Current_Portion_of_LT_Debt.txt' -> Annualized (variable 'Annualized_22').\n",
      "  'work_subset_Total_Assets.txt' -> Annualized (variable 'Annualized_23').\n",
      "  'work_subset_Total_Liabilities.txt' -> Annualized (variable 'Annualized_24').\n",
      "  'work_subset_Unspecified_Other_Loans.txt' -> Annualized (variable 'Annualized_25').\n",
      "\n",
      "Variable creation complete.\n",
      "Created 25 Annualized variables.\n",
      "Created 11 Mixed variables.\n",
      "Created 13 Special variables.\n",
      "\n",
      "Annualized Variables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Annualized_1': 'Accounts_Payable',\n",
       " 'Annualized_2': 'Cash__Short_Term_Investments',\n",
       " 'Annualized_3': 'Common_Equity',\n",
       " 'Annualized_4': 'Current_Assets___Total',\n",
       " 'Annualized_5': 'Current_Liabilities___Total',\n",
       " 'Annualized_6': 'Deferred_Taxes',\n",
       " 'Annualized_7': 'Income_Taxes_Payable',\n",
       " 'Annualized_8': 'Inventories___Total',\n",
       " 'Annualized_9': 'Investments_in_Associated_Companies',\n",
       " 'Annualized_10': 'Investments_in_Sales__Direct_Financing_Leases',\n",
       " 'Annualized_11': 'Long_Term_Debt',\n",
       " 'Annualized_12': 'Long_Term_Receivables',\n",
       " 'Annualized_13': 'Minority_Interest',\n",
       " 'Annualized_14': 'Other_Assets___Total',\n",
       " 'Annualized_15': 'Other_Current_Assets',\n",
       " 'Annualized_16': 'Other_Current_Liabilities',\n",
       " 'Annualized_17': 'Other_Investments',\n",
       " 'Annualized_18': 'Other_Liabilities',\n",
       " 'Annualized_19': 'Preferred_Stock',\n",
       " 'Annualized_20': 'Property_Plant__Equipment___Net',\n",
       " 'Annualized_21': 'ReceivablesNet',\n",
       " 'Annualized_22': 'Short_Term_Debt__Current_Portion_of_LT_Debt',\n",
       " 'Annualized_23': 'Total_Assets',\n",
       " 'Annualized_24': 'Total_Liabilities',\n",
       " 'Annualized_25': 'Unspecified_Other_Loans'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixed Variables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Mixed_1': 'Cost_of_Goods_Sold_Excl_Depreciation',\n",
       " 'Mixed_2': 'Depreciation_Depletion__Amortization',\n",
       " 'Mixed_3': 'Earnings_Per_Share_Fiscal_Year_End',\n",
       " 'Mixed_4': 'Income_Taxes',\n",
       " 'Mixed_5': 'Interest_Expense___Total',\n",
       " 'Mixed_6': 'Net_Income_Before_Extra_Items_Preferred_Divs',\n",
       " 'Mixed_7': 'Net_Income_Used_to_Calculate_Basic_EPS',\n",
       " 'Mixed_8': 'Net_Sales_or_Revenues',\n",
       " 'Mixed_9': 'Operating_Income',\n",
       " 'Mixed_10': 'Sales_Per_Share',\n",
       " 'Mixed_11': 'Selling_General__Administrative_Expenses'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Special Variables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Special_1': 'Capital_Expenditures_Addtns_to_Fixed_Assets',\n",
       " 'Special_2': 'Cash_Dividends_Paid___Total',\n",
       " 'Special_3': 'Com_Pfd_Redeemed_Retired_Converted_Etc.',\n",
       " 'Special_4': 'Disposal_of_Fixed_Assets',\n",
       " 'Special_5': 'Extraordinary_Items',\n",
       " 'Special_6': 'Funds_From_For_Other_Operating_Activities',\n",
       " 'Special_7': 'Funds_From_Operations',\n",
       " 'Special_8': 'Long_Term_Borrowings',\n",
       " 'Special_9': 'Net_Cash_Flow___Financing',\n",
       " 'Special_10': 'Net_Cash_Flow___Investing',\n",
       " 'Special_11': 'Net_Cash_Flow___Operating_Activities',\n",
       " 'Special_12': 'Net_Proceeds_From_Sale_Issue_of_Com__Pfd',\n",
       " 'Special_13': 'Reduction_in_Long_Term_Debt'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# CELL 2 maps work_subset_*.txt files to categories (\"Annualized\", \"Mixed\", \"Special\")\n",
    "# based on the ItemName_Sanitized that was derived in the previous cell.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Check that the required categorized DataFrames (annualized_items, mixed_items,\n",
    "#      special_items) and the temporary directory path Temp_file_path_DP exist.\n",
    "#   2. Build three sets of sanitized item names (annualized_names, mixed_names,\n",
    "#      special_names) from those DataFrames.\n",
    "#   3. List all files in Temp_file_path_DP and filter for those matching\n",
    "#      \"work_subset_*.txt\".\n",
    "#   4. For each work_subset file:\n",
    "#        - Extract the sanitized item name from the filename.\n",
    "#        - Determine whether it belongs to Mixed, Annualized, or Special based on\n",
    "#          the sets created in step 2.\n",
    "#        - Assign it a variable name (Mixed_n, Annualized_n, Special_n) and store\n",
    "#          that mapping in dicts mixed_vars, annualized_vars, special_vars.\n",
    "#   5. Store these dicts in globals() for use in later cells.\n",
    "#   6. Print summary information and display the created dictionaries.\n",
    "#   7. Perform garbage collection at the end.\n",
    "#\n",
    "# If any of the prerequisites are missing, it prints a message and skips the mapping.\n",
    "\n",
    "\n",
    "# CELL 2 — Map work_subset files to categories using ItemName_Sanitized\n",
    "\n",
    "if ('annualized_items' not in globals() or annualized_items is None or\n",
    "    'mixed_items' not in globals() or mixed_items is None or\n",
    "    'special_items' not in globals() or special_items is None or\n",
    "    'Temp_file_path_DP' not in globals()):\n",
    "    # If required DataFrames or directory path are missing, do not proceed\n",
    "    print(\"Required DataFrames (annualized_items, mixed_items, special_items) or Temp_file_path_DP not found. Please run the categorization cell.\")\n",
    "else:\n",
    "    print(\"Identifying work_subset files and creating variables based on categories...\")\n",
    "\n",
    "    # Sets of sanitized names that are final Annualized/Mixed/Special\n",
    "    annualized_names = set(annualized_items['ItemName_Sanitized'].dropna())\n",
    "    mixed_names      = set(mixed_items['ItemName_Sanitized'].dropna())\n",
    "    special_names    = set(special_items['ItemName_Sanitized'].dropna())\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Get a list of work_subset files from the temp directory\n",
    "    # ------------------------------------------------------------------\n",
    "    temp_files = os.listdir(Temp_file_path_DP)\n",
    "    work_subset_files = [\n",
    "        f for f in temp_files\n",
    "        if f.startswith('work_subset_') and f.endswith('.txt')\n",
    "    ]\n",
    "\n",
    "    # Dictionaries to hold mappings:\n",
    "    #   \"Annualized_1\" -> \"SomeItemName\"\n",
    "    #   \"Mixed_1\"      -> \"AnotherItemName\"\n",
    "    #   \"Special_1\"    -> \"SpecialItemName\"\n",
    "    annualized_vars = {}\n",
    "    mixed_vars = {}\n",
    "    special_vars = {}\n",
    "\n",
    "    print(f\"\\nFound {len(work_subset_files)} work_subset files in Temp directory.\")\n",
    "\n",
    "    # Sort files to have deterministic order when assigning variable names\n",
    "    work_subset_files.sort()\n",
    "\n",
    "    # Counters for how many items fall into each category; used for variable suffixes\n",
    "    annualized_count = 0\n",
    "    mixed_count = 0\n",
    "    special_count = 0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Iterate over each work_subset file and map it to a category\n",
    "    # ------------------------------------------------------------------\n",
    "    for file_name in work_subset_files:\n",
    "        # Extract sanitized item name from filename, expecting \"work_subset_<name>.txt\"\n",
    "        match = re.match(r'work_subset_(.+)\\.txt$', file_name)\n",
    "        if not match:\n",
    "            print(f\"  Filename format not as expected for '{file_name}'. Skipping processing.\")\n",
    "            continue\n",
    "\n",
    "        sanitized_item_name = match.group(1)\n",
    "\n",
    "        # Use the resolved sets. No more ambiguous precedence:\n",
    "        # priority Mixed -> Annualized -> Special, in this order of checks.\n",
    "        if sanitized_item_name in mixed_names:\n",
    "            mixed_count += 1\n",
    "            var_name = f\"Mixed_{mixed_count}\"\n",
    "            mixed_vars[var_name] = sanitized_item_name\n",
    "            print(f\"  '{file_name}' -> Mixed (variable '{var_name}').\")\n",
    "\n",
    "        elif sanitized_item_name in annualized_names:\n",
    "            annualized_count += 1\n",
    "            var_name = f\"Annualized_{annualized_count}\"\n",
    "            annualized_vars[var_name] = sanitized_item_name\n",
    "            print(f\"  '{file_name}' -> Annualized (variable '{var_name}').\")\n",
    "\n",
    "        elif sanitized_item_name in special_names:\n",
    "            special_count += 1\n",
    "            var_name = f\"Special_{special_count}\"\n",
    "            special_vars[var_name] = sanitized_item_name\n",
    "            print(f\"  '{file_name}' -> Special (variable '{var_name}').\")\n",
    "\n",
    "        else:\n",
    "            # No category mapping found for this sanitized name\n",
    "            print(f\"  '{file_name}' -> No matching Category (might be unmapped or ambiguous). Skipping.\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Expose the mapping dictionaries globally for later use\n",
    "    # ------------------------------------------------------------------\n",
    "    globals()['annualized_vars'] = annualized_vars\n",
    "    globals()['mixed_vars'] = mixed_vars\n",
    "    globals()['special_vars'] = special_vars\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Summary output and inspection\n",
    "    # ------------------------------------------------------------------\n",
    "    print(f\"\\nVariable creation complete.\")\n",
    "    print(f\"Created {len(annualized_vars)} Annualized variables.\")\n",
    "    print(f\"Created {len(mixed_vars)} Mixed variables.\")\n",
    "    print(f\"Created {len(special_vars)} Special variables.\")\n",
    "\n",
    "    print(\"\\nAnnualized Variables:\")\n",
    "    display(annualized_vars)\n",
    "\n",
    "    print(\"\\nMixed Variables:\")\n",
    "    display(mixed_vars)\n",
    "\n",
    "    print(\"\\nSpecial Variables:\")\n",
    "    display(special_vars)\n",
    "\n",
    "    # Run garbage collection after building mappings\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh7zifBYNg5C"
   },
   "source": [
    "## 4.3. Cash Flow Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIi7hX5Qoivw"
   },
   "source": [
    "### Special 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bk2WeU2yvY5"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1764881571632,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "VFzTQhiDyvY_",
    "outputId": "542a6949-225c-4632-8208-c95bfe54bcf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_1  ->  ItemName: 'Capital_Expenditures_Addtns_to_Fixed_Assets'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 1  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GldMcNUZyvZB"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1764881571701,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Rji1urHlyvZC",
    "outputId": "c5ecae43-e7ae-4003-f8b8-6d6ace6317e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Capital_Expenditures_Addtns_to_Fixed_Assets' ...\n",
      "Full dataset loaded successfully: 1,956,042 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4601</td>\n",
       "      <td>24.714956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4601</td>\n",
       "      <td>63.338554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4601</td>\n",
       "      <td>85.214171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4601</td>\n",
       "      <td>106.86508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4601</td>\n",
       "      <td>53.811107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode      Value  \n",
       "0     4601  24.714956  \n",
       "1     4601  63.338554  \n",
       "2     4601  85.214171  \n",
       "3     4601  106.86508  \n",
       "4     4601  53.811107  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hz56X8vZyvZD"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1764881571796,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Mk6voezByvZD",
    "outputId": "2a120a11-5e97-4ea6-c139-bf1cc1639e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Capital_Expenditures_Addtns_to_Fixed_Assets' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y04' 'Q1Y04' ... 'Q1Y10' 'Q3Y18' 'Q3Y18']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4601</td>\n",
       "      <td>24.714956</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4601</td>\n",
       "      <td>63.338554</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4601</td>\n",
       "      <td>85.214171</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4601</td>\n",
       "      <td>106.86508</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4601</td>\n",
       "      <td>53.811107</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode      Value Str_FiscalPrd  \n",
       "0     4601  24.714956           Y92  \n",
       "1     4601  63.338554           Y93  \n",
       "2     4601  85.214171           Y94  \n",
       "3     4601  106.86508           Y95  \n",
       "4     4601  53.811107           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Psbi_E3LyvZE"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1140,
     "status": "ok",
     "timestamp": 1764881572938,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Am-oNLHTyvZE",
    "outputId": "e96c6835-be1d-496a-f4ae-f8f701473ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,956,042 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 1338239\n",
      "                mean: 23398.703863129445\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.66793694815519\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 18,321\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 1319918\n",
      "                mean: 100.04664907447778\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.89225201982978\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Capital_Expenditures_Addtns_to_Fixed_Assets_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Capital_Expenditures_Addtns_to_Fixed_Assets_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,956,042\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 18,321\n",
      "Output rows (final):            1,937,721\n",
      "Check: excluded + dropped + output = 1,956,042\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYXQZ0uGRpJB"
   },
   "source": [
    "### Special 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uo2xvZcRpJO"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1764881573043,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Qcap9QYRRpJP",
    "outputId": "beee276d-8b25-476e-a709-825d67b9471f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_2  ->  ItemName: 'Cash_Dividends_Paid___Total'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 2  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9Bvm_ywRpJS"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1764881573368,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "NFU32xZ2RpJT",
    "outputId": "1436aa5d-722f-404a-88f7-d4ec8b162958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Cash_Dividends_Paid___Total' ...\n",
      "Full dataset loaded successfully: 1,787,365 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4551</td>\n",
       "      <td>39.696982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4551</td>\n",
       "      <td>9.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4551</td>\n",
       "      <td>7.402716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4551</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4551</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode      Value  \n",
       "0     4551  39.696982  \n",
       "1     4551      9.838  \n",
       "2     4551   7.402716  \n",
       "3     4551        0.0  \n",
       "4     4551        0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIYACqyWRpJU"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1764881573684,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "m5tiINI-RpJV",
    "outputId": "a916944d-5c6c-4686-be75-01f7a1d32887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Cash_Dividends_Paid___Total' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y07' 'Q1Y08' ... 'Q1Y13' 'Q3Y18' 'Q3Y18']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4551</td>\n",
       "      <td>39.696982</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4551</td>\n",
       "      <td>9.838</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4551</td>\n",
       "      <td>7.402716</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode      Value Str_FiscalPrd  \n",
       "0     4551  39.696982           Y92  \n",
       "1     4551      9.838           Y93  \n",
       "2     4551   7.402716           Y94  \n",
       "3     4551        0.0           Y95  \n",
       "4     4551        0.0           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKCZrzKiRpJW"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3255,
     "status": "ok",
     "timestamp": 1764881576940,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Fm8DhR18RpJW",
    "outputId": "42c902fe-dfa4-44a4-e4c4-315c91e9c1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,787,365 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 693677\n",
      "                mean: 24674.33980832419\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 97.72495743899829\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 16,187\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 677490\n",
      "                mean: 100.10068114999407\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.00451661769004\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Cash_Dividends_Paid___Total_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Cash_Dividends_Paid___Total_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,787,365\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 16,187\n",
      "Output rows (final):            1,771,178\n",
      "Check: excluded + dropped + output = 1,787,365\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aftAGj8aRr1F"
   },
   "source": [
    "### Special 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmoBKm4SyxX2"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96,
     "status": "ok",
     "timestamp": 1764881577039,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Xgf16_NmyxX3",
    "outputId": "bc5e9f96-06c0-417c-9cc3-18800bdd68d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_3  ->  ItemName: 'Com_Pfd_Redeemed_Retired_Converted_Etc.'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 3  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRMVkEtMyxX4"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1764881577354,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "zjJQGlNkyxX4",
    "outputId": "a1c39bdf-d12c-47aa-a0cc-045e71030384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Com_Pfd_Redeemed_Retired_Converted_Etc.' ...\n",
      "Full dataset loaded successfully: 1,447,068 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4751</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4751</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4751</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-31</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4751</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4751</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-31         A        NaN         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode Value  \n",
       "0     4751   0.0  \n",
       "1     4751   0.0  \n",
       "2     4751   0.0  \n",
       "3     4751   0.0  \n",
       "4     4751   0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUcX3hteyxX5"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 179,
     "status": "ok",
     "timestamp": 1764881577535,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "thNbFyOByxX5",
    "outputId": "f2915bb4-d24b-4af3-cd31-ab391234bb37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Com_Pfd_Redeemed_Retired_Converted_Etc.' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y07' 'Q1Y08' 'Q1Y07' ... 'Q1Y13' 'Q3Y18' 'Q3Y18']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-31</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-31         A        NaN          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode Value Str_FiscalPrd  \n",
       "0     4751   0.0           Y92  \n",
       "1     4751   0.0           Y93  \n",
       "2     4751   0.0           Y94  \n",
       "3     4751   0.0           Y95  \n",
       "4     4751   0.0           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3lmUEdCyxX6"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2686,
     "status": "ok",
     "timestamp": 1764881580223,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "qq_3zzGEyxX7",
    "outputId": "4b030d2f-70e8-40ce-d180-501577e9e874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,447,068 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 213288\n",
      "                mean: 2619.227878871928\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 96.70892479603569\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 7,290\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 205998\n",
      "                mean: 99.99207121506225\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.99754611413671\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Com_Pfd_Redeemed_Retired_Converted_Etc._complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Com_Pfd_Redeemed_Retired_Converted_Etc._complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,447,068\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 7,290\n",
      "Output rows (final):            1,439,778\n",
      "Check: excluded + dropped + output = 1,447,068\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxWj5w0FRwEl"
   },
   "source": [
    "### Special 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgrMgMjoyyPp"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1764881580324,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "dywBLOhNyyPq",
    "outputId": "36b33bde-7414-4f89-fd0c-743febf45725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_4  ->  ItemName: 'Disposal_of_Fixed_Assets'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 4  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jcNuQYjyyPr"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1764881580632,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "S41VQZC_yyPs",
    "outputId": "f337194c-80d1-4333-abff-1d0484d339d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Disposal_of_Fixed_Assets' ...\n",
      "Full dataset loaded successfully: 1,484,067 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4351</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4351</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4351</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4351</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4351</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode Value  \n",
       "0     4351   0.0  \n",
       "1     4351   0.0  \n",
       "2     4351   0.0  \n",
       "3     4351   0.0  \n",
       "4     4351   0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXNm207fyyPs"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1764881580889,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "CBHGNJEUyyPt",
    "outputId": "97bffcba-7526-4b8c-8d46-facdf8025329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Disposal_of_Fixed_Assets' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y07' 'Q1Y08' 'Q1Y08' ... 'Q1Y13' 'Q3Y18' 'Q3Y18']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode Value Str_FiscalPrd  \n",
       "0     4351   0.0           Y92  \n",
       "1     4351   0.0           Y93  \n",
       "2     4351   0.0           Y94  \n",
       "3     4351   0.0           Y95  \n",
       "4     4351   0.0           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFulM8SxyyPt"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2773,
     "status": "ok",
     "timestamp": 1764881583665,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "IZDcy1ayyyPu",
    "outputId": "0caa70b2-a466-4f87-94c5-202ad2e34130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,484,067 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 741801\n",
      "                mean: 26126.07958369098\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 97.76615146707655\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 26,353\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 715448\n",
      "                mean: 99.8558697048265\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.73330749796511\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Disposal_of_Fixed_Assets_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Disposal_of_Fixed_Assets_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,484,067\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 26,353\n",
      "Output rows (final):            1,457,714\n",
      "Check: excluded + dropped + output = 1,484,067\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1z9K-FW7Rw1B"
   },
   "source": [
    "### Special 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2cNBCFVyzAd"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1764881583736,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "78LnHOoIyzAe",
    "outputId": "bcb62032-aa8c-467d-8d99-f6e9884a713d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_5  ->  ItemName: 'Extraordinary_Items'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 5  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lauN3Cf2yzAf"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1764881584218,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "UI9tItmZyzAf",
    "outputId": "0c06d130-7b1c-4a99-c018-55105142fc8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Extraordinary_Items' ...\n",
      "Full dataset loaded successfully: 1,503,226 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4225</td>\n",
       "      <td>-8.619558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4225</td>\n",
       "      <td>-25.858138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4225</td>\n",
       "      <td>-83.558991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4225</td>\n",
       "      <td>-13.904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     4225         0.0  \n",
       "1     4225   -8.619558  \n",
       "2     4225  -25.858138  \n",
       "3     4225  -83.558991  \n",
       "4     4225     -13.904  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9FPLH8XyzAg"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1764881584425,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "flqYdqxsyzAg",
    "outputId": "7b86a8ac-9852-4968-e958-a6974c08eb17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Extraordinary_Items' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y07' 'Q1Y08' ... 'Q1Y13' 'Q3Y18' 'Q3Y18']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4225</td>\n",
       "      <td>-8.619558</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4225</td>\n",
       "      <td>-25.858138</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4225</td>\n",
       "      <td>-83.558991</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4225</td>\n",
       "      <td>-13.904</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     4225         0.0           Y92  \n",
       "1     4225   -8.619558           Y93  \n",
       "2     4225  -25.858138           Y94  \n",
       "3     4225  -83.558991           Y95  \n",
       "4     4225     -13.904           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWZ3U42GyzAh"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2861,
     "status": "ok",
     "timestamp": 1764881587314,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "l9AhdyaDyzAh",
    "outputId": "abcc6d5c-ad5f-431e-c8ab-70a9caae4b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,503,226 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 5663\n",
      "                mean: 91651.86503264453\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 96.70751525972483\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 194\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 5469\n",
      "                mean: 100.03079414107599\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.0\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Extraordinary_Items_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Extraordinary_Items_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,503,226\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 194\n",
      "Output rows (final):            1,503,032\n",
      "Check: excluded + dropped + output = 1,503,226\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgUXp_70Rxum"
   },
   "source": [
    "### Special 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rkv_WysDy0YU"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1764881587380,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "xTZ0ql0Oy0YV",
    "outputId": "2e47d96b-5069-42e2-c748-84fbed638a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_6  ->  ItemName: 'Funds_From_For_Other_Operating_Activities'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 6  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7f6TNJzy0YX"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 1941,
     "status": "ok",
     "timestamp": 1764881589323,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "knkqDAuWy0YY",
    "outputId": "9616c781-7534-4504-d53b-e84cef0dd027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Funds_From_For_Other_Operating_Activities' ...\n",
      "Full dataset loaded successfully: 1,694,729 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4831</td>\n",
       "      <td>-87.320244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4831</td>\n",
       "      <td>200.312706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4831</td>\n",
       "      <td>47.756416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4831</td>\n",
       "      <td>361.917728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4831</td>\n",
       "      <td>246.214426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     4831  -87.320244  \n",
       "1     4831  200.312706  \n",
       "2     4831   47.756416  \n",
       "3     4831  361.917728  \n",
       "4     4831  246.214426  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLzMvGo5y0Ya"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1764881589538,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "kXgiXuh4y0Ya",
    "outputId": "2fc46ddc-2c1b-4819-8d12-4c5f8c020d45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Funds_From_For_Other_Operating_Activities' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y02' 'Q1Y02' ... 'Q1Y10' 'Q1Y01' 'Q1Y10']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4831</td>\n",
       "      <td>-87.320244</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4831</td>\n",
       "      <td>200.312706</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4831</td>\n",
       "      <td>47.756416</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4831</td>\n",
       "      <td>361.917728</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4831</td>\n",
       "      <td>246.214426</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     4831  -87.320244           Y92  \n",
       "1     4831  200.312706           Y93  \n",
       "2     4831   47.756416           Y94  \n",
       "3     4831  361.917728           Y95  \n",
       "4     4831  246.214426           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKZNWo8sy0Yb"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3198,
     "status": "ok",
     "timestamp": 1764881592776,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "LMoNTc3zy0Yb",
    "outputId": "bddddd64-5ae4-4d5a-bfae-5bd52afd2cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,694,729 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 1218296\n",
      "                mean: 18445.996633235434\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 98.49069658802186\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 40,085\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 1178211\n",
      "                mean: 100.29888366758064\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.07450366491875\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Funds_From_For_Other_Operating_Activities_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Funds_From_For_Other_Operating_Activities_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,694,729\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 40,085\n",
      "Output rows (final):            1,654,644\n",
      "Check: excluded + dropped + output = 1,694,729\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZGQktb9Rydw"
   },
   "source": [
    "### Special 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJxFPf3Hy1No"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1764881592849,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "TTcq2u8Oy1Np",
    "outputId": "c61ffd8b-ef4c-4d0e-d428-f5e1696f6a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_7  ->  ItemName: 'Funds_From_Operations'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 7  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9LF9TH-y1Np"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1764881593219,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "O6HnA14ty1Nq",
    "outputId": "279248db-3f85-4e8c-e1be-02331ccaaed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Funds_From_Operations' ...\n",
      "Full dataset loaded successfully: 2,433,080 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4201</td>\n",
       "      <td>70.253522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4201</td>\n",
       "      <td>240.925386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4201</td>\n",
       "      <td>317.659199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4201</td>\n",
       "      <td>112.426119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4201</td>\n",
       "      <td>129.039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     4201   70.253522  \n",
       "1     4201  240.925386  \n",
       "2     4201  317.659199  \n",
       "3     4201  112.426119  \n",
       "4     4201     129.039  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwcgXbD5y1Nq"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1764881593607,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "IY2Xq9jBy1Nr",
    "outputId": "bf476800-dc47-4042-9c26-32a1c89a6367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Funds_From_Operations' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y02' 'Q1Y99' 'Q1Y02' ... 'Q3Y18' 'Q1Y23' 'Q1Y24']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4201</td>\n",
       "      <td>70.253522</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4201</td>\n",
       "      <td>240.925386</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4201</td>\n",
       "      <td>317.659199</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4201</td>\n",
       "      <td>112.426119</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4201</td>\n",
       "      <td>129.039</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     4201   70.253522           Y92  \n",
       "1     4201  240.925386           Y93  \n",
       "2     4201  317.659199           Y94  \n",
       "3     4201  112.426119           Y95  \n",
       "4     4201     129.039           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLRlPTn2y1Nr"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4261,
     "status": "ok",
     "timestamp": 1764881597870,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "iGcmYQFTy1Nr",
    "outputId": "4fb850db-54b3-48f7-9c1b-a63af9e9af3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 2,433,080 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 1778807\n",
      "                mean: 18071.225460238547\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 98.8033106519789\n",
      "                 p10: 92.66924215631829\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 101.17604413937995\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 65,076\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 1713731\n",
      "                mean: 100.09592602971993\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.87557291676661\n",
      "                 p10: 96.8199926268833\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.48232692861423\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Funds_From_Operations_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Funds_From_Operations_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     2,433,080\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 65,076\n",
      "Output rows (final):            2,368,004\n",
      "Check: excluded + dropped + output = 2,433,080\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rCgWaFPRzKG"
   },
   "source": [
    "### Special 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suhVrZ8sy2UK"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1764881597989,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "tVUMfPQNy2UL",
    "outputId": "425c316e-ddbd-4b48-f6bb-d589989efc5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_8  ->  ItemName: 'Long_Term_Borrowings'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 8  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyF0fOGiy2UL"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1764881598416,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "85m6WfUfy2UL",
    "outputId": "856968de-bc3c-40ff-df99-1edf7fadf178"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Long_Term_Borrowings' ...\n",
      "Full dataset loaded successfully: 1,381,622 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4401</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4401</td>\n",
       "      <td>90.276215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4401</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4401</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4401</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode      Value  \n",
       "0     4401        0.0  \n",
       "1     4401  90.276215  \n",
       "2     4401        0.0  \n",
       "3     4401        0.0  \n",
       "4     4401        0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTOH8xMuy2UM"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1764881598536,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "kZ9R1Hyay2UM",
    "outputId": "d401eb09-6deb-4a26-e419-a45275e94c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Long_Term_Borrowings' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y07' 'Q1Y07' 'Q1Y08' ... 'Q1Y13' 'Q3Y18' 'Q3Y18']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4401</td>\n",
       "      <td>90.276215</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode      Value Str_FiscalPrd  \n",
       "0     4401        0.0           Y92  \n",
       "1     4401  90.276215           Y93  \n",
       "2     4401        0.0           Y94  \n",
       "3     4401        0.0           Y95  \n",
       "4     4401        0.0           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-fg38eMy2UN"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2532,
     "status": "ok",
     "timestamp": 1764881601071,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Qlov5Lzuy2UN",
    "outputId": "86af26eb-9b18-464c-f8af-d2f2df8dbb56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,381,622 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 615196\n",
      "                mean: 19679.64721642216\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 97.67438845014577\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 16,579\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 598617\n",
      "                mean: 99.88085057765244\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.74627172771028\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Long_Term_Borrowings_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Long_Term_Borrowings_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,381,622\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 16,579\n",
      "Output rows (final):            1,365,043\n",
      "Check: excluded + dropped + output = 1,381,622\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlqChPY3RzyV"
   },
   "source": [
    "### Special 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrpobRiJy3sY"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1764881601181,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "xb7EuBvTy3sa",
    "outputId": "72d65291-ab20-466b-c1c1-a0eae44e4b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_9  ->  ItemName: 'Net_Cash_Flow___Financing'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 9  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKh1KsqSy3sb"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1764881601466,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "SJ5WfoX-y3sb",
    "outputId": "a84f17ac-639d-4f20-9d6e-0e3fd670e64e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Net_Cash_Flow___Financing' ...\n",
      "Full dataset loaded successfully: 1,975,430 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>June</td>\n",
       "      <td>4890</td>\n",
       "      <td>40.829043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>June</td>\n",
       "      <td>4890</td>\n",
       "      <td>-45.73678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-11-01</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>June</td>\n",
       "      <td>4890</td>\n",
       "      <td>-12.83298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1997-10-31</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1997</td>\n",
       "      <td>June</td>\n",
       "      <td>4890</td>\n",
       "      <td>72.094518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-12-04</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1998</td>\n",
       "      <td>September</td>\n",
       "      <td>4890</td>\n",
       "      <td>-55.529309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName ImplCountry  \\\n",
       "0  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "1  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "3  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "4  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "\n",
       "  CurrentCurrency HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod  \\\n",
       "0             Ars          Ars  1996-05-03         A          3         1994   \n",
       "1             Ars          Ars  1996-05-03         A          3         1995   \n",
       "2             Ars          Ars  1996-11-01         A          3         1996   \n",
       "3             Ars          Ars  1997-10-31         A          3         1997   \n",
       "4             Ars          Ars  1998-12-04         A          3         1998   \n",
       "\n",
       "   FYE Month ItemCode       Value  \n",
       "0       June     4890   40.829043  \n",
       "1       June     4890   -45.73678  \n",
       "2       June     4890   -12.83298  \n",
       "3       June     4890   72.094518  \n",
       "4  September     4890  -55.529309  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3N_RLA5y3sc"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1764881601708,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "yFaVpQCTy3sc",
    "outputId": "702a0498-a5be-409e-e2ba-db70786a353e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Net_Cash_Flow___Financing' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y02' 'Q1Y02' ... 'Q3Y18' 'Q1Y23' 'Q1Y24']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>June</td>\n",
       "      <td>4890</td>\n",
       "      <td>40.829043</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>June</td>\n",
       "      <td>4890</td>\n",
       "      <td>-45.73678</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-11-01</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>June</td>\n",
       "      <td>4890</td>\n",
       "      <td>-12.83298</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1997-10-31</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1997</td>\n",
       "      <td>June</td>\n",
       "      <td>4890</td>\n",
       "      <td>72.094518</td>\n",
       "      <td>Y97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-12-04</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1998</td>\n",
       "      <td>September</td>\n",
       "      <td>4890</td>\n",
       "      <td>-55.529309</td>\n",
       "      <td>Y98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName ImplCountry  \\\n",
       "0  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "1  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "3  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "4  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "\n",
       "  CurrentCurrency HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod  \\\n",
       "0             Ars          Ars  1996-05-03         A          3          1994   \n",
       "1             Ars          Ars  1996-05-03         A          3          1995   \n",
       "2             Ars          Ars  1996-11-01         A          3          1996   \n",
       "3             Ars          Ars  1997-10-31         A          3          1997   \n",
       "4             Ars          Ars  1998-12-04         A          3          1998   \n",
       "\n",
       "   FYE Month ItemCode       Value Str_FiscalPrd  \n",
       "0       June     4890   40.829043           Y94  \n",
       "1       June     4890   -45.73678           Y95  \n",
       "2       June     4890   -12.83298           Y96  \n",
       "3       June     4890   72.094518           Y97  \n",
       "4  September     4890  -55.529309           Y98  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioMkfJA4y3sd"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3628,
     "status": "ok",
     "timestamp": 1764881605378,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "KZuSLbVsy3sd",
    "outputId": "e11eb66f-c0b6-40ec-8007-7e7cef5a061c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,975,430 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 1334356\n",
      "                mean: 22951.9780547885\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 98.81137799681824\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 25,573\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 1308783\n",
      "                mean: 100.08163361124764\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.91052107770894\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Net_Cash_Flow___Financing_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Net_Cash_Flow___Financing_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,975,430\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 25,573\n",
      "Output rows (final):            1,949,857\n",
      "Check: excluded + dropped + output = 1,975,430\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKBVA-oIjsfn"
   },
   "source": [
    "### Special 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4hE9PwgjsgH"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1764881605478,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "OzTCEZa-jsgL",
    "outputId": "1f8b1f08-b802-4ee1-c420-4374d3c44de9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_10  ->  ItemName: 'Net_Cash_Flow___Investing'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 10  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WIwn8mSjsgO"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1764881605851,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "pOrdQ7X4jsgQ",
    "outputId": "7ae13dde-c2bc-488f-b929-7c9819c4ee63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Net_Cash_Flow___Investing' ...\n",
      "Full dataset loaded successfully: 1,979,139 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>June</td>\n",
       "      <td>4870</td>\n",
       "      <td>47.224268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>June</td>\n",
       "      <td>4870</td>\n",
       "      <td>28.910173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-11-01</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>June</td>\n",
       "      <td>4870</td>\n",
       "      <td>33.23571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1997-10-31</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1997</td>\n",
       "      <td>June</td>\n",
       "      <td>4870</td>\n",
       "      <td>38.539571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-12-04</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1998</td>\n",
       "      <td>September</td>\n",
       "      <td>4870</td>\n",
       "      <td>41.47087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName ImplCountry  \\\n",
       "0  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "1  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "3  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "4  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "\n",
       "  CurrentCurrency HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod  \\\n",
       "0             Ars          Ars  1996-05-03         A          3         1994   \n",
       "1             Ars          Ars  1996-05-03         A          3         1995   \n",
       "2             Ars          Ars  1996-11-01         A          3         1996   \n",
       "3             Ars          Ars  1997-10-31         A          3         1997   \n",
       "4             Ars          Ars  1998-12-04         A          3         1998   \n",
       "\n",
       "   FYE Month ItemCode      Value  \n",
       "0       June     4870  47.224268  \n",
       "1       June     4870  28.910173  \n",
       "2       June     4870   33.23571  \n",
       "3       June     4870  38.539571  \n",
       "4  September     4870   41.47087  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJkiqx51jsgR"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1764881606186,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "IeZJWkSEjsgR",
    "outputId": "024513a9-9228-43b2-f46c-c86cc857ef26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Net_Cash_Flow___Investing' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y02' 'Q1Y02' ... 'Q3Y18' 'Q1Y23' 'Q1Y24']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>June</td>\n",
       "      <td>4870</td>\n",
       "      <td>47.224268</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>June</td>\n",
       "      <td>4870</td>\n",
       "      <td>28.910173</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-11-01</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>June</td>\n",
       "      <td>4870</td>\n",
       "      <td>33.23571</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1997-10-31</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1997</td>\n",
       "      <td>June</td>\n",
       "      <td>4870</td>\n",
       "      <td>38.539571</td>\n",
       "      <td>Y97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-12-04</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1998</td>\n",
       "      <td>September</td>\n",
       "      <td>4870</td>\n",
       "      <td>41.47087</td>\n",
       "      <td>Y98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName ImplCountry  \\\n",
       "0  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "1  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "3  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "4  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "\n",
       "  CurrentCurrency HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod  \\\n",
       "0             Ars          Ars  1996-05-03         A          3          1994   \n",
       "1             Ars          Ars  1996-05-03         A          3          1995   \n",
       "2             Ars          Ars  1996-11-01         A          3          1996   \n",
       "3             Ars          Ars  1997-10-31         A          3          1997   \n",
       "4             Ars          Ars  1998-12-04         A          3          1998   \n",
       "\n",
       "   FYE Month ItemCode      Value Str_FiscalPrd  \n",
       "0       June     4870  47.224268           Y94  \n",
       "1       June     4870  28.910173           Y95  \n",
       "2       June     4870   33.23571           Y96  \n",
       "3       June     4870  38.539571           Y97  \n",
       "4  September     4870   41.47087           Y98  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwmMjCg1jsgT"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3674,
     "status": "ok",
     "timestamp": 1764881609862,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "rOG6JcDYjsgT",
    "outputId": "849822c0-2b0c-4166-97af-cb8110a2a6be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,979,139 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 1339411\n",
      "                mean: 21564.019765097186\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.09259777790926\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 22,262\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 1317149\n",
      "                mean: 100.06684577111258\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.9006871573958\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Net_Cash_Flow___Investing_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Net_Cash_Flow___Investing_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,979,139\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 22,262\n",
      "Output rows (final):            1,956,877\n",
      "Check: excluded + dropped + output = 1,979,139\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7vnArWRjwXJ"
   },
   "source": [
    "### Special 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWaDDsTVjwXL"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1764881609968,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "HtpOiPoWjwXL",
    "outputId": "46b8837e-9761-4902-b2c3-cb42baf376ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_11  ->  ItemName: 'Net_Cash_Flow___Operating_Activities'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 11  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEnZ2lFejwXM"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1764881610383,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "AluINJiNjwXN",
    "outputId": "f407a68c-db1d-491f-9f13-130078f6bdd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Net_Cash_Flow___Operating_Activities' ...\n",
      "Full dataset loaded successfully: 2,052,576 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>June</td>\n",
       "      <td>4860</td>\n",
       "      <td>3.574132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>June</td>\n",
       "      <td>4860</td>\n",
       "      <td>66.404052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-11-01</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>June</td>\n",
       "      <td>4860</td>\n",
       "      <td>41.129327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1997-10-31</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1997</td>\n",
       "      <td>June</td>\n",
       "      <td>4860</td>\n",
       "      <td>-15.875458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-12-04</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1998</td>\n",
       "      <td>September</td>\n",
       "      <td>4860</td>\n",
       "      <td>111.195883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName ImplCountry  \\\n",
       "0  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "1  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "3  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "4  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "\n",
       "  CurrentCurrency HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod  \\\n",
       "0             Ars          Ars  1996-05-03         A          3         1994   \n",
       "1             Ars          Ars  1996-05-03         A          3         1995   \n",
       "2             Ars          Ars  1996-11-01         A          3         1996   \n",
       "3             Ars          Ars  1997-10-31         A          3         1997   \n",
       "4             Ars          Ars  1998-12-04         A          3         1998   \n",
       "\n",
       "   FYE Month ItemCode       Value  \n",
       "0       June     4860    3.574132  \n",
       "1       June     4860   66.404052  \n",
       "2       June     4860   41.129327  \n",
       "3       June     4860  -15.875458  \n",
       "4  September     4860  111.195883  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ajfPwZdjwXN"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1764881610570,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Mml1V_4BjwXO",
    "outputId": "a5fbf593-a56f-423d-919d-ba249aea7742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Net_Cash_Flow___Operating_Activities' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y02' 'Q1Y02' ... 'Q3Y18' 'Q1Y23' 'Q1Y24']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>June</td>\n",
       "      <td>4860</td>\n",
       "      <td>3.574132</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>June</td>\n",
       "      <td>4860</td>\n",
       "      <td>66.404052</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-11-01</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>June</td>\n",
       "      <td>4860</td>\n",
       "      <td>41.129327</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1997-10-31</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1997</td>\n",
       "      <td>June</td>\n",
       "      <td>4860</td>\n",
       "      <td>-15.875458</td>\n",
       "      <td>Y97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-12-04</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1998</td>\n",
       "      <td>September</td>\n",
       "      <td>4860</td>\n",
       "      <td>111.195883</td>\n",
       "      <td>Y98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName ImplCountry  \\\n",
       "0  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "1  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "3  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "4  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "\n",
       "  CurrentCurrency HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod  \\\n",
       "0             Ars          Ars  1996-05-03         A          3          1994   \n",
       "1             Ars          Ars  1996-05-03         A          3          1995   \n",
       "2             Ars          Ars  1996-11-01         A          3          1996   \n",
       "3             Ars          Ars  1997-10-31         A          3          1997   \n",
       "4             Ars          Ars  1998-12-04         A          3          1998   \n",
       "\n",
       "   FYE Month ItemCode       Value Str_FiscalPrd  \n",
       "0       June     4860    3.574132           Y94  \n",
       "1       June     4860   66.404052           Y95  \n",
       "2       June     4860   41.129327           Y96  \n",
       "3       June     4860  -15.875458           Y97  \n",
       "4  September     4860  111.195883           Y98  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnddt3xljwXO"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3684,
     "status": "ok",
     "timestamp": 1764881614262,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "WrRgNqcOjwXP",
    "outputId": "055c802f-b738-4564-8ce4-2186c31cc278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 2,052,576 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 1411558\n",
      "                mean: 20985.548626357453\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.251528170489\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 23,166\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 1388392\n",
      "                mean: 100.13877387747178\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.96372439823251\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Net_Cash_Flow___Operating_Activities_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Net_Cash_Flow___Operating_Activities_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     2,052,576\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 23,166\n",
      "Output rows (final):            2,029,410\n",
      "Check: excluded + dropped + output = 2,052,576\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5fadnHYj4jI"
   },
   "source": [
    "### Special 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLC2QIXJj4jJ"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1764881614405,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "aTLRK8wJj4jK",
    "outputId": "821691c5-a90f-4b7c-9558-094598a365f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_12  ->  ItemName: 'Net_Proceeds_From_Sale_Issue_of_Com__Pfd'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 12  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeayAOdKj4jL"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1764881614724,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "TE2P3xoHj4jL",
    "outputId": "1667c7d1-7914-4574-cbc8-d545c4b678cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Net_Proceeds_From_Sale_Issue_of_Com__Pfd' ...\n",
      "Full dataset loaded successfully: 1,611,038 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4251</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4251</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4251</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4251</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4251</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode Value  \n",
       "0     4251   0.0  \n",
       "1     4251   0.0  \n",
       "2     4251   0.0  \n",
       "3     4251   0.0  \n",
       "4     4251   0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Dt0Dw3Qj4jM"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1764881614947,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Ow-DF4o-j4jM",
    "outputId": "753cb04c-2201-46cd-9c1a-83e7cb9c5590"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Net_Proceeds_From_Sale_Issue_of_Com__Pfd' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y07' 'Q1Y08' 'Q1Y07' ... 'Q1Y13' 'Q3Y18' 'Q3Y18']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode Value Str_FiscalPrd  \n",
       "0     4251   0.0           Y92  \n",
       "1     4251   0.0           Y93  \n",
       "2     4251   0.0           Y94  \n",
       "3     4251   0.0           Y95  \n",
       "4     4251   0.0           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJyg_XAyj4jN"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3107,
     "status": "ok",
     "timestamp": 1764881618085,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "36ewI1_Ej4jN",
    "outputId": "7ba83c3b-5b2c-4f19-d65c-ec60465cfddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,611,038 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 507219\n",
      "                mean: 5393.4765422721775\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 96.46907483123637\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 18,973\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 488246\n",
      "                mean: 100.00050179781469\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.96976408475548\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Net_Proceeds_From_Sale_Issue_of_Com__Pfd_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Net_Proceeds_From_Sale_Issue_of_Com__Pfd_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,611,038\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 18,973\n",
      "Output rows (final):            1,592,065\n",
      "Check: excluded + dropped + output = 1,611,038\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6QPwOcNj9RD"
   },
   "source": [
    "### Special 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MikmKl9pj9RF"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1764881618193,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "nq6kVoozj9RG",
    "outputId": "57bae82f-e742-4f6c-bdab-13f2ce50a4dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Special_13  ->  ItemName: 'Reduction_in_Long_Term_Debt'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE SPECIAL_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Chooses which Special_* item (from special_vars) should be processed.\n",
    "#   2. Validates that special_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Sets a base_output_filename for downstream output files.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Adjust `special_index` to run a different Special_* dataset (e.g., 2, 3, 10 ...).\n",
    "#   - Assumes `special_vars` was created in the categorization step and\n",
    "#     `Temp_file_path_DP` was defined in the environment setup.\n",
    "\n",
    "# === Select which Special_* item to run ===\n",
    "special_index = 13  # Change this to run another dataset, e.g. 10\n",
    "\n",
    "# special_vars should look like: {'Special_1': 'SomeItem', 'Special_2': 'OtherItem', ...}\n",
    "assert 'special_vars' in globals(), \"special_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key for the chosen index and find the corresponding item name\n",
    "item_key = f\"Special_{special_index}\"\n",
    "target_item_name = special_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in special_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing your globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file for this item (produced by previous merging steps)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for output files created by the \"Special\" pipeline\n",
    "base_output_filename = f\"Special_{target_item_name}_complete\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xX4KZvhCj9RG"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1764881618471,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "yPqUcXosj9RH",
    "outputId": "73837821-73ff-4024-9275-178714bf88ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Reduction_in_Long_Term_Debt' ...\n",
      "Full dataset loaded successfully: 1,478,266 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4701</td>\n",
       "      <td>0.615867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4701</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4701</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4701</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4701</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode     Value  \n",
       "0     4701  0.615867  \n",
       "1     4701       0.0  \n",
       "2     4701       0.0  \n",
       "3     4701       0.0  \n",
       "4     4701       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    special_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if special_raw is not None and not special_raw.empty:\n",
    "        print(f\"Full dataset loaded successfully: {len(special_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(special_raw.head())\n",
    "        except Exception:\n",
    "            print(special_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        special_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    special_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZV6Ztp7j9RH"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1764881618775,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "51JNZ0_Gj9RI",
    "outputId": "2516dc58-3325-4761-a23d-829a96c39b63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Special dataset for 'Reduction_in_Long_Term_Debt' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952323/2997685217.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y08' 'Q1Y09' 'Q1Y10' ... 'Q1Y13' 'Q1Y13' 'Q3Y18']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>4701</td>\n",
       "      <td>0.615867</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>4701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>4701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>4701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>4701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode     Value Str_FiscalPrd  \n",
       "0     4701  0.615867           Y92  \n",
       "1     4701       0.0           Y93  \n",
       "2     4701       0.0           Y94  \n",
       "3     4701       0.0           Y95  \n",
       "4     4701       0.0           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# (unchanged documentation)\n",
    "# ...\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    # (function identical to your version—no changes needed)\n",
    "    df = df.copy()\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce')) |\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to special_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'special_raw' in globals() and special_raw is not None and not special_raw.empty:\n",
    "    print(f\"Applying encoding to Special dataset for '{target_item_name}' ...\")\n",
    "    special_encoded = add_str_fiscalprd(special_raw)\n",
    "    display(special_encoded.head())\n",
    "else:\n",
    "    print(\"special_raw not found or empty. Cannot perform encoding.\")\n",
    "    special_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_kCKzdEj9RI"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2730,
     "status": "ok",
     "timestamp": 1764881621509,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "r9parID2j9RJ",
    "outputId": "b7e83164-2bea-4a4d-b030-a3e21c223e69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,478,266 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 833668\n",
      "                mean: 15650.415999260122\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 97.73801315381671\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >250 or <25): 25,406\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 808262\n",
      "                mean: 99.7696399239103\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.59769860231407\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Reduction_in_Long_Term_Debt_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Special_Reduction_in_Long_Term_Debt_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,478,266\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 25,406\n",
      "Output rows (final):            1,452,860\n",
      "Check: excluded + dropped + output = 1,478,266\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   - Implements a fast \"as-of join\" between two DataFrames based on PIT dates\n",
    "#     and key columns (asof_numpy).\n",
    "#   - Provides helpers for percentile summaries and winsorized statistics.\n",
    "#   - Builds annualized \"AnnPITValue\" values from:\n",
    "#       * true annual data (A/B frequency) and\n",
    "#       * sums of quarterly data (Q1..Q4) when available.\n",
    "#   - Performs various quality checks (future-dated values, extreme percentages).\n",
    "#   - Produces a processed \"special_processed\" DataFrame and saves:\n",
    "#       * a full export and\n",
    "#       * a subset export.\n",
    "#   - Prints a row-accounting overview so drops and exclusions are transparent.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Build a combined string key from multiple columns by concatenating them\n",
    "    with '||' to use as a group key.\n",
    "    \"\"\"\n",
    "    # Convert all key columns to string and join them row-wise with '||'\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of float values (same length as left_df) containing the matched\n",
    "        values from right_df (or NaN if none found).\n",
    "    out_dates : np.ndarray\n",
    "        Array of datetime64 values containing the matched dates (or NaT).\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaN/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns in left/right for the as-of join\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask to filter rows with all required fields present\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If no valid rows on either side, return empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on copies of the filtered frames\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT dates to daily granularity\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build grouping keys on both sides\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and PIT Date so we can binary-search later\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract numpy arrays for fast vectorized operations\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Determine contiguous slices of rows for each unique key in right_df\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]                         # start index for this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end index\n",
    "        slices[k] = (rdt[s:e], rval[s:e])    # store date and value slices\n",
    "\n",
    "    # Original indices of left rows (to write back results correctly)\n",
    "    l_idx = l.index.to_numpy()\n",
    "    # Keys and dates of left rows\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left rows by key (stable sort) for block processing\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Iterate over blocks of the same key in left_df\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current key\n",
    "        j = i + 1\n",
    "        # Find the end of this key's block\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Block of PIT dates and corresponding positions (indices) for this key\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # Only process if the key exists in the right-hand slices\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "\n",
    "            # For each left PIT date, find index of right PIT <= left PIT\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # only those with at least one right date <= left date\n",
    "\n",
    "            # Write results back to output arrays\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "\n",
    "        # Move to the next block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile (quantile) helper that returns NaN on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and selected percentiles for a numeric series.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "\n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures winsorize gets a writable array\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "\n",
    "    # Compute mean, median, winsorized mean and deciles\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Period prioritization ----------\n",
    "# Priority ranking for period labels when deciding between multiple candidates\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # Full annual has highest priority\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map column names to period labels used in _PERIOD_PRIORITY.\n",
    "    Currently only special-cases 'A'.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# ---------- Helpers for AnnPITValue using A + Q1..Q4 sum ----------\n",
    "def full_year_from_quarters(row, pit, cutoff):\n",
    "    \"\"\"\n",
    "    Build a full-year candidate from Q1..Q4:\n",
    "\n",
    "      - Requires ALL Q1..Q4 to have:\n",
    "          * non-missing value\n",
    "          * non-missing date\n",
    "          * non-missing OriginFP\n",
    "      - All dates must be within [cutoff, pit].\n",
    "      - origin_fp = max(OriginFP of Q1..Q4) (i.e., newest year among quarters)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from the working DataFrame.\n",
    "    pit : datetime-like\n",
    "        PIT Date of the row.\n",
    "    cutoff : datetime-like\n",
    "        Lower bound for valid quarter dates (pit - 365 days).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (dt, val_sum, origin_fp) or (NaT, NaN, NaN)\n",
    "        dt        : latest quarter date among Q1..Q4\n",
    "        val_sum   : sum of Q1..Q4 values\n",
    "        origin_fp : max OriginFP among Q1..Q4\n",
    "    \"\"\"\n",
    "    labels = [f'Q{i}' for i in range(1, 5)]\n",
    "    vals, dts, fps = [], [], []\n",
    "\n",
    "    # Check all four quarters\n",
    "    for lbl in labels:\n",
    "        v = row.get(lbl, np.nan)\n",
    "        d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "        o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "        # Require non-missing value, date, and OriginFP\n",
    "        if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        # Ensure Date is valid and within [cutoff, pit]\n",
    "        d = pd.to_datetime(d, errors='coerce')\n",
    "        if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "            return pd.NaT, np.nan, np.nan\n",
    "\n",
    "        vals.append(float(v))\n",
    "        dts.append(d)\n",
    "        fps.append(int(o))\n",
    "\n",
    "    # If all checks pass, compute sum, latest date, and max OriginFP\n",
    "    total_val = float(np.nansum(vals))\n",
    "    latest_dt = max(dts)\n",
    "    origin_fp = max(fps)\n",
    "    return latest_dt, total_val, origin_fp\n",
    "\n",
    "\n",
    "def pick_annpit_sum_with_origin(row):\n",
    "    \"\"\"\n",
    "    Compute AnnPITValue using annual (A) and quarterly (Q1..Q4) data.\n",
    "    \"\"\"\n",
    "    pit = row['PIT Date']\n",
    "    # If PIT Date is missing, no AnnPITValue can be computed\n",
    "    if pd.isna(pit):\n",
    "        return np.nan\n",
    "\n",
    "    # Lower bound for acceptable dates (365 days before PIT)\n",
    "    # This works now because timedelta is imported\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Extract fiscal period as integer if possible\n",
    "    fp = row.get('FiscalPeriod', np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    # Collect candidate tuples as:\n",
    "    # (label, period_priority, date, value, origin_fp)\n",
    "    candidates = []\n",
    "\n",
    "    # --- A: actual annual (0 is allowed) ---\n",
    "    A_val = row.get('A', np.nan)\n",
    "    A_dt  = row.get('A_Date', pd.NaT)\n",
    "    A_ofp = row.get('A_OriginFP', np.nan)\n",
    "    if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "        A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "        if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "            candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "    # --- Q4 candidate: sum of Q1..Q4 (0 is allowed) ---\n",
    "    q4_dt, q4_val, q4_fp = full_year_from_quarters(row, pit, cutoff)\n",
    "    if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "        candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "    # No candidates at all -> return NaN\n",
    "    if not candidates:\n",
    "        return np.nan\n",
    "\n",
    "    # Filter only those with non-NaN values (0 is allowed)\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "    # Relative year relation between candidate OriginFP and the row's FiscalPeriod\n",
    "    def rel(c):\n",
    "        _, _, _, _, ofp = c\n",
    "        if fp_int is None or ofp is None:\n",
    "            return 'unknown'\n",
    "        if ofp == fp_int:\n",
    "            return 'same'\n",
    "        if ofp == fp_int - 1:\n",
    "            return 'prior'\n",
    "        return 'other'\n",
    "\n",
    "    # 1) Same-year A\n",
    "    same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "    if same_A:\n",
    "        # Pick latest A by date\n",
    "        best = max(same_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 2) Same-year Q4 sum\n",
    "    same_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'same')\n",
    "    if same_Q4:\n",
    "        # Higher priority first, then latest date\n",
    "        best = max(same_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 3) Prior-year A\n",
    "    prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "    if prior_A:\n",
    "        best = max(prior_A, key=lambda x: x[2])\n",
    "        return best[3]\n",
    "\n",
    "    # 4) Prior-year Q4 sum\n",
    "    prior_Q4 = valid(c for c in candidates if c[0] == 'Q4' and rel(c) == 'prior')\n",
    "    if prior_Q4:\n",
    "        best = max(prior_Q4, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return best[3]\n",
    "\n",
    "    # If all else fails (should not normally happen), return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "if 'special_encoded' in globals() and special_encoded is not None:\n",
    "    # Remember the number of input rows for row-accounting\n",
    "    input_rows = len(special_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy so we do not mutate the original DataFrame\n",
    "    working = special_encoded.copy()\n",
    "\n",
    "    # Exclude certain frequencies (E, L, R, U) from further processing\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # Convert key columns to appropriate types\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Ensure some ID-like columns are strings\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # Parse Q/S/T numbers from Str_FiscalPrd (e.g. 'Q1Y23' -> QNUM=1)\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Define all period/value and period/date column names\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1,5)] + \\\n",
    "                                       [f'S{i}' for i in range(1,3)] + \\\n",
    "                                       [f'T{i}' for i in range(1,4)]] + ['A_Date']\n",
    "\n",
    "    # Ensure all period value columns exist (initialize if missing)\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Ensure all period date columns exist (initialize if missing)\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Keys used to identify time series in as-of joins\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) TrueValue from annuals: build reference \"TrueValue\" per ID/FiscalPeriod\n",
    "    # -------------------------------------------------------------------------\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Merge TrueValue back on keys\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping for each frequency (no prior-year / no forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Annual (A/B) as-of\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period for A\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # Quarterly (Q/C) as-of, by quarter number\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Subset source to a specific quarter\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        # Set OriginFP where we have a newly filled quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Semiannual (S/F) as-of, by half-year number\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # Trimester (T/K) as-of, by term number\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels and normalize dates (only as-of results, no ffill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    # Ensure all date columns are valid datetimes at day precision\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) AnnPITValue with new logic (A + Q1..Q4 sum, zeros allowed)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue'] = working.apply(\n",
    "        pick_annpit_sum_with_origin,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check (period date > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Only use date columns that actually exist\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    # For each period date column, check if it's after PIT Date\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        # Count violations per column\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        # Track rows with any violation across all period dates\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flag rows with at least one future-date violation\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct and quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Percentage of AnnPITValue relative to TrueValue (%)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Summary BEFORE dropping outliers\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    # Flag infinities\n",
    "    is_inf = np.isinf(pct)\n",
    "    # Flag finite out-of-range values outside [25, 250]\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 250) | (pct < 25))\n",
    "    # Combined drop mask: infinities or out-of-range finite values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    # Count dropped rows due to quality rules\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >250 or <25): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that pass the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Summary AFTER dropping outliers\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns and cleanup\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core columns that describe each row\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Period-related columns (Dates and Values)\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Columns we want to keep in the final output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns such as OriginFP and intermediate numeric helpers\n",
    "    drop_cols = [c for c in working.columns\n",
    "                 if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder to the final column set\n",
    "    special_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Save outputs (requires Temp_file_path_DP and base_output_filename)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Both variables must be defined in a previous setup cell\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Full export path and write to pipe-delimited text\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    special_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Subset export with a small selection of columns\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in special_processed.columns]\n",
    "    subset_df = special_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df  # free some memory\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(special_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "\n",
    "    # Sanity check: excluded + dropped + final should equal original\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows.\")\n",
    "\n",
    "    # Optional: trigger garbage collection (import gc must exist elsewhere)\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # Early exit if special_encoded is not defined or is None\n",
    "    print(\"special_encoded not found or None; skipping.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPAPKOVeFPpXXKQVkoYFX5a",
   "collapsed_sections": [
    "b06f6df4",
    "87ba0a55",
    "FmcsWZB0oDMl",
    "9WkpqfMTNN5n",
    "3Nb7Pqn0eFOw",
    "wh7zifBYNg5C",
    "sIi7hX5Qoivw",
    "sYXQZ0uGRpJB",
    "9uo2xvZcRpJO",
    "J9Bvm_ywRpJS",
    "XIYACqyWRpJU",
    "VKCZrzKiRpJW",
    "aftAGj8aRr1F",
    "WxWj5w0FRwEl",
    "1z9K-FW7Rw1B",
    "qgUXp_70Rxum",
    "FZGQktb9Rydw",
    "1rCgWaFPRzKG",
    "qlqChPY3RzyV"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
