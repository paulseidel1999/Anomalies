{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b06f6df4"
   },
   "source": [
    "### Mount Google Drive, Import Libraries and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22325,
     "status": "ok",
     "timestamp": 1764881041038,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "DVbbtM4ztajO",
    "outputId": "6989c266-bce5-4f3f-fde6-d766ed042707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: False\n",
      "BASE_PATH: /home/jovyan/work/hpool1/pseidel/test\n",
      "Importing numpy ...\n",
      "numpy OK\n",
      "Importing scipy ...\n",
      "scipy OK\n",
      "Importing pandas ...\n",
      "pandas OK\n",
      "Importing linearmodels ...\n",
      "linearmodels OK\n",
      "Importing xlsxwriter ...\n",
      "xlsxwriter OK\n",
      "Paths configured. Temp outputs -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview\n",
      "Example input path -> /home/jovyan/work/hpool1/pseidel/test/Input/WSFV_f_20250131.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT SETUP + PATH CONFIGURATION (SERVER / COLAB COMPATIBLE)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) HARD SAFETY: cap native thread usage (prevents pthread_create EAGAIN)\n",
    "#    MUST be set before importing numpy / scipy / pandas\n",
    "# -----------------------------------------------------------------------------\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"BLIS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Detect environment\n",
    "# -----------------------------------------------------------------------------\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) (Colab only) Mount Google Drive\n",
    "# -----------------------------------------------------------------------------\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "else:\n",
    "    # Server base path (your target)\n",
    "    BASE_PATH = \"/home/jovyan/work/hpool1/pseidel/test\"\n",
    "\n",
    "print(\"IN_COLAB:\", IN_COLAB)\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Sanity checks: path exists + write permission\n",
    "# -----------------------------------------------------------------------------\n",
    "BASE = Path(BASE_PATH)\n",
    "if not BASE.exists():\n",
    "    raise FileNotFoundError(f\"BASE_PATH does not exist: {BASE}\")\n",
    "\n",
    "# quick write test (fails fast if you don't have permissions)\n",
    "test_file = BASE / \".write_test_tmp\"\n",
    "try:\n",
    "    test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "    test_file.unlink()\n",
    "except Exception as e:\n",
    "    raise PermissionError(f\"No write permission in {BASE}. Error: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Environment check: ensure required packages import cleanly\n",
    "# -----------------------------------------------------------------------------\n",
    "required_packages = [\"numpy\", \"scipy\", \"pandas\", \"linearmodels\", \"xlsxwriter\"]\n",
    "\n",
    "for pkg in required_packages:\n",
    "    print(f\"Importing {pkg} ...\")\n",
    "    importlib.import_module(pkg)\n",
    "    print(f\"{pkg} OK\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Base paths and input/output locations\n",
    "# -----------------------------------------------------------------------------\n",
    "Input_file_path   = str(BASE / \"Input\")\n",
    "Temp_file_path    = str(BASE / \"Temp\")\n",
    "Output_file_path  = str(BASE / \"Output\")\n",
    "\n",
    "Fundamentals_file_path = f\"{Input_file_path}/WSFV_f_20250131.txt\"\n",
    "Current_file_path      = f\"{Input_file_path}/WSCurrent_f_20250131.txt\"\n",
    "Calendar_file_path     = f\"{Input_file_path}/WSCalendarPrd_f_20250131.txt\"\n",
    "Meta_file_path         = f\"{Input_file_path}/WSMetaData_f_20250131.txt\"\n",
    "Excel_file_path        = f\"{Input_file_path}/WS PIT Table Definitions V5 with start dates.xls\"\n",
    "\n",
    "MarketValues_file_path          = f\"{Input_file_path}/Daily MV USD\"\n",
    "MarketValues_file_path_LC       = f\"{Input_file_path}/Daily MV LC\"\n",
    "DailyTotalReturns_file_path     = f\"{Input_file_path}/Daily Returns USD\"\n",
    "DailyIndexReturns_file_path     = f\"{Input_file_path}/Daily Index Returns USD\"\n",
    "Constituents_file_path          = f\"{Input_file_path}/Constituents.01.csv\"\n",
    "UniversalMatching_file_path     = f\"{Input_file_path}/Universal Matching File\"\n",
    "\n",
    "Temp_file_path_GO  = f\"{Temp_file_path}/TempGeneralOverview\"\n",
    "Temp_file_path_EoC = f\"{Temp_file_path}/TempExtractionofCharacteristics\"\n",
    "Temp_file_path_DP  = f\"{Temp_file_path}/TempDataPreparation\"\n",
    "Temp_file_path_A   = f\"{Temp_file_path}/TempAnomalies\"\n",
    "Temp_file_path_R   = f\"{Temp_file_path}/TempRegressionModel\"\n",
    "\n",
    "Relevant_items_path   = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_A = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_B = f\"{Input_file_path}/RelevantItemsB.txt\"\n",
    "Relevant_items_path_C = f\"{Input_file_path}/RelevantItemsC.txt\"\n",
    "Relevant_items_path_D = f\"{Input_file_path}/RelevantItemsD.txt\"\n",
    "\n",
    "Subset_file_path = f\"{Temp_file_path_GO}/Subsets\"\n",
    "Fundamentals_clean_file_path = f\"{Temp_file_path_GO}/Fundamentals_clean.txt\"\n",
    "Current_clean_file_path      = f\"{Temp_file_path_GO}/Current_clean.txt\"\n",
    "Calendar_clean_file_path     = f\"{Temp_file_path_GO}/Input/Calendar_clean.txt\"\n",
    "Meta_clean_file_path         = f\"{Temp_file_path_GO}/Input/Meta_clean.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Ensure required directories exist\n",
    "# -----------------------------------------------------------------------------\n",
    "Path(Output_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_GO).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_EoC).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_A).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(Subset_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Path(Calendar_clean_file_path).parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Streaming / deduplication settings\n",
    "# -----------------------------------------------------------------------------\n",
    "CHUNK_SIZE = 2_000_000\n",
    "DATE_COL = \"PIT Date\"\n",
    "DEDUP_KEYS = [\"ID\", \"ItemCode\", DATE_COL]\n",
    "\n",
    "print(\"Paths configured. Temp outputs ->\", Temp_file_path_GO)\n",
    "print(\"Example input path ->\", Fundamentals_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           754Gi       196Gi       292Gi        55Mi       274Gi       558Gi\n",
      "Swap:             0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dam2y7Z-02IX"
   },
   "source": [
    "# 1.0. Import Data for Characteristics Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87ba0a55"
   },
   "source": [
    "### Import Data Files to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4826,
     "status": "ok",
     "timestamp": 1764881085906,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "94c3b5e3",
    "outputId": "be1e9d6b-edcd-404b-b3ac-9f1e4afe5341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imported RelevantItems.txt as DataFrame 'RelevantItems'\n",
      "Preview of 'RelevantItems':\n",
      "  ItemCode\n",
      "0    01001\n",
      "1    01051\n",
      "2    01075\n",
      "3    01101\n",
      "4    01151 \n",
      "\n",
      "\n",
      "Imported CountryCodes.txt as DataFrame 'CountryCodes'\n",
      "Preview of 'CountryCodes':\n",
      "  NatCo ImplCountry\n",
      "0   012     Algeria\n",
      "1   440   Lithuania\n",
      "2   025   Argentina\n",
      "3   442  Luxembourg\n",
      "4   036   Australia \n",
      "\n",
      "\n",
      "Imported ADR_clean.txt as DataFrame 'ADR_clean'\n",
      "Preview of 'ADR_clean':\n",
      "          ID ADRIndicator\n",
      "0  C036F63D0            N\n",
      "1  C056879S0            X\n",
      "2  C2461T100            N\n",
      "3  C2504O500            N\n",
      "4  C250C9180            N \n",
      "\n",
      "\n",
      "Imported CompanyName_clean.txt as DataFrame 'CompanyName_clean'\n",
      "Preview of 'CompanyName_clean':\n",
      "          ID                               CompanyName\n",
      "0  C00948205             AGRIFORCE GROWING SYSTEMS LTD\n",
      "1  C02500770            PEUGEOT CITROEN ARGENTINA S.A.\n",
      "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA\n",
      "3  C02520220                       ALPARGATAS S.A.I.C.\n",
      "4  C02520230               ALUAR ALUMINIO ARGENTINO SA \n",
      "\n",
      "\n",
      "Imported CurrencyCodes_clean.txt as DataFrame 'CurrencyCodes_clean'\n",
      "Preview of 'CurrencyCodes_clean':\n",
      "          ID CurrencyCode1 SwitchDate1 CurrencyCode2 SwitchDate2  \\\n",
      "0  C00948205           Usd  2021-07-09           NaN         NaN   \n",
      "1  C02500770           Ars  1995-12-29           NaN         NaN   \n",
      "2  C0250077A           Ars  1999-10-01           NaN         NaN   \n",
      "3  C0250077B           Ars  1999-10-01           NaN         NaN   \n",
      "4  C0250077C           Ars  1999-10-01           NaN         NaN   \n",
      "\n",
      "  CurrencyCode3 SwitchDate3 CurrentCurrency  \n",
      "0           NaN         NaN             Usd  \n",
      "1           NaN         NaN             Ars  \n",
      "2           NaN         NaN             Ars  \n",
      "3           NaN         NaN             Ars  \n",
      "4           NaN         NaN             Ars   \n",
      "\n",
      "\n",
      "Imported FYE_clean.txt as DataFrame 'FYE_clean'\n",
      "Preview of 'FYE_clean':\n",
      "          ID    FY FYE Month\n",
      "0  C00948205  2018  December\n",
      "1  C00948205  2019  December\n",
      "2  C00948205  2020     March\n",
      "3  C00948205  2021  December\n",
      "4  C00948205  2022  December \n",
      "\n",
      "\n",
      "Imported ID_clean.txt as DataFrame 'ID_clean'\n",
      "Preview of 'ID_clean':\n",
      "          ID\n",
      "0  C02500770\n",
      "1  C02520200\n",
      "2  C02520220\n",
      "3  C02520230\n",
      "4  C02520240 \n",
      "\n",
      "\n",
      "Imported UpdateCodes_clean.txt as DataFrame 'UpdateCodes_clean'\n",
      "Preview of 'UpdateCodes_clean':\n",
      "          ID    PIT Date Frequency FiscalPeriod UpdateCode\n",
      "0  C02500770  1995-12-29         A         1985          3\n",
      "1  C02500770  1995-12-29         A         1986          3\n",
      "2  C02500770  1995-12-29         A         1987          3\n",
      "3  C02500770  1995-12-29         A         1988          3\n",
      "4  C02500770  1995-12-29         A         1989          3 \n",
      "\n",
      "\n",
      "Imported ValueCoding.txt as DataFrame 'ValueCoding'\n",
      "Preview of 'ValueCoding':\n",
      "  ItemCode                           ItemName  Source\n",
      "0    05006               Market Price Current  Market\n",
      "1    05007      Market Price YTD High Current  Market\n",
      "2    05008       Market Price YTD Low Current  Market\n",
      "3    05009              Date of Current Price  Market\n",
      "4    05091  Market Price 52 Week High Current  Market \n",
      "\n",
      "\n",
      "Identifying subset files to process...\n",
      "  Found subset_01001.txt\n",
      "  Found subset_01051.txt\n",
      "  Found subset_01075.txt\n",
      "  Found subset_01101.txt\n",
      "  Found subset_01151.txt\n",
      "  Found subset_01250.txt\n",
      "  Found subset_01451.txt\n",
      "  Found subset_01551.txt\n",
      "  Found subset_01706.txt\n",
      "  Found subset_02001.txt\n",
      "  Found subset_02051.txt\n",
      "  Found subset_02101.txt\n",
      "  Found subset_02149.txt\n",
      "  Found subset_02201.txt\n",
      "  Found subset_02250.txt\n",
      "  Found subset_02256.txt\n",
      "  Found subset_02257.txt\n",
      "  Found subset_02258.txt\n",
      "  Found subset_02263.txt\n",
      "  Found subset_02501.txt\n",
      "  Found subset_02652.txt\n",
      "  Found subset_02999.txt\n",
      "  Found subset_03040.txt\n",
      "  Found subset_03051.txt\n",
      "  Found subset_03063.txt\n",
      "  Found subset_03066.txt\n",
      "  Found subset_03101.txt\n",
      "  Found subset_03251.txt\n",
      "  Found subset_03263.txt\n",
      "  Found subset_03273.txt\n",
      "  Found subset_03351.txt\n",
      "  Found subset_03426.txt\n",
      "  Found subset_03451.txt\n",
      "  Found subset_03501.txt\n",
      "  Found subset_04251.txt\n",
      "  Found subset_04401.txt\n",
      "  Found subset_04551.txt\n",
      "  Found subset_04601.txt\n",
      "  Found subset_04701.txt\n",
      "  Found subset_04751.txt\n",
      "  Found subset_04860.txt\n",
      "  Found subset_04870.txt\n",
      "  Found subset_04890.txt\n",
      "  Found subset_05202.txt\n",
      "  Found subset_04201.txt\n",
      "  Found subset_04225.txt\n",
      "  Found subset_04831.txt\n",
      "  Found subset_04351.txt\n",
      "  Found subset_05508.txt\n",
      "\n",
      "Identified 49 subset files for processing.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell:\n",
    "#\n",
    "#   1. Defines a helper function `import_file_to_dataframe` that reads a pipe-delimited\n",
    "#      text file into a pandas DataFrame (all columns as string; returns None on error).\n",
    "#   2. Imports a list of \"input\" files from Input_file_path into DataFrames\n",
    "#      (RelevantItems, CountryCodes, ...), storing them in globals() by filename.\n",
    "#   3. Imports a list of \"temp\" files from Temp_file_path_EoC into DataFrames\n",
    "#      (ADR_clean, CompanyName_clean, CurrencyCodes_clean, FYE_clean, ID_clean,\n",
    "#       UpdateCodes_clean, ValueCoding), also stored in globals().\n",
    "#   4. Identifies which subset_*.txt files exist in Subset_file_path based on the IDs\n",
    "#      listed in RelevantItems.txt, and records their names (without .txt) in\n",
    "#      `successful_subset_names`.\n",
    "#\n",
    "# No actual subset data is loaded here; that is deferred to later steps to keep\n",
    "# memory usage under control.\n",
    "\n",
    "\n",
    "# Function to import a file and return a pandas DataFrame\n",
    "def import_file_to_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Import a pipe-separated text file as a pandas DataFrame.\n",
    "\n",
    "    - Uses sep='|' to read pipe-delimited files.\n",
    "    - Reads all columns as strings (dtype=str), which helps preserve things like\n",
    "      leading zeros in codes (e.g., NatCo, ItemCode).\n",
    "    - Returns None on failure and prints a brief error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='|', dtype=str)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error importing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Import files from Input directory\n",
    "# -------------------------------------------------------------------------\n",
    "input_files_to_import = [\"RelevantItems.txt\", \"CountryCodes.txt\"]\n",
    "\n",
    "for file_name in input_files_to_import:\n",
    "    file_path = os.path.join(Input_file_path, file_name)\n",
    "    var_name = file_name.replace(\".txt\", \"\")  # e.g. \"RelevantItems\"\n",
    "    globals()[var_name] = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if globals()[var_name] is not None:\n",
    "        print(f\"\\nImported {file_name} as DataFrame '{var_name}'\")\n",
    "        print(f\"Preview of '{var_name}':\")\n",
    "        print(globals()[var_name].head(), \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Import files from Temp directory (end-of-cleaning stage)\n",
    "# -------------------------------------------------------------------------\n",
    "temp_files_to_import = [\n",
    "    \"ADR_clean.txt\",\n",
    "    \"CompanyName_clean.txt\",\n",
    "    \"CurrencyCodes_clean.txt\",\n",
    "    \"FYE_clean.txt\",\n",
    "    \"ID_clean.txt\",\n",
    "    \"UpdateCodes_clean.txt\",\n",
    "    \"ValueCoding.txt\"\n",
    "]\n",
    "\n",
    "for file_name in temp_files_to_import:\n",
    "    file_path = os.path.join(Temp_file_path_EoC, file_name)\n",
    "    var_name = file_name.replace(\".txt\", \"\")  # e.g. \"ADR_clean\"\n",
    "    globals()[var_name] = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if globals()[var_name] is not None:\n",
    "        print(f\"\\nImported {file_name} as DataFrame '{var_name}'\")\n",
    "        print(f\"Preview of '{var_name}':\")\n",
    "        print(globals()[var_name].head(), \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Identify subset files that exist for the relevant items\n",
    "# -------------------------------------------------------------------------\n",
    "successful_subset_names = []\n",
    "\n",
    "if 'RelevantItems' in globals() and RelevantItems is not None:\n",
    "    # Assume first column of RelevantItems holds the item IDs used in subset filenames\n",
    "    relevant_ids = RelevantItems.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "    print(\"\\nIdentifying subset files to process...\")\n",
    "    for item_id in relevant_ids:\n",
    "        file_name = f\"subset_{item_id}.txt\"\n",
    "        file_path = os.path.join(Subset_file_path, file_name)\n",
    "\n",
    "        # Check the existence of each candidate subset file\n",
    "        if os.path.exists(file_path):\n",
    "            successful_subset_names.append(f\"subset_{item_id}\")\n",
    "            print(f\"  Found {file_name}\")\n",
    "        else:\n",
    "            print(f\"  File not found: {file_name}. Skipping.\")\n",
    "\n",
    "    print(f\"\\nIdentified {len(successful_subset_names)} subset files for processing.\")\n",
    "else:\n",
    "    print(\"RelevantItems DataFrame not found or is empty. Cannot identify subset files.\")\n",
    "\n",
    "# Note: actual loading and processing of subset files happens later, in\n",
    "# batch-based steps, to manage memory usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5cb8ea8"
   },
   "source": [
    "### Check Unique IDs in DataFrames (Takes long, just Overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39829,
     "status": "ok",
     "timestamp": 1764881125744,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "729ed06c",
    "outputId": "1960cf57-76c5-4dbc-c0e4-e948886379a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique IDs in each relevant DataFrame:\n",
      "- CompanyName_clean: 104061\n",
      "- CurrencyCodes_clean: 121007\n",
      "- FYE_clean: 120948\n",
      "- ID_clean: 55367\n",
      "- UpdateCodes_clean: 111895\n",
      "- ADR_clean: 2297\n",
      "- RelevantItems: 49\n",
      "- ValueCoding: 477\n",
      "\n",
      "Number of unique IDs in each subset DataFrame:\n",
      "- subset_01001: 103974\n",
      "- subset_01051: 95686\n",
      "- subset_01075: 31347\n",
      "- subset_01101: 93284\n",
      "- subset_01151: 99264\n",
      "- subset_01250: 103792\n",
      "- subset_01451: 103210\n",
      "- subset_01551: 104010\n",
      "- subset_01706: 104003\n",
      "- subset_02001: 98243\n",
      "- subset_02051: 97864\n",
      "- subset_02101: 97742\n",
      "- subset_02149: 90304\n",
      "- subset_02201: 90853\n",
      "- subset_02250: 96558\n",
      "- subset_02256: 101294\n",
      "- subset_02257: 2367\n",
      "- subset_02258: 82485\n",
      "- subset_02263: 3934\n",
      "- subset_02501: 101374\n",
      "- subset_02652: 103894\n",
      "- subset_02999: 103985\n",
      "- subset_03040: 88043\n",
      "- subset_03051: 103339\n",
      "- subset_03063: 70004\n",
      "- subset_03066: 89836\n",
      "- subset_03101: 90852\n",
      "- subset_03251: 103886\n",
      "- subset_03263: 90934\n",
      "- subset_03273: 103941\n",
      "- subset_03351: 103982\n",
      "- subset_03426: 103286\n",
      "- subset_03451: 103532\n",
      "- subset_03501: 103996\n",
      "- subset_04251: 100332\n",
      "- subset_04401: 93111\n",
      "- subset_04551: 102969\n",
      "- subset_04601: 101287\n",
      "- subset_04701: 93854\n",
      "- subset_04751: 96088\n",
      "- subset_04860: 100313\n",
      "- subset_04870: 99434\n",
      "- subset_04890: 100204\n",
      "- subset_05202: 120325\n",
      "- subset_04201: 103520\n",
      "- subset_04225: 97426\n",
      "- subset_04831: 96328\n",
      "- subset_04351: 95104\n",
      "- subset_05508: 119179\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This diagnostic cell is intended for emergency checks only.\n",
    "#\n",
    "# It does two things:\n",
    "#   1) For a set of key DataFrames (CompanyName_clean, CurrencyCodes_clean, FYE_clean,\n",
    "#      ID_clean, UpdateCodes_clean, ADR_clean, RelevantItems, ValueCoding), it prints\n",
    "#      how many unique IDs each one contains.\n",
    "#   2) For each subset_X file listed in `successful_subset_names`, it loads the file,\n",
    "#      counts the number of unique IDs, prints that count, and then immediately frees\n",
    "#      the DataFrame from memory.\n",
    "#\n",
    "# The goal is to compare ID coverage across all relevant datasets and subsets while\n",
    "# keeping memory usage low by only loading one subset at a time.\n",
    "\n",
    "# Nur im Notfall laufen lassen\n",
    "# (Only run this for debugging / sanity checks; it can be slow on large datasets.)\n",
    "\n",
    "# List of DataFrames to check for unique IDs\n",
    "dataframes_to_check = {\n",
    "    'CompanyName_clean': CompanyName_clean,\n",
    "    'CurrencyCodes_clean': CurrencyCodes_clean,\n",
    "    'FYE_clean': FYE_clean,\n",
    "    'ID_clean': ID_clean,\n",
    "    'UpdateCodes_clean': UpdateCodes_clean,\n",
    "    'ADR_clean': ADR_clean,\n",
    "    'RelevantItems': RelevantItems,\n",
    "    'ValueCoding': ValueCoding,\n",
    "}\n",
    "\n",
    "print(\"Number of unique IDs in each relevant DataFrame:\")\n",
    "\n",
    "for df_name, df in dataframes_to_check.items():\n",
    "    if df is not None:\n",
    "        # If an 'ID' column exists, use it; otherwise fall back to the first column\n",
    "        id_column = 'ID' if 'ID' in df.columns else df.columns[0]\n",
    "        if id_column in df.columns:\n",
    "            unique_ids_count = df[id_column].nunique()\n",
    "            print(f\"- {df_name}: {unique_ids_count}\")\n",
    "        else:\n",
    "            print(f\"- {df_name}: Does not contain an 'ID' column or similar.\")\n",
    "    else:\n",
    "        print(f\"- {df_name}: DataFrame is None (not imported or empty).\")\n",
    "\n",
    "print(\"\\nNumber of unique IDs in each subset DataFrame:\")\n",
    "\n",
    "# Process subset files one by one to get summary statistics and manage memory\n",
    "if 'successful_subset_names' in globals() and successful_subset_names:\n",
    "    for subset_name in successful_subset_names:\n",
    "        # Derive raw item_id from subset_X name\n",
    "        item_id = subset_name.replace(\"subset_\", \"\")\n",
    "        file_name = f\"subset_{item_id}.txt\"\n",
    "        file_path = os.path.join(Subset_file_path, file_name)\n",
    "\n",
    "        # Import the subset file\n",
    "        subset_df = import_file_to_dataframe(file_path)\n",
    "\n",
    "        if subset_df is not None:\n",
    "            # Again, use 'ID' if present, otherwise the first column\n",
    "            id_column = 'ID' if 'ID' in subset_df.columns else subset_df.columns[0]\n",
    "            if id_column in subset_df.columns:\n",
    "                unique_ids_count = subset_df[id_column].nunique()\n",
    "                print(f\"- {subset_name}: {unique_ids_count}\")\n",
    "            else:\n",
    "                print(f\"- {subset_name}: Does not contain an 'ID' column or similar.\")\n",
    "\n",
    "            # Explicitly delete the subset_df to free memory\n",
    "            del subset_df\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"- {subset_name}: Could not be imported.\")\n",
    "else:\n",
    "    print(\"No successful subset file names found to process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHfKNOfs9oZh"
   },
   "source": [
    "# 2.0. Create Characteristics DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20284010"
   },
   "source": [
    "### Initialize Characteristics DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1764881125795,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "d9826bad",
    "outputId": "237b3f89-5e4f-4053-f59d-42741259678f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing characteristics_df with ID_clean and merging CompanyName_clean...\n",
      "Initial characteristics_df created with 55,367 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520220</td>\n",
       "      <td>ALPARGATAS S.A.I.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520230</td>\n",
       "      <td>ALUAR ALUMINIO ARGENTINO SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520240</td>\n",
       "      <td>ASTRA COMPANIA ARGENTINA DE PETROLEO SA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName\n",
       "0  C02500770            PEUGEOT CITROEN ARGENTINA S.A.\n",
       "1  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA\n",
       "2  C02520220                       ALPARGATAS S.A.I.C.\n",
       "3  C02520230               ALUAR ALUMINIO ARGENTINO SA\n",
       "4  C02520240   ASTRA COMPANIA ARGENTINA DE PETROLEO SA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block initializes a characteristics_df DataFrame that will hold core\n",
    "# company-level attributes.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Check that ID_clean and CompanyName_clean exist, are not None, and are non-empty.\n",
    "#   2. Normalize the 'ID' column in both DataFrames to string (for safe merging).\n",
    "#   3. Build characteristics_df from the unique IDs in ID_clean.\n",
    "#   4. Merge the corresponding 'CompanyName' from CompanyName_clean onto characteristics_df.\n",
    "#   5. Print basic info (row count) and show a sample.\n",
    "#   6. If prerequisites are missing, create an empty characteristics_df instead.\n",
    "#   7. Trigger garbage collection at the end.\n",
    "\n",
    "\n",
    "if (\n",
    "    'ID_clean' in globals() and ID_clean is not None and not ID_clean.empty and\n",
    "    'CompanyName_clean' in globals() and CompanyName_clean is not None and not CompanyName_clean.empty\n",
    "):\n",
    "    print(\"Initializing characteristics_df with ID_clean and merging CompanyName_clean...\")\n",
    "\n",
    "    # Ensure IDs are comparable between ID_clean and CompanyName_clean\n",
    "    ID_clean['ID'] = ID_clean['ID'].astype(str)\n",
    "    CompanyName_clean['ID'] = CompanyName_clean['ID'].astype(str)\n",
    "\n",
    "    # Start from unique IDs only to avoid duplicates in the base characteristics table\n",
    "    characteristics_df = ID_clean[['ID']].drop_duplicates().copy()\n",
    "\n",
    "    # Merge CompanyName from CompanyName_clean onto characteristics_df by ID\n",
    "    characteristics_df = characteristics_df.merge(\n",
    "        CompanyName_clean[['ID', 'CompanyName']].drop_duplicates(),\n",
    "        on='ID',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Column already named 'CompanyName'; rename is effectively a no-op but kept for clarity\n",
    "    characteristics_df.rename(columns={'CompanyName': 'CompanyName'}, inplace=True)\n",
    "\n",
    "    print(f\"Initial characteristics_df created with {len(characteristics_df):,} rows.\")\n",
    "    display(characteristics_df.head())\n",
    "\n",
    "else:\n",
    "    # If either ID_clean or CompanyName_clean is missing or empty, initialize an empty DataFrame\n",
    "    print(\"ID_clean or CompanyName_clean not found or empty. Cannot initialize characteristics_df.\")\n",
    "    characteristics_df = pd.DataFrame()\n",
    "\n",
    "# Run garbage collection to clean up any unused objects\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00e2167f"
   },
   "source": [
    "### Extract and Merge Implied Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1764881125837,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "f2c70aa4",
    "outputId": "b5847829-1445-4231-fe97-65ffc64bbe13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplNatCo</th>\n",
       "      <th>ImplCountry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>025</td>\n",
       "      <td>Argentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>025</td>\n",
       "      <td>Argentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520220</td>\n",
       "      <td>ALPARGATAS S.A.I.C.</td>\n",
       "      <td>025</td>\n",
       "      <td>Argentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520230</td>\n",
       "      <td>ALUAR ALUMINIO ARGENTINO SA</td>\n",
       "      <td>025</td>\n",
       "      <td>Argentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520240</td>\n",
       "      <td>ASTRA COMPANIA ARGENTINA DE PETROLEO SA</td>\n",
       "      <td>025</td>\n",
       "      <td>Argentina</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName ImplNatCo ImplCountry\n",
       "0  C02500770            PEUGEOT CITROEN ARGENTINA S.A.       025   Argentina\n",
       "1  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA       025   Argentina\n",
       "2  C02520220                       ALPARGATAS S.A.I.C.       025   Argentina\n",
       "3  C02520230               ALUAR ALUMINIO ARGENTINO SA       025   Argentina\n",
       "4  C02520240   ASTRA COMPANIA ARGENTINA DE PETROLEO SA       025   Argentina"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty values per column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ID             0\n",
       "CompanyName    0\n",
       "ImplNatCo      0\n",
       "ImplCountry    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique IDs in characteristics_df: 55367\n"
     ]
    }
   ],
   "source": [
    "# Extract the 2nd to 4th digit from the 'ID' column and create 'ImplNatCo'\n",
    "characteristics_df['ImplNatCo'] = characteristics_df['ID'].str[1:4]\n",
    "\n",
    "# Convert 'NatCo' in CountryCodes to string type to ensure consistent data types for merging\n",
    "CountryCodes['NatCo'] = CountryCodes['NatCo'].astype(str)\n",
    "\n",
    "# Merge characteristics_df with CountryCodes\n",
    "characteristics_df = pd.merge(characteristics_df, CountryCodes, left_on='ImplNatCo', right_on='NatCo', how='left')\n",
    "\n",
    "# Drop the redundant 'NatCo' column after merging\n",
    "characteristics_df.drop('NatCo', axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "display(characteristics_df.head())\n",
    "\n",
    "# Check for empty values per column\n",
    "empty_values_per_column = characteristics_df.isnull().sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Number of empty values per column:\")\n",
    "display(empty_values_per_column)\n",
    "\n",
    "# Print the number of unique IDs in characteristics_df\n",
    "print(f\"\\nNumber of unique IDs in characteristics_df: {characteristics_df['ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79336c1f"
   },
   "source": [
    "### Merge Current Currency Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1764881125882,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "12df1e41",
    "outputId": "aa082f8c-7a50-4390-ee2b-02914de552fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplNatCo</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>025</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>025</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520220</td>\n",
       "      <td>ALPARGATAS S.A.I.C.</td>\n",
       "      <td>025</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520230</td>\n",
       "      <td>ALUAR ALUMINIO ARGENTINO SA</td>\n",
       "      <td>025</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520240</td>\n",
       "      <td>ASTRA COMPANIA ARGENTINA DE PETROLEO SA</td>\n",
       "      <td>025</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName ImplNatCo ImplCountry  \\\n",
       "0  C02500770            PEUGEOT CITROEN ARGENTINA S.A.       025   Argentina   \n",
       "1  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA       025   Argentina   \n",
       "2  C02520220                       ALPARGATAS S.A.I.C.       025   Argentina   \n",
       "3  C02520230               ALUAR ALUMINIO ARGENTINO SA       025   Argentina   \n",
       "4  C02520240   ASTRA COMPANIA ARGENTINA DE PETROLEO SA       025   Argentina   \n",
       "\n",
       "  CurrentCurrency  \n",
       "0             Ars  \n",
       "1             Ars  \n",
       "2             Ars  \n",
       "3             Ars  \n",
       "4             Ars  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty values per column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "CompanyName        0\n",
       "ImplNatCo          0\n",
       "ImplCountry        0\n",
       "CurrentCurrency    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique IDs in characteristics_df: 55367\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block enriches characteristics_df by adding each entity's CurrentCurrency\n",
    "# from CurrencyCodes_clean.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Select only the necessary columns (ID, CurrentCurrency) from CurrencyCodes_clean.\n",
    "#   2. Perform a left-merge onto characteristics_df using ID as the join key.\n",
    "#   3. Display a preview of the updated characteristics_df.\n",
    "#   4. Count how many missing (NaN) values appear in each column after the merge.\n",
    "#   5. Report the number of unique IDs present.\n",
    "\n",
    "\n",
    "# Extract the minimal set of columns needed for the merge\n",
    "currency_subset = CurrencyCodes_clean[['ID', 'CurrentCurrency']].copy()\n",
    "\n",
    "# Merge currency data onto characteristics_df by ID\n",
    "characteristics_df = pd.merge(characteristics_df, currency_subset, on='ID', how='left')\n",
    "\n",
    "# Show a sample of the enriched DataFrame\n",
    "display(characteristics_df.head())\n",
    "\n",
    "# Count missing values across all columns\n",
    "empty_values_per_column = characteristics_df.isnull().sum()\n",
    "\n",
    "print(\"Number of empty values per column:\")\n",
    "display(empty_values_per_column)\n",
    "\n",
    "# Report the number of unique IDs\n",
    "print(f\"\\nNumber of unique IDs in characteristics_df: {characteristics_df['ID'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf38b32a"
   },
   "source": [
    "### Save Cleaned Characteristics DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1764881125925,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "e2c67ac0",
    "outputId": "3bda4e94-d16b-435e-d024-ccb13888f659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved successfully to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Characteristics_clean.txt\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block writes the finalized characteristics_df to disk as\n",
    "# Characteristics_clean.txt.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Construct the output file path inside Temp_file_path_DP.\n",
    "#   2. Remove the ImplNatCo column before saving.\n",
    "#   3. Save the cleaned DataFrame as a pipe-delimited text file.\n",
    "#   4. Confirm completion.\n",
    "\n",
    "\n",
    "# Define the output path for the characteristics file\n",
    "output_file = os.path.join(Temp_file_path_DP, \"Characteristics_clean.txt\")\n",
    "\n",
    "# Remove ImplNatCo prior to saving (ADRIndicator was already absent or removed)\n",
    "characteristics_df_to_save = characteristics_df.drop(columns=['ImplNatCo'])\n",
    "\n",
    "# Save characteristics_df as a pipe-delimited text file\n",
    "characteristics_df_to_save.to_csv(output_file, sep='|', index=False)\n",
    "\n",
    "print(f\"DataFrame saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnoP3Nt_cV8O"
   },
   "source": [
    "\n",
    "# 3.0. Preparation of Main Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QytTd1m1VzRK"
   },
   "source": [
    "## Worldscope PIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d2759d6"
   },
   "source": [
    "### Merge Subset Data with Characteristics (Batch Processing) and Removal of Rows (e.g. ADR/Security Rows Which are still in DFs but not in ID_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15307,
     "status": "ok",
     "timestamp": 1764881141237,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "80f23ddc",
    "outputId": "0b6709b8-c90a-4604-b7a5-32603e98dc91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded characteristics data from /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Characteristics_clean.txt\n",
      "Identified 49 subset files to process.\n",
      "\n",
      "Filtering subset files to include only IDs present in characteristics_clean_df (55367 valid IDs).\n",
      "Processing batch 1 of subsets: ['subset_01001', 'subset_01051', 'subset_01075', 'subset_01101', 'subset_01151', 'subset_01250', 'subset_01451', 'subset_01551', 'subset_01706', 'subset_02001']\n",
      "  Reading subset_01001.txt from disk...\n",
      "  Processing subset_01001 (shape: (6805130, 6))\n",
      "    Filtered subset_01001: 2802001 rows removed.\n",
      "  Merging filtered subset_01001 (shape: (4003129, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_01001 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01001.txt\n",
      "  Reading subset_01051.txt from disk...\n",
      "  Processing subset_01051 (shape: (6188353, 6))\n",
      "    Filtered subset_01051: 1961350 rows removed.\n",
      "  Merging filtered subset_01051 (shape: (4227003, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_01051 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01051.txt\n",
      "  Reading subset_01075.txt from disk...\n",
      "  Processing subset_01075 (shape: (868117, 6))\n",
      "    Filtered subset_01075: 747088 rows removed.\n",
      "  Merging filtered subset_01075 (shape: (121029, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_01075 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01075.txt\n",
      "  Reading subset_01101.txt from disk...\n",
      "  Processing subset_01101 (shape: (4668450, 6))\n",
      "    Filtered subset_01101: 1589585 rows removed.\n",
      "  Merging filtered subset_01101 (shape: (3078865, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_01101 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01101.txt\n",
      "  Reading subset_01151.txt from disk...\n",
      "  Processing subset_01151 (shape: (5324276, 6))\n",
      "    Filtered subset_01151: 1934054 rows removed.\n",
      "  Merging filtered subset_01151 (shape: (3390222, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_01151 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01151.txt\n",
      "  Reading subset_01250.txt from disk...\n",
      "  Processing subset_01250 (shape: (7067874, 6))\n",
      "    Filtered subset_01250: 2813961 rows removed.\n",
      "  Merging filtered subset_01250 (shape: (4253913, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_01250 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01250.txt\n",
      "  Reading subset_01451.txt from disk...\n",
      "  Processing subset_01451 (shape: (5866087, 6))\n",
      "    Filtered subset_01451: 2309312 rows removed.\n",
      "  Merging filtered subset_01451 (shape: (3556775, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_01451 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01451.txt\n",
      "  Reading subset_01551.txt from disk...\n",
      "  Processing subset_01551 (shape: (6549897, 6))\n",
      "    Filtered subset_01551: 2643098 rows removed.\n",
      "  Merging filtered subset_01551 (shape: (3906799, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_01551 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01551.txt\n",
      "  Reading subset_01706.txt from disk...\n",
      "  Processing subset_01706 (shape: (6235577, 6))\n",
      "    Filtered subset_01706: 2484879 rows removed.\n",
      "  Merging filtered subset_01706 (shape: (3750698, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_01706 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01706.txt\n",
      "  Reading subset_02001.txt from disk...\n",
      "  Processing subset_02001 (shape: (5610152, 6))\n",
      "    Filtered subset_02001: 1986000 rows removed.\n",
      "  Merging filtered subset_02001 (shape: (3624152, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02001 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02001.txt\n",
      "Processing batch 2 of subsets: ['subset_02051', 'subset_02101', 'subset_02149', 'subset_02201', 'subset_02250', 'subset_02256', 'subset_02257', 'subset_02258', 'subset_02263', 'subset_02501']\n",
      "  Reading subset_02051.txt from disk...\n",
      "  Processing subset_02051 (shape: (5605736, 6))\n",
      "    Filtered subset_02051: 1847194 rows removed.\n",
      "  Merging filtered subset_02051 (shape: (3758542, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02051 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02051.txt\n",
      "  Reading subset_02101.txt from disk...\n",
      "  Processing subset_02101 (shape: (5127941, 6))\n",
      "    Filtered subset_02101: 1675167 rows removed.\n",
      "  Merging filtered subset_02101 (shape: (3452774, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02101 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02101.txt\n",
      "  Reading subset_02149.txt from disk...\n",
      "  Processing subset_02149 (shape: (5973856, 6))\n",
      "    Filtered subset_02149: 1716428 rows removed.\n",
      "  Merging filtered subset_02149 (shape: (4257428, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02149 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02149.txt\n",
      "  Reading subset_02201.txt from disk...\n",
      "  Processing subset_02201 (shape: (5082397, 6))\n",
      "    Filtered subset_02201: 1530756 rows removed.\n",
      "  Merging filtered subset_02201 (shape: (3551641, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02201 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02201.txt\n",
      "  Reading subset_02250.txt from disk...\n",
      "  Processing subset_02250 (shape: (2001452, 6))\n",
      "    Filtered subset_02250: 858390 rows removed.\n",
      "  Merging filtered subset_02250 (shape: (1143062, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02250 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02250.txt\n",
      "  Reading subset_02256.txt from disk...\n",
      "  Processing subset_02256 (shape: (4663969, 6))\n",
      "    Filtered subset_02256: 1902696 rows removed.\n",
      "  Merging filtered subset_02256 (shape: (2761273, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02256 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02256.txt\n",
      "  Reading subset_02257.txt from disk...\n",
      "  Processing subset_02257 (shape: (20579, 6))\n",
      "    Filtered subset_02257: 20106 rows removed.\n",
      "  Merging filtered subset_02257 (shape: (473, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02257 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02257.txt\n",
      "  Reading subset_02258.txt from disk...\n",
      "  Processing subset_02258 (shape: (1182054, 6))\n",
      "    Filtered subset_02258: 343379 rows removed.\n",
      "  Merging filtered subset_02258 (shape: (838675, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02258 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02258.txt\n",
      "  Reading subset_02263.txt from disk...\n",
      "  Processing subset_02263 (shape: (60031, 6))\n",
      "    Filtered subset_02263: 60011 rows removed.\n",
      "  Merging filtered subset_02263 (shape: (20, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02263 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02263.txt\n",
      "  Reading subset_02501.txt from disk...\n",
      "  Processing subset_02501 (shape: (5821650, 6))\n",
      "    Filtered subset_02501: 2207993 rows removed.\n",
      "  Merging filtered subset_02501 (shape: (3613657, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02501 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02501.txt\n",
      "Processing batch 3 of subsets: ['subset_02652', 'subset_02999', 'subset_03040', 'subset_03051', 'subset_03063', 'subset_03066', 'subset_03101', 'subset_03251', 'subset_03263', 'subset_03273']\n",
      "  Reading subset_02652.txt from disk...\n",
      "  Processing subset_02652 (shape: (6914882, 6))\n",
      "    Filtered subset_02652: 2931263 rows removed.\n",
      "  Merging filtered subset_02652 (shape: (3983619, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02652 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02652.txt\n",
      "  Reading subset_02999.txt from disk...\n",
      "  Processing subset_02999 (shape: (5978838, 6))\n",
      "    Filtered subset_02999: 2387862 rows removed.\n",
      "  Merging filtered subset_02999 (shape: (3590976, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_02999 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02999.txt\n",
      "  Reading subset_03040.txt from disk...\n",
      "  Processing subset_03040 (shape: (4404164, 6))\n",
      "    Filtered subset_03040: 1345301 rows removed.\n",
      "  Merging filtered subset_03040 (shape: (3058863, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_03040 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03040.txt\n",
      "  Reading subset_03051.txt from disk...\n",
      "  Processing subset_03051 (shape: (5757531, 6))\n",
      "    Filtered subset_03051: 2222207 rows removed.\n",
      "  Merging filtered subset_03051 (shape: (3535324, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_03051 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03051.txt\n",
      "  Reading subset_03063.txt from disk...\n",
      "  Processing subset_03063 (shape: (2669447, 6))\n",
      "    Filtered subset_03063: 689811 rows removed.\n",
      "  Merging filtered subset_03063 (shape: (1979636, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_03063 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03063.txt\n",
      "  Reading subset_03066.txt from disk...\n",
      "  Processing subset_03066 (shape: (6006262, 6))\n",
      "    Filtered subset_03066: 1734579 rows removed.\n",
      "  Merging filtered subset_03066 (shape: (4271683, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_03066 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03066.txt\n",
      "  Reading subset_03101.txt from disk...\n",
      "  Processing subset_03101 (shape: (5107218, 6))\n",
      "    Filtered subset_03101: 1538954 rows removed.\n",
      "  Merging filtered subset_03101 (shape: (3568264, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_03101 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03101.txt\n",
      "  Reading subset_03251.txt from disk...\n",
      "  Processing subset_03251 (shape: (5762661, 6))\n",
      "    Filtered subset_03251: 2293628 rows removed.\n",
      "  Merging filtered subset_03251 (shape: (3469033, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_03251 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03251.txt\n",
      "  Reading subset_03263.txt from disk...\n",
      "  Processing subset_03263 (shape: (4453589, 6))\n",
      "    Filtered subset_03263: 1591793 rows removed.\n",
      "  Merging filtered subset_03263 (shape: (2861796, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_03263 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03263.txt\n",
      "  Reading subset_03273.txt from disk...\n",
      "  Processing subset_03273 (shape: (6575549, 6))\n",
      "    Filtered subset_03273: 2824911 rows removed.\n",
      "  Merging filtered subset_03273 (shape: (3750638, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_03273 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03273.txt\n",
      "Processing batch 4 of subsets: ['subset_03351', 'subset_03426', 'subset_03451', 'subset_03501', 'subset_04251', 'subset_04401', 'subset_04551', 'subset_04601', 'subset_04701', 'subset_04751']\n",
      "  Reading subset_03351.txt from disk...\n",
      "  Processing subset_03351 (shape: (6159877, 6))\n",
      "    Filtered subset_03351: 2428361 rows removed.\n",
      "  Merging filtered subset_03351 (shape: (3731516, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_03351 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03351.txt\n",
      "  Reading subset_03426.txt from disk...\n",
      "  Processing subset_03426 (shape: (5398258, 6))\n",
      "    Filtered subset_03426: 2133877 rows removed.\n",
      "  Merging filtered subset_03426 (shape: (3264381, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_03426 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03426.txt\n",
      "  Reading subset_03451.txt from disk...\n",
      "  Processing subset_03451 (shape: (5401443, 6))\n",
      "    Filtered subset_03451: 2140210 rows removed.\n",
      "  Merging filtered subset_03451 (shape: (3261233, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_03451 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03451.txt\n",
      "  Reading subset_03501.txt from disk...\n",
      "  Processing subset_03501 (shape: (6040635, 6))\n",
      "    Filtered subset_03501: 2407362 rows removed.\n",
      "  Merging filtered subset_03501 (shape: (3633273, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_03501 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03501.txt\n",
      "  Reading subset_04251.txt from disk...\n",
      "  Processing subset_04251 (shape: (2711818, 6))\n",
      "    Filtered subset_04251: 1100780 rows removed.\n",
      "  Merging filtered subset_04251 (shape: (1611038, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04251 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04251.txt\n",
      "  Reading subset_04401.txt from disk...\n",
      "  Processing subset_04401 (shape: (2212031, 6))\n",
      "    Filtered subset_04401: 830409 rows removed.\n",
      "  Merging filtered subset_04401 (shape: (1381622, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04401 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04401.txt\n",
      "  Reading subset_04551.txt from disk...\n",
      "  Processing subset_04551 (shape: (2985507, 6))\n",
      "    Filtered subset_04551: 1198142 rows removed.\n",
      "  Merging filtered subset_04551 (shape: (1787365, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04551 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04551.txt\n",
      "  Reading subset_04601.txt from disk...\n",
      "  Processing subset_04601 (shape: (3138358, 6))\n",
      "    Filtered subset_04601: 1182316 rows removed.\n",
      "  Merging filtered subset_04601 (shape: (1956042, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04601 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04601.txt\n",
      "  Reading subset_04701.txt from disk...\n",
      "  Processing subset_04701 (shape: (2358225, 6))\n",
      "    Filtered subset_04701: 879959 rows removed.\n",
      "  Merging filtered subset_04701 (shape: (1478266, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04701 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04701.txt\n",
      "  Reading subset_04751.txt from disk...\n",
      "  Processing subset_04751 (shape: (2408046, 6))\n",
      "    Filtered subset_04751: 960978 rows removed.\n",
      "  Merging filtered subset_04751 (shape: (1447068, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04751 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04751.txt\n",
      "Processing batch 5 of subsets: ['subset_04860', 'subset_04870', 'subset_04890', 'subset_05202', 'subset_04201', 'subset_04225', 'subset_04831', 'subset_04351', 'subset_05508']\n",
      "  Reading subset_04860.txt from disk...\n",
      "  Processing subset_04860 (shape: (3398342, 6))\n",
      "    Filtered subset_04860: 1345766 rows removed.\n",
      "  Merging filtered subset_04860 (shape: (2052576, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04860 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04860.txt\n",
      "  Reading subset_04870.txt from disk...\n",
      "  Processing subset_04870 (shape: (3242884, 6))\n",
      "    Filtered subset_04870: 1263745 rows removed.\n",
      "  Merging filtered subset_04870 (shape: (1979139, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04870 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04870.txt\n",
      "  Reading subset_04890.txt from disk...\n",
      "  Processing subset_04890 (shape: (3255288, 6))\n",
      "    Filtered subset_04890: 1279858 rows removed.\n",
      "  Merging filtered subset_04890 (shape: (1975430, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04890 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04890.txt\n",
      "  Reading subset_05202.txt from disk...\n",
      "  Processing subset_05202 (shape: (16142369, 6))\n",
      "    Filtered subset_05202: 7342457 rows removed.\n",
      "  Merging filtered subset_05202 (shape: (8799912, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_05202 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_05202.txt\n",
      "  Reading subset_04201.txt from disk...\n",
      "  Processing subset_04201 (shape: (3979485, 6))\n",
      "    Filtered subset_04201: 1546405 rows removed.\n",
      "  Merging filtered subset_04201 (shape: (2433080, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04201 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04201.txt\n",
      "  Reading subset_04225.txt from disk...\n",
      "  Processing subset_04225 (shape: (2514173, 6))\n",
      "    Filtered subset_04225: 1010947 rows removed.\n",
      "  Merging filtered subset_04225 (shape: (1503226, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04225 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04225.txt\n",
      "  Reading subset_04831.txt from disk...\n",
      "  Processing subset_04831 (shape: (2811572, 6))\n",
      "    Filtered subset_04831: 1116843 rows removed.\n",
      "  Merging filtered subset_04831 (shape: (1694729, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04831 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04831.txt\n",
      "  Reading subset_04351.txt from disk...\n",
      "  Processing subset_04351 (shape: (2437500, 6))\n",
      "    Filtered subset_04351: 953433 rows removed.\n",
      "  Merging filtered subset_04351 (shape: (1484067, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_04351 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04351.txt\n",
      "  Reading subset_05508.txt from disk...\n",
      "  Processing subset_05508 (shape: (3597776, 6))\n",
      "    Filtered subset_05508: 1668133 rows removed.\n",
      "  Merging filtered subset_05508 (shape: (1929643, 6)) with characteristics_clean_df\n",
      "    Saved work_subset_05508 to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_05508.txt\n",
      "\n",
      "Merging complete. All valid subset DataFrames were merged with characteristics_clean_df and saved as work_subset_X.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell merges many subset_*.txt files with the master characteristics_clean_df.\n",
    "# Imports and library loading have been intentionally removed (you said you will add\n",
    "# them at the beginning).\n",
    "#\n",
    "# The workflow:\n",
    "#   1. Load Characteristics_clean.txt into characteristics_clean_df.\n",
    "#   2. Determine which subset files should be processed (successful_subset_names).\n",
    "#   3. Extract valid IDs from characteristics_clean_df to filter subset files.\n",
    "#   4. Process subset files in batches to avoid high memory usage:\n",
    "#         - Read subset file from disk\n",
    "#         - Filter rows by valid IDs\n",
    "#         - Merge with characteristics_clean_df (left merge on \"ID\")\n",
    "#         - Reorder columns so characteristics appear first\n",
    "#         - Save result as work_subset_<name>.txt in Temp_file_path_DP\n",
    "#         - Delete temporary DataFrames to free memory\n",
    "#   5. Print progress messages and summary at the end.\n",
    "#\n",
    "# No merged DataFrames are kept in memory — everything is streamed batch-wise.\n",
    "\n",
    "\n",
    "# Iterate and merge for all subset DataFrames in batches to manage memory\n",
    "merged_subset_files = {}\n",
    "batch_size = 10  # Adjust batch size as needed\n",
    "\n",
    "# Path of Characteristics_clean dataset\n",
    "characteristics_clean_file_path = os.path.join(Temp_file_path_DP, \"Characteristics_clean.txt\")\n",
    "\n",
    "# Load Characteristics_clean.txt\n",
    "characteristics_clean_df = import_file_to_dataframe(characteristics_clean_file_path)\n",
    "\n",
    "if characteristics_clean_df is not None and not characteristics_clean_df.empty:\n",
    "    print(f\"Loaded characteristics data from {characteristics_clean_file_path}\")\n",
    "\n",
    "    # Save list of characteristics columns so we can reorder merged files later\n",
    "    characteristics_cols = characteristics_clean_df.columns.tolist()\n",
    "\n",
    "    # Retrieve set of subset names from earlier processing\n",
    "    if 'successful_subset_names' in globals() and successful_subset_names:\n",
    "        subset_names_to_process = successful_subset_names\n",
    "        print(f\"Identified {len(subset_names_to_process)} subset files to process.\")\n",
    "    else:\n",
    "        subset_names_to_process = []\n",
    "        print(\"No successful subset file names found to process.\")\n",
    "\n",
    "    # Ensure target directory exists\n",
    "    os.makedirs(Temp_file_path_DP, exist_ok=True)\n",
    "\n",
    "    # Extract the list of valid IDs from the characteristics file\n",
    "    valid_ids = []\n",
    "    if 'ID' in characteristics_clean_df.columns:\n",
    "        valid_ids = characteristics_clean_df['ID'].tolist()\n",
    "        print(f\"\\nFiltering subset files to include only IDs present in characteristics_clean_df ({len(valid_ids)} valid IDs).\")\n",
    "    else:\n",
    "        print(\"\\ncharacteristics_clean_df has no 'ID' column. Skipping ID filtering.\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # Process subset files in batches\n",
    "    # ----------------------------------------------------------------------\n",
    "    for i in range(0, len(subset_names_to_process), batch_size):\n",
    "        batch_names = subset_names_to_process[i:i + batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1} of subsets: {batch_names}\")\n",
    "\n",
    "        for subset_name in batch_names:\n",
    "\n",
    "            # Reconstruct file name on disk\n",
    "            item_id = subset_name.replace(\"subset_\", \"\")\n",
    "            file_name = f\"subset_{item_id}.txt\"\n",
    "            file_path = os.path.join(Subset_file_path, file_name)\n",
    "\n",
    "            print(f\"  Reading {file_name} from disk...\")\n",
    "            subset_df = import_file_to_dataframe(file_path)\n",
    "\n",
    "            if subset_df is not None:\n",
    "                print(f\"  Processing {subset_name} (shape: {subset_df.shape})\")\n",
    "\n",
    "                # ---------------------------\n",
    "                # Apply ID filtering\n",
    "                # ---------------------------\n",
    "                if valid_ids and 'ID' in subset_df.columns:\n",
    "                    initial_rows = len(subset_df)\n",
    "                    subset_df = subset_df[subset_df['ID'].isin(valid_ids)].copy()\n",
    "                    removed = initial_rows - len(subset_df)\n",
    "                    print(f\"    Filtered {subset_name}: {removed} rows removed.\")\n",
    "                elif not valid_ids:\n",
    "                    print(f\"    No valid IDs available — skipping ID filtering.\")\n",
    "                else:\n",
    "                    print(f\"    'ID' column missing — skipping ID filtering.\")\n",
    "\n",
    "                # ---------------------------\n",
    "                # Merge with characteristics_clean_df\n",
    "                # ---------------------------\n",
    "                print(f\"  Merging filtered {subset_name} (shape: {subset_df.shape}) with characteristics_clean_df\")\n",
    "                merged_df = pd.merge(subset_df, characteristics_clean_df, on='ID', how='left')\n",
    "\n",
    "                # ---------------------------\n",
    "                # Reorder columns: characteristics first\n",
    "                # ---------------------------\n",
    "                subset_cols = [c for c in merged_df.columns if c not in characteristics_cols]\n",
    "                merged_df = merged_df[characteristics_cols + subset_cols]\n",
    "\n",
    "                # ---------------------------\n",
    "                # Save merged result\n",
    "                # ---------------------------\n",
    "                work_subset_name = subset_name.replace(\"subset_\", \"work_subset_\")\n",
    "                output_file = os.path.join(Temp_file_path_DP, f\"{work_subset_name}.txt\")\n",
    "                merged_df.to_csv(output_file, sep='|', index=False)\n",
    "                print(f\"    Saved {work_subset_name} to {output_file}\")\n",
    "\n",
    "                # Free memory immediately\n",
    "                del subset_df\n",
    "                del merged_df\n",
    "                gc.collect()\n",
    "\n",
    "    print(\"\\nMerging complete. All valid subset DataFrames were merged with characteristics_clean_df and saved as work_subset_X.\")\n",
    "else:\n",
    "    print(f\"Error: Could not load characteristics data from {characteristics_clean_file_path}. Skipping subset merging.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e99GuzVzbNTy"
   },
   "source": [
    "### Check for Missing Values (Should be 0) and Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14934,
     "status": "ok",
     "timestamp": 1764881156186,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ed228cfa",
    "outputId": "c45eb126-f50b-451a-d257-28496bbd0a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping rows with 'n' in Value column from work_subset files...\n",
      "\n",
      "--- Processing work_subset_01101 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_01101 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01101.txt\n",
      "\n",
      "--- Processing work_subset_03351 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_03351 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03351.txt\n",
      "\n",
      "--- Processing work_subset_04751 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04751 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04751.txt\n",
      "\n",
      "--- Processing work_subset_03263 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_03263 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03263.txt\n",
      "\n",
      "--- Processing work_subset_04890 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04890 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04890.txt\n",
      "\n",
      "--- Processing work_subset_03101 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_03101 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03101.txt\n",
      "\n",
      "--- Processing work_subset_01001 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_01001 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01001.txt\n",
      "\n",
      "--- Processing work_subset_04401 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04401 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04401.txt\n",
      "\n",
      "--- Processing work_subset_03251 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_03251 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03251.txt\n",
      "\n",
      "--- Processing work_subset_03040 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_03040 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03040.txt\n",
      "\n",
      "--- Processing work_subset_03273 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_03273 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03273.txt\n",
      "\n",
      "--- Processing work_subset_01250 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_01250 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01250.txt\n",
      "\n",
      "--- Processing work_subset_03426 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_03426 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03426.txt\n",
      "\n",
      "--- Processing work_subset_04701 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04701 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04701.txt\n",
      "\n",
      "--- Processing work_subset_01051 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_01051 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01051.txt\n",
      "\n",
      "--- Processing work_subset_03063 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_03063 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03063.txt\n",
      "\n",
      "--- Processing work_subset_04551 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04551 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04551.txt\n",
      "\n",
      "--- Processing work_subset_02999 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02999 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02999.txt\n",
      "\n",
      "--- Processing work_subset_04601 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04601 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04601.txt\n",
      "\n",
      "--- Processing work_subset_01151 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_01151 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01151.txt\n",
      "\n",
      "--- Processing work_subset_01075 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_01075 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01075.txt\n",
      "\n",
      "--- Processing work_subset_02501 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02501 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02501.txt\n",
      "\n",
      "--- Processing work_subset_03051 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_03051 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03051.txt\n",
      "\n",
      "--- Processing work_subset_04251 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04251 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04251.txt\n",
      "\n",
      "--- Processing work_subset_02652 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02652 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02652.txt\n",
      "\n",
      "--- Processing work_subset_02201 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02201 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02201.txt\n",
      "\n",
      "--- Processing work_subset_04831 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04831 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04831.txt\n",
      "\n",
      "--- Processing work_subset_03501 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_03501 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03501.txt\n",
      "\n",
      "--- Processing work_subset_02051 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02051 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02051.txt\n",
      "\n",
      "--- Processing work_subset_04351 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04351 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04351.txt\n",
      "\n",
      "--- Processing work_subset_02149 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02149 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02149.txt\n",
      "\n",
      "--- Processing work_subset_04860 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04860 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04860.txt\n",
      "\n",
      "--- Processing work_subset_02257 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02257 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02257.txt\n",
      "\n",
      "--- Processing work_subset_02250 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02250 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02250.txt\n",
      "\n",
      "--- Processing work_subset_05202 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_05202 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_05202.txt\n",
      "\n",
      "--- Processing work_subset_05508 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_05508 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_05508.txt\n",
      "\n",
      "--- Processing work_subset_01706 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_01706 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01706.txt\n",
      "\n",
      "--- Processing work_subset_01451 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_01451 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01451.txt\n",
      "\n",
      "--- Processing work_subset_02001 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02001 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02001.txt\n",
      "\n",
      "--- Processing work_subset_04870 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04870 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04870.txt\n",
      "\n",
      "--- Processing work_subset_03066 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_03066 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03066.txt\n",
      "\n",
      "--- Processing work_subset_04225 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04225 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04225.txt\n",
      "\n",
      "--- Processing work_subset_02256 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02256 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02256.txt\n",
      "\n",
      "--- Processing work_subset_02258 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02258 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02258.txt\n",
      "\n",
      "--- Processing work_subset_02101 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02101 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02101.txt\n",
      "\n",
      "--- Processing work_subset_01551 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_01551 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_01551.txt\n",
      "\n",
      "--- Processing work_subset_03451 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_03451 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_03451.txt\n",
      "\n",
      "--- Processing work_subset_02263 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_02263 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_02263.txt\n",
      "\n",
      "--- Processing work_subset_04201 ---\n",
      "  No rows with 'n' in Value column found.\n",
      "  Saved cleaned work_subset_04201 back to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_04201.txt\n",
      "\n",
      "Summary of rows dropped due to 'n' in Value column from work_subset files:\n",
      "- work_subset_01101: 0 rows dropped\n",
      "- work_subset_03351: 0 rows dropped\n",
      "- work_subset_04751: 0 rows dropped\n",
      "- work_subset_03263: 0 rows dropped\n",
      "- work_subset_04890: 0 rows dropped\n",
      "- work_subset_03101: 0 rows dropped\n",
      "- work_subset_01001: 0 rows dropped\n",
      "- work_subset_04401: 0 rows dropped\n",
      "- work_subset_03251: 0 rows dropped\n",
      "- work_subset_03040: 0 rows dropped\n",
      "- work_subset_03273: 0 rows dropped\n",
      "- work_subset_01250: 0 rows dropped\n",
      "- work_subset_03426: 0 rows dropped\n",
      "- work_subset_04701: 0 rows dropped\n",
      "- work_subset_01051: 0 rows dropped\n",
      "- work_subset_03063: 0 rows dropped\n",
      "- work_subset_04551: 0 rows dropped\n",
      "- work_subset_02999: 0 rows dropped\n",
      "- work_subset_04601: 0 rows dropped\n",
      "- work_subset_01151: 0 rows dropped\n",
      "- work_subset_01075: 0 rows dropped\n",
      "- work_subset_02501: 0 rows dropped\n",
      "- work_subset_03051: 0 rows dropped\n",
      "- work_subset_04251: 0 rows dropped\n",
      "- work_subset_02652: 0 rows dropped\n",
      "- work_subset_02201: 0 rows dropped\n",
      "- work_subset_04831: 0 rows dropped\n",
      "- work_subset_03501: 0 rows dropped\n",
      "- work_subset_02051: 0 rows dropped\n",
      "- work_subset_04351: 0 rows dropped\n",
      "- work_subset_02149: 0 rows dropped\n",
      "- work_subset_04860: 0 rows dropped\n",
      "- work_subset_02257: 0 rows dropped\n",
      "- work_subset_02250: 0 rows dropped\n",
      "- work_subset_05202: 0 rows dropped\n",
      "- work_subset_05508: 0 rows dropped\n",
      "- work_subset_01706: 0 rows dropped\n",
      "- work_subset_01451: 0 rows dropped\n",
      "- work_subset_02001: 0 rows dropped\n",
      "- work_subset_04870: 0 rows dropped\n",
      "- work_subset_03066: 0 rows dropped\n",
      "- work_subset_04225: 0 rows dropped\n",
      "- work_subset_02256: 0 rows dropped\n",
      "- work_subset_02258: 0 rows dropped\n",
      "- work_subset_02101: 0 rows dropped\n",
      "- work_subset_01551: 0 rows dropped\n",
      "- work_subset_03451: 0 rows dropped\n",
      "- work_subset_02263: 0 rows dropped\n",
      "- work_subset_04201: 0 rows dropped\n",
      "\n",
      "Dropping rows with 'n' from work_subsets complete.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell removes all rows where the column \"Value\" contains the string \"n\"\n",
    "# from each work_subset_*.txt file in Temp_file_path_DP.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Identify all work_subset files.\n",
    "#   2. Load each file.\n",
    "#   3. If the column \"Value\" exists, remove rows where Value == \"n\".\n",
    "#   4. Overwrite the file with the cleaned version.\n",
    "#   5. Record how many rows were removed per file.\n",
    "#   6. Print a summary at the end.\n",
    "\n",
    "\n",
    "print(\"Dropping rows with 'n' in Value column from work_subset files...\")\n",
    "\n",
    "# Find all work_subset files in the Temp directory\n",
    "work_subset_file_names = [\n",
    "    f for f in os.listdir(Temp_file_path_DP)\n",
    "    if f.startswith('work_subset_') and f.endswith('.txt')\n",
    "]\n",
    "\n",
    "# Dictionary to store the count of dropped rows for each file\n",
    "dropped_rows_summary_work_subsets = {}\n",
    "\n",
    "# Process each work_subset file\n",
    "for file_name in work_subset_file_names:\n",
    "    ws_name = file_name.replace(\".txt\", \"\")\n",
    "    file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "    print(f\"\\n--- Processing {ws_name} ---\")\n",
    "\n",
    "    ws_df = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if ws_df is not None and 'Value' in ws_df.columns:\n",
    "        rows_before_drop = len(ws_df)\n",
    "\n",
    "        # Remove rows where Value == \"n\"\n",
    "        ws_df_cleaned = ws_df[ws_df['Value'] != 'n'].copy()\n",
    "\n",
    "        rows_after_drop = len(ws_df_cleaned)\n",
    "        rows_dropped = rows_before_drop - rows_after_drop\n",
    "        dropped_rows_summary_work_subsets[ws_name] = rows_dropped\n",
    "\n",
    "        if rows_dropped > 0:\n",
    "            print(f\"  Dropped {rows_dropped} rows with 'n' in Value column.\")\n",
    "        else:\n",
    "            print(\"  No rows with 'n' in Value column found.\")\n",
    "\n",
    "        # Save cleaned file\n",
    "        ws_df_cleaned.to_csv(file_path, sep='|', index=False)\n",
    "        print(f\"  Saved cleaned {ws_name} back to {file_path}\")\n",
    "\n",
    "        del ws_df\n",
    "        del ws_df_cleaned\n",
    "        gc.collect()\n",
    "\n",
    "    else:\n",
    "        # Skip files without a Value column\n",
    "        print(f\"  'Value' column not found in {ws_name}. Skipping.\")\n",
    "        dropped_rows_summary_work_subsets[ws_name] = 'N/A - No Value column'\n",
    "\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary of rows dropped due to 'n' in Value column from work_subset files:\")\n",
    "for ws_name, count in dropped_rows_summary_work_subsets.items():\n",
    "    print(f\"- {ws_name}: {count} rows dropped\")\n",
    "\n",
    "print(\"\\nDropping rows with 'n' from work_subsets complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21b17226"
   },
   "source": [
    "### Rename Work Subset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1764881156303,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "4167b337",
    "outputId": "9674c170-8a4d-48d5-a5ff-16a52910af93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ItemCode to ItemName mapping from ValueCoding.\n",
      "\n",
      "Found 52 files in Temp directory.\n",
      "Identified 49 files with numerical IDs for potential renaming.\n",
      "  Renamed 'work_subset_01101.txt' to 'work_subset_Selling_General__Administrative_Expenses.txt'.\n",
      "  Renamed 'work_subset_03351.txt' to 'work_subset_Total_Liabilities.txt'.\n",
      "  Renamed 'work_subset_04751.txt' to 'work_subset_Com_Pfd_Redeemed_Retired_Converted_Etc..txt'.\n",
      "  Renamed 'work_subset_03263.txt' to 'work_subset_Deferred_Taxes.txt'.\n",
      "  Renamed 'work_subset_04890.txt' to 'work_subset_Net_Cash_Flow___Financing.txt'.\n",
      "  Renamed 'work_subset_03101.txt' to 'work_subset_Current_Liabilities___Total.txt'.\n",
      "  Renamed 'work_subset_01001.txt' to 'work_subset_Net_Sales_or_Revenues.txt'.\n",
      "  Renamed 'work_subset_04401.txt' to 'work_subset_Long_Term_Borrowings.txt'.\n",
      "  Renamed 'work_subset_03251.txt' to 'work_subset_Long_Term_Debt.txt'.\n",
      "  Renamed 'work_subset_03040.txt' to 'work_subset_Accounts_Payable.txt'.\n",
      "  Renamed 'work_subset_03273.txt' to 'work_subset_Other_Liabilities.txt'.\n",
      "  Renamed 'work_subset_01250.txt' to 'work_subset_Operating_Income.txt'.\n",
      "  Renamed 'work_subset_03426.txt' to 'work_subset_Minority_Interest.txt'.\n",
      "  Renamed 'work_subset_04701.txt' to 'work_subset_Reduction_in_Long_Term_Debt.txt'.\n",
      "  Renamed 'work_subset_01051.txt' to 'work_subset_Cost_of_Goods_Sold_Excl_Depreciation.txt'.\n",
      "  Renamed 'work_subset_03063.txt' to 'work_subset_Income_Taxes_Payable.txt'.\n",
      "  Renamed 'work_subset_04551.txt' to 'work_subset_Cash_Dividends_Paid___Total.txt'.\n",
      "  Renamed 'work_subset_02999.txt' to 'work_subset_Total_Assets.txt'.\n",
      "  Renamed 'work_subset_04601.txt' to 'work_subset_Capital_Expenditures_Addtns_to_Fixed_Assets.txt'.\n",
      "  Renamed 'work_subset_01151.txt' to 'work_subset_Depreciation_Depletion__Amortization.txt'.\n",
      "  Renamed 'work_subset_01075.txt' to 'work_subset_Interest_Expense___Total.txt'.\n",
      "  Renamed 'work_subset_02501.txt' to 'work_subset_Property_Plant__Equipment___Net.txt'.\n",
      "  Renamed 'work_subset_03051.txt' to 'work_subset_Short_Term_Debt__Current_Portion_of_LT_Debt.txt'.\n",
      "  Renamed 'work_subset_04251.txt' to 'work_subset_Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt'.\n",
      "  Renamed 'work_subset_02652.txt' to 'work_subset_Other_Assets___Total.txt'.\n",
      "  Renamed 'work_subset_02201.txt' to 'work_subset_Current_Assets___Total.txt'.\n",
      "  Renamed 'work_subset_04831.txt' to 'work_subset_Funds_From_For_Other_Operating_Activities.txt'.\n",
      "  Renamed 'work_subset_03501.txt' to 'work_subset_Common_Equity.txt'.\n",
      "  Renamed 'work_subset_02051.txt' to 'work_subset_ReceivablesNet.txt'.\n",
      "  Renamed 'work_subset_04351.txt' to 'work_subset_Disposal_of_Fixed_Assets.txt'.\n",
      "  Renamed 'work_subset_02149.txt' to 'work_subset_Other_Current_Assets.txt'.\n",
      "  Renamed 'work_subset_04860.txt' to 'work_subset_Net_Cash_Flow___Operating_Activities.txt'.\n",
      "  Renamed 'work_subset_02257.txt' to 'work_subset_Investments_in_Sales__Direct_Financing_Leases.txt'.\n",
      "  Renamed 'work_subset_02250.txt' to 'work_subset_Other_Investments.txt'.\n",
      "  Renamed 'work_subset_05202.txt' to 'work_subset_Earnings_Per_Share_Fiscal_Year_End.txt'.\n",
      "  Renamed 'work_subset_05508.txt' to 'work_subset_Sales_Per_Share.txt'.\n",
      "  Renamed 'work_subset_01706.txt' to 'work_subset_Net_Income_Used_to_Calculate_Basic_EPS.txt'.\n",
      "  Renamed 'work_subset_01451.txt' to 'work_subset_Income_Taxes.txt'.\n",
      "  Renamed 'work_subset_02001.txt' to 'work_subset_Cash__Short_Term_Investments.txt'.\n",
      "  Renamed 'work_subset_04870.txt' to 'work_subset_Net_Cash_Flow___Investing.txt'.\n",
      "  Renamed 'work_subset_03066.txt' to 'work_subset_Other_Current_Liabilities.txt'.\n",
      "  Renamed 'work_subset_04225.txt' to 'work_subset_Extraordinary_Items.txt'.\n",
      "  Renamed 'work_subset_02256.txt' to 'work_subset_Investments_in_Associated_Companies.txt'.\n",
      "  Renamed 'work_subset_02258.txt' to 'work_subset_Long_Term_Receivables.txt'.\n",
      "  Renamed 'work_subset_02101.txt' to 'work_subset_Inventories___Total.txt'.\n",
      "  Renamed 'work_subset_01551.txt' to 'work_subset_Net_Income_Before_Extra_Items_Preferred_Divs.txt'.\n",
      "  Renamed 'work_subset_03451.txt' to 'work_subset_Preferred_Stock.txt'.\n",
      "  Renamed 'work_subset_02263.txt' to 'work_subset_Unspecified_Other_Loans.txt'.\n",
      "  Renamed 'work_subset_04201.txt' to 'work_subset_Funds_From_Operations.txt'.\n",
      "\n",
      "Renaming and cleanup process complete. Successfully renamed: 49, Deleted existing renamed files: 0, Skipped: 0, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell renames work_subset_*.txt files that still use a numeric ItemCode\n",
    "# in their filename (e.g. work_subset_1051.txt) to use the corresponding\n",
    "# textual ItemName instead (e.g. work_subset_Revenue.txt).\n",
    "#\n",
    "# Steps:\n",
    "#   1. Build a mapping from ItemCode -> ItemName from the ValueCoding DataFrame\n",
    "#      (using the first column as ItemCode, second as ItemName).\n",
    "#   2. Scan Temp_file_path_DP for work_subset_*.txt files whose names contain\n",
    "#      only a numeric code.\n",
    "#   3. For each such file:\n",
    "#        - Look up its ItemCode in the mapping.\n",
    "#        - If found, sanitize the ItemName into a filesystem-safe string.\n",
    "#        - Build a new filename of the form work_subset_<sanitized_name>.txt.\n",
    "#        - If a file with that new name already exists, delete it first.\n",
    "#        - Rename the original numeric file to the new, name-based filename.\n",
    "#        - Track counts for renamed, skipped, and error cases.\n",
    "#   4. Print a final summary of the renaming process.\n",
    "\n",
    "\n",
    "# Ensure ValueCoding DataFrame is available and usable\n",
    "if 'ValueCoding' in globals() and ValueCoding is not None and not ValueCoding.empty:\n",
    "    # Ensure there are at least two columns: first = ItemCode, second = ItemName\n",
    "    if ValueCoding.shape[1] >= 2:\n",
    "        # Build a mapping ItemCode (as string) -> ItemName\n",
    "        # ValueCoding.iloc[:, 0]  -> first column (ItemCode)\n",
    "        # ValueCoding.iloc[:, 1]  -> second column (ItemName)\n",
    "        item_code_to_name = pd.Series(\n",
    "            ValueCoding.iloc[:, 1].values,\n",
    "            index=ValueCoding.iloc[:, 0].astype(str)  # convert to str for consistent lookup\n",
    "        ).to_dict()\n",
    "        print(\"Created ItemCode to ItemName mapping from ValueCoding.\")\n",
    "    else:\n",
    "        item_code_to_name = {}\n",
    "        print(\"ValueCoding DataFrame does not have enough columns for mapping.\")\n",
    "else:\n",
    "    # Fallback: empty mapping if ValueCoding is not available\n",
    "    item_code_to_name = {}\n",
    "    print(\"ValueCoding DataFrame not found or is empty. Cannot rename subset files.\")\n",
    "\n",
    "\n",
    "# Get all files in the Temp directory\n",
    "temp_files = os.listdir(Temp_file_path_DP)\n",
    "\n",
    "# Select files that still follow the numeric naming pattern:\n",
    "#   work_subset_<digits>.txt\n",
    "work_subset_files_numerical_ids = [\n",
    "    f for f in temp_files\n",
    "    if re.match(r'work_subset_(\\d+)\\.txt$', f)\n",
    "]\n",
    "\n",
    "print(f\"\\nFound {len(temp_files)} files in Temp directory.\")\n",
    "print(f\"Identified {len(work_subset_files_numerical_ids)} files with numerical IDs for potential renaming.\")\n",
    "\n",
    "\n",
    "# Counters for statistics\n",
    "renamed_count = 0\n",
    "deleted_existing_renamed_count = 0\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "\n",
    "# Iterate over each numerically-named work_subset file\n",
    "for file_name in work_subset_files_numerical_ids:\n",
    "    old_file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "    # Extract the numeric ItemCode from the filename via regex\n",
    "    match = re.match(r'work_subset_(\\d+)\\.txt$', file_name)\n",
    "    if match:\n",
    "        item_code = match.group(1)  # e.g. \"1051\"\n",
    "\n",
    "        # Look up the corresponding ItemName in the mapping\n",
    "        item_name = item_code_to_name.get(item_code)\n",
    "\n",
    "        if item_name:\n",
    "            # Sanitize the ItemName for use in filenames:\n",
    "            #   - replace common separators with underscore\n",
    "            #   - then remove any remaining disallowed characters\n",
    "            sanitized_item_name = (\n",
    "                item_name\n",
    "                .replace(' ', '_')\n",
    "                .replace('-', '_')\n",
    "                .replace('/', '_')\n",
    "                .replace('\\\\', '_')\n",
    "                .replace(':', '_')\n",
    "                .replace('*', '_')\n",
    "                .replace('?', '_')\n",
    "                .replace('\"', '_')\n",
    "                .replace('<', '_')\n",
    "                .replace('>', '_')\n",
    "                .replace('|', '_')\n",
    "            )\n",
    "            sanitized_item_name = re.sub(r'[^\\w.-]', '', sanitized_item_name)\n",
    "\n",
    "            # Build the new filename using the sanitized name\n",
    "            new_file_name = f\"work_subset_{sanitized_item_name}.txt\"\n",
    "            new_file_path = os.path.join(Temp_file_path_DP, new_file_name)\n",
    "\n",
    "            # If the target name already exists, remove it so we can rename safely\n",
    "            if os.path.exists(new_file_path):\n",
    "                print(f\"  Target renamed file '{new_file_name}' already exists. Deleting it before renaming.\")\n",
    "                try:\n",
    "                    os.remove(new_file_path)\n",
    "                    deleted_existing_renamed_count += 1\n",
    "                except Exception as e:\n",
    "                    # If deletion fails, record an error and skip this file\n",
    "                    print(f\"  Error deleting existing renamed file '{new_file_path}': {e}. Skipping rename for this file.\")\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "\n",
    "            # Perform the actual rename from numeric to name-based filename\n",
    "            try:\n",
    "                os.rename(old_file_path, new_file_path)\n",
    "                print(f\"  Renamed '{file_name}' to '{new_file_name}'.\")\n",
    "                renamed_count += 1\n",
    "            except OSError as e:\n",
    "                print(f\"  OSError renaming '{file_name}' to '{new_file_path}': {e}. Skipping rename.\")\n",
    "                error_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  Error renaming '{file_name}' to '{new_file_path}': {e}. Skipping rename.\")\n",
    "                error_count += 1\n",
    "\n",
    "        else:\n",
    "            # No mapping found for this ItemCode, so we cannot rename meaningfully\n",
    "            print(f\"  Could not find ItemName for ItemCode '{item_code}' in ValueCoding. Skipping processing for '{file_name}'.\")\n",
    "            skipped_count += 1\n",
    "    else:\n",
    "        # Should not happen due to the initial regex filter, kept as safety net\n",
    "        print(f\"  Filename format not as expected for '{file_name}'. Skipping processing.\")\n",
    "        skipped_count += 1\n",
    "\n",
    "# Final summary of the renaming process\n",
    "print(\n",
    "    f\"\\nRenaming and cleanup process complete. \"\n",
    "    f\"Successfully renamed: {renamed_count}, \"\n",
    "    f\"Deleted existing renamed files: {deleted_existing_renamed_count}, \"\n",
    "    f\"Skipped: {skipped_count}, \"\n",
    "    f\"Errors: {error_count}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59bf1295"
   },
   "source": [
    "### Merge Currency, Update Codes, and FYE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24239,
     "status": "ok",
     "timestamp": 1764881180562,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "S4VcEFfIaBpZ",
    "outputId": "966b5a51-b809-4bac-839b-39b6b9206456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared CurrencyCodes_clean for merging.\n",
      "Prepared UpdateCodes_clean for merging.\n",
      "Prepared FYE_clean for merging.\n",
      "Processing work_subset DataFrames: merging currency, update codes, FYE data, adding HistCurrency...\n",
      "\n",
      "--- Processing work_subset_Extraordinary_Items ---\n",
      "  Merged FYE data with work_subset_Extraordinary_Items.\n",
      "    Saved updated work_subset_Extraordinary_Items to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Extraordinary_Items.txt\n",
      "\n",
      "--- Processing work_subset_Com_Pfd_Redeemed_Retired_Converted_Etc. ---\n",
      "  Merged FYE data with work_subset_Com_Pfd_Redeemed_Retired_Converted_Etc..\n",
      "    Saved updated work_subset_Com_Pfd_Redeemed_Retired_Converted_Etc. to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Com_Pfd_Redeemed_Retired_Converted_Etc..txt\n",
      "\n",
      "--- Processing work_subset_Net_Income_Used_to_Calculate_Basic_EPS ---\n",
      "  Merged FYE data with work_subset_Net_Income_Used_to_Calculate_Basic_EPS.\n",
      "    Saved updated work_subset_Net_Income_Used_to_Calculate_Basic_EPS to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Net_Income_Used_to_Calculate_Basic_EPS.txt\n",
      "\n",
      "--- Processing work_subset_Unspecified_Other_Loans ---\n",
      "  Merged FYE data with work_subset_Unspecified_Other_Loans.\n",
      "    Saved updated work_subset_Unspecified_Other_Loans to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Unspecified_Other_Loans.txt\n",
      "\n",
      "--- Processing work_subset_Net_Cash_Flow___Operating_Activities ---\n",
      "  Merged FYE data with work_subset_Net_Cash_Flow___Operating_Activities.\n",
      "    Saved updated work_subset_Net_Cash_Flow___Operating_Activities to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Net_Cash_Flow___Operating_Activities.txt\n",
      "\n",
      "--- Processing work_subset_Net_Proceeds_From_Sale_Issue_of_Com__Pfd ---\n",
      "  Merged FYE data with work_subset_Net_Proceeds_From_Sale_Issue_of_Com__Pfd.\n",
      "    Saved updated work_subset_Net_Proceeds_From_Sale_Issue_of_Com__Pfd to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt\n",
      "\n",
      "--- Processing work_subset_Other_Current_Assets ---\n",
      "  Merged FYE data with work_subset_Other_Current_Assets.\n",
      "    Saved updated work_subset_Other_Current_Assets to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Other_Current_Assets.txt\n",
      "\n",
      "--- Processing work_subset_Property_Plant__Equipment___Net ---\n",
      "  Merged FYE data with work_subset_Property_Plant__Equipment___Net.\n",
      "    Saved updated work_subset_Property_Plant__Equipment___Net to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Property_Plant__Equipment___Net.txt\n",
      "\n",
      "--- Processing work_subset_Net_Cash_Flow___Investing ---\n",
      "  Merged FYE data with work_subset_Net_Cash_Flow___Investing.\n",
      "    Saved updated work_subset_Net_Cash_Flow___Investing to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Net_Cash_Flow___Investing.txt\n",
      "\n",
      "--- Processing work_subset_Inventories___Total ---\n",
      "  Merged FYE data with work_subset_Inventories___Total.\n",
      "    Saved updated work_subset_Inventories___Total to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Inventories___Total.txt\n",
      "\n",
      "--- Processing work_subset_Interest_Expense___Total ---\n",
      "  Merged FYE data with work_subset_Interest_Expense___Total.\n",
      "    Saved updated work_subset_Interest_Expense___Total to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Interest_Expense___Total.txt\n",
      "\n",
      "--- Processing work_subset_Long_Term_Receivables ---\n",
      "  Merged FYE data with work_subset_Long_Term_Receivables.\n",
      "    Saved updated work_subset_Long_Term_Receivables to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Long_Term_Receivables.txt\n",
      "\n",
      "--- Processing work_subset_Other_Assets___Total ---\n",
      "  Merged FYE data with work_subset_Other_Assets___Total.\n",
      "    Saved updated work_subset_Other_Assets___Total to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Other_Assets___Total.txt\n",
      "\n",
      "--- Processing work_subset_Cash__Short_Term_Investments ---\n",
      "  Merged FYE data with work_subset_Cash__Short_Term_Investments.\n",
      "    Saved updated work_subset_Cash__Short_Term_Investments to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Cash__Short_Term_Investments.txt\n",
      "\n",
      "--- Processing work_subset_Investments_in_Associated_Companies ---\n",
      "  Merged FYE data with work_subset_Investments_in_Associated_Companies.\n",
      "    Saved updated work_subset_Investments_in_Associated_Companies to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Investments_in_Associated_Companies.txt\n",
      "\n",
      "--- Processing work_subset_Total_Liabilities ---\n",
      "  Merged FYE data with work_subset_Total_Liabilities.\n",
      "    Saved updated work_subset_Total_Liabilities to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Total_Liabilities.txt\n",
      "\n",
      "--- Processing work_subset_Total_Assets ---\n",
      "  Merged FYE data with work_subset_Total_Assets.\n",
      "    Saved updated work_subset_Total_Assets to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Total_Assets.txt\n",
      "\n",
      "--- Processing work_subset_Net_Cash_Flow___Financing ---\n",
      "  Merged FYE data with work_subset_Net_Cash_Flow___Financing.\n",
      "    Saved updated work_subset_Net_Cash_Flow___Financing to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Net_Cash_Flow___Financing.txt\n",
      "\n",
      "--- Processing work_subset_Operating_Income ---\n",
      "  Merged FYE data with work_subset_Operating_Income.\n",
      "    Saved updated work_subset_Operating_Income to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Operating_Income.txt\n",
      "\n",
      "--- Processing work_subset_Long_Term_Debt ---\n",
      "  Merged FYE data with work_subset_Long_Term_Debt.\n",
      "    Saved updated work_subset_Long_Term_Debt to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Long_Term_Debt.txt\n",
      "\n",
      "--- Processing work_subset_Income_Taxes ---\n",
      "  Merged FYE data with work_subset_Income_Taxes.\n",
      "    Saved updated work_subset_Income_Taxes to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Income_Taxes.txt\n",
      "\n",
      "--- Processing work_subset_Minority_Interest ---\n",
      "  Merged FYE data with work_subset_Minority_Interest.\n",
      "    Saved updated work_subset_Minority_Interest to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Minority_Interest.txt\n",
      "\n",
      "--- Processing work_subset_Income_Taxes_Payable ---\n",
      "  Merged FYE data with work_subset_Income_Taxes_Payable.\n",
      "    Saved updated work_subset_Income_Taxes_Payable to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Income_Taxes_Payable.txt\n",
      "\n",
      "--- Processing work_subset_Other_Current_Liabilities ---\n",
      "  Merged FYE data with work_subset_Other_Current_Liabilities.\n",
      "    Saved updated work_subset_Other_Current_Liabilities to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Other_Current_Liabilities.txt\n",
      "\n",
      "--- Processing work_subset_ReceivablesNet ---\n",
      "  Merged FYE data with work_subset_ReceivablesNet.\n",
      "    Saved updated work_subset_ReceivablesNet to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_ReceivablesNet.txt\n",
      "\n",
      "--- Processing work_subset_Sales_Per_Share ---\n",
      "  Merged FYE data with work_subset_Sales_Per_Share.\n",
      "    Saved updated work_subset_Sales_Per_Share to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Sales_Per_Share.txt\n",
      "\n",
      "--- Processing work_subset_Cost_of_Goods_Sold_Excl_Depreciation ---\n",
      "  Merged FYE data with work_subset_Cost_of_Goods_Sold_Excl_Depreciation.\n",
      "    Saved updated work_subset_Cost_of_Goods_Sold_Excl_Depreciation to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Cost_of_Goods_Sold_Excl_Depreciation.txt\n",
      "\n",
      "--- Processing work_subset_Long_Term_Borrowings ---\n",
      "  Merged FYE data with work_subset_Long_Term_Borrowings.\n",
      "    Saved updated work_subset_Long_Term_Borrowings to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Long_Term_Borrowings.txt\n",
      "\n",
      "--- Processing work_subset_Other_Investments ---\n",
      "  Merged FYE data with work_subset_Other_Investments.\n",
      "    Saved updated work_subset_Other_Investments to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Other_Investments.txt\n",
      "\n",
      "--- Processing work_subset_Net_Sales_or_Revenues ---\n",
      "  Merged FYE data with work_subset_Net_Sales_or_Revenues.\n",
      "    Saved updated work_subset_Net_Sales_or_Revenues to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Net_Sales_or_Revenues.txt\n",
      "\n",
      "--- Processing work_subset_Reduction_in_Long_Term_Debt ---\n",
      "  Merged FYE data with work_subset_Reduction_in_Long_Term_Debt.\n",
      "    Saved updated work_subset_Reduction_in_Long_Term_Debt to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Reduction_in_Long_Term_Debt.txt\n",
      "\n",
      "--- Processing work_subset_Funds_From_For_Other_Operating_Activities ---\n",
      "  Merged FYE data with work_subset_Funds_From_For_Other_Operating_Activities.\n",
      "    Saved updated work_subset_Funds_From_For_Other_Operating_Activities to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Funds_From_For_Other_Operating_Activities.txt\n",
      "\n",
      "--- Processing work_subset_Capital_Expenditures_Addtns_to_Fixed_Assets ---\n",
      "  Merged FYE data with work_subset_Capital_Expenditures_Addtns_to_Fixed_Assets.\n",
      "    Saved updated work_subset_Capital_Expenditures_Addtns_to_Fixed_Assets to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Capital_Expenditures_Addtns_to_Fixed_Assets.txt\n",
      "\n",
      "--- Processing work_subset_Preferred_Stock ---\n",
      "  Merged FYE data with work_subset_Preferred_Stock.\n",
      "    Saved updated work_subset_Preferred_Stock to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Preferred_Stock.txt\n",
      "\n",
      "--- Processing work_subset_Common_Equity ---\n",
      "  Merged FYE data with work_subset_Common_Equity.\n",
      "    Saved updated work_subset_Common_Equity to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Common_Equity.txt\n",
      "\n",
      "--- Processing work_subset_Cash_Dividends_Paid___Total ---\n",
      "  Merged FYE data with work_subset_Cash_Dividends_Paid___Total.\n",
      "    Saved updated work_subset_Cash_Dividends_Paid___Total to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Cash_Dividends_Paid___Total.txt\n",
      "\n",
      "--- Processing work_subset_Investments_in_Sales__Direct_Financing_Leases ---\n",
      "  Merged FYE data with work_subset_Investments_in_Sales__Direct_Financing_Leases.\n",
      "    Saved updated work_subset_Investments_in_Sales__Direct_Financing_Leases to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Investments_in_Sales__Direct_Financing_Leases.txt\n",
      "\n",
      "--- Processing work_subset_Funds_From_Operations ---\n",
      "  Merged FYE data with work_subset_Funds_From_Operations.\n",
      "    Saved updated work_subset_Funds_From_Operations to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Funds_From_Operations.txt\n",
      "\n",
      "--- Processing work_subset_Short_Term_Debt__Current_Portion_of_LT_Debt ---\n",
      "  Merged FYE data with work_subset_Short_Term_Debt__Current_Portion_of_LT_Debt.\n",
      "    Saved updated work_subset_Short_Term_Debt__Current_Portion_of_LT_Debt to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Short_Term_Debt__Current_Portion_of_LT_Debt.txt\n",
      "\n",
      "--- Processing work_subset_Current_Assets___Total ---\n",
      "  Merged FYE data with work_subset_Current_Assets___Total.\n",
      "    Saved updated work_subset_Current_Assets___Total to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Current_Assets___Total.txt\n",
      "\n",
      "--- Processing work_subset_Accounts_Payable ---\n",
      "  Merged FYE data with work_subset_Accounts_Payable.\n",
      "    Saved updated work_subset_Accounts_Payable to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Accounts_Payable.txt\n",
      "\n",
      "--- Processing work_subset_Disposal_of_Fixed_Assets ---\n",
      "  Merged FYE data with work_subset_Disposal_of_Fixed_Assets.\n",
      "    Saved updated work_subset_Disposal_of_Fixed_Assets to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Disposal_of_Fixed_Assets.txt\n",
      "\n",
      "--- Processing work_subset_Deferred_Taxes ---\n",
      "  Merged FYE data with work_subset_Deferred_Taxes.\n",
      "    Saved updated work_subset_Deferred_Taxes to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Deferred_Taxes.txt\n",
      "\n",
      "--- Processing work_subset_Depreciation_Depletion__Amortization ---\n",
      "  Merged FYE data with work_subset_Depreciation_Depletion__Amortization.\n",
      "    Saved updated work_subset_Depreciation_Depletion__Amortization to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Depreciation_Depletion__Amortization.txt\n",
      "\n",
      "--- Processing work_subset_Earnings_Per_Share_Fiscal_Year_End ---\n",
      "  Merged FYE data with work_subset_Earnings_Per_Share_Fiscal_Year_End.\n",
      "    Saved updated work_subset_Earnings_Per_Share_Fiscal_Year_End to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Earnings_Per_Share_Fiscal_Year_End.txt\n",
      "\n",
      "--- Processing work_subset_Current_Liabilities___Total ---\n",
      "  Merged FYE data with work_subset_Current_Liabilities___Total.\n",
      "    Saved updated work_subset_Current_Liabilities___Total to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Current_Liabilities___Total.txt\n",
      "\n",
      "--- Processing work_subset_Other_Liabilities ---\n",
      "  Merged FYE data with work_subset_Other_Liabilities.\n",
      "    Saved updated work_subset_Other_Liabilities to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Other_Liabilities.txt\n",
      "\n",
      "--- Processing work_subset_Net_Income_Before_Extra_Items_Preferred_Divs ---\n",
      "  Merged FYE data with work_subset_Net_Income_Before_Extra_Items_Preferred_Divs.\n",
      "    Saved updated work_subset_Net_Income_Before_Extra_Items_Preferred_Divs to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Net_Income_Before_Extra_Items_Preferred_Divs.txt\n",
      "\n",
      "--- Processing work_subset_Selling_General__Administrative_Expenses ---\n",
      "  Merged FYE data with work_subset_Selling_General__Administrative_Expenses.\n",
      "    Saved updated work_subset_Selling_General__Administrative_Expenses to /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/work_subset_Selling_General__Administrative_Expenses.txt\n",
      "\n",
      "Processing complete (currency, update codes, FYE, HistCurrency).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell enriches all work_subset_*.txt files in Temp_file_path_DP with:\n",
    "#\n",
    "#   1. Historical currency (HistCurrency) computed from CurrencyCodes_clean and PIT Date.\n",
    "#   2. Update codes from UpdateCodes_clean (matched by ID, PIT Date, Frequency, FiscalPeriod).\n",
    "#   3. Fiscal year-end information (FYE Month) from FYE_clean (optional).\n",
    "#\n",
    "# Main steps:\n",
    "#   - Prepare a slimmed-down CurrencyCodes_clean with numeric switch dates and renamed\n",
    "#     columns (Currency1/2/3, Date1/2/3).\n",
    "#   - Prepare a slimmed-down UpdateCodes_clean with normalized keys.\n",
    "#   - Optionally prepare FYE_clean if present and valid.\n",
    "#   - For each work_subset file:\n",
    "#       * Load it, normalize key columns (PIT Date, Frequency, FiscalPeriod).\n",
    "#       * Merge currency info and compute HistCurrency based on PIT Date vs Date1/2/3.\n",
    "#       * Merge update codes.\n",
    "#       * Merge FYE data (if available).\n",
    "#       * Reorder columns (keeping all existing columns).\n",
    "#       * Persist back to disk with PIT Date formatted as 'YYYY-MM-DD'.\n",
    "#       * Clean up intermediate DataFrames to manage memory.\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1) Prepare CurrencyCodes_clean (currency + switch dates)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Select only the needed columns from CurrencyCodes_clean\n",
    "currency_cols_to_merge = CurrencyCodes_clean[\n",
    "    ['ID', 'CurrencyCode1', 'SwitchDate1',\n",
    "     'CurrencyCode2', 'SwitchDate2',\n",
    "     'CurrencyCode3', 'SwitchDate3']\n",
    "].copy()\n",
    "\n",
    "# Parse all SwitchDate* columns to datetime, assuming 'YYYY-MM-DD' format\n",
    "for col in ['SwitchDate1', 'SwitchDate2', 'SwitchDate3']:\n",
    "    currency_cols_to_merge[col] = pd.to_datetime(\n",
    "        currency_cols_to_merge[col],\n",
    "        format='%Y-%m-%d',\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "# Rename columns for clarity and to avoid name clashes during merge\n",
    "currency_cols_to_merge.rename(columns={\n",
    "    'CurrencyCode1': 'Currency1',\n",
    "    'SwitchDate1': 'Date1',\n",
    "    'CurrencyCode2': 'Currency2',\n",
    "    'SwitchDate2': 'Date2',\n",
    "    'CurrencyCode3': 'Currency3',\n",
    "    'SwitchDate3': 'Date3'\n",
    "}, inplace=True)\n",
    "\n",
    "print(\"Prepared CurrencyCodes_clean for merging.\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2) Prepare UpdateCodes_clean\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Select only the columns needed for joining in UpdateCodes_clean\n",
    "update_codes_to_merge = UpdateCodes_clean[\n",
    "    ['ID', 'PIT Date', 'Frequency', 'FiscalPeriod', 'UpdateCode']\n",
    "].copy()\n",
    "\n",
    "# Parse PIT Date as datetime (format 'YYYY-MM-DD')\n",
    "update_codes_to_merge['PIT Date'] = pd.to_datetime(\n",
    "    update_codes_to_merge['PIT Date'],\n",
    "    format='%Y-%m-%d',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Normalize Frequency to stripped string for consistent joins\n",
    "update_codes_to_merge['Frequency'] = (\n",
    "    update_codes_to_merge['Frequency'].astype(str).str.strip()\n",
    ")\n",
    "\n",
    "# Normalize FiscalPeriod to stripped string for consistent joins\n",
    "update_codes_to_merge['FiscalPeriod'] = (\n",
    "    update_codes_to_merge['FiscalPeriod'].astype(str).str.strip()\n",
    ")\n",
    "\n",
    "print(\"Prepared UpdateCodes_clean for merging.\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3) Prepare FYE_clean (optional)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Check presence and validity of FYE_clean before using it\n",
    "if (\n",
    "    'FYE_clean' in globals()\n",
    "    and FYE_clean is not None\n",
    "    and all(col in FYE_clean.columns for col in ['ID', 'FY', 'FYE Month'])\n",
    "):\n",
    "    # Keep only relevant columns and rename FY -> FiscalPeriod\n",
    "    fye_cols_to_merge = FYE_clean[['ID', 'FY', 'FYE Month']].copy()\n",
    "    fye_cols_to_merge.rename(columns={'FY': 'FiscalPeriod'}, inplace=True)\n",
    "\n",
    "    # Normalize FiscalPeriod to stripped string for joins\n",
    "    fye_cols_to_merge['FiscalPeriod'] = (\n",
    "        fye_cols_to_merge['FiscalPeriod'].astype(str).str.strip()\n",
    "    )\n",
    "\n",
    "    print(\"Prepared FYE_clean for merging.\")\n",
    "else:\n",
    "    # If not available or invalid, skip FYE merge later\n",
    "    fye_cols_to_merge = None\n",
    "    print(\"FYE_clean DataFrame not found or invalid. FYE data will not be merged.\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4) Process work_subset files\n",
    "# --------------------------------------------------\n",
    "\n",
    "summary_data = []  # Placeholder if you want to collect per-file stats later\n",
    "\n",
    "print(\"Processing work_subset DataFrames: merging currency, update codes, FYE data, adding HistCurrency...\")\n",
    "\n",
    "# Find all work_subset_*.txt files in the temp directory\n",
    "work_subset_file_names = [\n",
    "    f for f in os.listdir(Temp_file_path_DP)\n",
    "    if f.startswith('work_subset_') and f.endswith('.txt')\n",
    "]\n",
    "\n",
    "# Iterate over each work_subset file\n",
    "for ws_file_name in work_subset_file_names:\n",
    "    ws_name = ws_file_name.replace(\".txt\", \"\")  # e.g. work_subset_Revenue\n",
    "    file_path = os.path.join(Temp_file_path_DP, ws_file_name)\n",
    "\n",
    "    print(f\"\\n--- Processing {ws_name} ---\")\n",
    "\n",
    "    # Read the current work_subset file from disk\n",
    "    ws_df = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if ws_df is None:\n",
    "        # If loading fails, skip this file\n",
    "        print(f\"--- Could not load {ws_name} from disk. Skipping processing. ---\")\n",
    "        continue\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4.1 Normalize PIT Date & key columns in ws_df\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    # Ensure PIT Date is parsed as datetime\n",
    "    if 'PIT Date' in ws_df.columns:\n",
    "        ws_df['PIT Date'] = pd.to_datetime(\n",
    "            ws_df['PIT Date'],\n",
    "            format='%Y-%m-%d',\n",
    "            errors='coerce'\n",
    "        )\n",
    "    else:\n",
    "        print(f\"  Warning: 'PIT Date' column missing in {ws_name}.\")\n",
    "\n",
    "    # Normalize Frequency as stripped string\n",
    "    if 'Frequency' in ws_df.columns:\n",
    "        ws_df['Frequency'] = ws_df['Frequency'].astype(str).str.strip()\n",
    "\n",
    "    # Normalize FiscalPeriod as stripped string\n",
    "    if 'FiscalPeriod' in ws_df.columns:\n",
    "        ws_df['FiscalPeriod'] = ws_df['FiscalPeriod'].astype(str).str.strip()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4.2 Merge with CurrencyCodes_clean & compute HistCurrency\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    # Merge currency switch information onto the subset by ID\n",
    "    merged_df = pd.merge(ws_df, currency_cols_to_merge, on='ID', how='left')\n",
    "\n",
    "    # PIT Date and Date1/2/3 are already datetime at this point\n",
    "\n",
    "    # Build conditions for choosing HistCurrency based on PIT Date\n",
    "    conditions = [\n",
    "        # Case 1: Only first currency known (no Date2/Date3) and PIT >= Date1\n",
    "        (merged_df['PIT Date'] >= merged_df['Date1']) &\n",
    "        (merged_df['Date2'].isna()) &\n",
    "        (merged_df['Date3'].isna()),\n",
    "\n",
    "        # Case 2: between Date1 and Date2\n",
    "        (merged_df['PIT Date'] >= merged_df['Date1']) &\n",
    "        (merged_df['PIT Date'] < merged_df['Date2']),\n",
    "\n",
    "        # Case 3: between Date2 and Date3 (or Date3 missing -> open-ended)\n",
    "        (merged_df['PIT Date'] >= merged_df['Date2']) &\n",
    "        (\n",
    "            (merged_df['PIT Date'] < merged_df['Date3']) |\n",
    "            (merged_df['Date3'].isna())\n",
    "        ),\n",
    "\n",
    "        # Case 4: on or after Date3\n",
    "        (merged_df['PIT Date'] >= merged_df['Date3']) &\n",
    "        (merged_df['Date3'].notna())\n",
    "    ]\n",
    "\n",
    "    # Corresponding currency choices for each condition above\n",
    "    choices = [\n",
    "        merged_df['Currency1'],  # Case 1\n",
    "        merged_df['Currency1'],  # Case 2 (still Currency1)\n",
    "        merged_df['Currency2'],  # Case 3\n",
    "        merged_df['Currency3']   # Case 4\n",
    "    ]\n",
    "\n",
    "    # Assign HistCurrency using np.select over the conditions\n",
    "    merged_df['HistCurrency'] = np.select(\n",
    "        conditions,\n",
    "        choices,\n",
    "        default=np.nan\n",
    "    )\n",
    "\n",
    "    # If PIT Date is before Date1 but Date1 exists, assume Currency1 as well\n",
    "    mask_before_first = (\n",
    "        merged_df['PIT Date'].notna() &\n",
    "        merged_df['Date1'].notna() &\n",
    "        (merged_df['PIT Date'] < merged_df['Date1'])\n",
    "    )\n",
    "    merged_df.loc[mask_before_first, 'HistCurrency'] = merged_df.loc[mask_before_first, 'Currency1']\n",
    "\n",
    "    # Drop currency helper columns (keep only HistCurrency from this merge)\n",
    "    merged_df = merged_df.drop(\n",
    "        columns=['Currency1', 'Date1', 'Currency2', 'Date2', 'Currency3', 'Date3'],\n",
    "        errors='ignore'\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4.3 Merge with UpdateCodes_clean\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    # Normalize key columns again (in case merges changed types)\n",
    "    if 'Frequency' in merged_df.columns:\n",
    "        merged_df['Frequency'] = merged_df['Frequency'].astype(str).str.strip()\n",
    "    if 'FiscalPeriod' in merged_df.columns:\n",
    "        merged_df['FiscalPeriod'] = merged_df['FiscalPeriod'].astype(str).str.strip()\n",
    "\n",
    "    # Merge update codes on the full key\n",
    "    merged_df = pd.merge(\n",
    "        merged_df,\n",
    "        update_codes_to_merge,\n",
    "        on=['ID', 'PIT Date', 'Frequency', 'FiscalPeriod'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4.4 Merge with FYE_clean (if available)\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    if fye_cols_to_merge is not None:\n",
    "        # Normalize FiscalPeriod for the join\n",
    "        if 'FiscalPeriod' in merged_df.columns:\n",
    "            merged_df['FiscalPeriod'] = merged_df['FiscalPeriod'].astype(str).str.strip()\n",
    "\n",
    "        # Merge FYE data on ID + FiscalPeriod\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            fye_cols_to_merge,\n",
    "            on=['ID', 'FiscalPeriod'],\n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"  Merged FYE data with {ws_name}.\")\n",
    "    else:\n",
    "        print(f\"  Skipping merge with FYE_clean for {ws_name}.\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4.5 Reorder columns but KEEP everything\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    # Preferred order for key columns in the final file\n",
    "    desired_column_order = [\n",
    "        'ID',\n",
    "        'CompanyName',\n",
    "        'ImplCountry',\n",
    "        'CurrentCurrency',\n",
    "        'HistCurrency',\n",
    "        'PIT Date',\n",
    "        'Frequency',\n",
    "        'UpdateCode',\n",
    "        'FiscalPeriod',\n",
    "        'FYE Month',\n",
    "        'ItemCode',\n",
    "        'Value'\n",
    "    ]\n",
    "\n",
    "    # Columns that exist in merged_df and appear in the desired order\n",
    "    cols_in_df = [c for c in desired_column_order if c in merged_df.columns]\n",
    "    # All remaining columns that were not explicitly ordered\n",
    "    other_cols = [c for c in merged_df.columns if c not in cols_in_df]\n",
    "\n",
    "    # Reorder DataFrame to have key columns first, then everything else\n",
    "    merged_df = merged_df[cols_in_df + other_cols]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4.6 Keep date format as 'YYYY-MM-DD' in output\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    if 'PIT Date' in merged_df.columns:\n",
    "        # Format PIT Date as string with 'YYYY-MM-DD'\n",
    "        merged_df['PIT Date'] = merged_df['PIT Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Note: Currency switch dates remain only in source tables; not stored here.\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4.7 Save back to file\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    # Overwrite the original work_subset file with the enriched data\n",
    "    merged_df.to_csv(file_path, sep='|', index=False)\n",
    "    print(f\"    Saved updated {ws_name} to {file_path}\")\n",
    "\n",
    "    # Clean up to free memory before moving on to the next file\n",
    "    del ws_df\n",
    "    del merged_df\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\nProcessing complete (currency, update codes, FYE, HistCurrency).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM9IyzlhV4tE"
   },
   "source": [
    "## Datastream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQgnee2VXFPd"
   },
   "source": [
    "### Calculate Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 29884,
     "status": "ok",
     "timestamp": 1764881229508,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "IpG7jK24WawW",
    "outputId": "760b87ac-2f66-40db-e482-161aec36b3cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file:      /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/TRI_clean.txt\n",
      "Destination file: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Returns_clean.txt\n",
      "Source exists: True\n",
      "No existing destination file – creating a new one.\n",
      "Finished computing returns file:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Returns_clean.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Compute daily returns for the full TRI dataset in memory\n",
    "#   - Reads TRI_clean.txt from Temp_file_path_GO (on Google Drive).\n",
    "#   - Loads the full file into memory (no chunking).\n",
    "#   - Sorts by ID and DayDate, computes returns from TRI per ID.\n",
    "#   - Stores returns in column 'ret_bps' in basis points with 4 decimals.\n",
    "#       ret_bps_t = (TRI_t / TRI_{t-1} - 1) * 10,000\n",
    "#   - Drops TRI column and writes the result to Returns_clean.txt\n",
    "#     in Temp_file_path_DP with pipe separation.\n",
    "#   - Ensures the output directory exists.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Path configuration: use the paths you defined in your setup cell\n",
    "# -------------------------------------------------------------------------\n",
    "# IMPORTANT: this must be the ORIGINAL TRI file, NOT the returns file\n",
    "src_file = f\"{Temp_file_path_GO}/TRI_clean.txt\"   # original TRI file\n",
    "dst_dir  = Temp_file_path_DP                      # output directory\n",
    "Path(dst_dir).mkdir(parents=True, exist_ok=True)  # ensure output dir exists\n",
    "dst_file = f\"{dst_dir}/Returns_clean.txt\"         # final output file path\n",
    "\n",
    "print(f\"Source file:      {src_file}\")\n",
    "print(f\"Destination file: {dst_file}\")\n",
    "print(\"Source exists:\", os.path.exists(src_file))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# If an old Returns_clean.txt exists at the destination, remove it\n",
    "# so we can write a fresh file.\n",
    "# -------------------------------------------------------------------------\n",
    "if os.path.exists(dst_file):\n",
    "    os.remove(dst_file)\n",
    "    print(\"Existing destination file removed.\")\n",
    "else:\n",
    "    print(\"No existing destination file – creating a new one.\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Read full TRI file into memory\n",
    "# -------------------------------------------------------------------------\n",
    "df = pd.read_csv(\n",
    "    src_file,\n",
    "    sep=\"|\",                  # pipe-separated input\n",
    "    dtype={\"ID\": str},        # ensure ID is handled as string\n",
    "    parse_dates=[\"DayDate\"],  # parse DayDate as datetime\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Sort by ID and DayDate so pct_change computes correct consecutive returns\n",
    "# -------------------------------------------------------------------------\n",
    "df = df.sort_values([\"ID\", \"DayDate\"])\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Compute returns per ID using pct_change\n",
    "# ret_bps_t = (TRI_t / TRI_{t-1} - 1) * 10,000\n",
    "# -------------------------------------------------------------------------\n",
    "df[\"ret_bps\"] = (\n",
    "    df.groupby(\"ID\")[\"TRI\"]\n",
    "      .pct_change() * 10000\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Round returns to 4 decimals as requested\n",
    "# -------------------------------------------------------------------------\n",
    "df[\"ret_bps\"] = df[\"ret_bps\"].round(4)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Drop TRI column after computing returns\n",
    "# -------------------------------------------------------------------------\n",
    "df = df.drop(columns=[\"TRI\"])\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Ensure that the destination directory exists right now\n",
    "# (in case Drive was remounted / folder disappeared)\n",
    "# -------------------------------------------------------------------------\n",
    "Path(dst_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Write full result to destination file (single shot, no append)\n",
    "# -------------------------------------------------------------------------\n",
    "df.to_csv(\n",
    "    dst_file,\n",
    "    sep=\"|\",          # pipe-separated output\n",
    "    index=False,      # no index column\n",
    "    mode=\"w\",         # overwrite\n",
    "    header=True,      # write header\n",
    ")\n",
    "\n",
    "print(\"Finished computing returns file:\")\n",
    "print(dst_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRf_FtD90oCI"
   },
   "source": [
    "### Merge with LC_MV & USD MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading ID list...\n",
      "2. Loading LC MV file...\n",
      "3. Loading Returns file...\n",
      "4. Merging LC MV + Returns...\n",
      "   Step 1 Rows: 207,708,087\n",
      "   Saved intermediate file to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/MarketData_Step1.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY OF THIS SCRIPT (Market Data Merge - Step 1)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This script performs the first phase of merging disparate market data sources\n",
    "# into a unified dataset.\n",
    "#\n",
    "# LOGIC:\n",
    "# 1. Loads the Valid ID List: ensures only relevant companies are processed.\n",
    "# 2. Loads Local Currency Market Values (LC_MV):\n",
    "#    - Renames 'MV' column to 'MV_LC' to distinguish it from USD values later.\n",
    "#    - Filters for valid IDs and standardizes dates.\n",
    "# 3. Loads Stock Returns:\n",
    "#    - Filters for valid IDs and standardizes dates.\n",
    "# 4. Performs an INNER JOIN on [ID, DayDate]:\n",
    "#    - Creates a dataset containing only rows where BOTH Local Currency Market Value\n",
    "#      and Stock Return data exist for that specific day.\n",
    "# 5. Saves the intermediate result to disk (\"MarketData_Step1.txt\") to free up RAM\n",
    "#    before the next merge step.\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "# Inputs\n",
    "lc_mv_file  = f\"{Temp_file_path_GO}/LC_MV_clean.txt\"          \n",
    "ret_file    = f\"{Temp_file_path_DP}/Returns_clean.txt\"     \n",
    "id_file     = f\"{Temp_file_path_EoC}/ID_clean.txt\"         \n",
    "\n",
    "# Output\n",
    "dst_dir     = Temp_file_path_DP                            \n",
    "Path(dst_dir).mkdir(parents=True, exist_ok=True)\n",
    "intermediate_file = f\"{dst_dir}/MarketData_Step1.txt\"        \n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD IDs\n",
    "# =============================================================================\n",
    "print(\"1. Loading ID list...\")\n",
    "id_df = pd.read_csv(id_file, sep=\"|\", dtype=str, usecols=[\"ID\"])\n",
    "valid_ids = set(id_df[\"ID\"])\n",
    "del id_df\n",
    "gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 2. LOAD LC MV FILE (Rename MV -> MV_LC)\n",
    "# =============================================================================\n",
    "print(\"2. Loading LC MV file...\")\n",
    "mv_lc_df = pd.read_csv(\n",
    "    lc_mv_file, \n",
    "    sep=\"|\", \n",
    "    dtype={\"ID\": \"string\", \"MV\": \"float32\", \"PCUR\": \"string\"},\n",
    "    usecols=[\"ID\", \"DayDate\", \"MV\", \"PCUR\"]\n",
    ")\n",
    "\n",
    "# Rename MV to MV_LC immediately\n",
    "mv_lc_df.rename(columns={\"MV\": \"MV_LC\"}, inplace=True)\n",
    "\n",
    "# Filter & Convert\n",
    "mv_lc_df = mv_lc_df[mv_lc_df[\"ID\"].isin(valid_ids)]\n",
    "mv_lc_df[\"DayDate\"] = pd.to_datetime(mv_lc_df[\"DayDate\"], format=\"mixed\", errors=\"coerce\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. LOAD RETURNS FILE\n",
    "# =============================================================================\n",
    "print(\"3. Loading Returns file...\")\n",
    "ret_df = pd.read_csv(\n",
    "    ret_file, \n",
    "    sep=\"|\", \n",
    "    dtype={\"ID\": \"string\", \"ret_bps\": \"float32\"}, \n",
    "    usecols=[\"ID\", \"DayDate\", \"ret_bps\"]\n",
    ")\n",
    "\n",
    "# Filter & Convert\n",
    "ret_df = ret_df[ret_df[\"ID\"].isin(valid_ids)]\n",
    "ret_df[\"DayDate\"] = pd.to_datetime(ret_df[\"DayDate\"], format=\"mixed\", errors=\"coerce\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MERGE STEP 1\n",
    "# =============================================================================\n",
    "print(\"4. Merging LC MV + Returns...\")\n",
    "step1_df = pd.merge(mv_lc_df, ret_df, on=[\"ID\", \"DayDate\"], how=\"inner\")\n",
    "\n",
    "print(f\"   Step 1 Rows: {len(step1_df):,}\")\n",
    "\n",
    "# Save intermediate result\n",
    "step1_df.to_csv(intermediate_file, sep=\"|\", index=False)\n",
    "print(f\"   Saved intermediate file to: {intermediate_file}\")\n",
    "\n",
    "# Clean up\n",
    "del mv_lc_df, ret_df, step1_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Step 1 Dataset...\n",
      "   Loaded rows: 207,708,087\n",
      "2. Loading USD MV file...\n",
      "3. Merging with USD Data (Left Join)...\n",
      "   Rows after merge: 208,304,865\n",
      "4. Removing rows with missing values...\n",
      "   - Rows before filter: 208,304,865\n",
      "   - Rows dropped:       57,637,159\n",
      "   - Rows remaining:     150,667,706\n",
      "5. Saving Final Dataset (150,667,706 rows)...\n",
      "\n",
      "Preview of Final Columns:\n",
      "['ID', 'PCUR', 'DayDate', 'MV_LC', 'ret_bps', 'MV_USD']\n",
      "             ID PCUR    DayDate      MV_LC  ret_bps  MV_USD\n",
      "2723  C025L1660   AP 2003-12-31  81.379997   0.0671   27.75\n",
      "2724  C025L1660   AP 2003-12-31  81.379997   0.0000   27.75\n",
      "2725  C025L1660   AP 2003-12-31  81.379997   0.0671   27.75\n",
      "2726  C025L1660   AP 2003-12-31  81.379997   0.0000   27.75\n",
      "2727  C025L1660   AP 2004-01-01  81.379997   0.0000   27.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY OF THIS SCRIPT (Market Data Merge - Step 2)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This script performs the final phase of the data merge.\n",
    "#\n",
    "# LOGIC:\n",
    "# 1. Loads the Intermediate Dataset (LC MV + Returns) from Step 1.\n",
    "# 2. Loads USD Market Values (MV_USD).\n",
    "# 3. Performs a LEFT JOIN to add USD data.\n",
    "# 4. [NEW] FILTERS CLEAN: Removes ANY row that has a missing value in:\n",
    "#      - MV_LC (Local Currency Market Value)\n",
    "#      - ret_bps (Return in basis points)\n",
    "#      - MV_USD (USD Market Value)\n",
    "#    This ensures the final dataset is \"complete\" (no empty cells in key columns).\n",
    "# 5. Saves the final consolidated file (\"MarketData_pre_clean.txt\").\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "# Inputs\n",
    "usd_mv_file = f\"{Temp_file_path_GO}/MV_clean.txt\"\n",
    "input_file  = f\"{Temp_file_path_DP}/MarketData_Step1.txt\" # Result from Cell 1\n",
    "\n",
    "# Output\n",
    "final_file  = f\"{Temp_file_path_DP}/MarketData_pre_clean.txt\"\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD STEP 1 DATASET\n",
    "# =============================================================================\n",
    "print(\"1. Loading Step 1 Dataset...\")\n",
    "main_df = pd.read_csv(\n",
    "    input_file,\n",
    "    sep=\"|\",\n",
    "    # Explicit types to keep memory usage low\n",
    "    dtype={\"ID\": \"string\", \"MV_LC\": \"float32\", \"ret_bps\": \"float32\", \"PCUR\": \"string\"}\n",
    ")\n",
    "main_df[\"DayDate\"] = pd.to_datetime(main_df[\"DayDate\"], format=\"mixed\", errors=\"coerce\")\n",
    "print(f\"   Loaded rows: {len(main_df):,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. LOAD USD MV FILE (Rename MV -> MV_USD)\n",
    "# =============================================================================\n",
    "print(\"2. Loading USD MV file...\")\n",
    "mv_usd_df = pd.read_csv(\n",
    "    usd_mv_file, \n",
    "    sep=\"|\", \n",
    "    dtype={\"ID\": \"string\", \"MV\": \"float32\"},\n",
    "    usecols=[\"ID\", \"DayDate\", \"MV\"]\n",
    ")\n",
    "\n",
    "# Rename MV to MV_USD immediately\n",
    "mv_usd_df.rename(columns={\"MV\": \"MV_USD\"}, inplace=True)\n",
    "\n",
    "# Convert Date\n",
    "mv_usd_df[\"DayDate\"] = pd.to_datetime(mv_usd_df[\"DayDate\"], format=\"mixed\", errors=\"coerce\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MERGE STEP 2 (LEFT JOIN)\n",
    "# =============================================================================\n",
    "print(\"3. Merging with USD Data (Left Join)...\")\n",
    "\n",
    "final_df = pd.merge(main_df, mv_usd_df, on=[\"ID\", \"DayDate\"], how=\"left\")\n",
    "print(f\"   Rows after merge: {len(final_df):,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. [NEW] FILTER MISSING VALUES\n",
    "# =============================================================================\n",
    "print(\"4. Removing rows with missing values...\")\n",
    "\n",
    "rows_before = len(final_df)\n",
    "\n",
    "# Drop rows where ANY of these specific columns are NaN\n",
    "cols_to_check = [\"MV_LC\", \"ret_bps\", \"MV_USD\"]\n",
    "final_df.dropna(subset=cols_to_check, inplace=True)\n",
    "\n",
    "rows_after = len(final_df)\n",
    "dropped_count = rows_before - rows_after\n",
    "\n",
    "print(f\"   - Rows before filter: {rows_before:,}\")\n",
    "print(f\"   - Rows dropped:       {dropped_count:,}\")\n",
    "print(f\"   - Rows remaining:     {rows_after:,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. SAVE FINAL DATASET\n",
    "# =============================================================================\n",
    "if not final_df.empty:\n",
    "    print(f\"5. Saving Final Dataset ({len(final_df):,} rows)...\")\n",
    "    final_df.to_csv(final_file, sep=\"|\", index=False)\n",
    "    \n",
    "    # Preview\n",
    "    print(\"\\nPreview of Final Columns:\")\n",
    "    print(final_df.columns.tolist())\n",
    "    print(final_df.head())\n",
    "else:\n",
    "    print(\"Warning: All rows were dropped! Check if the datasets actually overlap.\")\n",
    "\n",
    "# Clean up\n",
    "del main_df, mv_usd_df, final_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNHlamBB3d_J"
   },
   "source": [
    "### Get Summary of Data for Table 2 from the Original Paper (Indication, not Final!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5687,
     "status": "ok",
     "timestamp": 1764881287092,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "fw3wD4Xf4FLZ",
    "outputId": "fe1c2061-35d4-4a42-9da1-601f7ec0b46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using merged file:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/MarketData_pre_clean.txt\n",
      "\n",
      "--- Loading Data and Applying Filters ---\n",
      "Total rows loaded: 150,667,706\n",
      "Filter Thresholds: MV_USD > 5,000,000 | Return > 100,000 bps (1000%)\n",
      " - Rows with High MV: 3,289\n",
      " - Rows with High Return: 12,898\n",
      " - Total Unique Rows Dropped: 16,187\n",
      "Rows remaining for analysis: 150,651,519\n",
      "\n",
      "--- Analyzing column: MV_USD ---\n",
      "  Performing ID-level winsorization for MV_USD...\n",
      "\n",
      "--- Analyzing column: ret_bps ---\n",
      "  Performing ID-level winsorization for ret_bps...\n",
      "\n",
      "=== SUMMARY STATISTICS (Post-Filtering) ===\n",
      "    Column  Observations_Used  Excluded_High_MV_USD  Excluded_High_Ret  \\\n",
      "0   MV_USD          150651519                  3289              12898   \n",
      "1  ret_bps          150651519                  3289              12898   \n",
      "\n",
      "   Total_Rows_Dropped     Mean_Raw   Std_Dev_Raw  Median_Raw  Mean_Winsorized  \\\n",
      "0               16187  1911.139135  17954.744053      157.05      1908.180445   \n",
      "1               16187     7.505989    421.058291        0.00         5.417396   \n",
      "\n",
      "   Winsorization_Alpha  \n",
      "0                 0.01  \n",
      "1                 0.01  \n",
      "\n",
      "Excel summary successfully saved to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/MarketData_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: SUMMARY STATISTICS (WITH MV AND RETURN THRESHOLD FILTERING)\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Loads the full merged MarketData_pre_clean.txt file.\n",
    "# 2) **FILTERING STEP**:\n",
    "#       - Identifies rows where MV > 5,000,000.\n",
    "#       - Identifies rows where Return > 1000% (converted to 100,000 bps).\n",
    "#       - Excludes these rows from ALL subsequent statistics.\n",
    "#       - Tracks the count of exclusions for both criteria.\n",
    "# 3) Computes statistics (Mean, Std, Median, Percentiles) on the filtered data.\n",
    "# 4) Performs ID-level winsorization (configurable).\n",
    "# 5) Saves Summary and Distribution tables to Excel.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Paths\n",
    "merged_file_path       = f\"{Temp_file_path_DP}/MarketData_pre_clean.txt\"\n",
    "stats_output_dir       = Temp_file_path_DP\n",
    "stats_output_name_xlsx = \"MarketData_summary.xlsx\"\n",
    "stats_output_path_xlsx = f\"{stats_output_dir}/{stats_output_name_xlsx}\"\n",
    "\n",
    "# Filtering Thresholds\n",
    "mv_filter_threshold          = 5_000_000   # Exclude if MV > 5,000,000\n",
    "ret_filter_threshold_percent = 1000        # Exclude if Return > 1000%\n",
    "ret_filter_threshold_bps     = ret_filter_threshold_percent * 100  # Convert % to bps\n",
    "\n",
    "# Winsorization configuration\n",
    "winsorize_data   = True      # set to False to skip winsorization\n",
    "winsor_alpha     = 0.01      # 1% winsorization in each tail\n",
    "winsor_id_column = \"ID\"      # column name for ID-level winsorization\n",
    "\n",
    "# Distribution quantiles configuration (in percent)\n",
    "distribution_percentiles = [0, 1, 5, 10, 20, 30, 40, 50,\n",
    "                            60, 70, 80, 90, 95, 99, 100]\n",
    "\n",
    "# Create index labels for rows (min, p01, ... max)\n",
    "dist_index_labels = []\n",
    "for p in distribution_percentiles:\n",
    "    if p == 0:\n",
    "        dist_index_labels.append(\"min\")\n",
    "    elif p == 100:\n",
    "        dist_index_labels.append(\"max\")\n",
    "    else:\n",
    "        dist_index_labels.append(f\"p{int(p):02d}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAFETY CHECK\n",
    "# =============================================================================\n",
    "\n",
    "if not os.path.exists(merged_file_path):\n",
    "    raise FileNotFoundError(f\"Merged file not found at: {merged_file_path}\")\n",
    "\n",
    "print(\"Using merged file:\")\n",
    "print(merged_file_path)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING AND FILTERING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Loading Data and Applying Filters ---\")\n",
    "\n",
    "cols_to_load = [\"MV_USD\", \"ret_bps\"]\n",
    "if winsorize_data:\n",
    "    cols_to_load.append(winsor_id_column)\n",
    "cols_to_load = list(set(cols_to_load))\n",
    "\n",
    "df_full = pd.read_csv(\n",
    "    merged_file_path,\n",
    "    sep=\"|\",\n",
    "    usecols=cols_to_load\n",
    ")\n",
    "\n",
    "total_rows_loaded = len(df_full)\n",
    "print(f\"Total rows loaded: {total_rows_loaded:,}\")\n",
    "\n",
    "# 1. Identify High MV rows\n",
    "high_mv_mask = df_full[\"MV_USD\"] > mv_filter_threshold\n",
    "count_high_mv = high_mv_mask.sum()\n",
    "\n",
    "# 2. Identify High Return rows (> 1000% -> > 100,000 bps)\n",
    "high_ret_mask = df_full[\"ret_bps\"] > ret_filter_threshold_bps\n",
    "count_high_ret = high_ret_mask.sum()\n",
    "\n",
    "# 3. Combine filters (Exclude if EITHER is true)\n",
    "combined_exclusion_mask = high_mv_mask | high_ret_mask\n",
    "total_dropped = combined_exclusion_mask.sum()\n",
    "\n",
    "# Apply Filter\n",
    "df_working = df_full[~combined_exclusion_mask].copy()\n",
    "remaining_rows = len(df_working)\n",
    "\n",
    "print(f\"Filter Thresholds: MV_USD > {mv_filter_threshold:,.0f} | Return > {ret_filter_threshold_bps:,} bps ({ret_filter_threshold_percent}%)\")\n",
    "print(f\" - Rows with High MV: {count_high_mv:,}\")\n",
    "print(f\" - Rows with High Return: {count_high_ret:,}\")\n",
    "print(f\" - Total Unique Rows Dropped: {total_dropped:,}\")\n",
    "print(f\"Rows remaining for analysis: {remaining_rows:,}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTION: Analyze a single numeric column (on filtered data)\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_column(df_in: pd.DataFrame, col_name: str):\n",
    "    print(f\"\\n--- Analyzing column: {col_name} ---\")\n",
    "\n",
    "    # Clean numeric column: replace +/- inf with NaN\n",
    "    series_data = df_in[col_name].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if winsorize_data:\n",
    "        temp_df = pd.DataFrame({\n",
    "            col_name: series_data,\n",
    "            winsor_id_column: df_in[winsor_id_column]\n",
    "        })\n",
    "        valid = temp_df.dropna(subset=[col_name])\n",
    "    else:\n",
    "        valid = series_data.dropna().to_frame(name=col_name)\n",
    "\n",
    "    if valid.empty:\n",
    "        print(f\"WARNING: No valid observations found in {col_name} after cleaning.\")\n",
    "        return {k: np.nan for k in [\"count\", \"mean_raw\", \"std_raw\", \"median_raw\", \"winsor_mean\"]} | {\"raw_q\": None, \"winsor_q\": None}\n",
    "\n",
    "    values_all = valid[col_name].to_numpy()\n",
    "\n",
    "    # Raw statistics\n",
    "    count = values_all.size\n",
    "    mean_raw = values_all.mean()\n",
    "    std_raw = values_all.std(ddof=1) if count > 1 else np.nan\n",
    "    median_raw = np.median(values_all)\n",
    "    raw_q = np.percentile(values_all, distribution_percentiles)\n",
    "\n",
    "    # Winsorized statistics\n",
    "    winsor_mean = np.nan\n",
    "    winsor_q = None\n",
    "\n",
    "    if winsorize_data:\n",
    "        print(f\"  Performing ID-level winsorization for {col_name}...\")\n",
    "        lower_q = valid.groupby(winsor_id_column)[col_name].transform(lambda x: x.quantile(winsor_alpha))\n",
    "        upper_q = valid.groupby(winsor_id_column)[col_name].transform(lambda x: x.quantile(1.0 - winsor_alpha))\n",
    "        winsor_vals = valid[col_name].clip(lower=lower_q, upper=upper_q)\n",
    "        winsor_mean = winsor_vals.mean()\n",
    "        winsor_q = np.percentile(winsor_vals.to_numpy(), distribution_percentiles)\n",
    "    else:\n",
    "        print(f\"  Winsorization disabled for {col_name}.\")\n",
    "\n",
    "    return {\n",
    "        \"count\": count,\n",
    "        \"mean_raw\": mean_raw,\n",
    "        \"std_raw\": std_raw,\n",
    "        \"median_raw\": median_raw,\n",
    "        \"raw_q\": raw_q,\n",
    "        \"winsor_mean\": winsor_mean,\n",
    "        \"winsor_q\": winsor_q\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "mv_results = analyze_column(df_working, \"MV_USD\")\n",
    "ret_results = analyze_column(df_working, \"ret_bps\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BUILD SUMMARY DATAFRAME\n",
    "# =============================================================================\n",
    "\n",
    "stats_df = pd.DataFrame({\n",
    "    \"Column\": [\"MV_USD\", \"ret_bps\"],\n",
    "    \"Observations_Used\": [mv_results[\"count\"], ret_results[\"count\"]],\n",
    "    \"Excluded_High_MV_USD\": [count_high_mv, count_high_mv],\n",
    "    \"Excluded_High_Ret\": [count_high_ret, count_high_ret],\n",
    "    \"Total_Rows_Dropped\": [total_dropped, total_dropped],\n",
    "    \"Mean_Raw\": [mv_results[\"mean_raw\"], ret_results[\"mean_raw\"]],\n",
    "    \"Std_Dev_Raw\": [mv_results[\"std_raw\"], ret_results[\"std_raw\"]],\n",
    "    \"Median_Raw\": [mv_results[\"median_raw\"], ret_results[\"median_raw\"]],\n",
    "    \"Mean_Winsorized\": [mv_results[\"winsor_mean\"], ret_results[\"winsor_mean\"]],\n",
    "    \"Winsorization_Alpha\": [winsor_alpha if winsorize_data else np.nan] * 2\n",
    "})\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BUILD DISTRIBUTION TABLES\n",
    "# =============================================================================\n",
    "\n",
    "if mv_results[\"raw_q\"] is not None and ret_results[\"raw_q\"] is not None:\n",
    "    raw_distribution_df = pd.DataFrame(\n",
    "        {\"MV_USD_raw\": mv_results[\"raw_q\"], \"ret_bps_raw\": ret_results[\"raw_q\"]},\n",
    "        index=dist_index_labels\n",
    "    )\n",
    "else:\n",
    "    raw_distribution_df = pd.DataFrame()\n",
    "\n",
    "winsor_distribution_df = None\n",
    "if winsorize_data and (mv_results[\"winsor_q\"] is not None) and (ret_results[\"winsor_q\"] is not None):\n",
    "    winsor_distribution_df = pd.DataFrame(\n",
    "        {\"MV_USD_winsorized\": mv_results[\"winsor_q\"], \"ret_bps_winsorized\": ret_results[\"winsor_q\"]},\n",
    "        index=dist_index_labels\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS TO EXCEL\n",
    "# =============================================================================\n",
    "\n",
    "os.makedirs(stats_output_dir, exist_ok=True)\n",
    "\n",
    "with pd.ExcelWriter(stats_output_path_xlsx) as writer:\n",
    "    stats_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    raw_distribution_df.to_excel(writer, sheet_name=\"Raw_Distribution\", index=True)\n",
    "    if winsor_distribution_df is not None:\n",
    "        winsor_distribution_df.to_excel(writer, sheet_name=\"Winsorized_Distribution\", index=True)\n",
    "\n",
    "print(\"\\n=== SUMMARY STATISTICS (Post-Filtering) ===\")\n",
    "print(stats_df)\n",
    "print(\"\\nExcel summary successfully saved to:\")\n",
    "print(stats_output_path_xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ExqzYD05cCJ"
   },
   "source": [
    "### Winsorize Returns (1% per ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12056,
     "status": "ok",
     "timestamp": 1764881299169,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "uBNb08oW5owt",
    "outputId": "db1ab26d-70b0-486e-c631-80f89ed6520d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input file:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/MarketData_pre_clean.txt\n",
      "Original row count: 150,667,706\n",
      "\n",
      "--- Applying Exclusion Filters ---\n",
      "Thresholds: MV_USD > 5,000,000 | Return > 100,000 bps\n",
      "Rows dropped: 16,187\n",
      "Rows remaining: 150,651,519\n",
      "\n",
      "--- Winsorizing column: MV_USD ---\n",
      "\n",
      "--- Winsorizing column: ret_bps ---\n",
      "\n",
      "Full winsorized dataset created in memory.\n",
      "Cleaned and winsorized dataset saved to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/MarketData_clean.txt\n",
      "\n",
      "--- Computing winsorized statistics for: MV_USD ---\n",
      "\n",
      "--- Computing winsorized statistics for: ret_bps ---\n",
      "\n",
      "=== WINSORIZED SUMMARY STATISTICS ===\n",
      "    Column  Observations  Rows_Dropped_Filter  Mean_Winsorized  \\\n",
      "0   MV_USD     150651519                16187      1908.180445   \n",
      "1  ret_bps     150651519                16187         5.417396   \n",
      "\n",
      "   Std_Dev_Winsorized  Median_Winsorized  Winsorization_Alpha  \\\n",
      "0        17876.494586             157.01                 0.01   \n",
      "1          336.252210               0.00                 0.01   \n",
      "\n",
      "  Winsorization_By_ID_Column  \n",
      "0                         ID  \n",
      "1                         ID  \n",
      "\n",
      "=== WINSORIZED DISTRIBUTION ===\n",
      "     MV_USD_winsorized  ret_bps_winsorized\n",
      "min       1.000000e+01       -10000.000000\n",
      "p01       1.197000e+01         -868.748500\n",
      "p05       1.720000e+01         -454.266383\n",
      "p10       2.355000e+01         -291.262100\n",
      "p20       3.908000e+01         -145.308000\n",
      "p30       6.158000e+01          -63.694300\n",
      "p40       9.700000e+01           -5.727800\n",
      "p50       1.570100e+02            0.000000\n",
      "p60       2.641100e+02            0.000000\n",
      "p70       4.672500e+02           55.185100\n",
      "p80       9.242300e+02          138.869000\n",
      "p90       2.543370e+03          300.632900\n",
      "p95       6.058880e+03          493.856300\n",
      "p99       3.070432e+04         1005.082845\n",
      "max       3.453938e+06        72345.333880\n",
      "\n",
      "Winsorized summary Excel successfully saved to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/MarketData_winsorized_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CREATE FULLY CLEANED & WINSORIZED DATASET + SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Loads the entire MarketData_pre_clean.txt file into memory.\n",
    "# 2) **FILTERING STEP**:\n",
    "#       - Removes rows where MV > 5,000,000.\n",
    "#       - Removes rows where Return > 1000% (converted to basis points).\n",
    "#       - These rows are physically deleted from the dataframe.\n",
    "# 3) Performs ID-level winsorization at 1% in each tail for MV and ret_bps\n",
    "#    on the remaining data.\n",
    "# 4) Writes the fully cleaned & winsorized dataset to a new text file.\n",
    "# 5) Computes summary statistics and distribution tables for the winsorized\n",
    "#    columns only.\n",
    "# 6) Saves these results into an Excel file.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Input file: uses the same path as in Cell 1\n",
    "input_file_path = merged_file_path  # relies on variable defined in Cell 1\n",
    "\n",
    "# Output file for the full cleaned & winsorized dataset\n",
    "winsorized_file_path = f\"{Temp_file_path_DP}/MarketData_clean.txt\"\n",
    "\n",
    "# Excel summary for the winsorized data\n",
    "winsor_stats_output_dir       = Temp_file_path_DP\n",
    "winsor_stats_output_name_xlsx = \"MarketData_winsorized_summary.xlsx\"\n",
    "winsor_stats_output_path_xlsx = f\"{winsor_stats_output_dir}/{winsor_stats_output_name_xlsx}\"\n",
    "\n",
    "# Filtering Thresholds (Must match previous cell logic)\n",
    "mv_filter_threshold          = 5_000_000   # Exclude if MV > 5,000,000\n",
    "ret_filter_threshold_percent = 1000        # Exclude if Return > 1000%\n",
    "ret_filter_threshold_bps     = ret_filter_threshold_percent * 100\n",
    "\n",
    "# Winsorization configuration\n",
    "winsor_alpha     = 0.01                 # 1% in each tail\n",
    "winsor_id_column = \"ID\"                 # ID-level winsorization\n",
    "target_columns   = [\"MV_USD\", \"ret_bps\"]    # columns to winsorize\n",
    "\n",
    "# Distribution quantiles configuration (in percent)\n",
    "distribution_percentiles = [0, 1, 5, 10, 20, 30, 40, 50,\n",
    "                            60, 70, 80, 90, 95, 99, 100]\n",
    "\n",
    "# Create index labels for rows\n",
    "dist_index_labels = []\n",
    "for p in distribution_percentiles:\n",
    "    if p == 0:\n",
    "        dist_index_labels.append(\"min\")\n",
    "    elif p == 100:\n",
    "        dist_index_labels.append(\"max\")\n",
    "    else:\n",
    "        dist_index_labels.append(f\"p{int(p):02d}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SAFETY CHECK\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if not os.path.exists(input_file_path):\n",
    "    raise FileNotFoundError(f\"Input file not found at: {input_file_path}\")\n",
    "\n",
    "print(\"Processing input file:\")\n",
    "print(input_file_path)\n",
    "\n",
    "os.makedirs(winsor_stats_output_dir, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD FULL DATASET\n",
    "# =============================================================================\n",
    "\n",
    "df = pd.read_csv(\n",
    "    input_file_path,\n",
    "    sep=\"|\"\n",
    ")\n",
    "original_row_count = len(df)\n",
    "print(f\"Original row count: {original_row_count:,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1.5: APPLY FILTERS AND REMOVE ROWS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Applying Exclusion Filters ---\")\n",
    "\n",
    "# Identify rows to drop\n",
    "# We use fillna(False) so we don't accidentally drop NaNs here (NaNs handled in winsorization)\n",
    "high_mv_mask = df[\"MV_USD\"] > mv_filter_threshold\n",
    "high_ret_mask = df[\"ret_bps\"] > ret_filter_threshold_bps\n",
    "\n",
    "# Combine masks\n",
    "rows_to_drop_mask = high_mv_mask | high_ret_mask\n",
    "drop_count = rows_to_drop_mask.sum()\n",
    "\n",
    "# Perform the drop\n",
    "df = df[~rows_to_drop_mask].copy()\n",
    "new_row_count = len(df)\n",
    "\n",
    "print(f\"Thresholds: MV_USD > {mv_filter_threshold:,.0f} | Return > {ret_filter_threshold_bps:,} bps\")\n",
    "print(f\"Rows dropped: {drop_count:,}\")\n",
    "print(f\"Rows remaining: {new_row_count:,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: APPLY ID-LEVEL WINSORIZATION TO TARGET COLUMNS\n",
    "# =============================================================================\n",
    "\n",
    "for col in target_columns:\n",
    "    if col not in df.columns:\n",
    "        print(f\"Column {col} not found in input data. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Winsorizing column: {col} ---\")\n",
    "\n",
    "    # Clean numeric column: replace +/- inf with NaN\n",
    "    df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Compute lower and upper quantiles per ID on valid values only\n",
    "    lower_q = df.groupby(winsor_id_column)[col].transform(\n",
    "        lambda x: x.quantile(winsor_alpha)\n",
    "    )\n",
    "    upper_q = df.groupby(winsor_id_column)[col].transform(\n",
    "        lambda x: x.quantile(1.0 - winsor_alpha)\n",
    "    )\n",
    "\n",
    "    # Winsorize within [lower_q, upper_q] per ID.\n",
    "    mask_valid_vals = df[col].notna()\n",
    "    bounds_defined = mask_valid_vals & lower_q.notna() & upper_q.notna()\n",
    "\n",
    "    # Initialize with original values\n",
    "    winsor_col = df[col].copy()\n",
    "\n",
    "    # Apply clipping only where both bounds are defined\n",
    "    winsor_col.loc[bounds_defined] = winsor_col.loc[bounds_defined].clip(\n",
    "        lower=lower_q.loc[bounds_defined],\n",
    "        upper=upper_q.loc[bounds_defined]\n",
    "    )\n",
    "\n",
    "    df[col] = winsor_col\n",
    "\n",
    "print(\"\\nFull winsorized dataset created in memory.\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: SAVE THE FULL CLEANED & WINSORIZED DATASET\n",
    "# =============================================================================\n",
    "\n",
    "df.to_csv(\n",
    "    winsorized_file_path,\n",
    "    sep=\"|\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Cleaned and winsorized dataset saved to:\")\n",
    "print(winsorized_file_path)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: COMPUTE SUMMARY STATISTICS AND DISTRIBUTION FOR WINSORIZED DATA\n",
    "# =============================================================================\n",
    "\n",
    "summary_rows = []\n",
    "dist_data = {}\n",
    "\n",
    "for col in target_columns:\n",
    "    if col not in df.columns:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Computing winsorized statistics for: {col} ---\")\n",
    "\n",
    "    valid_vals = df[col].dropna()\n",
    "    if valid_vals.empty:\n",
    "        # If filtering removed everything, handle gracefully\n",
    "        print(f\"Warning: No valid observations left for {col}.\")\n",
    "        continue\n",
    "\n",
    "    count      = valid_vals.size\n",
    "    mean_w     = valid_vals.mean()\n",
    "    std_w      = valid_vals.std(ddof=1) if count > 1 else np.nan\n",
    "    median_w   = valid_vals.median()\n",
    "    q_vals     = np.percentile(valid_vals.to_numpy(), distribution_percentiles)\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"Column\": col,\n",
    "        \"Observations\": count,\n",
    "        \"Rows_Dropped_Filter\": drop_count,\n",
    "        \"Mean_Winsorized\": mean_w,\n",
    "        \"Std_Dev_Winsorized\": std_w,\n",
    "        \"Median_Winsorized\": median_w,\n",
    "        \"Winsorization_Alpha\": winsor_alpha,\n",
    "        \"Winsorization_By_ID_Column\": winsor_id_column\n",
    "    })\n",
    "\n",
    "    dist_data[f\"{col}_winsorized\"] = q_vals\n",
    "\n",
    "# Summary DataFrame\n",
    "winsor_stats_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Distribution DataFrame\n",
    "if dist_data:\n",
    "    winsor_distribution_df = pd.DataFrame(\n",
    "        dist_data,\n",
    "        index=dist_index_labels\n",
    "    )\n",
    "else:\n",
    "    winsor_distribution_df = pd.DataFrame()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: SAVE SUMMARY TO EXCEL\n",
    "# =============================================================================\n",
    "\n",
    "with pd.ExcelWriter(winsor_stats_output_path_xlsx) as writer:\n",
    "    winsor_stats_df.to_excel(writer, sheet_name=\"Summary_Winsorized\", index=False)\n",
    "    winsor_distribution_df.to_excel(writer, sheet_name=\"Distribution_Winsorized\", index=True)\n",
    "\n",
    "print(\"\\n=== WINSORIZED SUMMARY STATISTICS ===\")\n",
    "print(winsor_stats_df)\n",
    "\n",
    "print(\"\\n=== WINSORIZED DISTRIBUTION ===\")\n",
    "print(winsor_distribution_df)\n",
    "\n",
    "print(\"\\nWinsorized summary Excel successfully saved to:\")\n",
    "print(winsor_stats_output_path_xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oYu5Sa-xLLf"
   },
   "source": [
    "### Check ID Difference\n",
    "\n",
    " => Comment: Difference due to non-overlapping Dates in Returns_clean and MV_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1885,
     "status": "ok",
     "timestamp": 1764881401089,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "VPbHUKQjZ5pE",
    "outputId": "ef3a0cfe-9cbe-472a-f841-4b47d4e043ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764\n",
      "['C036000A0', 'C036001A0', 'C036001G0', 'C036001I0', 'C036001T0', 'C036002V0', 'C03601380', 'C03607230', 'C03607C00', 'C03617890', 'C0361C100', 'C03622200', 'C03627960', 'C03640640', 'C03642D00', 'C03655920', 'C03656110', 'C03656130', 'C03659670', 'C0365F500', 'C0365LF00', 'C03661390', 'C03665920', 'C03666170', 'C03667450', 'C03668570', 'C0367RB00', 'C0367S800', 'C03682060', 'C0368V500', 'C03694900', 'C036A1720', 'C036A2A00', 'C036BT320', 'C036BY900', 'C036CA510', 'C036CD530', 'C036CH450', 'C036CT200', 'C036E0280', 'C036EB500', 'C036EHB00', 'C036F00E0', 'C036F03W0', 'C036F05N0', 'C036F13G0', 'C036F4090', 'C036F41K0', 'C036F4520', 'C036F46B0', 'C036F49L0', 'C036F5160', 'C036F5170', 'C036F60T0', 'C036F6170', 'C036F6230', 'C036F6250', 'C036F62O0', 'C036F6780', 'C036F67J0', 'C036F6840', 'C036F68Q0', 'C036F69H0', 'C036F7220', 'C036F7440', 'C036F7660', 'C036F7840', 'C036F7930', 'C036F7D00', 'C036F8310', 'C036F8380', 'C036F8530', 'C036F8590', 'C036F9280', 'C036F9360', 'C036F9740', 'C036FHB00', 'C036FN710', 'C036FP130', 'C036G1260', 'C036G1450', 'C036G1540', 'C036G1550', 'C036G1670', 'C036G1740', 'C036G9B00', 'C036GUB00', 'C036HBQ10', 'C036HI700', 'C036HLF00', 'C036HNF00', 'C036IGE00', 'C036IJC00', 'C036JKF00', 'C036K3C00', 'C036K5810', 'C036KP000', 'C036LNC00', 'C036M2B00', 'C036M3C00', 'C036MF400', 'C036NBQ10', 'C036NEC00', 'C036PPC00', 'C036Q7D00', 'C036QGF00', 'C036QJ900', 'C036QPC00', 'C036RAF00', 'C036RR700', 'C036TMC00', 'C036TZA00', 'C036UZE00', 'C036WM400', 'C036Z2R10', 'C036ZL100', 'C036ZT400', 'C04003260', 'C040FG570', 'C056I15M0', 'C056TCF60', 'C060P5B00', 'C060P7EO0', 'C070003N0', 'C070003O0', 'C07620640', 'C0765SG70', 'C076F0F60', 'C076G20K0', 'C076G20L0', 'C076G20P0', 'C076G21M0', 'C076L0680', 'C076L5200', 'C076L5470', 'C076L5980', 'C076L6190', 'C100000A0', 'C10000290', 'C100002D0', 'C10000490', 'C100006J0', 'C100007T0', 'C100007U0', 'C1240Z000', 'C12438340', 'C12438830', 'C12438B00', 'C1244ORL0', 'C1244V000', 'C12450300', 'C12450480', 'C12450580', 'C1245A000', 'C1245T000', 'C12460640', 'C12465440', 'C1246FB00', 'C1248J990', 'C1249C000', 'C1249F100', 'C124A0750', 'C124AA410', 'C124AS010', 'C124AS050', 'C124AS200', 'C124CCB00', 'C124CE0Y0', 'C124CE6O0', 'C124CF670', 'C124CH130', 'C124CI250', 'C124CI610', 'C124D00Q0', 'C124D0240', 'C124D04C0', 'C124D04L0', 'C124D04Y0', 'C124D05A0', 'C124D07S0', 'C124D0900', 'C124D12F0', 'C124D1760', 'C124D19O0', 'C124D1Z50', 'C124D21H0', 'C124D22P0', 'C124D2300', 'C124D2ZL0', 'C124D30A0', 'C124D30R0', 'C124D3ZC0', 'C124DC010', 'C124DC160', 'C124DC370', 'C124DC4P0', 'C124DC560', 'C124DC5S0', 'C124DC6V0', 'C124DU000', 'C124GZ000', 'C124IFB00', 'C124II0Y0', 'C124IV000', 'C124K5940', 'C124K8360', 'C124LM000', 'C124OM000', 'C124SDB00', 'C124TT000', 'C124UC100', 'C124VE4K0', 'C124WC100', 'C124XCS10', 'C124Y0910', 'C124ZZ800', 'C1529T500', 'C152CE080', 'C152TM500', 'C152Z9460', 'C156000O0', 'C15600110', 'C1560PS10', 'C15678Q10', 'C156BJ0W0', 'C156BJ1H0', 'C156GD150', 'C156H00E0', 'C156H00J0', 'C156HK2H0', 'C156HK2O0', 'C156HK3L0', 'C156HK4F0', 'C156HKC60', 'C156OQR10', 'C156RCQ10', 'C156Z5Q10', 'C175000B0', 'C17562390', 'C191002A0', 'C191003E0', 'C196000G0', 'C19600430', 'C196004M0', 'C196004U0', 'C196005G0', 'C20848810', 'C208C2810', 'C208C4070', 'C24694390', 'C246BR150', 'C25000020', 'C25000430', 'C25029450', 'C25033420', 'C25034160', 'C25034200', 'C25034220', 'C25035220', 'C250464J0', 'C25047910', 'C25049240', 'C25075220', 'C250AT980', 'C250AV560', 'C250C8250', 'C250C8680', 'C250C8690', 'C250C9740', 'C250C9850', 'C250C99D0', 'C250C99H0', 'C250CC590', 'C250D4280', 'C250F2100', 'C250I1340', 'C250J2830', 'C250J2850', 'C250K6130', 'C250K8460', 'C250MZ300', 'C250YJ100', 'C28025260', 'C28028530', 'C28036330', 'C280660K0', 'C28066920', 'C28081030', 'C280AW560', 'C280BH6Q0', 'C280D5430', 'C280L3700', 'C280T4S10', 'C3009OSJ0', 'C300AV970', 'C300AX820', 'C300B9690', 'C300J0470', 'C344AQ060', 'C344AQ730', 'C344BT450', 'C344E1550', 'C344E1710', 'C344G4770', 'C35000010', 'C35073900', 'C356006E0', 'C356037I0', 'C35606000', 'C35627E00', 'C356AEF00', 'C356WFD00', 'C356XSD00', 'C366800S0', 'C36680170', 'C366843O0', 'C366845I0', 'C366846O0', 'C366BU820', 'C366CP960', 'C366CQ000', 'C366G6200', 'C366G6450', 'C366G6590', 'C37205380', 'C37251610', 'C37253150', 'C372EG560', 'C372J0090', 'C372J0650', 'C372J0950', 'C37600250', 'C376003W0', 'C376004M0', 'C37600580', 'C376005B0', 'C376005H0', 'C376005T0', 'C376006D0', 'C376006L0', 'C37600890', 'C37600950', 'C376LF1A0', 'C376ZB400', 'C380063H0', 'C380H7170', 'C380H9250', 'C392028G0', 'C39202900', 'C39202910', 'C39203Z40', 'C39203Z70', 'C39204010', 'C392040H0', 'C39204ZC0', 'C39204ZP0', 'C39204ZT0', 'C39290180', 'C39293470', 'C392AI0Q0', 'C392AI1M0', 'C392AI1X0', 'C392AI2O0', 'C392AI3K0', 'C392AI3O0', 'C392AI3V0', 'C392AI3W0', 'C392AI5P0', 'C392AJ180', 'C392AY450', 'C392BY540', 'C392BY910', 'C392GTF00', 'C392HS1G0', 'C392ITG00', 'C392J8250', 'C392K10X0', 'C392K10Y0', 'C392K12A0', 'C392NKQ10', 'C392WVA00', 'C392WYC00', 'C392X30K0', 'C392X30X0', 'C392X32I0', 'C392X32N0', 'C392X33B0', 'C392X33N0', 'C392X33O0', 'C392X33Q0', 'C392XUG00', 'C39800030', 'C39800070', 'C39800090', 'C398000H0', 'C39800100', 'C39800140', 'C39800180', 'C39800240', 'C400003C0', 'C400D1260', 'C41000060', 'C410000B0', 'C41007890', 'C41010ZP0', 'C41011880', 'C41019G00', 'C4103CG00', 'C4106QR10', 'C410ABG00', 'C410B6G00', 'C410BL740', 'C410E6G00', 'C410HCG00', 'C410QEG00', 'C410SYF00', 'C410UZF00', 'C410Y0G00', 'C414000A0', 'C458001D0', 'C4585D400', 'C458G8380', 'C458JI400', 'C458VM2Y0', 'C458VM490', 'C458VM590', 'C480000N0', 'C484637A0', 'C484GBS10', 'C484K2580', 'C52859480', 'C52883660', 'C52893780', 'C528AK110', 'C528AK230', 'C528C0720', 'C528CG700', 'C528E7500', 'C528G0670', 'C528G1310', 'C5540BC00', 'C554W9050', 'C566CQ200', 'C57800600', 'C5780NQV0', 'C5787YA00', 'C5788GQ10', 'C5789HS10', 'C578B7370', 'C578B7450', 'C578H9110', 'C578I4370', 'C578JLS10', 'C5825Z0B0', 'C586005V0', 'C58600620', 'C58600740', 'C58669540', 'C59700230', 'C597002F0', 'C59769910', 'C60857440', 'C608H29I0', 'C617003P0', 'C617010V0', 'C61701630', 'C617CH760', 'C62022440', 'C62098390', 'C620A9970', 'C620B9480', 'C620D2020', 'C620D3950', 'C620FF840', 'C642001H0', 'C64316E00', 'C643402V0', 'C643408O0', 'C643C00R0', 'C682AJD00', 'C688000E0', 'C688000K0', 'C688000N0', 'C68800160', 'C688001G0', 'C688001S0', 'C68800210', 'C68800230', 'C68800250', 'C68800260', 'C688002H0', 'C688002J0', 'C688002L0', 'C688002M0', 'C70200240', 'C70229540', 'C7022U800', 'C7026W600', 'C702B9140', 'C702CFQ10', 'C702GU600', 'C702IC500', 'C702KY800', 'C702R5G00', 'C702RU800', 'C702TM400', 'C702TS400', 'C702VI600', 'C702XO400', 'C702YO400', 'C7034BD00', 'C70400050', 'C704001Q0', 'C704002N0', 'C704002Q0', 'C704002V0', 'C704003C0', 'C704004I0', 'C704005Y0', 'C70400630', 'C704006G0', 'C704006S0', 'C704007O0', 'C704008E0', 'C704008G0', 'C704009T0', 'C70401040', 'C70401100', 'C704012C0', 'C704012U0', 'C704012V0', 'C70401420', 'C70401470', 'C704016H0', 'C704019Q0', 'C704019X0', 'C70401Z60', 'C70401Z90', 'C70401ZA0', 'C70402090', 'C704026N0', 'C704027P0', 'C70402ZI0', 'C704030I0', 'C70403150', 'C704032I0', 'C70403300', 'C704035I0', 'C704035K0', 'C704035N0', 'C704035R0', 'C704036R0', 'C704037H0', 'C704037Y0', 'C704038C0', 'C704038J0', 'C70403960', 'C704039M0', 'C70404060', 'C70404120', 'C704042U0', 'C70404Z10', 'C70404ZM0', 'C705NPD00', 'C705QPD00', 'C71000110', 'C710I7930', 'C710I8000', 'C710I8440', 'C710J1740', 'C710J1800', 'C710J1920', 'C710J3080', 'C710R0C00', 'C710RK800', 'C710UE600', 'C710Z0270', 'C72432620', 'C724C2930', 'C724C7330', 'C724K9160', 'C7520E300', 'C75216370', 'C752166S0', 'C7524AS10', 'C75262230', 'C7526UB20', 'C75271460', 'C752BV190', 'C752BW430', 'C752CD880', 'C752GE300', 'C752I9940', 'C752JB000', 'C752K9910', 'C752L4200', 'C752MD300', 'C752OE300', 'C752PQA00', 'C752S3D00', 'C752VD300', 'C752W2D70', 'C75692930', 'C756EW500', 'C756H5030', 'C760003E0', 'C760D0R10', 'C760WIQ10', 'C784LMG00', 'C80400090', 'C804000P0', 'C804001X0', 'C804002R0', 'C80400310', 'C804003B0', 'C804003C0', 'C80400470', 'C804004A0', 'C804004H0', 'C804004K0', 'C80400510', 'C804005T0', 'C80400600', 'C80400610', 'C807000U0', 'C82603S10', 'C82619410', 'C8261EQ10', 'C8261J900', 'C82625310', 'C82658720', 'C826650I0', 'C8266F600', 'C82672680', 'C82673020', 'C82680210', 'C82681560', 'C82682530', 'C82686020', 'C82691830', 'C82692010', 'C82692990', 'C82693330', 'C82693620', 'C82693920', 'C82694560', 'C826A2220', 'C826A2770', 'C826A2840', 'C826A2860', 'C826A3100', 'C826A5640', 'C826A81W0', 'C826A8210', 'C826A8290', 'C826A8410', 'C826A8840', 'C826A9030', 'C826A9960', 'C826AB120', 'C826AB170', 'C826AB360', 'C826AD710', 'C826AE230', 'C826AO690', 'C826AS290', 'C826AS510', 'C826AT210', 'C826AU960', 'C826AU980', 'C826AV300', 'C826AZ900', 'C826B4140', 'C826B6650', 'C826BU090', 'C826BW280', 'C826C0800', 'C826C4240', 'C826C4340', 'C826CJ130', 'C826CLA00', 'C826CO600', 'C826CQ800', 'C826CT360', 'C826D3S10', 'C826EG680', 'C826EV630', 'C826FB920', 'C826FC340', 'C826FHF00', 'C826H5R10', 'C826H81V0', 'C826H8770', 'C826H99P0', 'C826I40U0', 'C826IG900', 'C826K50E0', 'C826K7630', 'C826LM870', 'C826MQQ10', 'C826MR800', 'C826UH010', 'C826UH560', 'C826UI450', 'C826UJ550', 'C826UJ950', 'C826UK850', 'C826UL040', 'C826UL330', 'C826UL500', 'C826UM310', 'C826UM330', 'C826UM550', 'C826UM610', 'C826UM790', 'C826UN770', 'C826UO340', 'C826UO350', 'C826UO660', 'C826UP210', 'C826UQ410', 'C826US590', 'C826US800', 'C826UT090', 'C826UT340', 'C826UU100', 'C8405ZA00', 'C84077450', 'C8407G6A0', 'C8407G6O0', 'C8408L000', 'C840BT430', 'C840C05S0', 'C840C06F0', 'C840C07L0', 'C840CP170', 'C840DYQ10', 'C840I89A0', 'C840I89P0', 'C840IP0K0', 'C840L78U0', 'C840LS100', 'C840N0ZH0', 'C840N14B0', 'C840N2ZE0', 'C840N6950', 'C840N78E0', 'C840N90U0', 'C840N91I0', 'C840Q5760', 'C840RF900', 'C840S82A0', 'C840V0060', 'C840V3180', 'C840X1440', 'C840X7320', 'C840X8350', 'C840Y5480', 'C840Y7140', 'C840Z6350', 'C840ZV900', 'C86222680', 'C8970CG00', 'C8974GC00']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "mv_file  = f\"{Temp_file_path_GO}/MV_clean.txt\"\n",
    "ret_file = f\"{Temp_file_path_DP}/Returns_clean.txt\"\n",
    "md_file  = f\"{Temp_file_path_DP}/MarketData_clean.txt\"\n",
    "id_file  = f\"{Temp_file_path_EoC}/ID_clean.txt\"\n",
    "\n",
    "# All valid IDs from ID_clean\n",
    "ids_ref = set(\n",
    "    pd.read_csv(id_file, sep=\"|\", usecols=[\"ID\"], dtype={\"ID\": \"string\"})[\"ID\"]\n",
    ")\n",
    "\n",
    "# IDs in the merged MarketData_clean\n",
    "ids_md = set(\n",
    "    pd.read_csv(md_file, sep=\"|\", usecols=[\"ID\"], dtype={\"ID\": \"string\"})[\"ID\"]\n",
    ")\n",
    "\n",
    "missing_ids = ids_ref - ids_md\n",
    "print(len(missing_ids))\n",
    "print(sorted(missing_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1764881401380,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "gA-Yb3EYaBwV",
    "outputId": "d875bcde-02a7-4719-bb24-c7488d592c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MV dates: ['2024-11-20', '2024-11-21', '2024-11-22', '2024-11-25', '2024-11-26', '2024-11-27', '2024-11-28', '2024-11-29', '2024-12-02', '2024-12-03', '2024-12-04', '2024-12-05', '2024-12-06', '2024-12-09', '2024-12-10', '2024-12-11', '2024-12-12', '2024-12-13', '2024-12-16', '2024-12-17', '2024-12-18', '2024-12-19', '2024-12-20', '2024-12-23', '2024-12-24', '2024-12-25', '2024-12-26', '2024-12-27', '2024-12-30', '2024-12-31', '2025-01-01', '2025-01-02', '2025-01-03', '2025-01-06', '2025-01-07', '2025-01-08', '2025-01-09', '2025-01-10', '2025-01-13', '2025-01-14', '2025-01-15', '2025-01-16', '2025-01-17', '2025-01-20', '2025-01-21', '2025-01-22', '2025-01-23', '2025-01-24', '2025-01-27', '2025-01-28', '2025-01-29', '2025-01-30', '2025-01-31', '2025-02-03', '2025-02-04', '2025-02-05', '2025-02-06', '2025-02-07', '2025-02-10', '2025-02-11', '2025-02-12', '2025-02-13', '2025-02-14', '2025-02-17']\n",
      "Returns dates: ['2024-11-20', '2024-11-21', '2024-11-22', '2024-11-25', '2024-11-26', '2024-11-27', '2024-11-28', '2024-11-29', '2024-12-02', '2024-12-03', '2024-12-04', '2024-12-05', '2024-12-06', '2024-12-09', '2024-12-10', '2024-12-11', '2024-12-12', '2024-12-13', '2024-12-16', '2024-12-17', '2024-12-18', '2024-12-19', '2024-12-20', '2024-12-23', '2024-12-24', '2024-12-25', '2024-12-26', '2024-12-27', '2024-12-30', '2024-12-31', '2025-01-01', '2025-01-02', '2025-01-03', '2025-01-06', '2025-01-07', '2025-01-08', '2025-01-09', '2025-01-10', '2025-01-13', '2025-01-14', '2025-01-15', '2025-01-16', '2025-01-17', '2025-01-20', '2025-01-21', '2025-01-22', '2025-01-23', '2025-01-24', '2025-01-27', '2025-01-28', '2025-01-29', '2025-01-30', '2025-01-31', '2025-02-03', '2025-02-04', '2025-02-05', '2025-02-06', '2025-02-07', '2025-02-10', '2025-02-11', '2025-02-12', '2025-02-13']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# -------\n",
    "# This debugging snippet helps you investigate *why* certain IDs appear in\n",
    "# `missing_ids` — meaning they were found in MV_clean.txt but not in\n",
    "# Returns_clean.txt.\n",
    "#\n",
    "# What it does:\n",
    "#   1) Picks one example ID from the set `missing_ids`\n",
    "#   2) Loads only the necessary columns from MV and Returns\n",
    "#   3) Filters both datasets to this single ID\n",
    "#   4) Prints the sorted list of dates available for:\n",
    "#        - MV (DayDate)\n",
    "#        - Returns (DayDate)\n",
    "#\n",
    "# This makes it easy to check whether:\n",
    "#   - The ID truly has no matching dates,\n",
    "#   - There are mismatched date formats,\n",
    "#   - Or Returns simply doesn't contain this ID at all.\n",
    "# =============================================================================\n",
    "\n",
    "# Only proceed if there are actual missing IDs to debug\n",
    "if missing_ids:\n",
    "    # Pick one example ID from the set of IDs that exist in MV but do not match Returns\n",
    "    example_id = next(iter(missing_ids))\n",
    "\n",
    "    # Load selected columns from MV file (keeps memory usage low)\n",
    "    mv_example = pd.read_csv(\n",
    "        mv_file,\n",
    "        sep=\"|\",\n",
    "        usecols=[\"ID\", \"DayDate\", \"MV\"],\n",
    "        dtype={\"ID\": \"string\", \"DayDate\": \"string\"},\n",
    "    )\n",
    "\n",
    "    # Filter MV data to only the chosen ID\n",
    "    mv_example = mv_example[mv_example[\"ID\"] == example_id]\n",
    "\n",
    "    # Load selected columns from Returns file\n",
    "    ret_example = pd.read_csv(\n",
    "        ret_file,\n",
    "        sep=\"|\",\n",
    "        usecols=[\"ID\", \"DayDate\", \"ret_bps\"],\n",
    "        dtype={\"ID\": \"string\", \"DayDate\": \"string\"},\n",
    "    )\n",
    "\n",
    "    # Filter Returns data to the same ID\n",
    "    ret_example = ret_example[ret_example[\"ID\"] == example_id]\n",
    "\n",
    "    # Print available date values for this ID in both datasets\n",
    "    print(\"MV dates:\", sorted(mv_example[\"DayDate\"].unique()))\n",
    "    print(\"Returns dates:\", sorted(ret_example[\"DayDate\"].unique()))\n",
    "else:\n",
    "    print(\"No missing IDs found to debug. The 'missing_ids' set is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxiK7CD5XkXu"
   },
   "source": [
    "### Track IDs Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21509,
     "status": "ok",
     "timestamp": 1764881434270,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "4LTLwV3GXpfR",
    "outputId": "793ada97-d8a1-4ce6-ed96-0ca704e9700d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing MarketData_clean from /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/MarketData_clean.txt...\n",
      "  Successfully read 'ID' column.\n",
      "  Loaded 54603 unique IDs for MarketData_clean.\n",
      "Processing Returns_clean from /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Returns_clean.txt...\n",
      "  Successfully read 'ID' column.\n",
      "  Loaded 85918 unique IDs for Returns_clean.\n",
      "Processing TRI_clean from /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/TRI_clean.txt...\n",
      "  Successfully read 'ID' column.\n",
      "  Loaded 85918 unique IDs for TRI_clean.\n",
      "Processing MV_clean from /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/MV_clean.txt...\n",
      "  Successfully read 'ID' column.\n",
      "  Loaded 72079 unique IDs for MV_clean.\n",
      "Processing LC_MV_clean from /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/LC_MV_clean.txt...\n",
      "  Successfully read 'ID' column.\n",
      "  Loaded 85049 unique IDs for LC_MV_clean.\n",
      "Processing ID_mapping_clean from /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/ID_mapping_clean.txt...\n",
      "  Successfully read 'ID' column.\n",
      "  Loaded 95104 unique IDs for ID_mapping_clean.\n",
      "Processing filtered_ids from /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_ids.txt...\n",
      "  Warning: 'ID' column not found in '/home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_ids.txt'. Attempting to read as single column without header.\n",
      "  Successfully read first column as IDs.\n",
      "  Loaded 120815 unique IDs for filtered_ids.\n",
      "Processing ID_clean from /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/ID_clean.txt...\n",
      "  Successfully read 'ID' column.\n",
      "  Loaded 55367 unique IDs for ID_clean.\n",
      "Processing ADR_clean from /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/ADR_clean.txt...\n",
      "  Successfully read 'ID' column.\n",
      "  Loaded 2297 unique IDs for ADR_clean.\n",
      "Done. Overview saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/ID_Drop_Tracking.xlsx\n"
     ]
    }
   ],
   "source": [
    "# SUMMARY:\n",
    "# This script takes multiple '|'-separated text files, extracts ONLY the ID column\n",
    "# (for efficiency), combines all unique IDs across all datasets, and produces an\n",
    "# Excel file showing which ID appears in which dataset. The Excel contains one row\n",
    "# per unique ID and one column per input file, marked with \"x\" if the ID exists in it.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --------- INPUT SECTION ---------\n",
    "# List all your input txt files here\n",
    "file_paths = [\n",
    "    os.path.join(Temp_file_path_DP, \"MarketData_clean.txt\"),\n",
    "    os.path.join(Temp_file_path_DP, \"Returns_clean.txt\"),\n",
    "    os.path.join(Temp_file_path_GO, \"TRI_clean.txt\"),\n",
    "    os.path.join(Temp_file_path_GO, \"MV_clean.txt\"),\n",
    "    os.path.join(Temp_file_path_GO, \"LC_MV_clean.txt\"),\n",
    "    os.path.join(Temp_file_path_GO, \"ID_mapping_clean.txt\"),\n",
    "    os.path.join(Temp_file_path_EoC, \"filtered_ids.txt\"),\n",
    "    os.path.join(Temp_file_path_EoC, \"ID_clean.txt\"),\n",
    "    os.path.join(Temp_file_path_EoC, \"ADR_clean.txt\"),\n",
    "]\n",
    "\n",
    "id_col = \"ID\"  # <-- change this if your column name is different\n",
    "output_excel_path = os.path.join(Temp_file_path_DP, \"ID_Drop_Tracking.xlsx\")\n",
    "# ---------------------------------\n",
    "\n",
    "\n",
    "# 1) Read only the ID column from each file and store as sets\n",
    "id_sets = {}  # key: dataset name, value: set of IDs\n",
    "\n",
    "for path in file_paths:\n",
    "    dataset_name = os.path.splitext(os.path.basename(path))[0]\n",
    "    print(f\"Processing {dataset_name} from {path}...\")\n",
    "\n",
    "    try:\n",
    "        # Attempt to read with specified ID column name\n",
    "        ids = pd.read_csv(path, sep=\"|\", usecols=[id_col])[id_col]\n",
    "        print(f\"  Successfully read '{id_col}' column.\")\n",
    "    except ValueError as e:\n",
    "        # If 'ID' column not found, try reading without header and assume first column is ID\n",
    "        print(f\"  Warning: '{id_col}' column not found in '{path}'. Attempting to read as single column without header.\")\n",
    "        try:\n",
    "            temp_df = pd.read_csv(path, sep=\"|\", header=None)\n",
    "            # Assuming the first column (index 0) contains the IDs\n",
    "            if not temp_df.empty:\n",
    "                ids = temp_df.iloc[:, 0]\n",
    "                print(f\"  Successfully read first column as IDs.\")\n",
    "            else:\n",
    "                ids = pd.Series([]) # Empty Series if file is empty\n",
    "                print(f\"  File '{path}' is empty, no IDs to process.\")\n",
    "        except Exception as inner_e:\n",
    "            print(f\"  Error reading '{path}' as single column: {inner_e}. Skipping this file.\")\n",
    "            continue # Skip to the next file if even this fallback fails\n",
    "\n",
    "    id_sets[dataset_name] = set(ids.dropna())  # drop NaNs just in case\n",
    "    print(f\"  Loaded {len(id_sets[dataset_name])} unique IDs for {dataset_name}.\")\n",
    "\n",
    "\n",
    "# 2) Compute the union of all IDs\n",
    "all_ids = sorted(set().union(*id_sets.values()))\n",
    "\n",
    "# 3) Build presence matrix: rows = IDs, columns = datasets, values = \"x\"/\"\"\n",
    "presence_df = pd.DataFrame(index=all_ids)\n",
    "\n",
    "for dataset_name, ids in id_sets.items():\n",
    "    presence_df[dataset_name] = presence_df.index.to_series().isin(ids).map(\n",
    "        lambda present: \"x\" if present else \"\"\n",
    "    )\n",
    "\n",
    "# Move ID from index to a proper column for nicer Excel output\n",
    "presence_df.index.name = id_col\n",
    "presence_df.reset_index(inplace=True)\n",
    "\n",
    "# 4) Save to Excel\n",
    "presence_df.to_excel(output_excel_path, index=False)\n",
    "\n",
    "print(f\"Done. Overview saved to: {output_excel_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DZfDexpI22g"
   },
   "source": [
    "### Merge with US One Month Risk Free Rate (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24330,
     "status": "ok",
     "timestamp": 1764881458608,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "L96qnr4xI-Xh",
    "outputId": "2114062e-8ef7-453a-d8d3-cf4b200cb8a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF CSV file:    /home/jovyan/work/hpool1/pseidel/test/Input/FF_5F+Mom+RrR.csv\n",
      "Market file:    /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/MarketData_clean.txt\n",
      "Output file:    /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/FF_Benchmark_data_clean.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF CSV exists: True\n",
      "Market exists: True\n",
      "\n",
      "MarketData header: ['ID', 'PCUR', 'DayDate', 'MV_LC', 'ret_bps', 'MV_USD']\n",
      "FF CSV header: ['date', 'mktrf', 'smb', 'hml', 'rmw', 'cma', 'rf', 'umd']\n",
      "Using FF date column: 'date'\n",
      "Merging ONLY column:  'rf'\n",
      "\n",
      "Loading full MarketData_clean.txt into memory...\n",
      "Rows in MarketData_clean.txt before cleaning: 150,651,519\n",
      "Rows in MarketData_clean.txt after dropping rows without DayDate: 150,651,519\n",
      "Unique DayDate values in MarketData: 8,642\n",
      "\n",
      "Building FF 'rf' with forward-fill for all Market dates...\n",
      "FF 'rf' factor with forward-fill built for 8642 unique Market dates.\n",
      "\n",
      "Date intersection size (Market ∩ FF original dates): 8340\n",
      "\n",
      "Merging full MarketData with 'rf'...\n",
      "\n",
      "Deriving 'Country' column from 'ID' (digits 2-4)...\n",
      "\n",
      "Checking unique IDs per Country and filtering...\n",
      "Found 43 countries with <= 30 unique IDs.\n",
      "Dropping the following countries (Count of Unique IDs):\n",
      "   Country: 705 | Unique IDs: 30\n",
      "   Country: 634 | Unique IDs: 27\n",
      "   Country: 070 | Unique IDs: 27\n",
      "   Country: 182 | Unique IDs: 26\n",
      "   Country: 688 | Unique IDs: 25\n",
      "   Country: 352 | Unique IDs: 24\n",
      "   Country: 388 | Unique IDs: 24\n",
      "   Country: 136 | Unique IDs: 23\n",
      "   Country: 233 | Unique IDs: 22\n",
      "   Country: 275 | Unique IDs: 19\n",
      "   Country: 048 | Unique IDs: 18\n",
      "   Country: 807 | Unique IDs: 17\n",
      "   Country: 470 | Unique IDs: 17\n",
      "   Country: 499 | Unique IDs: 15\n",
      "   Country: 428 | Unique IDs: 15\n",
      "   Country: 897 | Unique IDs: 15\n",
      "   Country: 862 | Unique IDs: 14\n",
      "   Country: 072 | Unique IDs: 14\n",
      "   Country: 894 | Unique IDs: 12\n",
      "   Country: 780 | Unique IDs: 10\n",
      "   Country: 288 | Unique IDs: 10\n",
      "   Country: 834 | Unique IDs: 9\n",
      "   Country: 703 | Unique IDs: 9\n",
      "   Country: 092 | Unique IDs: 8\n",
      "   Country: 068 | Unique IDs: 5\n",
      "   Country: 800 | Unique IDs: 5\n",
      "   Country: 454 | Unique IDs: 5\n",
      "   Country: 516 | Unique IDs: 4\n",
      "   Country: 832 | Unique IDs: 4\n",
      "   Country: 831 | Unique IDs: 3\n",
      "   Country: 218 | Unique IDs: 3\n",
      "   Country: 234 | Unique IDs: 2\n",
      "   Country: 833 | Unique IDs: 2\n",
      "   Country: 422 | Unique IDs: 2\n",
      "   Country: 591 | Unique IDs: 2\n",
      "   Country: 178 | Unique IDs: 1\n",
      "   Country: 116 | Unique IDs: 1\n",
      "   Country: 044 | Unique IDs: 1\n",
      "   Country: 052 | Unique IDs: 1\n",
      "   Country: 496 | Unique IDs: 1\n",
      "   Country: 242 | Unique IDs: 1\n",
      "   Country: 736 | Unique IDs: 1\n",
      "   Country: 686 | Unique IDs: 1\n",
      "Rows dropped: 1,130,784\n",
      "Rows remaining: 149,520,735\n",
      "\n",
      "=== MERGE STATS ===\n",
      "Total merged rows (Market rows with forward-filled 'rf'): 149,520,735\n",
      "Unique Countries derived: 66\n",
      "\n",
      "=== DATES IN MARKETDATA NOT PRESENT IN ORIGINAL FF CSV (now forward-filled) ===\n",
      "Number of such dates: 302\n",
      "Examples: ['1992-01-01', '1992-02-17', '1992-04-17', '1992-05-25', '1992-07-03']\n",
      "\n",
      "Head of FF_Benchmark_data.txt (first 50 rows):\n",
      "\n",
      "       ID PCUR    DayDate  MV_LC     ret_bps MV_USD      rf Country\n",
      "C025L1660   AP 2003-12-31  81.38      0.0671  27.75 0.00000     025\n",
      "C025L1660   AP 2003-12-31  81.38         0.0  27.75 0.00000     025\n",
      "C025L1660   AP 2003-12-31  81.38      0.0671  27.75 0.00000     025\n",
      "C025L1660   AP 2003-12-31  81.38         0.0  27.75 0.00000     025\n",
      "C025L1660   AP 2004-01-01  81.38         0.0  27.75 0.00000     025\n",
      "C025L1660   AP 2004-01-02  87.34    770.1186  29.89 0.00000     025\n",
      "C025L1660   AP 2004-01-05  89.51    354.7217  30.95 0.00000     025\n",
      "C025L1660   AP 2004-01-06  88.43     -0.1805  30.95 0.00000     025\n",
      "C025L1660   AP 2004-01-07  85.71   -340.5904  29.89 0.00000     025\n",
      "C025L1660   AP 2004-01-08  88.97    325.6874  30.87 0.00000     025\n",
      "C025L1660   AP 2004-01-09  90.05     86.9382  31.13 0.00000     025\n",
      "C025L1660   AP 2004-01-12  91.14    208.8031  31.78 0.00000     025\n",
      "C025L1660   AP 2004-01-13  94.94     471.403  33.28 0.00000     025\n",
      "C025L1660   AP 2004-01-14  94.94     -69.603  33.05 0.00000     025\n",
      "C025L1660   AP 2004-01-15  94.94      15.607   33.1 0.00000     025\n",
      "C025L1660   AP 2004-01-16  97.65    278.6325  34.02 0.00000     025\n",
      "C025L1660   AP 2004-01-19  104.7    685.0597  36.36 0.00000     025\n",
      "C025L1660   AP 2004-01-20 106.33    128.9754  36.82 0.00000     025\n",
      "C025L1660   AP 2004-01-21 105.24   -119.1409  36.39 0.00000     025\n",
      "C025L1660   AP 2004-01-22 106.33     16.5308  36.45 0.00000     025\n",
      "C025L1660   AP 2004-01-23 106.33     68.9264   36.7 0.00000     025\n",
      "C025L1660   AP 2004-01-26 103.07   -347.7025  35.42 0.00000     025\n",
      "C025L1660   AP 2004-01-27  104.7    192.8365   36.1 0.00000     025\n",
      "C025L1660   AP 2004-01-28 103.07   -222.8171   35.3 0.00000     025\n",
      "C025L1660   AP 2004-01-29 100.36   -229.7413  34.49 0.00000     025\n",
      "C025L1660   AP 2004-01-30 100.36    -51.1323  34.31 0.00000     025\n",
      "C025L1660   AP 2004-02-02  99.82     -87.974  34.01 0.00000     025\n",
      "C025L1660   AP 2004-02-03 103.62    406.9252  35.39 0.00000     025\n",
      "C025L1660   AP 2004-02-04  99.82   -399.2213  33.98 0.00000     025\n",
      "C025L1660   AP 2004-02-05  92.22 -777.787917   31.1 0.00000     025\n",
      "C025L1660   AP 2004-02-06  87.88   -446.3815  29.72 0.00000     025\n",
      "C025L1660   AP 2004-02-09  94.94    848.3732  32.24 0.00000     025\n",
      "C025L1660   AP 2004-02-10  90.05   -445.3713   30.8 0.00000     025\n",
      "C025L1660   AP 2004-02-11  93.31    282.2785  31.67 0.00000     025\n",
      "C025L1660   AP 2004-02-12  99.82    761.6141  34.08 0.00000     025\n",
      "C025L1660   AP 2004-02-13 103.62    438.4077  35.58 0.00000     025\n",
      "C025L1660   AP 2004-02-16 104.16     33.3944   35.7 0.00000     025\n",
      "C025L1660   AP 2004-02-17 102.53   -154.6263  35.14 0.00000     025\n",
      "C025L1660   AP 2004-02-18 103.62     20.0822  35.21 0.00000     025\n",
      "C025L1660   AP 2004-02-19 106.33    270.4875  36.17 0.00000     025\n",
      "C025L1660   AP 2004-02-20 105.79     -42.581  36.01 0.00000     025\n",
      "C025L1660   AP 2004-02-23 106.33     94.2133  36.35 0.00000     025\n",
      "C025L1660   AP 2004-02-24 105.35    -91.7459  36.02 0.00000     025\n",
      "C025L1660   AP 2004-02-25 103.07   -203.8031  35.28 0.00000     025\n",
      "C025L1660   AP 2004-02-26 105.24    206.1431  36.01 0.00000     025\n",
      "C025L1660   AP 2004-02-27 103.62   -154.5613  35.46 0.00000     025\n",
      "C025L1660   AP 2004-03-01 103.07    -26.8387  35.36 0.00000     025\n",
      "C025L1660   AP 2004-03-02 103.94     54.0324  35.55 0.00000     025\n",
      "C025L1660   AP 2004-03-03 101.99   -149.9644  35.02 0.00000     025\n",
      "C025L1660   AP 2004-03-04 100.69     -144.59  34.51 0.00000     025\n",
      "\n",
      "Finished creating FF benchmark merge file:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/FF_Benchmark_data_clean.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NON-BUCKETED MERGE (Risk-Free Rate Only) WITH ID FILTERING:\n",
    "#   MarketData_clean.txt (pipe, has DayDate = YYYY-MM-DD)\n",
    "#   FF CSV (comma, has a date column \"date\" and \"rf\")\n",
    "#\n",
    "# Result: FF_Benchmark_data_clean.txt (pipe-separated)\n",
    "#\n",
    "# Features:\n",
    "#   - No bucketing, full in-memory merge.\n",
    "#   - ONLY the 'rf' column is merged from the FF file.\n",
    "#   - 'rf' is forward-filled for all Market dates (using merge_asof).\n",
    "#   - ADDS 'Country' column derived from 'ID' (2nd to 4th digit).\n",
    "#   - FILTERS: Drops countries with 30 or fewer unique IDs (logs dropped countries).\n",
    "#   - Final output does NOT contain the FF \"date\" column.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# =====================================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================================\n",
    "ff_csv_file = f'{Input_file_path}/FF_5F+Mom+RrR.csv'  # FF Factors\n",
    "dst_dir     = Temp_file_path_DP                       # same as in your benchmark script\n",
    "market_file = f\"{dst_dir}/MarketData_clean.txt\"       # output of your benchmark script\n",
    "out_file    = f\"{dst_dir}/FF_Benchmark_data_clean.txt\"      # final merged file\n",
    "\n",
    "FF_DATE_COL = \"date\"  # correct column name in FF CSV\n",
    "TARGET_COL  = \"rf\"    # We only want to merge this column\n",
    "\n",
    "print(f\"FF CSV file:    {ff_csv_file}\")\n",
    "print(f\"Market file:    {market_file}\")\n",
    "print(f\"Output file:    {out_file}\")\n",
    "print(\"FF CSV exists:\", os.path.exists(ff_csv_file))\n",
    "print(\"Market exists:\", os.path.exists(market_file))\n",
    "\n",
    "# =====================================================================\n",
    "# DATE NORMALISATION\n",
    "# =====================================================================\n",
    "def normalize_date(s: str):\n",
    "    \"\"\"\n",
    "    Normalize date strings to YYYY-MM-DD if possible.\n",
    "    Handles:\n",
    "      - YYYY-MM-DD (unchanged)\n",
    "      - YYYYMMDD  (FF style) -> YYYY-MM-DD\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    # Already looks like YYYY-MM-DD\n",
    "    if \"-\" in s and len(s) == 10:\n",
    "        return s\n",
    "\n",
    "    # Plain YYYYMMDD digits\n",
    "    if len(s) == 8 and s.isdigit():\n",
    "        return f\"{s[0:4]}-{s[4:6]}-{s[6:8]}\"\n",
    "\n",
    "    # Fallback: return as-is\n",
    "    return s\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 1: Read headers of both files (for logging)\n",
    "# =====================================================================\n",
    "with open(market_file, \"r\", newline=\"\") as f_mkt:\n",
    "    mkt_reader = csv.reader(f_mkt, delimiter=\"|\")\n",
    "    mkt_header = next(mkt_reader)\n",
    "\n",
    "with open(ff_csv_file, \"r\", newline=\"\") as f_ff:\n",
    "    ff_reader = csv.reader(f_ff, delimiter=\",\")\n",
    "    ff_header = next(ff_reader)\n",
    "\n",
    "print(\"\\nMarketData header:\", mkt_header)\n",
    "print(\"FF CSV header:\", ff_header)\n",
    "\n",
    "if FF_DATE_COL not in ff_header:\n",
    "    raise ValueError(f\"FF_DATE_COL = {FF_DATE_COL!r} not found in FF CSV header!\")\n",
    "if TARGET_COL not in ff_header:\n",
    "    raise ValueError(f\"Target column {TARGET_COL!r} not found in FF CSV header!\")\n",
    "\n",
    "print(f\"Using FF date column: {FF_DATE_COL!r}\")\n",
    "print(f\"Merging ONLY column:  {TARGET_COL!r}\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 2: Load full MarketData_clean.txt\n",
    "# =====================================================================\n",
    "print(\"\\nLoading full MarketData_clean.txt into memory...\")\n",
    "\n",
    "mkt_df = pd.read_csv(\n",
    "    market_file,\n",
    "    sep=\"|\",\n",
    "    dtype=\"string\",\n",
    "    engine=\"c\"\n",
    ")\n",
    "\n",
    "print(f\"Rows in MarketData_clean.txt before cleaning: {len(mkt_df):,}\")\n",
    "\n",
    "# Normalize DayDate and drop rows without a valid date\n",
    "mkt_df[\"DayDate\"] = mkt_df[\"DayDate\"].map(normalize_date)\n",
    "mkt_df = mkt_df.dropna(subset=[\"DayDate\"])\n",
    "\n",
    "print(f\"Rows in MarketData_clean.txt after dropping rows without DayDate: {len(mkt_df):,}\")\n",
    "\n",
    "# Collect unique market dates\n",
    "mkt_dates = set(mkt_df[\"DayDate\"].unique())\n",
    "print(f\"Unique DayDate values in MarketData: {len(mkt_dates):,}\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 3: Load FF CSV and build forward-filled 'rf' for all Market dates\n",
    "# =====================================================================\n",
    "print(f\"\\nBuilding FF '{TARGET_COL}' with forward-fill for all Market dates...\")\n",
    "\n",
    "# Load full FF CSV\n",
    "ff_raw = pd.read_csv(\n",
    "    ff_csv_file,\n",
    "    sep=\",\",\n",
    "    dtype=\"string\",\n",
    "    engine=\"c\"\n",
    ")\n",
    "\n",
    "# Normalize FF date column\n",
    "ff_raw[FF_DATE_COL] = ff_raw[FF_DATE_COL].map(normalize_date)\n",
    "ff_raw = ff_raw.dropna(subset=[FF_DATE_COL])\n",
    "\n",
    "# --- CRITICAL CHANGE: Keep ONLY Date and 'rf' ---\n",
    "ff_raw = ff_raw[[FF_DATE_COL, TARGET_COL]].copy()\n",
    "\n",
    "# Set of FF dates for statistics\n",
    "ff_dates = set(ff_raw[FF_DATE_COL])\n",
    "\n",
    "# Drop duplicate dates, keep last entry per date\n",
    "ff_raw = ff_raw.drop_duplicates(subset=[FF_DATE_COL], keep=\"last\")\n",
    "\n",
    "# Convert to datetime and sort\n",
    "ff_raw[\"_date_dt\"] = pd.to_datetime(ff_raw[FF_DATE_COL])\n",
    "ff_raw = ff_raw.sort_values(\"_date_dt\")\n",
    "\n",
    "# DataFrame with all Market dates\n",
    "mkt_dates_sorted = sorted(mkt_dates)\n",
    "dates_df = pd.DataFrame({\"DayDate\": mkt_dates_sorted})\n",
    "dates_df[\"_date_dt\"] = pd.to_datetime(dates_df[\"DayDate\"])\n",
    "\n",
    "# As-of merge: for each Market date, take last available FF row <= that date\n",
    "ff_full = pd.merge_asof(\n",
    "    dates_df.sort_values(\"_date_dt\"),\n",
    "    ff_raw,\n",
    "    left_on=\"_date_dt\",\n",
    "    right_on=\"_date_dt\",\n",
    "    direction=\"backward\"\n",
    ")\n",
    "\n",
    "# Set index to DayDate for fast join later\n",
    "ff_full = ff_full.set_index(\"DayDate\")\n",
    "\n",
    "# Drop helper datetime and FF date columns from ff_full\n",
    "# We only want to keep 'rf'\n",
    "cols_to_drop = [\"_date_dt\", FF_DATE_COL, \"date\"]\n",
    "existing_drop_cols = [c for c in cols_to_drop if c in ff_full.columns]\n",
    "if existing_drop_cols:\n",
    "    ff_full = ff_full.drop(columns=existing_drop_cols)\n",
    "\n",
    "print(f\"FF '{TARGET_COL}' factor with forward-fill built for {len(ff_full)} unique Market dates.\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 4: Quick sanity check – intersection of dates\n",
    "# =====================================================================\n",
    "intersection_size = len(mkt_dates & ff_dates)\n",
    "print(\"\\nDate intersection size (Market ∩ FF original dates):\", intersection_size)\n",
    "if intersection_size == 0:\n",
    "    print(\"WARNING: No common dates after normalization – all FF values (if any) will be NaN!\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 5: Merge full MarketData with pre-built forward-filled 'rf'\n",
    "# =====================================================================\n",
    "if os.path.exists(out_file):\n",
    "    os.remove(out_file)\n",
    "\n",
    "print(f\"\\nMerging full MarketData with '{TARGET_COL}'...\")\n",
    "\n",
    "merged = mkt_df.merge(\n",
    "    ff_full, # Contains only index (DayDate) and 'rf'\n",
    "    left_on=\"DayDate\",\n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# =====================================================================\n",
    "# NEW STEP: Derive 'Country' Column\n",
    "# =====================================================================\n",
    "print(\"\\nDeriving 'Country' column from 'ID' (digits 2-4)...\")\n",
    "# Logic: ID \"C28011490\" -> index 1:4 (\"280\")\n",
    "if \"ID\" in merged.columns:\n",
    "    merged[\"Country\"] = merged[\"ID\"].str[1:4]\n",
    "else:\n",
    "    print(\"WARNING: 'ID' column not found, skipping Country derivation.\")\n",
    "\n",
    "print(\"\\nChecking unique IDs per Country and filtering...\")\n",
    "\n",
    "if \"Country\" in merged.columns and \"ID\" in merged.columns:\n",
    "    # 1. Count unique IDs for each Country\n",
    "    country_id_counts = merged.groupby(\"Country\")[\"ID\"].nunique()\n",
    "\n",
    "    # 2. Identify which countries have <= 30 IDs (to drop)\n",
    "    #    and which have > 30 IDs (to keep)\n",
    "    countries_to_drop = country_id_counts[country_id_counts <= 30]\n",
    "    valid_countries = country_id_counts[country_id_counts > 30].index\n",
    "\n",
    "    # 3. Track/Log the countries being dropped\n",
    "    print(f\"Found {len(countries_to_drop)} countries with <= 30 unique IDs.\")\n",
    "    if not countries_to_drop.empty:\n",
    "        print(\"Dropping the following countries (Count of Unique IDs):\")\n",
    "        # Sorting just for cleaner log output\n",
    "        for ctry, count in countries_to_drop.sort_values(ascending=False).items():\n",
    "            print(f\"   Country: {ctry} | Unique IDs: {count}\")\n",
    "\n",
    "    # 4. Perform the filter\n",
    "    rows_before = len(merged)\n",
    "    merged = merged[merged[\"Country\"].isin(valid_countries)]\n",
    "    rows_after = len(merged)\n",
    "    \n",
    "    print(f\"Rows dropped: {rows_before - rows_after:,}\")\n",
    "    print(f\"Rows remaining: {rows_after:,}\")\n",
    "else:\n",
    "    print(\"WARNING: Skipping filtering because 'Country' or 'ID' column is missing.\")    \n",
    "\n",
    "total_merged_rows = len(merged)\n",
    "\n",
    "# Write merged rows (all columns from Market + 'rf' + 'Country')\n",
    "merged.to_csv(\n",
    "    out_file,\n",
    "    sep=\"|\",\n",
    "    index=False,\n",
    "    mode=\"w\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== MERGE STATS ===\")\n",
    "print(f\"Total merged rows (Market rows with forward-filled '{TARGET_COL}'): {total_merged_rows:,}\")\n",
    "if \"Country\" in merged.columns:\n",
    "    print(f\"Unique Countries derived: {merged['Country'].nunique()}\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 6: Dates in MarketData not covered by original FF CSV (before forward-fill)\n",
    "# =====================================================================\n",
    "missing_in_ff = sorted(mkt_dates - ff_dates)\n",
    "print(\"\\n=== DATES IN MARKETDATA NOT PRESENT IN ORIGINAL FF CSV (now forward-filled) ===\")\n",
    "print(f\"Number of such dates: {len(missing_in_ff):,}\")\n",
    "if missing_in_ff:\n",
    "    print(f\"Examples: {missing_in_ff[:5]}\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 7: Show head (first 50 rows) of FF_Benchmark_data.txt\n",
    "# =====================================================================\n",
    "print(\"\\nHead of FF_Benchmark_data.txt (first 50 rows):\\n\")\n",
    "\n",
    "if os.path.exists(out_file):\n",
    "    head_df = pd.read_csv(out_file, sep=\"|\", nrows=50, dtype=\"string\")\n",
    "    with pd.option_context(\"display.max_columns\", None,\n",
    "                           \"display.width\", 200,\n",
    "                           \"display.max_colwidth\", 50):\n",
    "        print(head_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"Output file does not exist; no head to display.\")\n",
    "\n",
    "print(\"\\nFinished creating FF benchmark merge file:\")\n",
    "print(out_file)\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 8: Cleanup – explicitly delete large variables to free memory\n",
    "# =====================================================================\n",
    "try:\n",
    "    del ff_full, ff_raw, dates_df, head_df, mkt_dates, ff_dates, missing_in_ff, mkt_df, merged\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMfEgkZinyxoUzSNGxiavlN",
   "collapsed_sections": [
    "b06f6df4",
    "Dam2y7Z-02IX",
    "87ba0a55",
    "e5cb8ea8",
    "oHfKNOfs9oZh",
    "20284010",
    "00e2167f",
    "79336c1f",
    "bf38b32a",
    "QytTd1m1VzRK",
    "0d2759d6",
    "e99GuzVzbNTy",
    "21b17226",
    "59bf1295",
    "rQgnee2VXFPd",
    "XRf_FtD90oCI",
    "nNHlamBB3d_J",
    "-ExqzYD05cCJ",
    "-oYu5Sa-xLLf",
    "CxiK7CD5XkXu",
    "5DZfDexpI22g"
   ],
   "history_visible": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
