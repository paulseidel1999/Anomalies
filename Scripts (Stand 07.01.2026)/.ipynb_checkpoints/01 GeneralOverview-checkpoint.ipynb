{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctXZHVs-zurM"
   },
   "source": [
    "### Mount Google Drive, Import Libraries and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25303,
     "status": "ok",
     "timestamp": 1764877642187,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ef11b3f9",
    "outputId": "505460e0-0a3d-4e7a-8a1a-7525af3014c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: False\n",
      "BASE_PATH: /home/jovyan/work/hpool1/pseidel/test\n",
      "Importing numpy ...\n",
      "numpy OK\n",
      "Importing scipy ...\n",
      "scipy OK\n",
      "Importing pandas ...\n",
      "pandas OK\n",
      "Importing linearmodels ...\n",
      "linearmodels OK\n",
      "Importing xlsxwriter ...\n",
      "xlsxwriter OK\n",
      "Paths configured. Temp outputs -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview\n",
      "Example input path -> /home/jovyan/work/hpool1/pseidel/test/Input/WSFV_f_20250131.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT SETUP + PATH CONFIGURATION (SERVER / COLAB COMPATIBLE)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) HARD SAFETY: cap native thread usage (prevents pthread_create EAGAIN)\n",
    "#    MUST be set before importing numpy / scipy / pandas\n",
    "# -----------------------------------------------------------------------------\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"BLIS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Detect environment\n",
    "# -----------------------------------------------------------------------------\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) (Colab only) Mount Google Drive\n",
    "# -----------------------------------------------------------------------------\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "else:\n",
    "    # Server base path (your target)\n",
    "    BASE_PATH = \"/home/jovyan/work/hpool1/pseidel/test\"\n",
    "\n",
    "print(\"IN_COLAB:\", IN_COLAB)\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Sanity checks: path exists + write permission\n",
    "# -----------------------------------------------------------------------------\n",
    "BASE = Path(BASE_PATH)\n",
    "if not BASE.exists():\n",
    "    raise FileNotFoundError(f\"BASE_PATH does not exist: {BASE}\")\n",
    "\n",
    "# quick write test (fails fast if you don't have permissions)\n",
    "test_file = BASE / \".write_test_tmp\"\n",
    "try:\n",
    "    test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "    test_file.unlink()\n",
    "except Exception as e:\n",
    "    raise PermissionError(f\"No write permission in {BASE}. Error: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Environment check: ensure required packages import cleanly\n",
    "# -----------------------------------------------------------------------------\n",
    "required_packages = [\"numpy\", \"scipy\", \"pandas\", \"linearmodels\", \"xlsxwriter\"]\n",
    "\n",
    "for pkg in required_packages:\n",
    "    print(f\"Importing {pkg} ...\")\n",
    "    importlib.import_module(pkg)\n",
    "    print(f\"{pkg} OK\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Base paths and input/output locations\n",
    "# -----------------------------------------------------------------------------\n",
    "Input_file_path   = str(BASE / \"Input\")\n",
    "Temp_file_path    = str(BASE / \"Temp\")\n",
    "Output_file_path  = str(BASE / \"Output\")\n",
    "\n",
    "Fundamentals_file_path = f\"{Input_file_path}/WSFV_f_20250131.txt\"\n",
    "Current_file_path      = f\"{Input_file_path}/WSCurrent_f_20250131.txt\"\n",
    "Calendar_file_path     = f\"{Input_file_path}/WSCalendarPrd_f_20250131.txt\"\n",
    "Meta_file_path         = f\"{Input_file_path}/WSMetaData_f_20250131.txt\"\n",
    "Excel_file_path        = f\"{Input_file_path}/WS PIT Table Definitions V5 with start dates.xls\"\n",
    "\n",
    "MarketValues_file_path          = f\"{Input_file_path}/Daily MV USD\"\n",
    "MarketValues_file_path_LC       = f\"{Input_file_path}/Daily MV LC\"\n",
    "DailyTotalReturns_file_path     = f\"{Input_file_path}/Daily Returns USD\"\n",
    "DailyIndexReturns_file_path     = f\"{Input_file_path}/Daily Index Returns USD\"\n",
    "Constituents_file_path          = f\"{Input_file_path}/Constituents.01.csv\"\n",
    "UniversalMatching_file_path     = f\"{Input_file_path}/Universal Matching File\"\n",
    "\n",
    "Temp_file_path_GO  = f\"{Temp_file_path}/TempGeneralOverview\"\n",
    "Temp_file_path_EoC = f\"{Temp_file_path}/TempExtractionofCharacteristics\"\n",
    "Temp_file_path_DP  = f\"{Temp_file_path}/TempDataPreparation\"\n",
    "Temp_file_path_A   = f\"{Temp_file_path}/TempAnomalies\"\n",
    "Temp_file_path_R   = f\"{Temp_file_path}/TempRegressionModel\"\n",
    "\n",
    "Relevant_items_path   = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_A = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_B = f\"{Input_file_path}/RelevantItemsB.txt\"\n",
    "Relevant_items_path_C = f\"{Input_file_path}/RelevantItemsC.txt\"\n",
    "Relevant_items_path_D = f\"{Input_file_path}/RelevantItemsD.txt\"\n",
    "\n",
    "Subset_file_path = f\"{Temp_file_path_GO}/Subsets\"\n",
    "Fundamentals_clean_file_path = f\"{Temp_file_path_GO}/Fundamentals_clean.txt\"\n",
    "Current_clean_file_path      = f\"{Temp_file_path_GO}/Current_clean.txt\"\n",
    "Calendar_clean_file_path     = f\"{Temp_file_path_GO}/Input/Calendar_clean.txt\"\n",
    "Meta_clean_file_path         = f\"{Temp_file_path_GO}/Input/Meta_clean.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Ensure required directories exist\n",
    "# -----------------------------------------------------------------------------\n",
    "Path(Output_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_GO).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_EoC).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_A).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(Subset_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Path(Calendar_clean_file_path).parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Streaming / deduplication settings\n",
    "# -----------------------------------------------------------------------------\n",
    "CHUNK_SIZE = 2_000_000\n",
    "DATE_COL = \"PIT Date\"\n",
    "DEDUP_KEYS = [\"ID\", \"ItemCode\", DATE_COL]\n",
    "\n",
    "print(\"Paths configured. Temp outputs ->\", Temp_file_path_GO)\n",
    "print(\"Example input path ->\", Fundamentals_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           754Gi       192Gi       313Gi        69Mi       256Gi       561Gi\n",
      "Swap:             0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "!free -h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTYJe_EqwwOd"
   },
   "source": [
    "# 1.0. Worldscope PIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a6_hk-ee7VZ"
   },
   "source": [
    "### Load relevant ItemCodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4420,
     "status": "ok",
     "timestamp": 1764877646619,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "5Ne8jNOAe84b",
    "outputId": "55116ab3-b702-4bb2-b1ad-d58f61aab850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant sets loaded: A=49  B=7  C=1  D=1\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell loads multiple sets of relevant item codes (A, B, C, D) from\n",
    "# text files into memory as Python sets of integers. The process is designed\n",
    "# to be robust and memory-efficient:\n",
    "#   - It first inspects the file to detect the correct column containing\n",
    "#     item codes, preferring a column explicitly named \"ItemCode\" and\n",
    "#     falling back to the first column otherwise.\n",
    "#   - It reads the full file in chunks to avoid loading the entire file\n",
    "#     into memory at once, which is important for large input files.\n",
    "#   - Within each chunk, it filters the column by coercing values to\n",
    "#     numeric, dropping any non-numeric or invalid entries, and casting\n",
    "#     valid values to 64-bit integers.\n",
    "#   - The item codes from all chunks are concatenated and converted to a\n",
    "#     set, enforcing uniqueness and providing fast membership tests for\n",
    "#     downstream filtering operations.\n",
    "#   - Four different relevant-item lists (A/B/C/D) are loaded and stored\n",
    "#     as separate sets to allow different filtering configurations in later\n",
    "#     processing steps.\n",
    "# =============================================================================\n",
    "\n",
    "# ---------- Load relevant item sets for A/B/C/D ----------\n",
    "\n",
    "def load_relevant_set(txt_path: str) -> set[int]:\n",
    "    \"\"\"\n",
    "    Loads a list of item codes (header present) and returns a set[int].\n",
    "    Detects a numeric column: 'ItemCode' if present else first column.\n",
    "    \"\"\"\n",
    "    import pandas as pd  # Local import to keep the function self-contained\n",
    "\n",
    "    # Attempt to read a small sample of rows assuming pipe-delimited format\n",
    "    # to infer the structure and available column names\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            txt_path,\n",
    "            sep=\"|\",               # Assume pipe as the primary delimiter\n",
    "            engine=\"c\",            # Use C engine for faster parsing\n",
    "            dtype=str,             # Read all columns as strings initially\n",
    "            encoding=\"latin1\",     # Encoding that can handle extended characters\n",
    "            nrows=50               # Only read a small sample to inspect header\n",
    "        )\n",
    "    except Exception:\n",
    "        # If reading with pipe delimiter fails, fall back to default delimiter\n",
    "        # (comma) to maximize robustness to different file formats\n",
    "        df = pd.read_csv(\n",
    "            txt_path,\n",
    "            engine=\"c\",\n",
    "            dtype=str,\n",
    "            encoding=\"latin1\",\n",
    "            nrows=50\n",
    "        )\n",
    "\n",
    "    # Identify columns whose name matches \"itemcode\" (case-insensitive)\n",
    "    cols = [c for c in df.columns if c.lower() == \"itemcode\"]\n",
    "\n",
    "    # Prefer a column explicitly named \"ItemCode\"; otherwise use the first column\n",
    "    # as the source of item codes\n",
    "    use_col = cols[0] if cols else df.columns[0]\n",
    "\n",
    "    # Prepare a list to accumulate cleaned numeric item codes from all chunks\n",
    "    codes = []\n",
    "\n",
    "    # Stream the full file in chunks using the detected column\n",
    "    for ch in pd.read_csv(\n",
    "        txt_path,\n",
    "        sep=\"|\",                  # Use pipe delimiter when streaming\n",
    "        engine=\"c\",\n",
    "        dtype=str,                # Read column as string before numeric conversion\n",
    "        encoding=\"latin1\",\n",
    "        usecols=[use_col],        # Only load the column with item codes\n",
    "        chunksize=200_000,        # Number of rows per chunk for streaming\n",
    "        on_bad_lines=\"skip\"       # Skip lines that cannot be parsed correctly\n",
    "    ):\n",
    "        # Convert the selected column to numeric, coercing invalid entries to NaN\n",
    "        numeric_series = pd.to_numeric(ch[use_col], errors=\"coerce\")\n",
    "\n",
    "        # Drop NaN values to retain only valid numeric item codes\n",
    "        numeric_series = numeric_series.dropna()\n",
    "\n",
    "        # Cast the numeric values to int64 for consistent integer representation\n",
    "        numeric_series = numeric_series.astype(\"int64\")\n",
    "\n",
    "        # Append this cleaned series of item codes to the accumulator list\n",
    "        codes.append(numeric_series)\n",
    "\n",
    "    # If no valid codes were accumulated, return an empty set\n",
    "    if not codes:\n",
    "        return set()\n",
    "\n",
    "    # Concatenate all numeric item code series into a single Series,\n",
    "    # convert to a Python list, and then to a set to ensure uniqueness\n",
    "    return set(pd.concat(codes, ignore_index=True).tolist())\n",
    "\n",
    "# Load relevant item code sets for each configuration (A, B, C, D)\n",
    "RELEVANT_ITEMCODES_A = load_relevant_set(Relevant_items_path_A)\n",
    "RELEVANT_ITEMCODES_B = load_relevant_set(Relevant_items_path_B)\n",
    "RELEVANT_ITEMCODES_C = load_relevant_set(Relevant_items_path_C)\n",
    "RELEVANT_ITEMCODES_D = load_relevant_set(Relevant_items_path_D)\n",
    "\n",
    "# Report the size of each loaded relevant item set for verification\n",
    "print(\n",
    "    \"Relevant sets loaded:\",\n",
    "    f\"A={len(RELEVANT_ITEMCODES_A)}  B={len(RELEVANT_ITEMCODES_B)}  \"\n",
    "    f\"C={len(RELEVANT_ITEMCODES_C)}  D={len(RELEVANT_ITEMCODES_D)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51RgNAd-j51P"
   },
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1764877646681,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "LqGYSlxKj9K7"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell defines reusable helper functions for data cleaning, type coercion,\n",
    "# file output, and audit tracking. In particular:\n",
    "#   - `standardize_strings` performs conservative text standardization:\n",
    "#       * keeps the \"ID\" column as a string without modifying content\n",
    "#       * for other text-like columns, trims surrounding whitespace and converts\n",
    "#         empty strings to missing values (<NA>). This is a key transformation\n",
    "#         step for normalizing string fields.\n",
    "#   - `coerce_types_schema` enforces a specific schema (\"A_num\", \"A_str\", \"B\")\n",
    "#       * converts core identifier and PIT date columns into appropriate string\n",
    "#         and datetime types\n",
    "#       * converts numeric fields like ItemCode and FiscalPeriod to nullable\n",
    "#         integer types (Int64)\n",
    "#       * converts Value either to numeric or string depending on the chosen\n",
    "#         schema. This ensures consistent typing across datasets prior to\n",
    "#         further filtering and aggregation.\n",
    "#   - `save_append_txt` appends DataFrames to pipe-separated text files with\n",
    "#     controlled date formatting. This supports incremental, chunk-based\n",
    "#     writing of large processed outputs.\n",
    "#   - `_na_increase` and `_trim_changes` are audit helpers:\n",
    "#       * `_na_increase` computes, per column, how many additional missing\n",
    "#         values were introduced by a transformation step.\n",
    "#       * `_trim_changes` counts how many cells in string-like columns changed\n",
    "#         after trimming operations (excluding the ID column), making it\n",
    "#         possible to quantify the impact of text-cleanup transformations.\n",
    "# =============================================================================\n",
    "\n",
    "# Helper imports and constants\n",
    "import pandas as pd                     # DataFrame operations and data manipulation\n",
    "\n",
    "OUTPUT_SEP = \"|\"   # Delimiter used for all .txt outputs written by this pipeline\n",
    "\n",
    "\n",
    "def standardize_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Conservative text cleanup:\n",
    "      - 'ID': keep as string; DO NOT strip/alter content.\n",
    "      - other object/string cols: cast to 'string', strip whitespace, empty -> <NA>.\n",
    "    \"\"\"\n",
    "    # Iterate over all columns to apply column-specific string standardization\n",
    "    for col in df.columns:\n",
    "        if col == \"ID\":\n",
    "            # Ensure ID is stored as pandas StringDtype without altering values\n",
    "            df[col] = df[col].astype(\"string\")\n",
    "            continue\n",
    "        # For all non-ID columns with object or string-like dtype, apply trimming\n",
    "        if pd.api.types.is_object_dtype(df[col]) or pd.api.types.is_string_dtype(df[col]):\n",
    "            # Convert column to pandas StringDtype and strip leading/trailing whitespace\n",
    "            df[col] = df[col].astype(\"string\").str.strip()\n",
    "            # Replace pure whitespace or empty strings with missing values (<NA>)\n",
    "            df[col] = df[col].replace(r\"^\\s*$\", pd.NA, regex=True)\n",
    "    # Return the standardized DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "def coerce_types_schema(df: pd.DataFrame, schema: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    schema:\n",
    "      - \"A_num\": ID str, PIT Date datetime, ItemCode Int64, Frequency str, FiscalPeriod Int64, Value numeric\n",
    "      - \"A_str\": ID str, PIT Date datetime, ItemCode Int64, Frequency str, FiscalPeriod Int64, Value string\n",
    "      - \"B\":     ID str, PIT Date datetime, ItemCode Int64, Value string\n",
    "    Dates: use format='mixed' to avoid per-element parsing warnings; later we CUT to 'YYYY-MM-DD'.\n",
    "    \"\"\"\n",
    "    # Ensure ID column (if present) is handled as a pandas string type\n",
    "    if \"ID\" in df.columns:\n",
    "        df[\"ID\"] = df[\"ID\"].astype(\"string\")\n",
    "\n",
    "    # Convert PIT Date column to datetime using mixed formats, coercing invalid values to NaT\n",
    "    if \"PIT Date\" in df.columns:\n",
    "        df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\", utc=False, format=\"mixed\")\n",
    "\n",
    "    # Convert ItemCode to a nullable integer type (Int64) after numeric coercion\n",
    "    if \"ItemCode\" in df.columns:\n",
    "        df[\"ItemCode\"] = pd.to_numeric(df[\"ItemCode\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Handle schema-specific field typing for schemas A_num and A_str\n",
    "    if schema in (\"A_num\", \"A_str\"):\n",
    "        # Convert Frequency to string if present\n",
    "        if \"Frequency\" in df.columns:\n",
    "            df[\"Frequency\"] = df[\"Frequency\"].astype(\"string\")\n",
    "        # Convert FiscalPeriod to nullable integer if present\n",
    "        if \"FiscalPeriod\" in df.columns:\n",
    "            df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        # For numeric-valued schema (A_num), coerce Value to numeric type\n",
    "        if schema == \"A_num\":\n",
    "            if \"Value\" in df.columns:\n",
    "                df[\"Value\"] = pd.to_numeric(df[\"Value\"], errors=\"coerce\")\n",
    "        else:  # For string-valued schema (A_str), keep Value as string\n",
    "            if \"Value\" in df.columns:\n",
    "                df[\"Value\"] = df[\"Value\"].astype(\"string\")\n",
    "\n",
    "    # Handle schema \"B\" where Value should be treated as string\n",
    "    elif schema == \"B\":\n",
    "        if \"Value\" in df.columns:\n",
    "            df[\"Value\"] = df[\"Value\"].astype(\"string\")\n",
    "\n",
    "    # Return DataFrame with coerced dtypes based on the selected schema\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_append_txt(df: pd.DataFrame, out_path: str, write_header: bool):\n",
    "    \"\"\"\n",
    "    Append to a single .txt file using OUTPUT_SEP (|).\n",
    "    Dates will be formatted compactly; we already cut PIT Date earlier to 'YYYY-MM-DD'.\n",
    "    \"\"\"\n",
    "    # Write the DataFrame to a pipe-separated text file in append mode\n",
    "    df.to_csv(\n",
    "        out_path,\n",
    "        index=False,                 # Do not write the index column\n",
    "        sep=OUTPUT_SEP,              # Use the global separator for consistency\n",
    "        mode=\"a\",                    # Append to existing file instead of overwriting\n",
    "        header=write_header,         # Conditionally write header (for first chunk only)\n",
    "        lineterminator=\"\\n\",         # Use newline as row terminator\n",
    "        date_format=\"%Y-%m-%d\"       # Format datetime columns in a compact date format\n",
    "    )\n",
    "\n",
    "\n",
    "# Audit helpers used to analyze changes introduced by cleaning steps\n",
    "def _na_increase(before_df: pd.DataFrame, after_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Compute the increase in missing values per column between two DataFrames.\n",
    "    Only columns that are present in both DataFrames are considered.\n",
    "    \"\"\"\n",
    "    # Determine the set of columns shared between the two DataFrames\n",
    "    cols = [c for c in after_df.columns if c in before_df.columns]\n",
    "    if not cols:\n",
    "        # If there are no common columns, return an empty dictionary\n",
    "        return {}\n",
    "    # Count missing values in the \"before\" DataFrame for the common columns\n",
    "    b = before_df[cols].isna().sum()\n",
    "    # Count missing values in the \"after\" DataFrame for the common columns\n",
    "    a = after_df[cols].isna().sum()\n",
    "    # Compute the increase in missing values per column and cast to int\n",
    "    inc = (a - b).astype(int)\n",
    "    # Keep only columns where the number of missing values increased\n",
    "    inc = inc[inc > 0]\n",
    "    # Return the increase as a plain dictionary {column_name: increase}\n",
    "    return inc.to_dict()\n",
    "\n",
    "\n",
    "def _trim_changes(before_df: pd.DataFrame, after_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Count cells changed by trimming in string-like columns (excluding 'ID').\n",
    "    \"\"\"\n",
    "    # Dictionary to accumulate the count of changed cells per column\n",
    "    changes = {}\n",
    "    # Determine columns present in both DataFrames, excluding the ID column\n",
    "    common_cols = [c for c in after_df.columns if c in before_df.columns and c != \"ID\"]\n",
    "    for c in common_cols:\n",
    "        # Only consider columns with object or string-like dtype in the \"before\" DataFrame\n",
    "        if not (pd.api.types.is_object_dtype(before_df[c]) or pd.api.types.is_string_dtype(before_df[c])):\n",
    "            continue\n",
    "        # Convert the column in both DataFrames to pandas StringDtype for comparison\n",
    "        b = before_df[c].astype(\"string\")\n",
    "        a = after_df[c].astype(\"string\")\n",
    "        # Build a mask of cells that are non-missing in both and where the value changed\n",
    "        mask = b.notna() & a.notna() & (b != a)\n",
    "        # Count how many values changed due to trimming or other string operations\n",
    "        cnt = int(mask.sum())\n",
    "        if cnt > 0:\n",
    "            # Record the number of changed cells for this column\n",
    "            changes[c] = changes.get(c, 0) + cnt\n",
    "    # Return a dictionary mapping column names to number of changed cells\n",
    "    return changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrNim8IySIQ"
   },
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1764877646716,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "8DEmANqUyV3H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: False\n",
      "USE_LOCAL_TMP: True\n",
      "LOCAL_TMP_DIR: /home/jovyan/work/hpool1/pseidel/test/Temp/tmp_clean\n",
      "CHUNK_SIZE: 1000000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell defines configuration parameters controlling how large input files\n",
    "# are processed and where intermediate cleaned data is written:\n",
    "#   - `CHUNK_SIZE` determines how many rows are read per chunk when streaming\n",
    "#     large files, which directly influences memory usage and I/O efficiency.\n",
    "#   - `REQUIRE_VALUE_AND_DATE` is a flag that indicates whether downstream\n",
    "#     cleaning steps must enforce the presence of both Value and PIT Date\n",
    "#     columns (e.g., filtering out rows with missing Value or PIT Date).\n",
    "#   - `USE_LOCAL_TMP` toggles whether intermediate files are first written to\n",
    "#     a local temporary directory in the Colab runtime for faster writes,\n",
    "#     before later being moved to Google Drive.\n",
    "#   - `LOCAL_TMP_DIR` specifies the local temporary directory path and is\n",
    "#     created if it does not already exist, ensuring that subsequent write\n",
    "#     operations to this location succeed without directory-related errors.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Maximum number of rows to process per chunk when reading large datasets\n",
    "CHUNK_SIZE = 1_000_000\n",
    "\n",
    "# Flag indicating whether cleaned outputs must have non-missing Value and PIT Date\n",
    "REQUIRE_VALUE_AND_DATE = True\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# Configure whether to use a local temporary directory for faster intermediate writes\n",
    "USE_LOCAL_TMP = True\n",
    "\n",
    "# IMPORTANT:\n",
    "# - In Colab, /content is valid.\n",
    "# - On your server, /content is not writable/doesn't exist.\n",
    "# So we pick a temp directory that is always valid.\n",
    "if USE_LOCAL_TMP:\n",
    "    if IN_COLAB:\n",
    "        LOCAL_TMP_DIR = Path(\"/content/tmp_clean\")\n",
    "    else:\n",
    "        # Use your project temp folder (must already be defined by your setup cell)\n",
    "        # Temp_file_path should be like: /home/jovyan/work/hpool1/pseidel/KGWMAT/Temp\n",
    "        LOCAL_TMP_DIR = Path(Temp_file_path) / \"tmp_clean\"\n",
    "\n",
    "    LOCAL_TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"IN_COLAB:\", IN_COLAB)\n",
    "print(\"USE_LOCAL_TMP:\", USE_LOCAL_TMP)\n",
    "print(\"LOCAL_TMP_DIR:\", LOCAL_TMP_DIR if USE_LOCAL_TMP else None)\n",
    "print(\"CHUNK_SIZE:\", CHUNK_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5Q8pv8Yj_kc"
   },
   "source": [
    "### Extract Relevant Rows Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42045,
     "status": "ok",
     "timestamp": 1764877688767,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "yiwjrux2kEM2",
    "outputId": "d814c0a0-2fbc-4433-d9ad-fcb2d26c7a79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[extract] A_relevant_raw.txt  chunk=1  kept=196,067  dropped_itemcode=738,377  dropped_freq=256,745\n",
      "[extract] A_relevant_raw.txt  chunk=2  kept=405,552  dropped_itemcode=1,454,354  dropped_freq=529,289\n",
      "[extract] A_relevant_raw.txt  chunk=3  kept=625,601  dropped_itemcode=2,168,525  dropped_freq=774,061\n",
      "[extract] A_relevant_raw.txt  chunk=4  kept=913,515  dropped_itemcode=2,878,961  dropped_freq=778,813\n",
      "[extract] A_relevant_raw.txt  chunk=5  kept=1,208,871  dropped_itemcode=3,582,632  dropped_freq=781,806\n",
      "[extract] A_relevant_raw.txt  chunk=6  kept=1,508,274  dropped_itemcode=4,282,589  dropped_freq=783,654\n",
      "[extract] A_relevant_raw.txt  chunk=7  kept=1,798,822  dropped_itemcode=4,990,889  dropped_freq=787,365\n",
      "[extract] A_relevant_raw.txt  chunk=8  kept=2,102,846  dropped_itemcode=5,686,378  dropped_freq=788,956\n",
      "[extract] A_relevant_raw.txt  chunk=9  kept=2,402,453  dropped_itemcode=6,385,760  dropped_freq=792,143\n",
      "[extract] A_relevant_raw.txt  chunk=10  kept=2,698,166  dropped_itemcode=7,087,885  dropped_freq=799,186\n",
      "[extract] A_relevant_raw.txt  chunk=11  kept=2,994,762  dropped_itemcode=7,790,956  dropped_freq=799,976\n",
      "[extract] A_relevant_raw.txt  chunk=12  kept=3,292,286  dropped_itemcode=8,491,567  dropped_freq=806,091\n",
      "[extract] A_relevant_raw.txt  chunk=13  kept=3,591,156  dropped_itemcode=9,191,579  dropped_freq=809,617\n",
      "[extract] A_relevant_raw.txt  chunk=14  kept=3,891,057  dropped_itemcode=9,890,406  dropped_freq=814,040\n",
      "[extract] A_relevant_raw.txt  chunk=15  kept=4,188,605  dropped_itemcode=10,591,375  dropped_freq=818,722\n",
      "[extract] A_relevant_raw.txt  chunk=16  kept=4,493,082  dropped_itemcode=11,284,729  dropped_freq=825,025\n",
      "[extract] A_relevant_raw.txt  chunk=17  kept=4,792,076  dropped_itemcode=11,984,630  dropped_freq=828,502\n",
      "[extract] A_relevant_raw.txt  chunk=18  kept=5,085,074  dropped_itemcode=12,689,635  dropped_freq=834,776\n",
      "[extract] A_relevant_raw.txt  chunk=19  kept=5,384,761  dropped_itemcode=13,389,294  dropped_freq=836,891\n",
      "[extract] A_relevant_raw.txt  chunk=20  kept=5,681,993  dropped_itemcode=14,090,096  dropped_freq=843,602\n",
      "[extract] A_relevant_raw.txt  chunk=21  kept=5,976,730  dropped_itemcode=14,794,232  dropped_freq=847,470\n",
      "[extract] A_relevant_raw.txt  chunk=22  kept=6,274,884  dropped_itemcode=15,494,498  dropped_freq=852,267\n",
      "[extract] A_relevant_raw.txt  chunk=23  kept=6,569,875  dropped_itemcode=16,198,193  dropped_freq=856,616\n",
      "[extract] A_relevant_raw.txt  chunk=24  kept=6,866,037  dropped_itemcode=16,900,928  dropped_freq=860,074\n",
      "[extract] A_relevant_raw.txt  chunk=25  kept=7,170,814  dropped_itemcode=17,592,927  dropped_freq=870,768\n",
      "[extract] A_relevant_raw.txt  chunk=26  kept=7,449,339  dropped_itemcode=18,295,861  dropped_freq=932,331\n",
      "[extract] A_relevant_raw.txt  chunk=27  kept=7,695,471  dropped_itemcode=18,993,349  dropped_freq=1,116,372\n",
      "[extract] A_relevant_raw.txt  chunk=28  kept=7,914,810  dropped_itemcode=19,712,166  dropped_freq=1,329,630\n",
      "[extract] A_relevant_raw.txt  chunk=29  kept=8,112,710  dropped_itemcode=20,441,881  dropped_freq=1,588,323\n",
      "[extract] A_relevant_raw.txt  chunk=30  kept=8,302,967  dropped_itemcode=21,183,798  dropped_freq=1,845,512\n",
      "[extract] A_relevant_raw.txt  chunk=31  kept=8,564,011  dropped_itemcode=21,882,097  dropped_freq=1,979,592\n",
      "[extract] A_relevant_raw.txt  chunk=32  kept=8,845,015  dropped_itemcode=22,580,036  dropped_freq=2,051,205\n",
      "[extract] A_relevant_raw.txt  chunk=33  kept=9,125,667  dropped_itemcode=23,271,211  dropped_freq=2,145,799\n",
      "[extract] A_relevant_raw.txt  chunk=34  kept=9,368,339  dropped_itemcode=23,971,143  dropped_freq=2,335,162\n",
      "[extract] A_relevant_raw.txt  chunk=35  kept=9,658,612  dropped_itemcode=24,634,124  dropped_freq=2,481,149\n",
      "[extract] A_relevant_raw.txt  chunk=36  kept=9,883,762  dropped_itemcode=25,377,202  dropped_freq=2,630,904\n",
      "[extract] A_relevant_raw.txt  chunk=37  kept=10,073,357  dropped_itemcode=26,140,637  dropped_freq=2,845,851\n",
      "[extract] A_relevant_raw.txt  chunk=38  kept=10,266,163  dropped_itemcode=26,887,057  dropped_freq=3,098,389\n",
      "[extract] A_relevant_raw.txt  chunk=39  kept=10,466,696  dropped_itemcode=27,617,214  dropped_freq=3,364,006\n",
      "[extract] A_relevant_raw.txt  chunk=40  kept=10,670,235  dropped_itemcode=28,348,226  dropped_freq=3,620,533\n",
      "[extract] A_relevant_raw.txt  chunk=41  kept=10,864,111  dropped_itemcode=29,095,244  dropped_freq=3,867,853\n",
      "[extract] A_relevant_raw.txt  chunk=42  kept=11,065,689  dropped_itemcode=29,830,945  dropped_freq=4,121,425\n",
      "[extract] A_relevant_raw.txt  chunk=43  kept=11,282,350  dropped_itemcode=30,537,104  dropped_freq=4,404,643\n",
      "[extract] A_relevant_raw.txt  chunk=44  kept=11,475,146  dropped_itemcode=31,280,346  dropped_freq=4,664,661\n",
      "[extract] A_relevant_raw.txt  chunk=45  kept=11,683,081  dropped_itemcode=32,013,912  dropped_freq=4,907,742\n",
      "[extract] A_relevant_raw.txt  chunk=46  kept=11,880,289  dropped_itemcode=32,762,844  dropped_freq=5,153,225\n",
      "[extract] A_relevant_raw.txt  chunk=47  kept=12,068,297  dropped_itemcode=33,510,945  dropped_freq=5,419,617\n",
      "[extract] A_relevant_raw.txt  chunk=48  kept=12,318,211  dropped_itemcode=34,190,234  dropped_freq=5,647,686\n",
      "[extract] A_relevant_raw.txt  chunk=49  kept=12,601,499  dropped_itemcode=34,821,429  dropped_freq=5,883,953\n",
      "[extract] A_relevant_raw.txt  chunk=50  kept=12,881,762  dropped_itemcode=35,456,864  dropped_freq=6,120,760\n",
      "[extract] A_relevant_raw.txt  chunk=51  kept=13,149,900  dropped_itemcode=36,108,045  dropped_freq=6,354,222\n",
      "[extract] A_relevant_raw.txt  chunk=52  kept=13,400,201  dropped_itemcode=36,783,810  dropped_freq=6,587,347\n",
      "[extract] A_relevant_raw.txt  chunk=53  kept=13,641,650  dropped_itemcode=37,465,938  dropped_freq=6,834,749\n",
      "[extract] A_relevant_raw.txt  chunk=54  kept=13,882,638  dropped_itemcode=38,157,777  dropped_freq=7,054,649\n",
      "[extract] A_relevant_raw.txt  chunk=55  kept=14,126,153  dropped_itemcode=38,844,608  dropped_freq=7,281,043\n",
      "[extract] A_relevant_raw.txt  chunk=56  kept=14,357,265  dropped_itemcode=39,539,195  dropped_freq=7,529,305\n",
      "[extract] A_relevant_raw.txt  chunk=57  kept=14,591,692  dropped_itemcode=40,232,195  dropped_freq=7,767,056\n",
      "[extract] A_relevant_raw.txt  chunk=58  kept=14,832,028  dropped_itemcode=40,917,384  dropped_freq=8,007,482\n",
      "[extract] A_relevant_raw.txt  chunk=59  kept=15,079,578  dropped_itemcode=41,595,195  dropped_freq=8,246,630\n",
      "[extract] A_relevant_raw.txt  chunk=60  kept=15,324,999  dropped_itemcode=42,276,351  dropped_freq=8,484,120\n",
      "[extract] A_relevant_raw.txt  chunk=61  kept=15,571,826  dropped_itemcode=42,963,093  dropped_freq=8,702,391\n",
      "[extract] A_relevant_raw.txt  chunk=62  kept=15,812,839  dropped_itemcode=43,650,127  dropped_freq=8,936,548\n",
      "[extract] A_relevant_raw.txt  chunk=63  kept=16,063,231  dropped_itemcode=44,336,637  dropped_freq=9,144,236\n",
      "[extract] A_relevant_raw.txt  chunk=64  kept=16,319,550  dropped_itemcode=45,022,908  dropped_freq=9,331,455\n",
      "[extract] A_relevant_raw.txt  chunk=65  kept=16,563,961  dropped_itemcode=45,707,450  dropped_freq=9,559,953\n",
      "[extract] A_relevant_raw.txt  chunk=66  kept=16,794,862  dropped_itemcode=46,398,193  dropped_freq=9,820,471\n",
      "[extract] A_relevant_raw.txt  chunk=67  kept=17,029,934  dropped_itemcode=47,083,729  dropped_freq=10,078,467\n",
      "[extract] A_relevant_raw.txt  chunk=68  kept=17,273,511  dropped_itemcode=47,762,999  dropped_freq=10,327,463\n",
      "[extract] A_relevant_raw.txt  chunk=69  kept=17,522,924  dropped_itemcode=48,429,181  dropped_freq=10,590,161\n",
      "[extract] A_relevant_raw.txt  chunk=70  kept=17,774,520  dropped_itemcode=49,088,933  dropped_freq=10,861,120\n",
      "[extract] A_relevant_raw.txt  chunk=71  kept=18,022,270  dropped_itemcode=49,756,463  dropped_freq=11,126,541\n",
      "[extract] A_relevant_raw.txt  chunk=72  kept=18,270,999  dropped_itemcode=50,421,017  dropped_freq=11,393,544\n",
      "[extract] A_relevant_raw.txt  chunk=73  kept=18,519,032  dropped_itemcode=51,086,276  dropped_freq=11,662,381\n",
      "[extract] A_relevant_raw.txt  chunk=74  kept=18,767,414  dropped_itemcode=51,748,657  dropped_freq=11,935,337\n",
      "[extract] A_relevant_raw.txt  chunk=75  kept=19,017,158  dropped_itemcode=52,412,716  dropped_freq=12,203,215\n",
      "[extract] A_relevant_raw.txt  chunk=76  kept=19,269,690  dropped_itemcode=53,071,026  dropped_freq=12,474,627\n",
      "[extract] A_relevant_raw.txt  chunk=77  kept=19,513,285  dropped_itemcode=53,741,792  dropped_freq=12,744,768\n",
      "[extract] A_relevant_raw.txt  chunk=78  kept=19,768,801  dropped_itemcode=54,396,623  dropped_freq=13,013,718\n",
      "[extract] A_relevant_raw.txt  chunk=79  kept=20,015,979  dropped_itemcode=55,061,256  dropped_freq=13,287,362\n",
      "[extract] A_relevant_raw.txt  chunk=80  kept=20,260,908  dropped_itemcode=55,731,581  dropped_freq=13,554,713\n",
      "[extract] A_relevant_raw.txt  chunk=81  kept=20,507,310  dropped_itemcode=56,398,630  dropped_freq=13,823,943\n",
      "[extract] A_relevant_raw.txt  chunk=82  kept=20,759,625  dropped_itemcode=57,060,367  dropped_freq=14,087,826\n",
      "[extract] A_relevant_raw.txt  chunk=83  kept=21,021,143  dropped_itemcode=57,721,944  dropped_freq=14,326,525\n",
      "[extract] A_relevant_raw.txt  chunk=84  kept=21,270,035  dropped_itemcode=58,392,988  dropped_freq=14,582,139\n",
      "[extract] A_relevant_raw.txt  chunk=85  kept=21,514,062  dropped_itemcode=59,067,259  dropped_freq=14,842,531\n",
      "[extract] A_relevant_raw.txt  chunk=86  kept=21,742,759  dropped_itemcode=59,760,629  dropped_freq=15,100,721\n",
      "[extract] A_relevant_raw.txt  chunk=87  kept=21,989,173  dropped_itemcode=60,427,555  dropped_freq=15,370,694\n",
      "[extract] A_relevant_raw.txt  chunk=88  kept=22,238,537  dropped_itemcode=61,090,637  dropped_freq=15,641,097\n",
      "[extract] A_relevant_raw.txt  chunk=89  kept=22,490,010  dropped_itemcode=61,753,902  dropped_freq=15,901,382\n",
      "[extract] A_relevant_raw.txt  chunk=90  kept=22,741,364  dropped_itemcode=62,417,582  dropped_freq=16,162,679\n",
      "[extract] A_relevant_raw.txt  chunk=91  kept=22,981,943  dropped_itemcode=63,102,197  dropped_freq=16,405,918\n",
      "[extract] A_relevant_raw.txt  chunk=92  kept=23,237,150  dropped_itemcode=63,771,420  dropped_freq=16,638,195\n",
      "[extract] A_relevant_raw.txt  chunk=93  kept=23,487,094  dropped_itemcode=64,447,650  dropped_freq=16,872,994\n",
      "[extract] A_relevant_raw.txt  chunk=94  kept=23,735,620  dropped_itemcode=65,131,149  dropped_freq=17,092,943\n",
      "[extract] A_relevant_raw.txt  chunk=95  kept=23,987,038  dropped_itemcode=65,805,129  dropped_freq=17,326,726\n",
      "[extract] A_relevant_raw.txt  chunk=96  kept=24,225,929  dropped_itemcode=66,492,325  dropped_freq=17,567,645\n",
      "[extract] A_relevant_raw.txt  chunk=97  kept=24,457,386  dropped_itemcode=67,178,369  dropped_freq=17,837,163\n",
      "[extract] A_relevant_raw.txt  chunk=98  kept=24,697,151  dropped_itemcode=67,860,039  dropped_freq=18,088,871\n",
      "[extract] A_relevant_raw.txt  chunk=99  kept=24,940,429  dropped_itemcode=68,539,605  dropped_freq=18,334,910\n",
      "[extract] A_relevant_raw.txt  chunk=100  kept=25,171,752  dropped_itemcode=69,227,778  dropped_freq=18,598,021\n",
      "[extract] A_relevant_raw.txt  chunk=101  kept=25,412,371  dropped_itemcode=69,910,327  dropped_freq=18,849,422\n",
      "[extract] A_relevant_raw.txt  chunk=102  kept=25,659,098  dropped_itemcode=70,588,559  dropped_freq=19,090,418\n",
      "[extract] A_relevant_raw.txt  chunk=103  kept=25,898,640  dropped_itemcode=71,272,204  dropped_freq=19,342,679\n",
      "[extract] A_relevant_raw.txt  chunk=104  kept=26,138,297  dropped_itemcode=71,959,132  dropped_freq=19,579,344\n",
      "[extract] A_relevant_raw.txt  chunk=105  kept=26,385,773  dropped_itemcode=72,637,644  dropped_freq=19,816,109\n",
      "[extract] A_relevant_raw.txt  chunk=106  kept=26,610,424  dropped_itemcode=73,327,746  dropped_freq=20,096,152\n",
      "[extract] A_relevant_raw.txt  chunk=107  kept=26,842,639  dropped_itemcode=74,016,502  dropped_freq=20,357,525\n",
      "[extract] A_relevant_raw.txt  chunk=108  kept=27,089,815  dropped_itemcode=74,689,686  dropped_freq=20,609,820\n",
      "[extract] A_relevant_raw.txt  chunk=109  kept=27,327,080  dropped_itemcode=75,372,887  dropped_freq=20,868,669\n",
      "[extract] A_relevant_raw.txt  chunk=110  kept=27,555,229  dropped_itemcode=76,064,806  dropped_freq=21,134,033\n",
      "[extract] A_relevant_raw.txt  chunk=111  kept=27,817,015  dropped_itemcode=76,725,780  dropped_freq=21,363,034\n",
      "[extract] A_relevant_raw.txt  chunk=112  kept=28,058,102  dropped_itemcode=77,408,978  dropped_freq=21,591,711\n",
      "[extract] A_relevant_raw.txt  chunk=113  kept=28,296,969  dropped_itemcode=78,090,331  dropped_freq=21,832,725\n",
      "[extract] A_relevant_raw.txt  chunk=114  kept=28,537,374  dropped_itemcode=78,769,225  dropped_freq=22,073,996\n",
      "[extract] A_relevant_raw.txt  chunk=115  kept=28,786,080  dropped_itemcode=79,443,527  dropped_freq=22,308,074\n",
      "[extract] A_relevant_raw.txt  chunk=116  kept=29,026,970  dropped_itemcode=80,136,826  dropped_freq=22,516,943\n",
      "[extract] A_relevant_raw.txt  chunk=117  kept=29,273,372  dropped_itemcode=80,822,249  dropped_freq=22,724,990\n",
      "[extract] A_relevant_raw.txt  chunk=118  kept=29,517,818  dropped_itemcode=81,512,532  dropped_freq=22,929,335\n",
      "[extract] A_relevant_raw.txt  chunk=119  kept=29,762,388  dropped_itemcode=82,197,304  dropped_freq=23,147,156\n",
      "[extract] A_relevant_raw.txt  chunk=120  kept=29,994,304  dropped_itemcode=82,893,403  dropped_freq=23,376,811\n",
      "[extract] A_relevant_raw.txt  chunk=121  kept=30,237,792  dropped_itemcode=83,577,053  dropped_freq=23,598,906\n",
      "[extract] A_relevant_raw.txt  chunk=122  kept=30,471,127  dropped_itemcode=84,272,758  dropped_freq=23,825,033\n",
      "[extract] A_relevant_raw.txt  chunk=123  kept=30,713,551  dropped_itemcode=84,959,870  dropped_freq=24,044,047\n",
      "[extract] A_relevant_raw.txt  chunk=124  kept=30,945,527  dropped_itemcode=85,661,921  dropped_freq=24,259,734\n",
      "[extract] A_relevant_raw.txt  chunk=125  kept=31,198,198  dropped_itemcode=86,336,807  dropped_freq=24,474,596\n",
      "[extract] A_relevant_raw.txt  chunk=126  kept=31,435,169  dropped_itemcode=87,033,176  dropped_freq=24,686,735\n",
      "[extract] A_relevant_raw.txt  chunk=127  kept=31,687,581  dropped_itemcode=87,711,204  dropped_freq=24,895,264\n",
      "[extract] A_relevant_raw.txt  chunk=128  kept=31,925,282  dropped_itemcode=88,407,492  dropped_freq=25,107,100\n",
      "[extract] A_relevant_raw.txt  chunk=129  kept=32,173,326  dropped_itemcode=89,092,006  dropped_freq=25,316,579\n",
      "[extract] A_relevant_raw.txt  chunk=130  kept=32,407,072  dropped_itemcode=89,799,988  dropped_freq=25,512,477\n",
      "[extract] A_relevant_raw.txt  chunk=131  kept=32,651,524  dropped_itemcode=90,487,571  dropped_freq=25,723,777\n",
      "[extract] A_relevant_raw.txt  chunk=132  kept=32,887,437  dropped_itemcode=91,192,716  dropped_freq=25,919,106\n",
      "[extract] A_relevant_raw.txt  chunk=133  kept=33,118,819  dropped_itemcode=91,894,317  dropped_freq=26,140,137\n",
      "[extract] A_relevant_raw.txt  chunk=134  kept=33,367,031  dropped_itemcode=92,580,871  dropped_freq=26,341,447\n",
      "[extract] A_relevant_raw.txt  chunk=135  kept=33,611,158  dropped_itemcode=93,270,201  dropped_freq=26,549,843\n",
      "[extract] A_relevant_raw.txt  chunk=136  kept=33,846,854  dropped_itemcode=93,963,062  dropped_freq=26,776,101\n",
      "[extract] A_relevant_raw.txt  chunk=137  kept=34,093,361  dropped_itemcode=94,660,254  dropped_freq=26,954,605\n",
      "[extract] A_relevant_raw.txt  chunk=138  kept=34,347,281  dropped_itemcode=95,342,363  dropped_freq=27,147,669\n",
      "[extract] A_relevant_raw.txt  chunk=139  kept=34,603,440  dropped_itemcode=96,019,262  dropped_freq=27,347,821\n",
      "[extract] A_relevant_raw.txt  chunk=140  kept=34,837,719  dropped_itemcode=96,721,944  dropped_freq=27,554,452\n",
      "[extract] A_relevant_raw.txt  chunk=141  kept=35,094,328  dropped_itemcode=97,396,064  dropped_freq=27,760,192\n",
      "[extract] A_relevant_raw.txt  chunk=142  kept=35,333,351  dropped_itemcode=98,087,117  dropped_freq=27,983,125\n",
      "[extract] A_relevant_raw.txt  chunk=143  kept=35,577,057  dropped_itemcode=98,773,148  dropped_freq=28,198,971\n",
      "[extract] A_relevant_raw.txt  chunk=144  kept=35,811,049  dropped_itemcode=99,468,218  dropped_freq=28,425,077\n",
      "[extract] A_relevant_raw.txt  chunk=145  kept=36,050,970  dropped_itemcode=100,159,387  dropped_freq=28,642,402\n",
      "[extract] A_relevant_raw.txt  chunk=146  kept=36,289,646  dropped_itemcode=100,851,191  dropped_freq=28,859,921\n",
      "[extract] A_relevant_raw.txt  chunk=147  kept=36,529,244  dropped_itemcode=101,544,806  dropped_freq=29,071,733\n",
      "[extract] A_relevant_raw.txt  chunk=148  kept=36,769,049  dropped_itemcode=102,239,936  dropped_freq=29,280,351\n",
      "[extract] A_relevant_raw.txt  chunk=149  kept=37,004,053  dropped_itemcode=102,936,198  dropped_freq=29,501,637\n",
      "[extract] A_relevant_raw.txt  chunk=150  kept=37,238,660  dropped_itemcode=103,631,854  dropped_freq=29,723,820\n",
      "[extract] A_relevant_raw.txt  chunk=151  kept=37,470,126  dropped_itemcode=104,330,599  dropped_freq=29,947,752\n",
      "[extract] A_relevant_raw.txt  chunk=152  kept=37,703,257  dropped_itemcode=105,030,353  dropped_freq=30,167,456\n",
      "[extract] A_relevant_raw.txt  chunk=153  kept=37,935,561  dropped_itemcode=105,729,178  dropped_freq=30,391,365\n",
      "[extract] A_relevant_raw.txt  chunk=154  kept=38,171,821  dropped_itemcode=106,422,463  dropped_freq=30,616,748\n",
      "[extract] A_relevant_raw.txt  chunk=155  kept=38,407,524  dropped_itemcode=107,116,594  dropped_freq=30,841,115\n",
      "[extract] A_relevant_raw.txt  chunk=156  kept=38,654,309  dropped_itemcode=107,805,486  dropped_freq=31,044,117\n",
      "[extract] A_relevant_raw.txt  chunk=157  kept=38,902,150  dropped_itemcode=108,493,947  dropped_freq=31,246,480\n",
      "[extract] A_relevant_raw.txt  chunk=158  kept=39,149,471  dropped_itemcode=109,183,165  dropped_freq=31,444,229\n",
      "[extract] A_relevant_raw.txt  chunk=159  kept=39,387,929  dropped_itemcode=109,875,092  dropped_freq=31,663,741\n",
      "[extract] A_relevant_raw.txt  chunk=160  kept=39,622,791  dropped_itemcode=110,570,242  dropped_freq=31,887,105\n",
      "[extract] A_relevant_raw.txt  chunk=161  kept=39,876,049  dropped_itemcode=111,251,650  dropped_freq=32,089,127\n",
      "[extract] A_relevant_raw.txt  chunk=162  kept=40,125,004  dropped_itemcode=111,932,727  dropped_freq=32,300,054\n",
      "[extract] A_relevant_raw.txt  chunk=163  kept=40,381,307  dropped_itemcode=112,607,400  dropped_freq=32,503,677\n",
      "[extract] A_relevant_raw.txt  chunk=164  kept=40,636,039  dropped_itemcode=113,282,052  dropped_freq=32,715,052\n",
      "[extract] A_relevant_raw.txt  chunk=165  kept=40,892,259  dropped_itemcode=113,954,212  dropped_freq=32,928,217\n",
      "[extract] A_relevant_raw.txt  chunk=166  kept=41,141,273  dropped_itemcode=114,633,495  dropped_freq=33,146,244\n",
      "[extract] A_relevant_raw.txt  chunk=167  kept=41,397,342  dropped_itemcode=115,304,460  dropped_freq=33,361,319\n",
      "[extract] A_relevant_raw.txt  chunk=168  kept=41,648,975  dropped_itemcode=115,979,520  dropped_freq=33,579,247\n",
      "[extract] A_relevant_raw.txt  chunk=169  kept=41,897,958  dropped_itemcode=116,658,799  dropped_freq=33,796,325\n",
      "[extract] A_relevant_raw.txt  chunk=170  kept=42,136,114  dropped_itemcode=117,350,626  dropped_freq=34,020,810\n",
      "[extract] A_relevant_raw.txt  chunk=171  kept=42,375,281  dropped_itemcode=118,045,865  dropped_freq=34,234,891\n",
      "[extract] A_relevant_raw.txt  chunk=172  kept=42,612,549  dropped_itemcode=118,739,600  dropped_freq=34,457,599\n",
      "[extract] A_relevant_raw.txt  chunk=173  kept=42,842,201  dropped_itemcode=119,439,692  dropped_freq=34,684,334\n",
      "[extract] A_relevant_raw.txt  chunk=174  kept=43,077,194  dropped_itemcode=120,135,628  dropped_freq=34,906,282\n",
      "[extract] A_relevant_raw.txt  chunk=175  kept=43,332,576  dropped_itemcode=120,807,918  dropped_freq=35,118,173\n",
      "[extract] A_relevant_raw.txt  chunk=176  kept=43,588,603  dropped_itemcode=121,483,022  dropped_freq=35,323,838\n",
      "[extract] A_relevant_raw.txt  chunk=177  kept=43,839,699  dropped_itemcode=122,161,306  dropped_freq=35,535,724\n",
      "[extract] A_relevant_raw.txt  chunk=178  kept=44,077,873  dropped_itemcode=122,854,754  dropped_freq=35,755,323\n",
      "[extract] A_relevant_raw.txt  chunk=179  kept=44,325,903  dropped_itemcode=123,535,151  dropped_freq=35,971,060\n",
      "[extract] A_relevant_raw.txt  chunk=180  kept=44,581,648  dropped_itemcode=124,206,052  dropped_freq=36,186,777\n",
      "[extract] A_relevant_raw.txt  chunk=181  kept=44,834,950  dropped_itemcode=124,879,923  dropped_freq=36,403,153\n",
      "[extract] A_relevant_raw.txt  chunk=182  kept=45,085,798  dropped_itemcode=125,556,230  dropped_freq=36,623,019\n",
      "[extract] A_relevant_raw.txt  chunk=183  kept=45,335,954  dropped_itemcode=126,233,931  dropped_freq=36,838,441\n",
      "[extract] A_relevant_raw.txt  chunk=184  kept=45,589,164  dropped_itemcode=126,908,405  dropped_freq=37,054,258\n",
      "[extract] A_relevant_raw.txt  chunk=185  kept=45,846,647  dropped_itemcode=127,577,190  dropped_freq=37,269,693\n",
      "[extract] A_relevant_raw.txt  chunk=186  kept=46,105,462  dropped_itemcode=128,242,066  dropped_freq=37,505,185\n",
      "[extract] A_relevant_raw.txt  chunk=187  kept=46,344,970  dropped_itemcode=128,946,431  dropped_freq=37,691,720\n",
      "[extract] A_relevant_raw.txt  chunk=188  kept=46,619,469  dropped_itemcode=129,582,167  dropped_freq=37,938,876\n",
      "[extract] A_relevant_raw.txt  chunk=189  kept=46,910,340  dropped_itemcode=130,200,028  dropped_freq=38,180,545\n",
      "[extract] A_relevant_raw.txt  chunk=190  kept=47,208,483  dropped_itemcode=130,835,210  dropped_freq=38,367,282\n",
      "[extract] A_relevant_raw.txt  chunk=191  kept=47,519,972  dropped_itemcode=131,495,378  dropped_freq=38,454,203\n",
      "[extract] A_relevant_raw.txt  chunk=192  kept=47,754,879  dropped_itemcode=132,215,752  dropped_freq=38,612,925\n",
      "[extract] A_relevant_raw.txt  chunk=193  kept=48,056,566  dropped_itemcode=132,880,202  dropped_freq=38,724,671\n",
      "[extract] A_relevant_raw.txt  chunk=194  kept=48,308,637  dropped_itemcode=133,576,151  dropped_freq=38,897,172\n",
      "[extract] A_relevant_raw.txt  chunk=195  kept=48,558,232  dropped_itemcode=134,278,278  dropped_freq=39,062,056\n",
      "[extract] A_relevant_raw.txt  chunk=196  kept=48,785,693  dropped_itemcode=134,988,904  dropped_freq=39,283,066\n",
      "[extract] A_relevant_raw.txt  chunk=197  kept=49,003,785  dropped_itemcode=135,693,794  dropped_freq=39,559,013\n",
      "[extract] A_relevant_raw.txt  chunk=198  kept=49,230,707  dropped_itemcode=136,387,228  dropped_freq=39,833,088\n",
      "[extract] A_relevant_raw.txt  chunk=199  kept=49,434,877  dropped_itemcode=137,111,490  dropped_freq=40,093,626\n",
      "[extract] A_relevant_raw.txt  chunk=200  kept=49,651,953  dropped_itemcode=137,818,757  dropped_freq=40,356,552\n",
      "[extract] A_relevant_raw.txt  chunk=201  kept=49,917,267  dropped_itemcode=138,501,570  dropped_freq=40,521,353\n",
      "[extract] A_relevant_raw.txt  chunk=202  kept=50,171,266  dropped_itemcode=139,195,804  dropped_freq=40,693,660\n",
      "[extract] A_relevant_raw.txt  chunk=203  kept=50,430,262  dropped_itemcode=139,874,153  dropped_freq=40,887,591\n",
      "[extract] A_relevant_raw.txt  chunk=204  kept=50,689,548  dropped_itemcode=140,559,332  dropped_freq=41,066,295\n",
      "[extract] A_relevant_raw.txt  chunk=205  kept=50,977,180  dropped_itemcode=141,240,381  dropped_freq=41,158,230\n",
      "[extract] A_relevant_raw.txt  chunk=206  kept=51,280,821  dropped_itemcode=141,928,444  dropped_freq=41,182,860\n",
      "[extract] A_relevant_raw.txt  chunk=207  kept=51,583,414  dropped_itemcode=142,617,614  dropped_freq=41,207,734\n",
      "[extract] A_relevant_raw.txt  chunk=208  kept=51,871,990  dropped_itemcode=143,316,569  dropped_freq=41,246,478\n",
      "[extract] A_relevant_raw.txt  chunk=209  kept=52,164,483  dropped_itemcode=144,014,910  dropped_freq=41,272,857\n",
      "[extract] A_relevant_raw.txt  chunk=210  kept=52,460,824  dropped_itemcode=144,702,608  dropped_freq=41,321,002\n",
      "[extract] A_relevant_raw.txt  chunk=211  kept=52,756,035  dropped_itemcode=145,394,299  dropped_freq=41,359,438\n",
      "[extract] A_relevant_raw.txt  chunk=212  kept=53,057,781  dropped_itemcode=146,085,131  dropped_freq=41,378,932\n",
      "[extract] A_relevant_raw.txt  chunk=213  kept=53,362,315  dropped_itemcode=146,772,333  dropped_freq=41,401,940\n",
      "[extract] A_relevant_raw.txt  chunk=214  kept=53,675,930  dropped_itemcode=147,450,545  dropped_freq=41,420,552\n",
      "[extract] A_relevant_raw.txt  chunk=215  kept=53,978,372  dropped_itemcode=148,137,307  dropped_freq=41,450,976\n",
      "[extract] A_relevant_raw.txt  chunk=216  kept=54,286,888  dropped_itemcode=148,819,858  dropped_freq=41,473,729\n",
      "[extract] A_relevant_raw.txt  chunk=217  kept=54,571,617  dropped_itemcode=149,519,692  dropped_freq=41,527,419\n",
      "[extract] A_relevant_raw.txt  chunk=218  kept=54,807,309  dropped_itemcode=150,227,573  dropped_freq=41,725,264\n",
      "[extract] A_relevant_raw.txt  chunk=219  kept=55,073,795  dropped_itemcode=150,929,775  dropped_freq=41,828,848\n",
      "[extract] A_relevant_raw.txt  chunk=220  kept=55,350,814  dropped_itemcode=151,626,955  dropped_freq=41,913,096\n",
      "[extract] A_relevant_raw.txt  chunk=221  kept=55,617,886  dropped_itemcode=152,320,344  dropped_freq=42,041,878\n",
      "[extract] A_relevant_raw.txt  chunk=222  kept=55,893,439  dropped_itemcode=153,001,767  dropped_freq=42,175,409\n",
      "[extract] A_relevant_raw.txt  chunk=223  kept=56,158,140  dropped_itemcode=153,688,488  dropped_freq=42,333,146\n",
      "[extract] A_relevant_raw.txt  chunk=224  kept=56,424,982  dropped_itemcode=154,373,529  dropped_freq=42,479,145\n",
      "[extract] A_relevant_raw.txt  chunk=225  kept=56,692,581  dropped_itemcode=155,062,134  dropped_freq=42,614,884\n",
      "[extract] A_relevant_raw.txt  chunk=226  kept=56,946,811  dropped_itemcode=155,755,923  dropped_freq=42,781,989\n",
      "[extract] A_relevant_raw.txt  chunk=227  kept=57,209,293  dropped_itemcode=156,438,013  dropped_freq=42,951,911\n",
      "[extract] A_relevant_raw.txt  chunk=228  kept=57,477,605  dropped_itemcode=157,108,830  dropped_freq=43,131,722\n",
      "[extract] A_relevant_raw.txt  chunk=229  kept=57,739,282  dropped_itemcode=157,799,189  dropped_freq=43,279,388\n",
      "[extract] A_relevant_raw.txt  chunk=230  kept=57,989,216  dropped_itemcode=158,496,647  dropped_freq=43,453,915\n",
      "[extract] A_relevant_raw.txt  chunk=231  kept=58,252,956  dropped_itemcode=159,176,350  dropped_freq=43,626,768\n",
      "[extract] A_relevant_raw.txt  chunk=232  kept=58,510,968  dropped_itemcode=159,867,736  dropped_freq=43,789,532\n",
      "[extract] A_relevant_raw.txt  chunk=233  kept=58,752,361  dropped_itemcode=160,568,963  dropped_freq=43,978,081\n",
      "[extract] A_relevant_raw.txt  chunk=234  kept=59,003,138  dropped_itemcode=161,270,985  dropped_freq=44,138,839\n",
      "[extract] A_relevant_raw.txt  chunk=235  kept=59,258,803  dropped_itemcode=161,964,545  dropped_freq=44,310,594\n",
      "[extract] A_relevant_raw.txt  chunk=236  kept=59,518,502  dropped_itemcode=162,650,844  dropped_freq=44,487,579\n",
      "[extract] A_relevant_raw.txt  chunk=237  kept=59,785,062  dropped_itemcode=163,326,443  dropped_freq=44,670,885\n",
      "[extract] A_relevant_raw.txt  chunk=238  kept=60,058,573  dropped_itemcode=164,037,855  dropped_freq=44,727,002\n",
      "[extract] A_relevant_raw.txt  chunk=239  kept=60,338,370  dropped_itemcode=164,745,594  dropped_freq=44,774,354\n",
      "[extract] A_relevant_raw.txt  chunk=240  kept=60,627,923  dropped_itemcode=165,449,216  dropped_freq=44,798,897\n",
      "[extract] A_relevant_raw.txt  chunk=241  kept=60,920,787  dropped_itemcode=166,151,585  dropped_freq=44,817,123\n",
      "[extract] A_relevant_raw.txt  chunk=242  kept=61,210,744  dropped_itemcode=166,855,146  dropped_freq=44,839,827\n",
      "[extract] A_relevant_raw.txt  chunk=243  kept=61,500,630  dropped_itemcode=167,558,773  dropped_freq=44,862,585\n",
      "[extract] A_relevant_raw.txt  chunk=244  kept=61,762,473  dropped_itemcode=168,270,778  dropped_freq=44,958,738\n",
      "[extract] A_relevant_raw.txt  chunk=245  kept=62,004,017  dropped_itemcode=168,999,660  dropped_freq=45,078,399\n",
      "[extract] A_relevant_raw.txt  chunk=246  kept=62,279,637  dropped_itemcode=169,708,936  dropped_freq=45,134,110\n",
      "[extract] A_relevant_raw.txt  chunk=247  kept=62,565,800  dropped_itemcode=170,419,677  dropped_freq=45,145,720\n",
      "[extract] A_relevant_raw.txt  chunk=248  kept=62,855,417  dropped_itemcode=171,126,994  dropped_freq=45,157,399\n",
      "[extract] A_relevant_raw.txt  chunk=249  kept=63,140,864  dropped_itemcode=171,837,865  dropped_freq=45,170,242\n",
      "[extract] A_relevant_raw.txt  chunk=250  kept=63,410,476  dropped_itemcode=172,549,939  dropped_freq=45,238,654\n",
      "[extract] A_relevant_raw.txt  chunk=251  kept=63,675,598  dropped_itemcode=173,260,610  dropped_freq=45,328,539\n",
      "[extract] A_relevant_raw.txt  chunk=252  kept=63,949,338  dropped_itemcode=173,961,477  dropped_freq=45,419,322\n",
      "[extract] A_relevant_raw.txt  chunk=253  kept=64,220,259  dropped_itemcode=174,666,776  dropped_freq=45,504,216\n",
      "[extract] A_relevant_raw.txt  chunk=254  kept=64,494,146  dropped_itemcode=175,367,380  dropped_freq=45,593,975\n",
      "[extract] A_relevant_raw.txt  chunk=255  kept=64,786,664  dropped_itemcode=176,062,364  dropped_freq=45,636,763\n",
      "[extract] A_relevant_raw.txt  chunk=256  kept=65,057,448  dropped_itemcode=176,769,623  dropped_freq=45,721,324\n",
      "[extract] A_relevant_raw.txt  chunk=257  kept=65,331,813  dropped_itemcode=177,471,740  dropped_freq=45,805,777\n",
      "[extract] A_relevant_raw.txt  chunk=258  kept=65,602,258  dropped_itemcode=178,180,540  dropped_freq=45,882,290\n",
      "[extract] A_relevant_raw.txt  chunk=259  kept=65,859,366  dropped_itemcode=178,864,606  dropped_freq=46,070,154\n",
      "[extract] A_relevant_raw.txt  chunk=260  kept=66,104,701  dropped_itemcode=179,562,437  dropped_freq=46,268,782\n",
      "[extract] A_relevant_raw.txt  chunk=261  kept=66,349,570  dropped_itemcode=180,266,728  dropped_freq=46,455,720\n",
      "[extract] A_relevant_raw.txt  chunk=262  kept=66,610,499  dropped_itemcode=180,955,985  dropped_freq=46,632,459\n",
      "[extract] A_relevant_raw.txt  chunk=263  kept=66,866,657  dropped_itemcode=181,647,848  dropped_freq=46,817,542\n",
      "[extract] A_relevant_raw.txt  chunk=264  kept=67,127,422  dropped_itemcode=182,336,916  dropped_freq=46,992,683\n",
      "[extract] A_relevant_raw.txt  chunk=265  kept=67,382,654  dropped_itemcode=183,029,848  dropped_freq=47,170,265\n",
      "[extract] A_relevant_raw.txt  chunk=266  kept=67,638,661  dropped_itemcode=183,718,693  dropped_freq=47,357,078\n",
      "[extract] A_relevant_raw.txt  chunk=267  kept=67,904,510  dropped_itemcode=184,406,262  dropped_freq=47,512,380\n",
      "[extract] A_relevant_raw.txt  chunk=268  kept=68,146,424  dropped_itemcode=185,107,902  dropped_freq=47,710,161\n",
      "[extract] A_relevant_raw.txt  chunk=269  kept=68,389,999  dropped_itemcode=185,817,576  dropped_freq=47,880,190\n",
      "[extract] A_relevant_raw.txt  chunk=270  kept=68,631,510  dropped_itemcode=186,527,050  dropped_freq=48,058,908\n",
      "[extract] A_relevant_raw.txt  chunk=271  kept=68,873,218  dropped_itemcode=187,238,732  dropped_freq=48,228,281\n",
      "[extract] A_relevant_raw.txt  chunk=272  kept=69,121,909  dropped_itemcode=187,945,643  dropped_freq=48,388,333\n",
      "[extract] A_relevant_raw.txt  chunk=273  kept=69,371,221  dropped_itemcode=188,648,247  dropped_freq=48,561,020\n",
      "[extract] A_relevant_raw.txt  chunk=274  kept=69,627,081  dropped_itemcode=189,341,514  dropped_freq=48,738,557\n",
      "[extract] A_relevant_raw.txt  chunk=275  kept=69,883,973  dropped_itemcode=190,031,907  dropped_freq=48,922,481\n",
      "[extract] A_relevant_raw.txt  chunk=276  kept=70,140,224  dropped_itemcode=190,722,442  dropped_freq=49,108,253\n",
      "[extract] A_relevant_raw.txt  chunk=277  kept=70,403,158  dropped_itemcode=191,406,055  dropped_freq=49,292,119\n",
      "[extract] A_relevant_raw.txt  chunk=278  kept=70,667,411  dropped_itemcode=192,094,816  dropped_freq=49,454,247\n",
      "[extract] A_relevant_raw.txt  chunk=279  kept=70,919,112  dropped_itemcode=192,796,102  dropped_freq=49,622,184\n",
      "[extract] A_relevant_raw.txt  chunk=280  kept=71,177,270  dropped_itemcode=193,490,032  dropped_freq=49,792,966\n",
      "[extract] A_relevant_raw.txt  chunk=281  kept=71,423,557  dropped_itemcode=194,199,009  dropped_freq=49,954,379\n",
      "[extract] A_relevant_raw.txt  chunk=282  kept=71,665,755  dropped_itemcode=194,909,219  dropped_freq=50,127,883\n",
      "[extract] A_relevant_raw.txt  chunk=283  kept=71,909,117  dropped_itemcode=195,618,750  dropped_freq=50,298,658\n",
      "[extract] A_relevant_raw.txt  chunk=284  kept=72,152,066  dropped_itemcode=196,328,183  dropped_freq=50,471,312\n",
      "[extract] A_relevant_raw.txt  chunk=285  kept=72,395,258  dropped_itemcode=197,038,547  dropped_freq=50,641,420\n",
      "[extract] A_relevant_raw.txt  chunk=286  kept=72,638,188  dropped_itemcode=197,746,924  dropped_freq=50,818,624\n",
      "[extract] A_relevant_raw.txt  chunk=287  kept=72,885,858  dropped_itemcode=198,451,414  dropped_freq=50,989,378\n",
      "[extract] A_relevant_raw.txt  chunk=288  kept=73,131,998  dropped_itemcode=199,161,694  dropped_freq=51,147,082\n",
      "[extract] A_relevant_raw.txt  chunk=289  kept=73,375,710  dropped_itemcode=199,871,665  dropped_freq=51,316,498\n",
      "[extract] A_relevant_raw.txt  chunk=290  kept=73,622,416  dropped_itemcode=200,574,680  dropped_freq=51,496,077\n",
      "[extract] A_relevant_raw.txt  chunk=291  kept=73,861,543  dropped_itemcode=201,287,152  dropped_freq=51,676,009\n",
      "[extract] A_relevant_raw.txt  chunk=292  kept=74,102,105  dropped_itemcode=201,996,084  dropped_freq=51,858,063\n",
      "[extract] A_relevant_raw.txt  chunk=293  kept=74,342,881  dropped_itemcode=202,708,873  dropped_freq=52,029,134\n",
      "[extract] A_relevant_raw.txt  chunk=294  kept=74,584,548  dropped_itemcode=203,418,819  dropped_freq=52,207,649\n",
      "[extract] A_relevant_raw.txt  chunk=295  kept=74,830,829  dropped_itemcode=204,125,081  dropped_freq=52,379,098\n",
      "[extract] A_relevant_raw.txt  chunk=296  kept=75,076,073  dropped_itemcode=204,829,526  dropped_freq=52,560,545\n",
      "[extract] A_relevant_raw.txt  chunk=297  kept=75,318,033  dropped_itemcode=205,537,711  dropped_freq=52,741,154\n",
      "[extract] A_relevant_raw.txt  chunk=298  kept=75,560,594  dropped_itemcode=206,246,139  dropped_freq=52,920,061\n",
      "[extract] A_relevant_raw.txt  chunk=299  kept=75,803,264  dropped_itemcode=206,954,058  dropped_freq=53,100,255\n",
      "[extract] A_relevant_raw.txt  chunk=300  kept=76,048,655  dropped_itemcode=207,662,016  dropped_freq=53,270,524\n",
      "[extract] A_relevant_raw.txt  chunk=301  kept=76,284,363  dropped_itemcode=208,360,918  dropped_freq=53,484,873\n",
      "[extract] A_relevant_raw.txt  chunk=302  kept=76,512,099  dropped_itemcode=209,060,113  dropped_freq=53,717,575\n",
      "[extract] A_relevant_raw.txt  chunk=303  kept=76,733,866  dropped_itemcode=209,762,421  dropped_freq=53,962,300\n",
      "[extract] A_relevant_raw.txt  chunk=304  kept=76,970,663  dropped_itemcode=210,447,426  dropped_freq=54,203,493\n",
      "[extract] A_relevant_raw.txt  chunk=305  kept=77,210,887  dropped_itemcode=211,123,054  dropped_freq=54,450,751\n",
      "[extract] A_relevant_raw.txt  chunk=306  kept=77,430,637  dropped_itemcode=211,830,524  dropped_freq=54,689,921\n",
      "[extract] A_relevant_raw.txt  chunk=307  kept=77,659,530  dropped_itemcode=212,527,126  dropped_freq=54,926,916\n",
      "[extract] A_relevant_raw.txt  chunk=308  kept=77,894,645  dropped_itemcode=213,216,647  dropped_freq=55,161,646\n",
      "[extract] A_relevant_raw.txt  chunk=309  kept=78,120,910  dropped_itemcode=213,917,403  dropped_freq=55,395,323\n",
      "[extract] A_relevant_raw.txt  chunk=310  kept=78,361,936  dropped_itemcode=214,597,477  dropped_freq=55,636,889\n",
      "[extract] A_relevant_raw.txt  chunk=311  kept=78,596,020  dropped_itemcode=215,285,855  dropped_freq=55,875,340\n",
      "[extract] A_relevant_raw.txt  chunk=312  kept=78,821,892  dropped_itemcode=215,980,413  dropped_freq=56,123,020\n",
      "[extract] A_relevant_raw.txt  chunk=313  kept=79,086,525  dropped_itemcode=216,677,047  dropped_freq=56,247,236\n",
      "[extract] A_relevant_raw.txt  chunk=314  kept=79,365,008  dropped_itemcode=217,362,173  dropped_freq=56,360,750\n",
      "[extract] A_relevant_raw.txt  chunk=315  kept=79,624,219  dropped_itemcode=218,023,933  dropped_freq=56,608,672\n",
      "[extract] A_relevant_raw.txt  chunk=316  kept=79,891,174  dropped_itemcode=218,676,542  dropped_freq=56,855,491\n",
      "[extract] A_relevant_raw.txt  chunk=317  kept=80,150,023  dropped_itemcode=219,339,640  dropped_freq=57,103,181\n",
      "[extract] A_relevant_raw.txt  chunk=318  kept=80,406,120  dropped_itemcode=220,006,662  dropped_freq=57,347,787\n",
      "[extract] A_relevant_raw.txt  chunk=319  kept=80,645,188  dropped_itemcode=220,698,191  dropped_freq=57,583,955\n",
      "[extract] A_relevant_raw.txt  chunk=320  kept=80,882,736  dropped_itemcode=221,392,855  dropped_freq=57,813,120\n",
      "[extract] A_relevant_raw.txt  chunk=321  kept=81,120,034  dropped_itemcode=222,085,501  dropped_freq=58,047,382\n",
      "[extract] A_relevant_raw.txt  chunk=322  kept=81,359,645  dropped_itemcode=222,771,489  dropped_freq=58,299,439\n",
      "[extract] A_relevant_raw.txt  chunk=323  kept=81,608,902  dropped_itemcode=223,457,909  dropped_freq=58,514,912\n",
      "[extract] A_relevant_raw.txt  chunk=324  kept=81,856,544  dropped_itemcode=224,152,514  dropped_freq=58,708,142\n",
      "[extract] A_relevant_raw.txt  chunk=325  kept=82,116,384  dropped_itemcode=224,855,285  dropped_freq=58,835,667\n",
      "[extract] A_relevant_raw.txt  chunk=326  kept=82,362,161  dropped_itemcode=225,557,056  dropped_freq=59,015,684\n",
      "[extract] A_relevant_raw.txt  chunk=327  kept=82,604,614  dropped_itemcode=226,270,075  dropped_freq=59,175,344\n",
      "[extract] A_relevant_raw.txt  chunk=328  kept=82,855,668  dropped_itemcode=226,965,839  dropped_freq=59,351,410\n",
      "[extract] A_relevant_raw.txt  chunk=329  kept=83,101,330  dropped_itemcode=227,681,915  dropped_freq=59,490,873\n",
      "[extract] A_relevant_raw.txt  chunk=330  kept=83,362,190  dropped_itemcode=228,385,701  dropped_freq=59,616,141\n",
      "[extract] A_relevant_raw.txt  chunk=331  kept=83,619,015  dropped_itemcode=229,080,893  dropped_freq=59,775,321\n",
      "[extract] A_relevant_raw.txt  chunk=332  kept=83,872,744  dropped_itemcode=229,774,344  dropped_freq=59,952,410\n",
      "[extract] A_relevant_raw.txt  chunk=333  kept=84,115,694  dropped_itemcode=230,454,917  dropped_freq=60,205,537\n",
      "[extract] A_relevant_raw.txt  chunk=334  kept=84,361,417  dropped_itemcode=231,147,170  dropped_freq=60,429,254\n",
      "[extract] A_relevant_raw.txt  chunk=335  kept=84,608,809  dropped_itemcode=231,839,915  dropped_freq=60,643,550\n",
      "[extract] A_relevant_raw.txt  chunk=336  kept=84,853,060  dropped_itemcode=232,536,898  dropped_freq=60,855,966\n",
      "[extract] A_relevant_raw.txt  chunk=337  kept=85,096,840  dropped_itemcode=233,232,912  dropped_freq=61,074,270\n",
      "[extract] A_relevant_raw.txt  chunk=338  kept=85,339,098  dropped_itemcode=233,931,170  dropped_freq=61,288,631\n",
      "[extract] A_relevant_raw.txt  chunk=339  kept=85,586,073  dropped_itemcode=234,624,588  dropped_freq=61,500,434\n",
      "[extract] A_relevant_raw.txt  chunk=340  kept=85,835,818  dropped_itemcode=235,316,943  dropped_freq=61,702,152\n",
      "[extract] A_relevant_raw.txt  chunk=341  kept=86,097,338  dropped_itemcode=235,985,716  dropped_freq=61,921,847\n",
      "[extract] A_relevant_raw.txt  chunk=342  kept=86,352,919  dropped_itemcode=236,667,135  dropped_freq=62,125,902\n",
      "[extract] A_relevant_raw.txt  chunk=343  kept=86,610,063  dropped_itemcode=237,344,231  dropped_freq=62,336,203\n",
      "[extract] A_relevant_raw.txt  chunk=344  kept=86,888,613  dropped_itemcode=238,002,474  dropped_freq=62,528,471\n",
      "[extract] A_relevant_raw.txt  chunk=345  kept=87,164,061  dropped_itemcode=238,666,907  dropped_freq=62,714,841\n",
      "[extract] A_relevant_raw.txt  chunk=346  kept=87,431,005  dropped_itemcode=239,339,392  dropped_freq=62,905,644\n",
      "[extract] A_relevant_raw.txt  chunk=347  kept=87,695,566  dropped_itemcode=240,009,967  dropped_freq=63,112,953\n",
      "[extract] A_relevant_raw.txt  chunk=348  kept=87,961,931  dropped_itemcode=240,679,514  dropped_freq=63,315,408\n",
      "[extract] A_relevant_raw.txt  chunk=349  kept=88,235,387  dropped_itemcode=241,343,297  dropped_freq=63,511,455\n",
      "[extract] A_relevant_raw.txt  chunk=350  kept=88,507,902  dropped_itemcode=242,008,009  dropped_freq=63,706,025\n",
      "[extract] A_relevant_raw.txt  chunk=351  kept=88,783,031  dropped_itemcode=242,670,678  dropped_freq=63,896,742\n",
      "[extract] A_relevant_raw.txt  chunk=352  kept=89,044,162  dropped_itemcode=243,340,510  dropped_freq=64,114,596\n",
      "[extract] A_relevant_raw.txt  chunk=353  kept=89,305,231  dropped_itemcode=244,017,658  dropped_freq=64,315,383\n",
      "[extract] A_relevant_raw.txt  chunk=354  kept=89,561,760  dropped_itemcode=244,694,923  dropped_freq=64,530,807\n",
      "[extract] A_relevant_raw.txt  chunk=355  kept=89,827,327  dropped_itemcode=245,365,083  dropped_freq=64,734,836\n",
      "[extract] A_relevant_raw.txt  chunk=356  kept=90,087,032  dropped_itemcode=246,037,877  dropped_freq=64,952,110\n",
      "[extract] A_relevant_raw.txt  chunk=357  kept=90,351,721  dropped_itemcode=246,708,993  dropped_freq=65,155,098\n",
      "[extract] A_relevant_raw.txt  chunk=358  kept=90,615,373  dropped_itemcode=247,378,826  dropped_freq=65,362,057\n",
      "[extract] A_relevant_raw.txt  chunk=359  kept=90,875,655  dropped_itemcode=248,052,932  dropped_freq=65,568,115\n",
      "[extract] A_relevant_raw.txt  chunk=360  kept=91,143,441  dropped_itemcode=248,720,757  dropped_freq=65,765,559\n",
      "[extract] A_relevant_raw.txt  chunk=361  kept=91,409,160  dropped_itemcode=249,386,746  dropped_freq=65,974,999\n",
      "[extract] A_relevant_raw.txt  chunk=362  kept=91,679,252  dropped_itemcode=250,051,480  dropped_freq=66,174,429\n",
      "[extract] A_relevant_raw.txt  chunk=363  kept=91,943,700  dropped_itemcode=250,728,542  dropped_freq=66,361,321\n",
      "[extract] A_relevant_raw.txt  chunk=364  kept=92,206,462  dropped_itemcode=251,405,399  dropped_freq=66,551,817\n",
      "[extract] A_relevant_raw.txt  chunk=365  kept=92,478,626  dropped_itemcode=252,073,622  dropped_freq=66,735,755\n",
      "[extract] A_relevant_raw.txt  chunk=366  kept=92,747,684  dropped_itemcode=252,741,130  dropped_freq=66,931,159\n",
      "[extract] A_relevant_raw.txt  chunk=367  kept=93,011,486  dropped_itemcode=253,415,693  dropped_freq=67,125,371\n",
      "[extract] A_relevant_raw.txt  chunk=368  kept=93,277,168  dropped_itemcode=254,088,177  dropped_freq=67,320,335\n",
      "[extract] A_relevant_raw.txt  chunk=369  kept=93,549,818  dropped_itemcode=254,752,837  dropped_freq=67,511,634\n",
      "[extract] A_relevant_raw.txt  chunk=370  kept=93,816,589  dropped_itemcode=255,427,358  dropped_freq=67,691,783\n",
      "[extract] A_relevant_raw.txt  chunk=371  kept=94,089,179  dropped_itemcode=256,089,269  dropped_freq=67,890,998\n",
      "[extract] A_relevant_raw.txt  chunk=372  kept=94,364,541  dropped_itemcode=256,745,357  dropped_freq=68,093,523\n",
      "[extract] A_relevant_raw.txt  chunk=373  kept=94,633,279  dropped_itemcode=257,405,633  dropped_freq=68,307,041\n",
      "[extract] A_relevant_raw.txt  chunk=374  kept=94,893,942  dropped_itemcode=258,073,355  dropped_freq=68,527,672\n",
      "[extract] A_relevant_raw.txt  chunk=375  kept=95,161,235  dropped_itemcode=258,741,839  dropped_freq=68,725,222\n",
      "[extract] A_relevant_raw.txt  chunk=376  kept=95,431,191  dropped_itemcode=259,411,114  dropped_freq=68,913,470\n",
      "[extract] A_relevant_raw.txt  chunk=377  kept=95,700,616  dropped_itemcode=260,072,850  dropped_freq=69,121,538\n",
      "[extract] A_relevant_raw.txt  chunk=378  kept=95,963,915  dropped_itemcode=260,743,488  dropped_freq=69,325,438\n",
      "[extract] A_relevant_raw.txt  chunk=379  kept=96,224,493  dropped_itemcode=261,413,329  dropped_freq=69,542,322\n",
      "[extract] A_relevant_raw.txt  chunk=380  kept=96,486,520  dropped_itemcode=262,085,938  dropped_freq=69,747,487\n",
      "[extract] A_relevant_raw.txt  chunk=381  kept=96,752,379  dropped_itemcode=262,750,115  dropped_freq=69,963,076\n",
      "[extract] A_relevant_raw.txt  chunk=382  kept=97,010,694  dropped_itemcode=263,420,469  dropped_freq=70,188,914\n",
      "[extract] A_relevant_raw.txt  chunk=383  kept=97,280,102  dropped_itemcode=264,079,512  dropped_freq=70,405,952\n",
      "[extract] A_relevant_raw.txt  chunk=384  kept=97,557,820  dropped_itemcode=264,730,123  dropped_freq=70,619,072\n",
      "[extract] A_relevant_raw.txt  chunk=385  kept=97,833,758  dropped_itemcode=265,383,318  dropped_freq=70,832,316\n",
      "[extract] A_relevant_raw.txt  chunk=386  kept=98,113,469  dropped_itemcode=266,030,736  dropped_freq=71,046,277\n",
      "[extract] A_relevant_raw.txt  chunk=387  kept=98,390,548  dropped_itemcode=266,682,392  dropped_freq=71,257,065\n",
      "[extract] A_relevant_raw.txt  chunk=388  kept=98,667,388  dropped_itemcode=267,333,143  dropped_freq=71,472,070\n",
      "[extract] A_relevant_raw.txt  chunk=389  kept=98,948,190  dropped_itemcode=267,979,983  dropped_freq=71,684,998\n",
      "[extract] A_relevant_raw.txt  chunk=390  kept=99,218,268  dropped_itemcode=268,639,206  dropped_freq=71,900,500\n",
      "[extract] A_relevant_raw.txt  chunk=391  kept=99,483,914  dropped_itemcode=269,303,910  dropped_freq=72,113,268\n",
      "[extract] A_relevant_raw.txt  chunk=392  kept=99,749,822  dropped_itemcode=269,971,213  dropped_freq=72,325,600\n",
      "[extract] A_relevant_raw.txt  chunk=393  kept=100,025,236  dropped_itemcode=270,626,649  dropped_freq=72,534,617\n",
      "[extract] A_relevant_raw.txt  chunk=394  kept=100,296,060  dropped_itemcode=271,283,952  dropped_freq=72,753,029\n",
      "[extract] A_relevant_raw.txt  chunk=395  kept=100,566,246  dropped_itemcode=271,943,213  dropped_freq=72,967,555\n",
      "[extract] A_relevant_raw.txt  chunk=396  kept=100,828,366  dropped_itemcode=272,608,453  dropped_freq=73,191,712\n",
      "[extract] A_relevant_raw.txt  chunk=397  kept=101,086,492  dropped_itemcode=273,277,529  dropped_freq=73,420,453\n",
      "[extract] A_relevant_raw.txt  chunk=398  kept=101,344,090  dropped_itemcode=273,948,338  dropped_freq=73,646,200\n",
      "[extract] A_relevant_raw.txt  chunk=399  kept=101,593,193  dropped_itemcode=274,630,039  dropped_freq=73,867,340\n",
      "[extract] A_relevant_raw.txt  chunk=400  kept=101,840,423  dropped_itemcode=275,323,335  dropped_freq=74,056,705\n",
      "[extract] A_relevant_raw.txt  chunk=401  kept=102,086,917  dropped_itemcode=276,019,665  dropped_freq=74,242,584\n",
      "[extract] A_relevant_raw.txt  chunk=402  kept=102,348,723  dropped_itemcode=276,694,775  dropped_freq=74,441,919\n",
      "[extract] A_relevant_raw.txt  chunk=403  kept=102,606,571  dropped_itemcode=277,364,292  dropped_freq=74,671,503\n",
      "[extract] A_relevant_raw.txt  chunk=404  kept=102,865,908  dropped_itemcode=278,025,288  dropped_freq=74,911,934\n",
      "[extract] A_relevant_raw.txt  chunk=405  kept=103,103,670  dropped_itemcode=278,707,171  dropped_freq=75,168,522\n",
      "[extract] A_relevant_raw.txt  chunk=406  kept=103,359,918  dropped_itemcode=279,374,572  dropped_freq=75,410,866\n",
      "[extract] A_relevant_raw.txt  chunk=407  kept=103,603,443  dropped_itemcode=280,054,956  dropped_freq=75,661,407\n",
      "[extract] A_relevant_raw.txt  chunk=408  kept=103,824,819  dropped_itemcode=280,778,845  dropped_freq=75,856,736\n",
      "[extract] A_relevant_raw.txt  chunk=409  kept=104,054,217  dropped_itemcode=281,487,169  dropped_freq=76,082,077\n",
      "[extract] A_relevant_raw.txt  chunk=410  kept=104,270,922  dropped_itemcode=282,195,894  dropped_freq=76,353,289\n",
      "[extract] A_relevant_raw.txt  chunk=411  kept=104,503,174  dropped_itemcode=282,887,281  dropped_freq=76,616,942\n",
      "[extract] A_relevant_raw.txt  chunk=412  kept=104,716,664  dropped_itemcode=283,595,341  dropped_freq=76,900,456\n",
      "[extract] A_relevant_raw.txt  chunk=413  kept=104,927,183  dropped_itemcode=284,312,847  dropped_freq=77,158,808\n",
      "[extract] A_relevant_raw.txt  chunk=414  kept=105,134,862  dropped_itemcode=285,039,888  dropped_freq=77,398,104\n",
      "[extract] A_relevant_raw.txt  chunk=415  kept=105,352,348  dropped_itemcode=285,752,342  dropped_freq=77,643,590\n",
      "[extract] A_relevant_raw.txt  chunk=416  kept=105,570,473  dropped_itemcode=286,468,678  dropped_freq=77,877,638\n",
      "[extract] A_relevant_raw.txt  chunk=417  kept=105,782,515  dropped_itemcode=287,186,652  dropped_freq=78,130,750\n",
      "[extract] A_relevant_raw.txt  chunk=418  kept=105,992,037  dropped_itemcode=287,905,272  dropped_freq=78,394,098\n",
      "[extract] A_relevant_raw.txt  chunk=419  kept=106,204,894  dropped_itemcode=288,621,225  dropped_freq=78,650,542\n",
      "[extract] A_relevant_raw.txt  chunk=420  kept=106,418,951  dropped_itemcode=289,334,280  dropped_freq=78,914,930\n",
      "[extract] A_relevant_raw.txt  chunk=421  kept=106,633,082  dropped_itemcode=290,045,699  dropped_freq=79,182,851\n",
      "[extract] A_relevant_raw.txt  chunk=422  kept=106,844,828  dropped_itemcode=290,759,911  dropped_freq=79,451,216\n",
      "[extract] A_relevant_raw.txt  chunk=423  kept=107,056,397  dropped_itemcode=291,476,834  dropped_freq=79,715,313\n",
      "[extract] A_relevant_raw.txt  chunk=424  kept=107,270,107  dropped_itemcode=292,188,136  dropped_freq=79,986,194\n",
      "[extract] A_relevant_raw.txt  chunk=425  kept=107,491,961  dropped_itemcode=292,892,516  dropped_freq=80,248,731\n",
      "[extract] A_relevant_raw.txt  chunk=426  kept=107,720,908  dropped_itemcode=293,587,854  dropped_freq=80,509,374\n",
      "[extract] A_relevant_raw.txt  chunk=427  kept=107,939,580  dropped_itemcode=294,296,524  dropped_freq=80,766,826\n",
      "[extract] A_relevant_raw.txt  chunk=428  kept=108,164,712  dropped_itemcode=294,998,258  dropped_freq=81,018,664\n",
      "[extract] A_relevant_raw.txt  chunk=429  kept=108,375,868  dropped_itemcode=295,717,283  dropped_freq=81,273,622\n",
      "[extract] A_relevant_raw.txt  chunk=430  kept=108,592,841  dropped_itemcode=296,430,067  dropped_freq=81,527,733\n",
      "[extract] A_relevant_raw.txt  chunk=431  kept=108,813,306  dropped_itemcode=297,136,056  dropped_freq=81,786,595\n",
      "[extract] A_relevant_raw.txt  chunk=432  kept=109,033,380  dropped_itemcode=297,844,397  dropped_freq=82,038,285\n",
      "[extract] A_relevant_raw.txt  chunk=433  kept=109,259,309  dropped_itemcode=298,542,994  dropped_freq=82,299,963\n",
      "[extract] A_relevant_raw.txt  chunk=434  kept=109,479,561  dropped_itemcode=299,249,661  dropped_freq=82,556,810\n",
      "[extract] A_relevant_raw.txt  chunk=435  kept=109,694,604  dropped_itemcode=299,959,008  dropped_freq=82,815,471\n",
      "[extract] A_relevant_raw.txt  chunk=436  kept=109,911,099  dropped_itemcode=300,667,346  dropped_freq=83,082,272\n",
      "[extract] A_relevant_raw.txt  chunk=437  kept=110,133,781  dropped_itemcode=301,372,391  dropped_freq=83,338,176\n",
      "[extract] A_relevant_raw.txt  chunk=438  kept=110,351,669  dropped_itemcode=302,083,322  dropped_freq=83,592,897\n",
      "[extract] A_relevant_raw.txt  chunk=439  kept=110,575,292  dropped_itemcode=302,785,398  dropped_freq=83,854,194\n",
      "[extract] A_relevant_raw.txt  chunk=440  kept=110,798,945  dropped_itemcode=303,486,542  dropped_freq=84,115,257\n",
      "[extract] A_relevant_raw.txt  chunk=441  kept=111,010,989  dropped_itemcode=304,203,103  dropped_freq=84,376,230\n",
      "[extract] A_relevant_raw.txt  chunk=442  kept=111,235,649  dropped_itemcode=304,902,246  dropped_freq=84,642,666\n",
      "[extract] A_relevant_raw.txt  chunk=443  kept=111,448,327  dropped_itemcode=305,614,785  dropped_freq=84,915,303\n",
      "[extract] A_relevant_raw.txt  chunk=444  kept=111,660,881  dropped_itemcode=306,329,781  dropped_freq=85,178,260\n",
      "[extract] A_relevant_raw.txt  chunk=445  kept=111,883,797  dropped_itemcode=307,030,748  dropped_freq=85,443,353\n",
      "[extract] A_relevant_raw.txt  chunk=446  kept=112,103,759  dropped_itemcode=307,735,261  dropped_freq=85,711,313\n",
      "[extract] A_relevant_raw.txt  chunk=447  kept=112,323,932  dropped_itemcode=308,440,306  dropped_freq=85,979,052\n",
      "[extract] A_relevant_raw.txt  chunk=448  kept=112,545,615  dropped_itemcode=309,143,230  dropped_freq=86,245,326\n",
      "[extract] A_relevant_raw.txt  chunk=449  kept=112,758,231  dropped_itemcode=309,853,733  dropped_freq=86,517,182\n",
      "[extract] A_relevant_raw.txt  chunk=450  kept=112,937,548  dropped_itemcode=310,602,880  dropped_freq=86,794,771\n",
      "[extract] A_relevant_raw.txt  chunk=451  kept=113,120,186  dropped_itemcode=311,347,949  dropped_freq=87,071,960\n",
      "[extract] A_relevant_raw.txt  chunk=452  kept=113,314,449  dropped_itemcode=312,077,621  dropped_freq=87,350,604\n",
      "[extract] A_relevant_raw.txt  chunk=453  kept=113,584,439  dropped_itemcode=312,728,892  dropped_freq=87,576,155\n",
      "[extract] A_relevant_raw.txt  chunk=454  kept=113,849,488  dropped_itemcode=313,388,575  dropped_freq=87,801,055\n",
      "[extract] A_relevant_raw.txt  chunk=455  kept=114,112,165  dropped_itemcode=314,076,178  dropped_freq=87,967,366\n",
      "[extract] A_relevant_raw.txt  chunk=456  kept=114,355,088  dropped_itemcode=314,755,563  dropped_freq=88,224,287\n",
      "[extract] A_relevant_raw.txt  chunk=457  kept=114,605,224  dropped_itemcode=315,434,445  dropped_freq=88,454,934\n",
      "[extract] A_relevant_raw.txt  chunk=458  kept=114,863,736  dropped_itemcode=316,110,336  dropped_freq=88,663,467\n",
      "[extract] A_relevant_raw.txt  chunk=459  kept=115,117,515  dropped_itemcode=316,792,077  dropped_freq=88,865,130\n",
      "[extract] A_relevant_raw.txt  chunk=460  kept=115,374,239  dropped_itemcode=317,469,376  dropped_freq=89,073,452\n",
      "[extract] A_relevant_raw.txt  chunk=461  kept=115,620,948  dropped_itemcode=318,151,474  dropped_freq=89,303,645\n",
      "[extract] A_relevant_raw.txt  chunk=462  kept=115,866,665  dropped_itemcode=318,841,805  dropped_freq=89,513,053\n",
      "[extract] A_relevant_raw.txt  chunk=463  kept=116,114,373  dropped_itemcode=319,524,248  dropped_freq=89,735,805\n",
      "[extract] A_relevant_raw.txt  chunk=464  kept=116,363,373  dropped_itemcode=320,208,110  dropped_freq=89,951,495\n",
      "[extract] A_relevant_raw.txt  chunk=465  kept=116,609,375  dropped_itemcode=320,892,887  dropped_freq=90,178,786\n",
      "[extract] A_relevant_raw.txt  chunk=466  kept=116,866,059  dropped_itemcode=321,565,098  dropped_freq=90,403,199\n",
      "[extract] A_relevant_raw.txt  chunk=467  kept=117,115,887  dropped_itemcode=322,248,487  dropped_freq=90,621,870\n",
      "[extract] A_relevant_raw.txt  chunk=468  kept=117,367,106  dropped_itemcode=322,926,582  dropped_freq=90,849,617\n",
      "[extract] A_relevant_raw.txt  chunk=469  kept=117,624,256  dropped_itemcode=323,597,509  dropped_freq=91,077,167\n",
      "[extract] A_relevant_raw.txt  chunk=470  kept=117,881,320  dropped_itemcode=324,268,769  dropped_freq=91,308,212\n",
      "[extract] A_relevant_raw.txt  chunk=471  kept=118,139,785  dropped_itemcode=324,938,658  dropped_freq=91,535,378\n",
      "[extract] A_relevant_raw.txt  chunk=472  kept=118,394,293  dropped_itemcode=325,614,613  dropped_freq=91,760,101\n",
      "[extract] A_relevant_raw.txt  chunk=473  kept=118,645,703  dropped_itemcode=326,294,297  dropped_freq=91,985,227\n",
      "[extract] A_relevant_raw.txt  chunk=474  kept=118,901,471  dropped_itemcode=326,967,475  dropped_freq=92,213,350\n",
      "[extract] A_relevant_raw.txt  chunk=475  kept=119,157,770  dropped_itemcode=327,638,604  dropped_freq=92,447,012\n",
      "[extract] A_relevant_raw.txt  chunk=476  kept=119,428,492  dropped_itemcode=328,302,876  dropped_freq=92,658,088\n",
      "[extract] A_relevant_raw.txt  chunk=477  kept=119,671,868  dropped_itemcode=328,989,017  dropped_freq=92,888,521\n",
      "[extract] A_relevant_raw.txt  chunk=478  kept=119,935,326  dropped_itemcode=329,669,809  dropped_freq=93,061,547\n",
      "[extract] A_relevant_raw.txt  chunk=479  kept=120,177,263  dropped_itemcode=330,354,272  dropped_freq=93,296,809\n",
      "[extract] A_relevant_raw.txt  chunk=480  kept=120,406,921  dropped_itemcode=331,052,471  dropped_freq=93,546,746\n",
      "[extract] A_relevant_raw.txt  chunk=481  kept=120,638,324  dropped_itemcode=331,749,140  dropped_freq=93,789,263\n",
      "[extract] A_relevant_raw.txt  chunk=482  kept=120,873,433  dropped_itemcode=332,433,435  dropped_freq=94,053,718\n",
      "[extract] A_relevant_raw.txt  chunk=483  kept=121,110,421  dropped_itemcode=333,131,128  dropped_freq=94,280,425\n",
      "[extract] A_relevant_raw.txt  chunk=484  kept=121,407,599  dropped_itemcode=333,816,228  dropped_freq=94,334,598\n",
      "[extract] A_relevant_raw.txt  chunk=485  kept=121,696,665  dropped_itemcode=334,502,650  dropped_freq=94,412,810\n",
      "[extract] A_relevant_raw.txt  chunk=486  kept=121,976,864  dropped_itemcode=335,189,592  dropped_freq=94,519,492\n",
      "[extract] A_relevant_raw.txt  chunk=487  kept=122,248,789  dropped_itemcode=335,872,198  dropped_freq=94,662,310\n",
      "[extract] A_relevant_raw.txt  chunk=488  kept=122,536,366  dropped_itemcode=336,562,444  dropped_freq=94,735,785\n",
      "[extract] A_relevant_raw.txt  chunk=489  kept=122,837,609  dropped_itemcode=337,258,132  dropped_freq=94,744,929\n",
      "[extract] A_relevant_raw.txt  chunk=490  kept=123,107,173  dropped_itemcode=337,952,782  dropped_freq=94,863,214\n",
      "[extract] A_relevant_raw.txt  chunk=491  kept=123,340,872  dropped_itemcode=338,644,254  dropped_freq=95,103,991\n",
      "[extract] A_relevant_raw.txt  chunk=492  kept=123,575,821  dropped_itemcode=339,342,966  dropped_freq=95,329,905\n",
      "[extract] A_relevant_raw.txt  chunk=493  kept=123,804,922  dropped_itemcode=340,054,322  dropped_freq=95,538,322\n",
      "[extract] A_relevant_raw.txt  chunk=494  kept=124,050,522  dropped_itemcode=340,748,906  dropped_freq=95,737,995\n",
      "[extract] A_relevant_raw.txt  chunk=495  kept=124,299,028  dropped_itemcode=341,438,028  dropped_freq=95,939,922\n",
      "[extract] A_relevant_raw.txt  chunk=496  kept=124,552,193  dropped_itemcode=342,122,938  dropped_freq=96,142,669\n",
      "[extract] A_relevant_raw.txt  chunk=497  kept=124,788,593  dropped_itemcode=342,827,561  dropped_freq=96,348,493\n",
      "[extract] A_relevant_raw.txt  chunk=498  kept=125,027,049  dropped_itemcode=343,521,715  dropped_freq=96,571,175\n",
      "[extract] A_relevant_raw.txt  chunk=499  kept=125,262,163  dropped_itemcode=344,205,242  dropped_freq=96,839,344\n",
      "[extract] A_relevant_raw.txt  chunk=500  kept=125,469,588  dropped_itemcode=344,924,752  dropped_freq=97,096,731\n",
      "[extract] A_relevant_raw.txt  chunk=501  kept=125,689,621  dropped_itemcode=345,625,795  dropped_freq=97,369,808\n",
      "[extract] A_relevant_raw.txt  chunk=502  kept=125,919,509  dropped_itemcode=346,315,890  dropped_freq=97,643,914\n",
      "[extract] A_relevant_raw.txt  chunk=503  kept=126,151,405  dropped_itemcode=347,006,143  dropped_freq=97,912,891\n",
      "[extract] A_relevant_raw.txt  chunk=504  kept=126,369,643  dropped_itemcode=347,713,341  dropped_freq=98,177,903\n",
      "[extract] A_relevant_raw.txt  chunk=505  kept=126,580,344  dropped_itemcode=348,432,858  dropped_freq=98,425,122\n",
      "[extract] A_relevant_raw.txt  chunk=506  kept=126,803,584  dropped_itemcode=349,139,741  dropped_freq=98,667,289\n",
      "[extract] A_relevant_raw.txt  chunk=507  kept=127,018,005  dropped_itemcode=349,855,699  dropped_freq=98,914,832\n",
      "[extract] A_relevant_raw.txt  chunk=508  kept=127,253,443  dropped_itemcode=350,541,852  dropped_freq=99,165,379\n",
      "[extract] A_relevant_raw.txt  chunk=509  kept=127,478,801  dropped_itemcode=351,240,819  dropped_freq=99,427,749\n",
      "[extract] A_relevant_raw.txt  chunk=510  kept=127,699,231  dropped_itemcode=351,952,350  dropped_freq=99,670,356\n",
      "[extract] A_relevant_raw.txt  chunk=511  kept=127,912,979  dropped_itemcode=352,676,591  dropped_freq=99,905,870\n",
      "[extract] A_relevant_raw.txt  chunk=512  kept=128,110,572  dropped_itemcode=353,415,895  dropped_freq=100,150,311\n",
      "[extract] A_relevant_raw.txt  chunk=513  kept=128,333,865  dropped_itemcode=354,121,770  dropped_freq=100,391,110\n",
      "[extract] A_relevant_raw.txt  chunk=514  kept=128,562,378  dropped_itemcode=354,821,926  dropped_freq=100,628,994\n",
      "[extract] A_relevant_raw.txt  chunk=515  kept=128,800,221  dropped_itemcode=355,506,264  dropped_freq=100,882,162\n",
      "[extract] A_relevant_raw.txt  chunk=516  kept=129,033,932  dropped_itemcode=356,197,297  dropped_freq=101,129,163\n",
      "[extract] A_relevant_raw.txt  chunk=517  kept=129,260,760  dropped_itemcode=356,899,706  dropped_freq=101,369,806\n",
      "[extract] A_relevant_raw.txt  chunk=518  kept=129,490,222  dropped_itemcode=357,595,655  dropped_freq=101,616,440\n",
      "[extract] A_relevant_raw.txt  chunk=519  kept=129,730,262  dropped_itemcode=358,278,111  dropped_freq=101,870,074\n",
      "[extract] A_relevant_raw.txt  chunk=520  kept=129,977,371  dropped_itemcode=358,952,138  dropped_freq=102,122,896\n",
      "[extract] A_relevant_raw.txt  chunk=521  kept=130,225,610  dropped_itemcode=359,623,883  dropped_freq=102,374,304\n",
      "[extract] A_relevant_raw.txt  chunk=522  kept=130,471,823  dropped_itemcode=360,294,894  dropped_freq=102,636,438\n",
      "[extract] A_relevant_raw.txt  chunk=523  kept=130,721,906  dropped_itemcode=360,965,739  dropped_freq=102,885,504\n",
      "[extract] A_relevant_raw.txt  chunk=524  kept=130,968,171  dropped_itemcode=361,644,681  dropped_freq=103,120,059\n",
      "[extract] A_relevant_raw.txt  chunk=525  kept=131,218,765  dropped_itemcode=362,317,209  dropped_freq=103,360,151\n",
      "[extract] A_relevant_raw.txt  chunk=526  kept=131,465,931  dropped_itemcode=362,991,386  dropped_freq=103,610,482\n",
      "[extract] A_relevant_raw.txt  chunk=527  kept=131,715,968  dropped_itemcode=363,662,825  dropped_freq=103,858,594\n",
      "[extract] A_relevant_raw.txt  chunk=528  kept=131,960,319  dropped_itemcode=364,343,505  dropped_freq=104,100,576\n",
      "[extract] A_relevant_raw.txt  chunk=529  kept=132,229,914  dropped_itemcode=365,026,452  dropped_freq=104,249,189\n",
      "[extract] A_relevant_raw.txt  chunk=530  kept=132,438,985  dropped_itemcode=365,749,113  dropped_freq=104,495,092\n",
      "[extract] A_relevant_raw.txt  chunk=531  kept=132,675,545  dropped_itemcode=366,454,861  dropped_freq=104,701,327\n",
      "[extract] A_relevant_raw.txt  chunk=532  kept=132,946,243  dropped_itemcode=367,127,295  dropped_freq=104,889,792\n",
      "[extract] A_relevant_raw.txt  chunk=533  kept=133,246,323  dropped_itemcode=367,787,140  dropped_freq=105,022,236\n",
      "[extract] A_relevant_raw.txt  chunk=534  kept=133,576,393  dropped_itemcode=368,413,525  dropped_freq=105,163,303\n",
      "[extract] A_relevant_raw.txt  chunk=535  kept=133,896,841  dropped_itemcode=369,046,091  dropped_freq=105,315,516\n",
      "[extract] A_relevant_raw.txt  chunk=536  kept=134,211,309  dropped_itemcode=369,689,504  dropped_freq=105,453,363\n",
      "[extract] A_relevant_raw.txt  chunk=537  kept=134,525,817  dropped_itemcode=370,332,560  dropped_freq=105,590,271\n",
      "[extract] A_relevant_raw.txt  chunk=538  kept=134,827,484  dropped_itemcode=370,987,684  dropped_freq=105,731,754\n",
      "[extract] A_relevant_raw.txt  chunk=539  kept=135,104,547  dropped_itemcode=371,671,862  dropped_freq=105,861,307\n",
      "[extract] A_relevant_raw.txt  chunk=540  kept=135,390,305  dropped_itemcode=372,350,980  dropped_freq=105,977,142\n",
      "[extract] A_relevant_raw.txt  chunk=541  kept=135,645,088  dropped_itemcode=373,047,825  dropped_freq=106,146,142\n",
      "[extract] A_relevant_raw.txt  chunk=542  kept=135,855,927  dropped_itemcode=373,758,896  dropped_freq=106,417,530\n",
      "[extract] A_relevant_raw.txt  chunk=543  kept=136,079,874  dropped_itemcode=374,471,035  dropped_freq=106,640,611\n",
      "[extract] A_relevant_raw.txt  chunk=544  kept=136,289,102  dropped_itemcode=375,182,457  dropped_freq=106,918,650\n",
      "[extract] A_relevant_raw.txt  chunk=545  kept=136,494,549  dropped_itemcode=375,898,840  dropped_freq=107,196,691\n",
      "[extract] A_relevant_raw.txt  chunk=546  kept=136,776,671  dropped_itemcode=376,578,659  dropped_freq=107,326,254\n",
      "[extract] A_relevant_raw.txt  chunk=547  kept=137,042,094  dropped_itemcode=377,259,935  dropped_freq=107,504,354\n",
      "[extract] A_relevant_raw.txt  chunk=548  kept=137,316,402  dropped_itemcode=377,939,291  dropped_freq=107,658,154\n",
      "[extract] A_relevant_raw.txt  chunk=549  kept=137,585,179  dropped_itemcode=378,627,592  dropped_freq=107,802,141\n",
      "[extract] A_relevant_raw.txt  chunk=550  kept=137,855,141  dropped_itemcode=379,313,281  dropped_freq=107,950,581\n",
      "[extract] A_relevant_raw.txt  chunk=551  kept=138,122,376  dropped_itemcode=379,999,429  dropped_freq=108,106,389\n",
      "[extract] A_relevant_raw.txt  chunk=552  kept=138,391,559  dropped_itemcode=380,687,124  dropped_freq=108,253,478\n",
      "[extract] A_relevant_raw.txt  chunk=553  kept=138,657,473  dropped_itemcode=381,371,234  dropped_freq=108,420,425\n",
      "[extract] A_relevant_raw.txt  chunk=554  kept=138,922,401  dropped_itemcode=382,060,333  dropped_freq=108,573,725\n",
      "[extract] A_relevant_raw.txt  chunk=555  kept=139,193,551  dropped_itemcode=382,742,331  dropped_freq=108,729,587\n",
      "[extract] A_relevant_raw.txt  chunk=556  kept=139,460,773  dropped_itemcode=383,423,142  dropped_freq=108,902,961\n",
      "[extract] A_relevant_raw.txt  chunk=557  kept=139,731,877  dropped_itemcode=384,102,292  dropped_freq=109,065,164\n",
      "[extract] A_relevant_raw.txt  chunk=558  kept=139,983,851  dropped_itemcode=384,798,740  dropped_freq=109,241,564\n",
      "[extract] A_relevant_raw.txt  chunk=559  kept=140,216,801  dropped_itemcode=385,489,497  dropped_freq=109,492,586\n",
      "[extract] A_relevant_raw.txt  chunk=560  kept=140,457,396  dropped_itemcode=386,172,283  dropped_freq=109,738,765\n",
      "[extract] A_relevant_raw.txt  chunk=561  kept=140,701,588  dropped_itemcode=386,849,782  dropped_freq=109,985,077\n",
      "[extract] A_relevant_raw.txt  chunk=562  kept=140,940,048  dropped_itemcode=387,535,921  dropped_freq=110,229,577\n",
      "[extract] A_relevant_raw.txt  chunk=563  kept=141,183,036  dropped_itemcode=388,215,395  dropped_freq=110,475,241\n",
      "[extract] A_relevant_raw.txt  chunk=564  kept=141,416,317  dropped_itemcode=388,904,055  dropped_freq=110,729,041\n",
      "[extract] A_relevant_raw.txt  chunk=565  kept=141,647,555  dropped_itemcode=389,595,955  dropped_freq=110,981,669\n",
      "[extract] A_relevant_raw.txt  chunk=566  kept=141,887,989  dropped_itemcode=390,277,496  dropped_freq=111,229,699\n",
      "[extract] A_relevant_raw.txt  chunk=567  kept=142,125,091  dropped_itemcode=390,963,875  dropped_freq=111,478,421\n",
      "[extract] A_relevant_raw.txt  chunk=568  kept=142,353,015  dropped_itemcode=391,660,004  dropped_freq=111,730,624\n",
      "[extract] A_relevant_raw.txt  chunk=569  kept=142,582,036  dropped_itemcode=392,354,399  dropped_freq=111,985,205\n",
      "[extract] A_relevant_raw.txt  chunk=570  kept=142,813,520  dropped_itemcode=393,041,733  dropped_freq=112,246,718\n",
      "[extract] A_relevant_raw.txt  chunk=571  kept=143,051,267  dropped_itemcode=393,721,569  dropped_freq=112,505,662\n",
      "[extract] A_relevant_raw.txt  chunk=572  kept=143,279,426  dropped_itemcode=394,412,146  dropped_freq=112,769,556\n",
      "[extract] A_relevant_raw.txt  chunk=573  kept=143,504,117  dropped_itemcode=395,109,539  dropped_freq=113,027,489\n",
      "[extract] A_relevant_raw.txt  chunk=574  kept=143,744,908  dropped_itemcode=395,798,206  dropped_freq=113,254,904\n",
      "[extract] A_relevant_raw.txt  chunk=575  kept=144,024,470  dropped_itemcode=396,498,049  dropped_freq=113,323,248\n",
      "[extract] A_relevant_raw.txt  chunk=576  kept=144,330,288  dropped_itemcode=397,190,629  dropped_freq=113,327,912\n",
      "[extract] A_relevant_raw.txt  chunk=577  kept=144,641,688  dropped_itemcode=397,876,110  dropped_freq=113,337,189\n",
      "[extract] A_relevant_raw.txt  chunk=578  kept=144,960,959  dropped_itemcode=398,550,762  dropped_freq=113,355,588\n",
      "[extract] A_relevant_raw.txt  chunk=579  kept=145,258,564  dropped_itemcode=399,249,796  dropped_freq=113,365,345\n",
      "[extract] A_relevant_raw.txt  chunk=580  kept=145,507,218  dropped_itemcode=399,970,552  dropped_freq=113,482,315\n",
      "[extract] A_relevant_raw.txt  chunk=581  kept=145,763,860  dropped_itemcode=400,677,289  dropped_freq=113,615,216\n",
      "[extract] A_relevant_raw.txt  chunk=582  kept=145,966,760  dropped_itemcode=401,439,926  dropped_freq=113,781,328\n",
      "[extract] A_relevant_raw.txt  chunk=583  kept=146,189,629  dropped_itemcode=402,181,860  dropped_freq=113,934,213\n",
      "[extract] A_relevant_raw.txt  chunk=584  kept=146,435,246  dropped_itemcode=402,895,724  dropped_freq=114,081,021\n",
      "[extract] A_relevant_raw.txt  chunk=585  kept=146,657,578  dropped_itemcode=403,603,005  dropped_freq=114,332,202\n",
      "[extract] A_relevant_raw.txt  chunk=586  kept=146,912,251  dropped_itemcode=404,271,407  dropped_freq=114,579,679\n",
      "[extract] A_relevant_raw.txt  chunk=587  kept=147,159,556  dropped_itemcode=404,946,116  dropped_freq=114,833,539\n",
      "[extract] A_relevant_raw.txt  chunk=588  kept=147,392,804  dropped_itemcode=405,637,663  dropped_freq=115,090,143\n",
      "[extract] A_relevant_raw.txt  chunk=589  kept=147,624,304  dropped_itemcode=406,332,813  dropped_freq=115,342,204\n",
      "[extract] A_relevant_raw.txt  chunk=590  kept=147,859,583  dropped_itemcode=407,031,926  dropped_freq=115,563,607\n",
      "[extract] A_relevant_raw.txt  chunk=591  kept=148,097,682  dropped_itemcode=407,723,810  dropped_freq=115,795,509\n",
      "[extract] A_relevant_raw.txt  chunk=592  kept=148,336,206  dropped_itemcode=408,417,321  dropped_freq=116,025,059\n",
      "[extract] A_relevant_raw.txt  chunk=593  kept=148,571,008  dropped_itemcode=409,125,008  dropped_freq=116,229,236\n",
      "[extract] A_relevant_raw.txt  chunk=594  kept=148,798,967  dropped_itemcode=409,843,942  dropped_freq=116,432,696\n",
      "[extract] A_relevant_raw.txt  chunk=595  kept=149,041,421  dropped_itemcode=410,542,693  dropped_freq=116,633,359\n",
      "[extract] A_relevant_raw.txt  chunk=596  kept=149,265,580  dropped_itemcode=411,264,162  dropped_freq=116,837,871\n",
      "[extract] A_relevant_raw.txt  chunk=597  kept=149,497,472  dropped_itemcode=411,966,660  dropped_freq=117,064,185\n",
      "[extract] A_relevant_raw.txt  chunk=598  kept=149,722,972  dropped_itemcode=412,683,034  dropped_freq=117,282,074\n",
      "[extract] A_relevant_raw.txt  chunk=599  kept=149,958,087  dropped_itemcode=413,383,455  dropped_freq=117,505,989\n",
      "[extract] A_relevant_raw.txt  chunk=600  kept=150,187,422  dropped_itemcode=414,094,687  dropped_freq=117,724,404\n",
      "[extract] A_relevant_raw.txt  chunk=601  kept=150,423,097  dropped_itemcode=414,802,947  dropped_freq=117,927,826\n",
      "[extract] A_relevant_raw.txt  chunk=602  kept=150,665,765  dropped_itemcode=415,496,717  dropped_freq=118,148,200\n",
      "[extract] A_relevant_raw.txt  chunk=603  kept=150,900,030  dropped_itemcode=416,198,547  dropped_freq=118,373,635\n",
      "[extract] A_relevant_raw.txt  chunk=604  kept=151,136,019  dropped_itemcode=416,901,165  dropped_freq=118,593,972\n",
      "[extract] A_relevant_raw.txt  chunk=605  kept=151,362,062  dropped_itemcode=417,613,745  dropped_freq=118,822,834\n",
      "[extract] A_relevant_raw.txt  chunk=606  kept=151,613,581  dropped_itemcode=418,330,831  dropped_freq=118,932,848\n",
      "[extract] A_relevant_raw.txt  chunk=607  kept=151,871,444  dropped_itemcode=419,048,655  dropped_freq=119,016,729\n",
      "[extract] A_relevant_raw.txt  chunk=608  kept=152,144,650  dropped_itemcode=419,755,486  dropped_freq=119,080,540\n",
      "[extract] A_relevant_raw.txt  chunk=609  kept=152,413,666  dropped_itemcode=420,473,259  dropped_freq=119,123,791\n",
      "[extract] A_relevant_raw.txt  chunk=610  kept=152,656,185  dropped_itemcode=421,203,080  dropped_freq=119,224,021\n",
      "[extract] A_relevant_raw.txt  chunk=611  kept=152,882,088  dropped_itemcode=421,915,489  dropped_freq=119,455,314\n",
      "[extract] A_relevant_raw.txt  chunk=612  kept=153,105,878  dropped_itemcode=422,629,478  dropped_freq=119,689,428\n",
      "[extract] A_relevant_raw.txt  chunk=613  kept=153,323,238  dropped_itemcode=423,353,965  dropped_freq=119,916,141\n",
      "[extract] A_relevant_raw.txt  chunk=614  kept=153,551,164  dropped_itemcode=424,067,649  dropped_freq=120,136,328\n",
      "[extract] A_relevant_raw.txt  chunk=615  kept=153,779,714  dropped_itemcode=424,785,854  dropped_freq=120,338,712\n",
      "[extract] A_relevant_raw.txt  chunk=616  kept=154,016,194  dropped_itemcode=425,502,156  dropped_freq=120,515,973\n",
      "[extract] A_relevant_raw.txt  chunk=617  kept=154,256,550  dropped_itemcode=426,210,739  dropped_freq=120,702,135\n",
      "[extract] A_relevant_raw.txt  chunk=618  kept=154,501,766  dropped_itemcode=426,921,172  dropped_freq=120,867,002\n",
      "[extract] A_relevant_raw.txt  chunk=619  kept=154,744,178  dropped_itemcode=427,629,515  dropped_freq=121,047,174\n",
      "[extract] A_relevant_raw.txt  chunk=620  kept=154,954,509  dropped_itemcode=428,354,629  dropped_freq=121,290,577\n",
      "[extract] A_relevant_raw.txt  chunk=621  kept=155,158,282  dropped_itemcode=429,086,968  dropped_freq=121,531,314\n",
      "[extract] A_relevant_raw.txt  chunk=622  kept=155,379,832  dropped_itemcode=429,798,424  dropped_freq=121,778,379\n",
      "[extract] A_relevant_raw.txt  chunk=623  kept=155,588,738  dropped_itemcode=430,524,481  dropped_freq=122,024,386\n",
      "[extract] A_relevant_raw.txt  chunk=624  kept=155,792,399  dropped_itemcode=431,254,625  dropped_freq=122,272,567\n",
      "[extract] A_relevant_raw.txt  chunk=625  kept=156,002,487  dropped_itemcode=431,978,275  dropped_freq=122,521,117\n",
      "[extract] A_relevant_raw.txt  chunk=626  kept=156,204,997  dropped_itemcode=432,710,012  dropped_freq=122,767,706\n",
      "[extract] A_relevant_raw.txt  chunk=627  kept=156,407,734  dropped_itemcode=433,442,537  dropped_freq=123,012,118\n",
      "[extract] A_relevant_raw.txt  chunk=628  kept=156,626,787  dropped_itemcode=434,155,838  dropped_freq=123,259,518\n",
      "[extract] A_relevant_raw.txt  chunk=629  kept=156,824,448  dropped_itemcode=434,897,251  dropped_freq=123,500,297\n",
      "[extract] A_relevant_raw.txt  chunk=630  kept=157,038,454  dropped_itemcode=435,615,935  dropped_freq=123,751,259\n",
      "[extract] A_relevant_raw.txt  chunk=631  kept=157,241,757  dropped_itemcode=436,344,780  dropped_freq=124,004,296\n",
      "[extract] A_relevant_raw.txt  chunk=632  kept=157,445,033  dropped_itemcode=437,076,931  dropped_freq=124,247,866\n",
      "[extract] A_relevant_raw.txt  chunk=633  kept=157,630,448  dropped_itemcode=437,826,104  dropped_freq=124,505,934\n",
      "[extract] A_relevant_raw.txt  chunk=634  kept=157,829,781  dropped_itemcode=438,560,351  dropped_freq=124,764,542\n",
      "[extract] A_relevant_raw.txt  chunk=635  kept=158,048,310  dropped_itemcode=439,273,563  dropped_freq=125,015,008\n",
      "[extract] A_relevant_raw.txt  chunk=636  kept=158,266,491  dropped_itemcode=439,989,311  dropped_freq=125,260,944\n",
      "[extract] A_relevant_raw.txt  chunk=637  kept=158,482,745  dropped_itemcode=440,705,125  dropped_freq=125,508,566\n",
      "[extract] A_relevant_raw.txt  chunk=638  kept=158,700,178  dropped_itemcode=441,422,260  dropped_freq=125,752,603\n",
      "[extract] A_relevant_raw.txt  chunk=639  kept=158,903,725  dropped_itemcode=442,154,769  dropped_freq=125,997,452\n",
      "[extract] A_relevant_raw.txt  chunk=640  kept=159,121,797  dropped_itemcode=442,870,377  dropped_freq=126,242,306\n",
      "[extract] A_relevant_raw.txt  chunk=641  kept=159,335,409  dropped_itemcode=443,591,751  dropped_freq=126,487,778\n",
      "[extract] A_relevant_raw.txt  chunk=642  kept=159,554,116  dropped_itemcode=444,306,735  dropped_freq=126,734,410\n",
      "[extract] A_relevant_raw.txt  chunk=643  kept=159,770,335  dropped_itemcode=445,023,743  dropped_freq=126,983,553\n",
      "[extract] A_relevant_raw.txt  chunk=644  kept=159,981,351  dropped_itemcode=445,747,730  dropped_freq=127,233,857\n",
      "[extract] A_relevant_raw.txt  chunk=645  kept=160,193,362  dropped_itemcode=446,470,892  dropped_freq=127,481,539\n",
      "[extract] A_relevant_raw.txt  chunk=646  kept=160,406,396  dropped_itemcode=447,194,032  dropped_freq=127,726,209\n",
      "[extract] A_relevant_raw.txt  chunk=647  kept=160,624,758  dropped_itemcode=447,910,341  dropped_freq=127,971,133\n",
      "[extract] A_relevant_raw.txt  chunk=648  kept=160,836,052  dropped_itemcode=448,632,651  dropped_freq=128,224,210\n",
      "[extract] A_relevant_raw.txt  chunk=649  kept=161,058,744  dropped_itemcode=449,338,856  dropped_freq=128,480,116\n",
      "[extract] A_relevant_raw.txt  chunk=650  kept=161,288,983  dropped_itemcode=450,036,482  dropped_freq=128,731,788\n",
      "[extract] A_relevant_raw.txt  chunk=651  kept=161,509,219  dropped_itemcode=450,752,297  dropped_freq=128,971,215\n",
      "[extract] A_relevant_raw.txt  chunk=652  kept=161,740,751  dropped_itemcode=451,448,886  dropped_freq=129,222,208\n",
      "[extract] A_relevant_raw.txt  chunk=653  kept=161,974,058  dropped_itemcode=452,143,648  dropped_freq=129,475,480\n",
      "[extract] A_relevant_raw.txt  chunk=654  kept=162,196,145  dropped_itemcode=452,852,060  dropped_freq=129,725,002\n",
      "[extract] A_relevant_raw.txt  chunk=655  kept=162,419,428  dropped_itemcode=453,560,982  dropped_freq=129,970,692\n",
      "[extract] A_relevant_raw.txt  chunk=656  kept=162,630,437  dropped_itemcode=454,284,690  dropped_freq=130,218,793\n",
      "[extract] A_relevant_raw.txt  chunk=657  kept=162,854,456  dropped_itemcode=454,980,091  dropped_freq=130,497,445\n",
      "[extract] A_relevant_raw.txt  chunk=658  kept=163,079,013  dropped_itemcode=455,673,525  dropped_freq=130,779,424\n",
      "[extract] A_relevant_raw.txt  chunk=659  kept=163,297,619  dropped_itemcode=456,374,542  dropped_freq=131,064,058\n",
      "[extract] A_relevant_raw.txt  chunk=660  kept=163,520,983  dropped_itemcode=457,072,776  dropped_freq=131,342,458\n",
      "[extract] A_relevant_raw.txt  chunk=661  kept=163,738,507  dropped_itemcode=457,777,170  dropped_freq=131,623,585\n",
      "[extract] A_relevant_raw.txt  chunk=662  kept=163,970,580  dropped_itemcode=458,467,405  dropped_freq=131,894,142\n",
      "[extract] A_relevant_raw.txt  chunk=663  kept=164,186,843  dropped_itemcode=459,178,614  dropped_freq=132,164,471\n",
      "[extract] A_relevant_raw.txt  chunk=664  kept=164,414,823  dropped_itemcode=459,875,251  dropped_freq=132,433,844\n",
      "[extract] A_relevant_raw.txt  chunk=665  kept=164,639,545  dropped_itemcode=460,573,751  dropped_freq=132,707,260\n",
      "[extract] A_relevant_raw.txt  chunk=666  kept=164,862,033  dropped_itemcode=461,277,293  dropped_freq=132,970,193\n",
      "[extract] A_relevant_raw.txt  chunk=667  kept=165,063,864  dropped_itemcode=461,997,416  dropped_freq=133,239,352\n",
      "[extract] A_relevant_raw.txt  chunk=668  kept=165,271,596  dropped_itemcode=462,714,475  dropped_freq=133,500,229\n",
      "[extract] A_relevant_raw.txt  chunk=669  kept=165,490,468  dropped_itemcode=463,444,136  dropped_freq=133,689,409\n",
      "[extract] A_relevant_raw.txt  chunk=670  kept=165,710,651  dropped_itemcode=464,169,372  dropped_freq=133,889,959\n",
      "[extract] A_relevant_raw.txt  chunk=671  kept=165,916,813  dropped_itemcode=464,901,143  dropped_freq=134,116,779\n",
      "[extract] A_relevant_raw.txt  chunk=672  kept=166,121,237  dropped_itemcode=465,629,382  dropped_freq=134,365,059\n",
      "[extract] A_relevant_raw.txt  chunk=673  kept=166,327,343  dropped_itemcode=466,356,360  dropped_freq=134,614,016\n",
      "[extract] A_relevant_raw.txt  chunk=674  kept=166,533,802  dropped_itemcode=467,079,401  dropped_freq=134,872,690\n",
      "[extract] A_relevant_raw.txt  chunk=675  kept=166,739,287  dropped_itemcode=467,800,443  dropped_freq=135,141,374\n",
      "[extract] A_relevant_raw.txt  chunk=676  kept=166,954,768  dropped_itemcode=468,509,045  dropped_freq=135,404,463\n",
      "[extract] A_relevant_raw.txt  chunk=677  kept=167,173,667  dropped_itemcode=469,209,253  dropped_freq=135,683,070\n",
      "[extract] A_relevant_raw.txt  chunk=678  kept=167,397,432  dropped_itemcode=469,904,627  dropped_freq=135,958,033\n",
      "[extract] A_relevant_raw.txt  chunk=679  kept=167,612,680  dropped_itemcode=470,611,923  dropped_freq=136,224,058\n",
      "[extract] A_relevant_raw.txt  chunk=680  kept=167,851,401  dropped_itemcode=471,316,760  dropped_freq=136,423,276\n",
      "[extract] A_relevant_raw.txt  chunk=681  kept=168,119,042  dropped_itemcode=472,002,099  dropped_freq=136,571,873\n",
      "[extract] A_relevant_raw.txt  chunk=682  kept=168,405,439  dropped_itemcode=472,715,209  dropped_freq=136,573,186\n",
      "[extract] A_relevant_raw.txt  chunk=683  kept=168,690,644  dropped_itemcode=473,427,889  dropped_freq=136,579,830\n",
      "[extract] A_relevant_raw.txt  chunk=684  kept=168,980,416  dropped_itemcode=474,131,518  dropped_freq=136,601,499\n",
      "[extract] A_relevant_raw.txt  chunk=685  kept=169,272,300  dropped_itemcode=474,832,431  dropped_freq=136,625,005\n",
      "[extract] A_relevant_raw.txt  chunk=686  kept=169,563,442  dropped_itemcode=475,536,611  dropped_freq=136,638,986\n",
      "[extract] A_relevant_raw.txt  chunk=687  kept=169,848,638  dropped_itemcode=476,238,238  dropped_freq=136,680,766\n",
      "[extract] A_relevant_raw.txt  chunk=688  kept=170,138,063  dropped_itemcode=476,944,046  dropped_freq=136,697,027\n",
      "[extract] A_relevant_raw.txt  chunk=689  kept=170,416,665  dropped_itemcode=477,661,289  dropped_freq=136,710,037\n",
      "[extract] A_relevant_raw.txt  chunk=690  kept=170,722,642  dropped_itemcode=478,348,612  dropped_freq=136,732,425\n",
      "[extract] A_relevant_raw.txt  chunk=691  kept=171,017,884  dropped_itemcode=479,050,136  dropped_freq=136,743,630\n",
      "[extract] A_relevant_raw.txt  chunk=692  kept=171,306,946  dropped_itemcode=479,755,981  dropped_freq=136,758,292\n",
      "[extract] A_relevant_raw.txt  chunk=693  kept=171,607,042  dropped_itemcode=480,452,528  dropped_freq=136,767,262\n",
      "[extract] A_relevant_raw.txt  chunk=694  kept=171,896,262  dropped_itemcode=481,153,164  dropped_freq=136,799,996\n",
      "[extract] A_relevant_raw.txt  chunk=695  kept=172,191,052  dropped_itemcode=481,855,074  dropped_freq=136,809,831\n",
      "[extract] A_relevant_raw.txt  chunk=696  kept=172,489,754  dropped_itemcode=482,549,284  dropped_freq=136,832,147\n",
      "[extract] A_relevant_raw.txt  chunk=697  kept=172,786,715  dropped_itemcode=483,248,565  dropped_freq=136,843,795\n",
      "[extract] A_relevant_raw.txt  chunk=698  kept=173,081,649  dropped_itemcode=483,951,487  dropped_freq=136,849,954\n",
      "[extract] A_relevant_raw.txt  chunk=699  kept=173,369,764  dropped_itemcode=484,655,239  dropped_freq=136,877,011\n",
      "[extract] A_relevant_raw.txt  chunk=700  kept=173,653,199  dropped_itemcode=485,364,366  dropped_freq=136,901,574\n",
      "[extract] A_relevant_raw.txt  chunk=701  kept=173,939,099  dropped_itemcode=486,073,286  dropped_freq=136,918,466\n",
      "[extract] A_relevant_raw.txt  chunk=702  kept=174,233,342  dropped_itemcode=486,774,092  dropped_freq=136,933,716\n",
      "[extract] A_relevant_raw.txt  chunk=703  kept=174,531,076  dropped_itemcode=487,470,700  dropped_freq=136,950,308\n",
      "[extract] A_relevant_raw.txt  chunk=704  kept=174,820,750  dropped_itemcode=488,170,918  dropped_freq=136,983,912\n",
      "[extract] A_relevant_raw.txt  chunk=705  kept=175,114,836  dropped_itemcode=488,870,979  dropped_freq=137,002,991\n",
      "[extract] A_relevant_raw.txt  chunk=706  kept=175,406,615  dropped_itemcode=489,576,335  dropped_freq=137,011,501\n",
      "[extract] A_relevant_raw.txt  chunk=707  kept=175,697,627  dropped_itemcode=490,284,794  dropped_freq=137,012,847\n",
      "[extract] A_relevant_raw.txt  chunk=708  kept=175,988,130  dropped_itemcode=490,993,771  dropped_freq=137,014,482\n",
      "[extract] A_relevant_raw.txt  chunk=709  kept=176,277,265  dropped_itemcode=491,704,497  dropped_freq=137,014,866\n",
      "[extract] A_relevant_raw.txt  chunk=710  kept=176,568,230  dropped_itemcode=492,411,674  dropped_freq=137,020,172\n",
      "[extract] A_relevant_raw.txt  chunk=711  kept=176,872,909  dropped_itemcode=493,100,545  dropped_freq=137,040,736\n",
      "[extract] A_relevant_raw.txt  chunk=712  kept=177,099,591  dropped_itemcode=493,805,935  dropped_freq=137,273,712\n",
      "[extract] A_relevant_raw.txt  chunk=713  kept=177,314,798  dropped_itemcode=494,516,369  dropped_freq=137,536,296\n",
      "[extract] A_relevant_raw.txt  chunk=714  kept=177,530,816  dropped_itemcode=495,225,304  dropped_freq=137,792,864\n",
      "[extract] A_relevant_raw.txt  chunk=715  kept=177,739,869  dropped_itemcode=495,943,270  dropped_freq=138,056,761\n",
      "[extract] A_relevant_raw.txt  chunk=716  kept=177,954,707  dropped_itemcode=496,655,653  dropped_freq=138,312,920\n",
      "[extract] A_relevant_raw.txt  chunk=717  kept=178,175,731  dropped_itemcode=497,358,950  dropped_freq=138,575,742\n",
      "[extract] A_relevant_raw.txt  chunk=718  kept=178,392,712  dropped_itemcode=498,064,560  dropped_freq=138,844,228\n",
      "[extract] A_relevant_raw.txt  chunk=719  kept=178,606,025  dropped_itemcode=498,779,300  dropped_freq=139,102,386\n",
      "[extract] A_relevant_raw.txt  chunk=720  kept=178,813,277  dropped_itemcode=499,498,304  dropped_freq=139,366,142\n",
      "[extract] A_relevant_raw.txt  chunk=721  kept=179,034,836  dropped_itemcode=500,201,169  dropped_freq=139,625,599\n",
      "[extract] A_relevant_raw.txt  chunk=722  kept=179,252,977  dropped_itemcode=500,905,546  dropped_freq=139,897,333\n",
      "[extract] A_relevant_raw.txt  chunk=723  kept=179,470,586  dropped_itemcode=501,620,037  dropped_freq=140,139,535\n",
      "[extract] A_relevant_raw.txt  chunk=724  kept=179,691,647  dropped_itemcode=502,322,467  dropped_freq=140,402,521\n",
      "[extract] A_relevant_raw.txt  chunk=725  kept=179,911,228  dropped_itemcode=503,029,021  dropped_freq=140,663,257\n",
      "[extract] A_relevant_raw.txt  chunk=726  kept=180,129,624  dropped_itemcode=503,734,906  dropped_freq=140,921,895\n",
      "[extract] A_relevant_raw.txt  chunk=727  kept=180,350,770  dropped_itemcode=504,459,228  dropped_freq=141,125,347\n",
      "[extract] A_relevant_raw.txt  chunk=728  kept=180,577,292  dropped_itemcode=505,159,810  dropped_freq=141,366,582\n",
      "[extract] A_relevant_raw.txt  chunk=729  kept=180,797,383  dropped_itemcode=505,858,442  dropped_freq=141,638,902\n",
      "[extract] A_relevant_raw.txt  chunk=730  kept=181,021,545  dropped_itemcode=506,558,503  dropped_freq=141,896,602\n",
      "[extract] A_relevant_raw.txt  chunk=731  kept=181,252,810  dropped_itemcode=507,253,318  dropped_freq=142,148,467\n",
      "[extract] A_relevant_raw.txt  chunk=732  kept=181,490,291  dropped_itemcode=507,928,122  dropped_freq=142,417,160\n",
      "[extract] A_relevant_raw.txt  chunk=733  kept=181,736,318  dropped_itemcode=508,589,244  dropped_freq=142,697,233\n",
      "[extract] A_relevant_raw.txt  chunk=734  kept=181,966,191  dropped_itemcode=509,271,091  dropped_freq=142,974,486\n",
      "[extract] A_relevant_raw.txt  chunk=735  kept=182,183,833  dropped_itemcode=509,972,955  dropped_freq=143,251,202\n",
      "[extract] A_relevant_raw.txt  chunk=736  kept=182,405,751  dropped_itemcode=510,672,799  dropped_freq=143,516,653\n",
      "[extract] A_relevant_raw.txt  chunk=737  kept=182,621,676  dropped_itemcode=511,384,287  dropped_freq=143,761,749\n",
      "[extract] A_relevant_raw.txt  chunk=738  kept=182,846,539  dropped_itemcode=512,096,409  dropped_freq=143,981,790\n",
      "[extract] A_relevant_raw.txt  chunk=739  kept=183,078,250  dropped_itemcode=512,787,334  dropped_freq=144,236,921\n",
      "[extract] A_relevant_raw.txt  chunk=740  kept=183,335,551  dropped_itemcode=513,442,688  dropped_freq=144,495,743\n",
      "[extract] A_relevant_raw.txt  chunk=741  kept=183,571,358  dropped_itemcode=514,124,922  dropped_freq=144,756,860\n",
      "[extract] A_relevant_raw.txt  chunk=742  kept=183,792,603  dropped_itemcode=514,831,273  dropped_freq=145,004,017\n",
      "[extract] A_relevant_raw.txt  chunk=743  kept=184,026,878  dropped_itemcode=515,513,789  dropped_freq=145,267,338\n",
      "[extract] A_relevant_raw.txt  chunk=744  kept=184,272,366  dropped_itemcode=516,187,816  dropped_freq=145,516,325\n",
      "[extract] A_relevant_raw.txt  chunk=745  kept=184,494,557  dropped_itemcode=516,888,884  dropped_freq=145,778,297\n",
      "[extract] A_relevant_raw.txt  chunk=746  kept=184,712,076  dropped_itemcode=517,599,111  dropped_freq=146,029,301\n",
      "[extract] A_relevant_raw.txt  chunk=747  kept=184,929,510  dropped_itemcode=518,304,564  dropped_freq=146,294,639\n",
      "[extract] A_relevant_raw.txt  chunk=748  kept=185,159,565  dropped_itemcode=519,009,170  dropped_freq=146,519,399\n",
      "[extract] A_relevant_raw.txt  chunk=749  kept=185,378,253  dropped_itemcode=519,716,007  dropped_freq=146,779,344\n",
      "[extract] A_relevant_raw.txt  chunk=750  kept=185,594,374  dropped_itemcode=520,424,056  dropped_freq=147,041,007\n",
      "[extract] A_relevant_raw.txt  chunk=751  kept=185,818,045  dropped_itemcode=521,128,594  dropped_freq=147,289,397\n",
      "[extract] A_relevant_raw.txt  chunk=752  kept=186,035,767  dropped_itemcode=521,835,873  dropped_freq=147,552,378\n",
      "[extract] A_relevant_raw.txt  chunk=753  kept=186,260,106  dropped_itemcode=522,540,446  dropped_freq=147,800,829\n",
      "[extract] A_relevant_raw.txt  chunk=754  kept=186,471,488  dropped_itemcode=523,251,832  dropped_freq=148,071,282\n",
      "[extract] A_relevant_raw.txt  chunk=755  kept=186,684,135  dropped_itemcode=523,968,762  dropped_freq=148,320,276\n",
      "[extract] A_relevant_raw.txt  chunk=756  kept=186,905,324  dropped_itemcode=524,670,543  dropped_freq=148,585,092\n",
      "[extract] A_relevant_raw.txt  chunk=757  kept=187,121,073  dropped_itemcode=525,376,510  dropped_freq=148,856,280\n",
      "[extract] A_relevant_raw.txt  chunk=758  kept=187,337,451  dropped_itemcode=526,081,721  dropped_freq=149,126,321\n",
      "[extract] A_relevant_raw.txt  chunk=759  kept=187,558,359  dropped_itemcode=526,784,421  dropped_freq=149,387,636\n",
      "[extract] A_relevant_raw.txt  chunk=760  kept=187,774,760  dropped_itemcode=527,490,190  dropped_freq=149,657,752\n",
      "[extract] A_relevant_raw.txt  chunk=761  kept=187,999,237  dropped_itemcode=528,193,243  dropped_freq=149,903,960\n",
      "[extract] A_relevant_raw.txt  chunk=762  kept=188,228,791  dropped_itemcode=528,893,907  dropped_freq=150,143,249\n",
      "[extract] A_relevant_raw.txt  chunk=763  kept=188,485,764  dropped_itemcode=529,587,743  dropped_freq=150,307,262\n",
      "[extract] A_relevant_raw.txt  chunk=764  kept=188,740,571  dropped_itemcode=530,280,052  dropped_freq=150,483,473\n",
      "[extract] A_relevant_raw.txt  chunk=765  kept=188,959,398  dropped_itemcode=530,983,003  dropped_freq=150,745,718\n",
      "[extract] A_relevant_raw.txt  chunk=766  kept=189,177,889  dropped_itemcode=531,688,216  dropped_freq=151,006,349\n",
      "[extract] A_relevant_raw.txt  chunk=767  kept=189,396,381  dropped_itemcode=532,394,991  dropped_freq=151,264,788\n",
      "[extract] A_relevant_raw.txt  chunk=768  kept=189,607,202  dropped_itemcode=533,110,308  dropped_freq=151,525,993\n",
      "[extract] A_relevant_raw.txt  chunk=769  kept=189,825,742  dropped_itemcode=533,816,732  dropped_freq=151,789,423\n",
      "[extract] A_relevant_raw.txt  chunk=770  kept=190,047,041  dropped_itemcode=534,519,742  dropped_freq=152,053,279\n",
      "[extract] A_relevant_raw.txt  chunk=771  kept=190,263,860  dropped_itemcode=535,230,541  dropped_freq=152,306,563\n",
      "[extract] A_relevant_raw.txt  chunk=772  kept=190,479,967  dropped_itemcode=535,939,420  dropped_freq=152,567,134\n",
      "[extract] A_relevant_raw.txt  chunk=773  kept=190,712,517  dropped_itemcode=536,636,206  dropped_freq=152,803,779\n",
      "[extract] A_relevant_raw.txt  chunk=774  kept=190,950,604  dropped_itemcode=537,325,875  dropped_freq=153,042,122\n",
      "[extract] A_relevant_raw.txt  chunk=775  kept=191,190,090  dropped_itemcode=538,012,302  dropped_freq=153,286,451\n",
      "[extract] A_relevant_raw.txt  chunk=776  kept=191,421,113  dropped_itemcode=538,709,779  dropped_freq=153,528,865\n",
      "[extract] A_relevant_raw.txt  chunk=777  kept=191,650,682  dropped_itemcode=539,419,208  dropped_freq=153,743,356\n",
      "[extract] A_relevant_raw.txt  chunk=778  kept=191,883,741  dropped_itemcode=540,111,209  dropped_freq=153,989,598\n",
      "[extract] A_relevant_raw.txt  chunk=779  kept=192,112,622  dropped_itemcode=540,810,365  dropped_freq=154,228,640\n",
      "[extract] A_relevant_raw.txt  chunk=780  kept=192,334,822  dropped_itemcode=541,517,325  dropped_freq=154,472,451\n",
      "[extract] A_relevant_raw.txt  chunk=781  kept=192,561,442  dropped_itemcode=542,217,132  dropped_freq=154,719,711\n",
      "[extract] A_relevant_raw.txt  chunk=782  kept=192,783,291  dropped_itemcode=542,927,761  dropped_freq=154,946,183\n",
      "[extract] A_relevant_raw.txt  chunk=783  kept=193,015,201  dropped_itemcode=543,622,526  dropped_freq=155,192,632\n",
      "[extract] A_relevant_raw.txt  chunk=784  kept=193,248,502  dropped_itemcode=544,316,970  dropped_freq=155,433,240\n",
      "[extract] A_relevant_raw.txt  chunk=785  kept=193,472,330  dropped_itemcode=545,019,724  dropped_freq=155,685,275\n",
      "[extract] A_relevant_raw.txt  chunk=786  kept=193,708,917  dropped_itemcode=545,712,587  dropped_freq=155,919,455\n",
      "[extract] A_relevant_raw.txt  chunk=787  kept=193,931,765  dropped_itemcode=546,417,217  dropped_freq=156,166,201\n",
      "[extract] A_relevant_raw.txt  chunk=788  kept=194,164,867  dropped_itemcode=547,111,261  dropped_freq=156,409,131\n",
      "[extract] A_relevant_raw.txt  chunk=789  kept=194,400,300  dropped_itemcode=547,803,273  dropped_freq=156,650,870\n",
      "[extract] A_relevant_raw.txt  chunk=790  kept=194,641,270  dropped_itemcode=548,490,329  dropped_freq=156,886,692\n",
      "[extract] A_relevant_raw.txt  chunk=791  kept=194,866,519  dropped_itemcode=549,189,593  dropped_freq=157,138,720\n",
      "[extract] A_relevant_raw.txt  chunk=792  kept=195,087,685  dropped_itemcode=549,902,638  dropped_freq=157,364,253\n",
      "[extract] A_relevant_raw.txt  chunk=793  kept=195,319,307  dropped_itemcode=550,595,829  dropped_freq=157,614,871\n",
      "[extract] A_relevant_raw.txt  chunk=794  kept=195,553,477  dropped_itemcode=551,288,135  dropped_freq=157,860,485\n",
      "[extract] A_relevant_raw.txt  chunk=795  kept=195,780,324  dropped_itemcode=551,992,028  dropped_freq=158,099,703\n",
      "[extract] A_relevant_raw.txt  chunk=796  kept=196,014,909  dropped_itemcode=552,687,097  dropped_freq=158,332,376\n",
      "[extract] A_relevant_raw.txt  chunk=797  kept=196,238,121  dropped_itemcode=553,390,838  dropped_freq=158,584,488\n",
      "[extract] A_relevant_raw.txt  chunk=798  kept=196,464,990  dropped_itemcode=554,092,822  dropped_freq=158,829,365\n",
      "[extract] A_relevant_raw.txt  chunk=799  kept=196,714,521  dropped_itemcode=554,764,244  dropped_freq=159,077,922\n",
      "[extract] A_relevant_raw.txt  chunk=800  kept=196,958,562  dropped_itemcode=555,442,597  dropped_freq=159,321,050\n",
      "[extract] A_relevant_raw.txt  chunk=801  kept=197,196,464  dropped_itemcode=556,126,361  dropped_freq=159,577,793\n",
      "[extract] A_relevant_raw.txt  chunk=802  kept=197,441,189  dropped_itemcode=556,802,421  dropped_freq=159,824,043\n",
      "[extract] A_relevant_raw.txt  chunk=803  kept=197,670,393  dropped_itemcode=557,498,269  dropped_freq=160,074,798\n",
      "[extract] A_relevant_raw.txt  chunk=804  kept=197,900,304  dropped_itemcode=558,194,119  dropped_freq=160,324,130\n",
      "[extract] A_relevant_raw.txt  chunk=805  kept=198,122,306  dropped_itemcode=558,896,343  dropped_freq=160,578,954\n",
      "[extract] A_relevant_raw.txt  chunk=806  kept=198,348,803  dropped_itemcode=559,596,319  dropped_freq=160,828,493\n",
      "[extract] A_relevant_raw.txt  chunk=807  kept=198,576,357  dropped_itemcode=560,293,754  dropped_freq=161,081,625\n",
      "[extract] A_relevant_raw.txt  chunk=808  kept=198,801,587  dropped_itemcode=560,996,388  dropped_freq=161,328,229\n",
      "[extract] A_relevant_raw.txt  chunk=809  kept=199,021,450  dropped_itemcode=561,699,087  dropped_freq=161,591,832\n",
      "[extract] A_relevant_raw.txt  chunk=810  kept=199,238,547  dropped_itemcode=562,405,687  dropped_freq=161,850,218\n",
      "[extract] A_relevant_raw.txt  chunk=811  kept=199,469,630  dropped_itemcode=563,103,333  dropped_freq=162,088,700\n",
      "[extract] A_relevant_raw.txt  chunk=812  kept=199,701,646  dropped_itemcode=563,795,443  dropped_freq=162,341,636\n",
      "[extract] A_relevant_raw.txt  chunk=813  kept=199,933,024  dropped_itemcode=564,490,080  dropped_freq=162,586,413\n",
      "[extract] A_relevant_raw.txt  chunk=814  kept=200,154,216  dropped_itemcode=565,197,374  dropped_freq=162,835,775\n",
      "[extract] A_relevant_raw.txt  chunk=815  kept=200,380,197  dropped_itemcode=565,900,911  dropped_freq=163,078,266\n",
      "[extract] A_relevant_raw.txt  chunk=816  kept=200,609,221  dropped_itemcode=566,599,898  dropped_freq=163,319,593\n",
      "[extract] A_relevant_raw.txt  chunk=817  kept=200,841,391  dropped_itemcode=567,296,735  dropped_freq=163,557,498\n",
      "[extract] A_relevant_raw.txt  chunk=818  kept=201,065,051  dropped_itemcode=567,997,748  dropped_freq=163,816,844\n",
      "[extract] A_relevant_raw.txt  chunk=819  kept=201,290,357  dropped_itemcode=568,700,533  dropped_freq=164,059,531\n",
      "[extract] A_relevant_raw.txt  chunk=820  kept=201,508,880  dropped_itemcode=569,411,110  dropped_freq=164,306,529\n",
      "[extract] A_relevant_raw.txt  chunk=821  kept=201,744,757  dropped_itemcode=570,109,755  dropped_freq=164,526,245\n",
      "[extract] A_relevant_raw.txt  chunk=822  kept=201,976,420  dropped_itemcode=570,809,108  dropped_freq=164,762,195\n",
      "[extract] A_relevant_raw.txt  chunk=823  kept=202,204,094  dropped_itemcode=571,508,271  dropped_freq=165,010,628\n",
      "[extract] A_relevant_raw.txt  chunk=824  kept=202,431,485  dropped_itemcode=572,205,714  dropped_freq=165,263,718\n",
      "[extract] A_relevant_raw.txt  chunk=825  kept=202,662,135  dropped_itemcode=572,901,296  dropped_freq=165,510,492\n",
      "[extract] A_relevant_raw.txt  chunk=826  kept=202,887,951  dropped_itemcode=573,601,543  dropped_freq=165,757,843\n",
      "[extract] A_relevant_raw.txt  chunk=827  kept=203,105,800  dropped_itemcode=574,306,646  dropped_freq=166,022,247\n",
      "[extract] A_relevant_raw.txt  chunk=828  kept=203,320,442  dropped_itemcode=575,021,552  dropped_freq=166,273,483\n",
      "[extract] A_relevant_raw.txt  chunk=829  kept=203,551,472  dropped_itemcode=575,718,708  dropped_freq=166,515,152\n",
      "[extract] A_relevant_raw.txt  chunk=830  kept=203,775,317  dropped_itemcode=576,422,401  dropped_freq=166,764,102\n",
      "[extract] A_relevant_raw.txt  chunk=831  kept=204,012,601  dropped_itemcode=577,110,464  dropped_freq=167,012,694\n",
      "[extract] A_relevant_raw.txt  chunk=832  kept=204,243,923  dropped_itemcode=577,803,599  dropped_freq=167,264,966\n",
      "[extract] A_relevant_raw.txt  chunk=833  kept=204,468,916  dropped_itemcode=578,508,231  dropped_freq=167,508,959\n",
      "[extract] A_relevant_raw.txt  chunk=834  kept=204,687,816  dropped_itemcode=579,218,805  dropped_freq=167,748,947\n",
      "[extract] A_relevant_raw.txt  chunk=835  kept=204,909,192  dropped_itemcode=579,924,027  dropped_freq=167,999,409\n",
      "[extract] A_relevant_raw.txt  chunk=836  kept=205,128,635  dropped_itemcode=580,630,724  dropped_freq=168,253,221\n",
      "[extract] A_relevant_raw.txt  chunk=837  kept=205,350,512  dropped_itemcode=581,336,367  dropped_freq=168,501,361\n",
      "[extract] A_relevant_raw.txt  chunk=838  kept=205,560,683  dropped_itemcode=582,057,539  dropped_freq=168,745,581\n",
      "[extract] A_relevant_raw.txt  chunk=839  kept=205,789,060  dropped_itemcode=582,753,062  dropped_freq=169,002,089\n",
      "[extract] A_relevant_raw.txt  chunk=840  kept=206,025,054  dropped_itemcode=583,443,920  dropped_freq=169,247,653\n",
      "[extract] A_relevant_raw.txt  chunk=841  kept=206,257,213  dropped_itemcode=584,136,620  dropped_freq=169,502,194\n",
      "[extract] A_relevant_raw.txt  chunk=842  kept=206,469,889  dropped_itemcode=584,847,070  dropped_freq=169,770,623\n",
      "[extract] A_relevant_raw.txt  chunk=843  kept=206,697,349  dropped_itemcode=585,541,428  dropped_freq=170,031,543\n",
      "[extract] A_relevant_raw.txt  chunk=844  kept=206,906,715  dropped_itemcode=586,262,317  dropped_freq=170,279,204\n",
      "[extract] A_relevant_raw.txt  chunk=845  kept=207,101,335  dropped_itemcode=586,999,323  dropped_freq=170,517,534\n",
      "[extract] A_relevant_raw.txt  chunk=846  kept=207,325,649  dropped_itemcode=587,702,821  dropped_freq=170,763,784\n",
      "[extract] A_relevant_raw.txt  chunk=847  kept=207,544,172  dropped_itemcode=588,411,294  dropped_freq=171,009,141\n",
      "[extract] A_relevant_raw.txt  chunk=848  kept=207,768,232  dropped_itemcode=589,113,216  dropped_freq=171,259,614\n",
      "[extract] A_relevant_raw.txt  chunk=849  kept=207,994,231  dropped_itemcode=589,812,570  dropped_freq=171,511,251\n",
      "[extract] A_relevant_raw.txt  chunk=850  kept=208,226,630  dropped_itemcode=590,505,440  dropped_freq=171,758,636\n",
      "[extract] A_relevant_raw.txt  chunk=851  kept=208,446,877  dropped_itemcode=591,210,136  dropped_freq=172,018,226\n",
      "[extract] A_relevant_raw.txt  chunk=852  kept=208,679,429  dropped_itemcode=591,901,506  dropped_freq=172,271,455\n",
      "[extract] A_relevant_raw.txt  chunk=853  kept=208,911,721  dropped_itemcode=592,595,854  dropped_freq=172,518,988\n",
      "[extract] A_relevant_raw.txt  chunk=854  kept=209,136,719  dropped_itemcode=593,297,025  dropped_freq=172,770,430\n",
      "[extract] A_relevant_raw.txt  chunk=855  kept=209,352,058  dropped_itemcode=594,007,385  dropped_freq=173,030,703\n",
      "[extract] A_relevant_raw.txt  chunk=856  kept=209,571,018  dropped_itemcode=594,709,992  dropped_freq=173,302,835\n",
      "[extract] A_relevant_raw.txt  chunk=857  kept=209,793,565  dropped_itemcode=595,415,023  dropped_freq=173,549,694\n",
      "[extract] A_relevant_raw.txt  chunk=858  kept=210,019,844  dropped_itemcode=596,113,108  dropped_freq=173,805,165\n",
      "[extract] A_relevant_raw.txt  chunk=859  kept=210,250,090  dropped_itemcode=596,806,593  dropped_freq=174,058,226\n",
      "[extract] A_relevant_raw.txt  chunk=860  kept=210,473,206  dropped_itemcode=597,507,515  dropped_freq=174,314,328\n",
      "[extract] A_relevant_raw.txt  chunk=861  kept=210,700,073  dropped_itemcode=598,204,971  dropped_freq=174,568,470\n",
      "[extract] A_relevant_raw.txt  chunk=862  kept=210,926,316  dropped_itemcode=598,906,385  dropped_freq=174,814,334\n",
      "[extract] A_relevant_raw.txt  chunk=863  kept=211,158,040  dropped_itemcode=599,601,181  dropped_freq=175,063,729\n",
      "[extract] A_relevant_raw.txt  chunk=864  kept=211,377,898  dropped_itemcode=600,308,558  dropped_freq=175,314,147\n",
      "[extract] A_relevant_raw.txt  chunk=865  kept=211,601,554  dropped_itemcode=601,010,792  dropped_freq=175,568,477\n",
      "[extract] A_relevant_raw.txt  chunk=866  kept=211,828,313  dropped_itemcode=601,710,606  dropped_freq=175,822,752\n",
      "[extract] A_relevant_raw.txt  chunk=867  kept=212,046,851  dropped_itemcode=602,417,092  dropped_freq=176,076,439\n",
      "[extract] A_relevant_raw.txt  chunk=868  kept=212,269,309  dropped_itemcode=603,119,170  dropped_freq=176,332,587\n",
      "[extract] A_relevant_raw.txt  chunk=869  kept=212,489,967  dropped_itemcode=603,820,837  dropped_freq=176,598,006\n",
      "[extract] A_relevant_raw.txt  chunk=870  kept=212,701,603  dropped_itemcode=604,536,480  dropped_freq=176,849,027\n",
      "[extract] A_relevant_raw.txt  chunk=871  kept=212,942,334  dropped_itemcode=605,230,362  dropped_freq=177,066,670\n",
      "[extract] A_relevant_raw.txt  chunk=872  kept=213,181,050  dropped_itemcode=605,946,283  dropped_freq=177,228,170\n",
      "[extract] A_relevant_raw.txt  chunk=873  kept=213,412,893  dropped_itemcode=606,662,544  dropped_freq=177,416,765\n",
      "[extract] A_relevant_raw.txt  chunk=874  kept=213,659,463  dropped_itemcode=607,368,199  dropped_freq=177,575,355\n",
      "[extract] A_relevant_raw.txt  chunk=875  kept=213,913,852  dropped_itemcode=608,063,293  dropped_freq=177,742,364\n",
      "[extract] A_relevant_raw.txt  chunk=876  kept=214,161,426  dropped_itemcode=608,763,011  dropped_freq=177,913,480\n",
      "[extract] A_relevant_raw.txt  chunk=877  kept=214,406,307  dropped_itemcode=609,468,712  dropped_freq=178,076,608\n",
      "[extract] A_relevant_raw.txt  chunk=878  kept=214,666,464  dropped_itemcode=610,168,108  dropped_freq=178,209,206\n",
      "[extract] A_relevant_raw.txt  chunk=879  kept=214,925,571  dropped_itemcode=610,859,815  dropped_freq=178,372,563\n",
      "[extract] A_relevant_raw.txt  chunk=880  kept=215,155,407  dropped_itemcode=611,561,922  dropped_freq=178,603,497\n",
      "[extract] A_relevant_raw.txt  chunk=881  kept=215,399,061  dropped_itemcode=612,254,739  dropped_freq=178,812,119\n",
      "[extract] A_relevant_raw.txt  chunk=882  kept=215,628,838  dropped_itemcode=612,957,308  dropped_freq=179,043,780\n",
      "[extract] A_relevant_raw.txt  chunk=883  kept=215,857,370  dropped_itemcode=613,659,047  dropped_freq=179,279,891\n",
      "[extract] A_relevant_raw.txt  chunk=884  kept=216,093,457  dropped_itemcode=614,359,780  dropped_freq=179,491,085\n",
      "[extract] A_relevant_raw.txt  chunk=885  kept=216,329,064  dropped_itemcode=615,060,959  dropped_freq=179,707,142\n",
      "[extract] A_relevant_raw.txt  chunk=886  kept=216,559,346  dropped_itemcode=615,762,669  dropped_freq=179,940,174\n",
      "[extract] A_relevant_raw.txt  chunk=887  kept=216,785,881  dropped_itemcode=616,467,757  dropped_freq=180,174,864\n",
      "[extract] A_relevant_raw.txt  chunk=888  kept=217,005,077  dropped_itemcode=617,183,654  dropped_freq=180,397,240\n",
      "[extract] A_relevant_raw.txt  chunk=889  kept=217,230,088  dropped_itemcode=617,885,754  dropped_freq=180,644,779\n",
      "[extract] A_relevant_raw.txt  chunk=890  kept=217,451,244  dropped_itemcode=618,588,405  dropped_freq=180,904,271\n",
      "[extract] A_relevant_raw.txt  chunk=891  kept=217,679,750  dropped_itemcode=619,289,495  dropped_freq=181,139,756\n",
      "[extract] A_relevant_raw.txt  chunk=892  kept=217,900,209  dropped_itemcode=619,992,369  dropped_freq=181,401,496\n",
      "[extract] A_relevant_raw.txt  chunk=893  kept=218,121,572  dropped_itemcode=620,692,739  dropped_freq=181,669,476\n",
      "[extract] A_relevant_raw.txt  chunk=894  kept=218,378,333  dropped_itemcode=621,375,752  dropped_freq=181,863,635\n",
      "[extract] A_relevant_raw.txt  chunk=895  kept=218,635,668  dropped_itemcode=622,070,527  dropped_freq=182,025,634\n",
      "[extract] A_relevant_raw.txt  chunk=896  kept=218,916,542  dropped_itemcode=622,754,097  dropped_freq=182,137,170\n",
      "[extract] A_relevant_raw.txt  chunk=897  kept=219,158,780  dropped_itemcode=623,452,328  dropped_freq=182,338,067\n",
      "[extract] A_relevant_raw.txt  chunk=898  kept=219,424,643  dropped_itemcode=624,141,194  dropped_freq=182,485,297\n",
      "[extract] A_relevant_raw.txt  chunk=899  kept=219,706,413  dropped_itemcode=624,823,079  dropped_freq=182,598,054\n",
      "[extract] A_relevant_raw.txt  chunk=900  kept=219,942,697  dropped_itemcode=625,517,889  dropped_freq=182,827,761\n",
      "[extract] A_relevant_raw.txt  chunk=901  kept=220,165,523  dropped_itemcode=626,230,875  dropped_freq=183,051,449\n",
      "[extract] A_relevant_raw.txt  chunk=902  kept=220,378,470  dropped_itemcode=626,943,186  dropped_freq=183,317,480\n",
      "[extract] A_relevant_raw.txt  chunk=903  kept=220,606,420  dropped_itemcode=627,640,194  dropped_freq=183,570,971\n",
      "[extract] A_relevant_raw.txt  chunk=904  kept=220,836,339  dropped_itemcode=628,335,022  dropped_freq=183,822,980\n",
      "[extract] A_relevant_raw.txt  chunk=905  kept=221,066,609  dropped_itemcode=629,028,530  dropped_freq=184,078,504\n",
      "[extract] A_relevant_raw.txt  chunk=906  kept=221,295,154  dropped_itemcode=629,725,689  dropped_freq=184,326,146\n",
      "[extract] A_relevant_raw.txt  chunk=907  kept=221,518,390  dropped_itemcode=630,423,634  dropped_freq=184,589,142\n",
      "[extract] A_relevant_raw.txt  chunk=908  kept=221,739,420  dropped_itemcode=631,122,822  dropped_freq=184,858,877\n",
      "[extract] A_relevant_raw.txt  chunk=909  kept=221,957,267  dropped_itemcode=631,827,665  dropped_freq=185,121,582\n",
      "[extract] A_relevant_raw.txt  chunk=910  kept=222,174,004  dropped_itemcode=632,532,388  dropped_freq=185,388,987\n",
      "[extract] A_relevant_raw.txt  chunk=911  kept=222,389,639  dropped_itemcode=633,238,911  dropped_freq=185,651,633\n",
      "[extract] A_relevant_raw.txt  chunk=912  kept=222,587,972  dropped_itemcode=633,968,470  dropped_freq=185,907,575\n",
      "[extract] A_relevant_raw.txt  chunk=913  kept=222,806,256  dropped_itemcode=634,671,482  dropped_freq=186,175,608\n",
      "[extract] A_relevant_raw.txt  chunk=914  kept=223,025,223  dropped_itemcode=635,375,371  dropped_freq=186,439,784\n",
      "[extract] A_relevant_raw.txt  chunk=915  kept=223,240,446  dropped_itemcode=636,083,321  dropped_freq=186,705,088\n",
      "[extract] A_relevant_raw.txt  chunk=916  kept=223,459,317  dropped_itemcode=636,788,866  dropped_freq=186,963,720\n",
      "[extract] A_relevant_raw.txt  chunk=917  kept=223,658,309  dropped_itemcode=637,517,320  dropped_freq=187,211,622\n",
      "[extract] A_relevant_raw.txt  chunk=918  kept=223,866,136  dropped_itemcode=638,234,112  dropped_freq=187,469,552\n",
      "[extract] A_relevant_raw.txt  chunk=919  kept=224,086,626  dropped_itemcode=638,935,836  dropped_freq=187,730,003\n",
      "[extract] A_relevant_raw.txt  chunk=920  kept=224,303,970  dropped_itemcode=639,640,050  dropped_freq=187,993,610\n",
      "[extract] A_relevant_raw.txt  chunk=921  kept=224,525,670  dropped_itemcode=640,338,996  dropped_freq=188,264,291\n",
      "[extract] A_relevant_raw.txt  chunk=922  kept=224,749,885  dropped_itemcode=641,039,033  dropped_freq=188,523,319\n",
      "[extract] A_relevant_raw.txt  chunk=923  kept=224,969,776  dropped_itemcode=641,741,374  dropped_freq=188,787,714\n",
      "[extract] A_relevant_raw.txt  chunk=924  kept=225,193,054  dropped_itemcode=642,440,819  dropped_freq=189,046,783\n",
      "[extract] A_relevant_raw.txt  chunk=925  kept=225,416,814  dropped_itemcode=643,135,501  dropped_freq=189,314,917\n",
      "[extract] A_relevant_raw.txt  chunk=926  kept=225,640,077  dropped_itemcode=643,832,739  dropped_freq=189,583,534\n",
      "[extract] A_relevant_raw.txt  chunk=927  kept=225,860,762  dropped_itemcode=644,532,794  dropped_freq=189,848,814\n",
      "[extract] A_relevant_raw.txt  chunk=928  kept=226,074,787  dropped_itemcode=645,240,887  dropped_freq=190,115,723\n",
      "[extract] A_relevant_raw.txt  chunk=929  kept=226,296,085  dropped_itemcode=645,941,334  dropped_freq=190,379,257\n",
      "[extract] A_relevant_raw.txt  chunk=930  kept=226,513,574  dropped_itemcode=646,647,660  dropped_freq=190,639,829\n",
      "[extract] A_relevant_raw.txt  chunk=931  kept=226,736,322  dropped_itemcode=647,351,185  dropped_freq=190,894,884\n",
      "[extract] A_relevant_raw.txt  chunk=932  kept=226,960,457  dropped_itemcode=648,046,875  dropped_freq=191,163,431\n",
      "[extract] A_relevant_raw.txt  chunk=933  kept=227,181,305  dropped_itemcode=648,747,398  dropped_freq=191,429,403\n",
      "[extract] A_relevant_raw.txt  chunk=934  kept=227,402,732  dropped_itemcode=649,448,808  dropped_freq=191,689,317\n",
      "[extract] A_relevant_raw.txt  chunk=935  kept=227,612,797  dropped_itemcode=650,162,043  dropped_freq=191,956,225\n",
      "[extract] A_relevant_raw.txt  chunk=936  kept=227,834,417  dropped_itemcode=650,862,115  dropped_freq=192,218,828\n",
      "[extract] A_relevant_raw.txt  chunk=937  kept=228,060,013  dropped_itemcode=651,564,386  dropped_freq=192,462,287\n",
      "[extract] A_relevant_raw.txt  chunk=938  kept=228,294,159  dropped_itemcode=652,265,752  dropped_freq=192,678,459\n",
      "[extract] A_relevant_raw.txt  chunk=939  kept=228,509,342  dropped_itemcode=652,979,976  dropped_freq=192,927,047\n",
      "[extract] A_relevant_raw.txt  chunk=940  kept=228,734,514  dropped_itemcode=653,679,500  dropped_freq=193,181,818\n",
      "[extract] A_relevant_raw.txt  chunk=941  kept=228,960,049  dropped_itemcode=654,380,283  dropped_freq=193,431,145\n",
      "[extract] A_relevant_raw.txt  chunk=942  kept=229,186,026  dropped_itemcode=655,076,302  dropped_freq=193,692,412\n",
      "[extract] A_relevant_raw.txt  chunk=943  kept=229,410,372  dropped_itemcode=655,778,576  dropped_freq=193,941,798\n",
      "[extract] A_relevant_raw.txt  chunk=944  kept=229,627,479  dropped_itemcode=656,498,123  dropped_freq=194,166,896\n",
      "[extract] A_relevant_raw.txt  chunk=945  kept=229,855,498  dropped_itemcode=657,076,159  dropped_freq=194,259,244\n",
      "[extract] moved final file to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/A_relevant_raw.txt\n",
      "[extract] DONE: wrote 229,855,498 rows to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/A_relevant_raw.txt\n",
      "[extract] B_relevant_raw.txt  chunk=1  kept=851  dropped_itemcode=999,149  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=2  kept=1,865  dropped_itemcode=1,998,135  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=3  kept=3,043  dropped_itemcode=2,996,957  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=4  kept=4,107  dropped_itemcode=3,995,893  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=5  kept=4,977  dropped_itemcode=4,995,023  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=6  kept=5,855  dropped_itemcode=5,994,145  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=7  kept=6,687  dropped_itemcode=6,993,313  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=8  kept=7,601  dropped_itemcode=7,992,399  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=9  kept=8,429  dropped_itemcode=8,991,571  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=10  kept=9,352  dropped_itemcode=9,990,648  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=11  kept=11,195  dropped_itemcode=10,988,805  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=12  kept=12,119  dropped_itemcode=11,987,881  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=13  kept=12,926  dropped_itemcode=12,987,074  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=14  kept=13,739  dropped_itemcode=13,986,261  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=15  kept=14,590  dropped_itemcode=14,985,410  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=16  kept=15,361  dropped_itemcode=15,984,639  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=17  kept=16,337  dropped_itemcode=16,983,663  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=18  kept=17,433  dropped_itemcode=17,982,567  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=19  kept=18,608  dropped_itemcode=18,981,392  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=20  kept=19,929  dropped_itemcode=19,980,071  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=21  kept=21,363  dropped_itemcode=20,978,637  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=22  kept=22,435  dropped_itemcode=21,977,565  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=23  kept=23,403  dropped_itemcode=22,976,597  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=24  kept=24,336  dropped_itemcode=23,975,664  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=25  kept=25,167  dropped_itemcode=24,974,833  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=26  kept=26,074  dropped_itemcode=25,973,926  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=27  kept=27,060  dropped_itemcode=26,972,940  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=28  kept=27,986  dropped_itemcode=27,972,014  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=29  kept=28,904  dropped_itemcode=28,971,096  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=30  kept=29,809  dropped_itemcode=29,970,191  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=31  kept=30,686  dropped_itemcode=30,969,314  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=32  kept=31,530  dropped_itemcode=31,968,470  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=33  kept=32,342  dropped_itemcode=32,967,658  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=34  kept=33,149  dropped_itemcode=33,966,851  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=35  kept=33,885  dropped_itemcode=34,966,115  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=36  kept=34,751  dropped_itemcode=35,965,249  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=37  kept=35,589  dropped_itemcode=36,964,411  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=38  kept=36,370  dropped_itemcode=37,963,630  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=39  kept=37,601  dropped_itemcode=38,962,399  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=40  kept=38,594  dropped_itemcode=39,961,406  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=41  kept=39,731  dropped_itemcode=40,960,269  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=42  kept=40,621  dropped_itemcode=41,959,379  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=43  kept=42,381  dropped_itemcode=42,957,619  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=44  kept=43,979  dropped_itemcode=43,956,021  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=45  kept=45,054  dropped_itemcode=44,954,946  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=46  kept=46,031  dropped_itemcode=45,953,969  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=47  kept=47,095  dropped_itemcode=46,952,905  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=48  kept=48,818  dropped_itemcode=47,951,182  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=49  kept=49,617  dropped_itemcode=48,950,383  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=50  kept=50,588  dropped_itemcode=49,949,412  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=51  kept=51,443  dropped_itemcode=50,948,557  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=52  kept=52,572  dropped_itemcode=51,947,428  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=53  kept=53,738  dropped_itemcode=52,946,262  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=54  kept=54,698  dropped_itemcode=53,945,302  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=55  kept=55,599  dropped_itemcode=54,944,401  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=56  kept=56,716  dropped_itemcode=55,943,284  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=57  kept=57,829  dropped_itemcode=56,942,171  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=58  kept=58,707  dropped_itemcode=57,941,293  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=59  kept=59,965  dropped_itemcode=58,940,035  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=60  kept=61,339  dropped_itemcode=59,938,661  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=61  kept=62,029  dropped_itemcode=60,937,971  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=62  kept=63,617  dropped_itemcode=61,936,383  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=63  kept=65,350  dropped_itemcode=62,934,650  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=64  kept=66,284  dropped_itemcode=63,933,716  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=65  kept=67,326  dropped_itemcode=64,932,674  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=66  kept=68,187  dropped_itemcode=65,931,813  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=67  kept=69,153  dropped_itemcode=66,930,847  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=68  kept=70,624  dropped_itemcode=67,929,376  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=69  kept=73,457  dropped_itemcode=68,926,543  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=70  kept=74,436  dropped_itemcode=69,925,564  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=71  kept=75,515  dropped_itemcode=70,924,485  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=72  kept=76,669  dropped_itemcode=71,923,331  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=73  kept=77,534  dropped_itemcode=72,922,466  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=74  kept=78,569  dropped_itemcode=73,921,431  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=75  kept=79,628  dropped_itemcode=74,920,372  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=76  kept=81,197  dropped_itemcode=75,918,803  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=77  kept=82,082  dropped_itemcode=76,917,918  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=78  kept=83,526  dropped_itemcode=77,916,474  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=79  kept=84,977  dropped_itemcode=78,915,023  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=80  kept=86,075  dropped_itemcode=79,913,925  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=81  kept=87,094  dropped_itemcode=80,912,906  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=82  kept=88,206  dropped_itemcode=81,911,794  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=83  kept=89,313  dropped_itemcode=82,910,687  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=84  kept=90,417  dropped_itemcode=83,909,583  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=85  kept=91,499  dropped_itemcode=84,908,501  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=86  kept=92,600  dropped_itemcode=85,907,400  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=87  kept=93,632  dropped_itemcode=86,906,368  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=88  kept=94,780  dropped_itemcode=87,905,220  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=89  kept=96,053  dropped_itemcode=88,903,947  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=90  kept=97,389  dropped_itemcode=89,902,611  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=91  kept=99,179  dropped_itemcode=90,900,821  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=92  kept=100,500  dropped_itemcode=91,899,500  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=93  kept=101,618  dropped_itemcode=92,898,382  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=94  kept=102,669  dropped_itemcode=93,897,331  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=95  kept=103,738  dropped_itemcode=94,896,262  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=96  kept=104,884  dropped_itemcode=95,895,116  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=97  kept=105,799  dropped_itemcode=96,894,201  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=98  kept=107,007  dropped_itemcode=97,892,993  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=99  kept=107,999  dropped_itemcode=98,892,001  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=100  kept=109,042  dropped_itemcode=99,890,958  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=101  kept=110,360  dropped_itemcode=100,889,640  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=102  kept=111,962  dropped_itemcode=101,888,038  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=103  kept=112,958  dropped_itemcode=102,887,042  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=104  kept=114,336  dropped_itemcode=103,885,664  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=105  kept=115,354  dropped_itemcode=104,884,646  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=106  kept=116,359  dropped_itemcode=105,883,641  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=107  kept=117,283  dropped_itemcode=106,882,717  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=108  kept=118,375  dropped_itemcode=107,881,625  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=109  kept=119,466  dropped_itemcode=108,880,534  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=110  kept=121,139  dropped_itemcode=109,878,861  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=111  kept=122,395  dropped_itemcode=110,877,605  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=112  kept=123,402  dropped_itemcode=111,876,598  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=113  kept=125,129  dropped_itemcode=112,874,871  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=114  kept=126,861  dropped_itemcode=113,873,139  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=115  kept=127,642  dropped_itemcode=114,872,358  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=116  kept=128,563  dropped_itemcode=115,871,437  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=117  kept=129,659  dropped_itemcode=116,870,341  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=118  kept=130,945  dropped_itemcode=117,869,055  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=119  kept=131,871  dropped_itemcode=118,868,129  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=120  kept=132,828  dropped_itemcode=119,867,172  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=121  kept=133,422  dropped_itemcode=120,866,578  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=122  kept=133,984  dropped_itemcode=121,866,016  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=123  kept=134,725  dropped_itemcode=122,865,275  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=124  kept=135,371  dropped_itemcode=123,864,629  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=125  kept=136,099  dropped_itemcode=124,863,901  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=126  kept=136,802  dropped_itemcode=125,863,198  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=127  kept=137,359  dropped_itemcode=126,862,641  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=128  kept=137,872  dropped_itemcode=127,862,128  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=129  kept=138,446  dropped_itemcode=128,861,554  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=130  kept=138,923  dropped_itemcode=129,861,077  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=131  kept=139,523  dropped_itemcode=130,860,477  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=132  kept=140,121  dropped_itemcode=131,859,879  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=133  kept=140,919  dropped_itemcode=132,859,081  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=134  kept=141,603  dropped_itemcode=133,858,397  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=135  kept=142,201  dropped_itemcode=134,857,799  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=136  kept=142,763  dropped_itemcode=135,857,237  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=137  kept=143,937  dropped_itemcode=136,856,063  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=138  kept=144,671  dropped_itemcode=137,855,329  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=139  kept=145,542  dropped_itemcode=138,854,458  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=140  kept=146,255  dropped_itemcode=139,853,745  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=141  kept=146,895  dropped_itemcode=140,853,105  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=142  kept=147,656  dropped_itemcode=141,852,344  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=143  kept=148,514  dropped_itemcode=142,851,486  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=144  kept=149,201  dropped_itemcode=143,850,799  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=145  kept=149,816  dropped_itemcode=144,850,184  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=146  kept=150,495  dropped_itemcode=145,849,505  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=147  kept=151,126  dropped_itemcode=146,848,874  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=148  kept=151,620  dropped_itemcode=147,848,380  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=149  kept=152,346  dropped_itemcode=148,847,654  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=150  kept=153,109  dropped_itemcode=149,846,891  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=151  kept=154,010  dropped_itemcode=150,845,990  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=152  kept=154,640  dropped_itemcode=151,845,360  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=153  kept=155,325  dropped_itemcode=152,844,675  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=154  kept=156,015  dropped_itemcode=153,843,985  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=155  kept=156,744  dropped_itemcode=154,843,256  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=156  kept=157,532  dropped_itemcode=155,842,468  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=157  kept=158,609  dropped_itemcode=156,841,391  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=158  kept=159,474  dropped_itemcode=157,840,526  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=159  kept=160,359  dropped_itemcode=158,839,641  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=160  kept=161,510  dropped_itemcode=159,838,490  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=161  kept=162,718  dropped_itemcode=160,837,282  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=162  kept=163,852  dropped_itemcode=161,836,148  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=163  kept=166,478  dropped_itemcode=162,833,522  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=164  kept=167,167  dropped_itemcode=163,832,833  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=165  kept=167,880  dropped_itemcode=164,832,120  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=166  kept=168,574  dropped_itemcode=165,831,426  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=167  kept=169,388  dropped_itemcode=166,830,612  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=168  kept=170,900  dropped_itemcode=167,829,100  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=169  kept=171,458  dropped_itemcode=168,828,542  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=170  kept=172,027  dropped_itemcode=169,827,973  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=171  kept=172,586  dropped_itemcode=170,827,414  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=172  kept=173,192  dropped_itemcode=171,826,808  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=173  kept=173,768  dropped_itemcode=172,826,232  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=174  kept=174,357  dropped_itemcode=173,825,643  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=175  kept=174,968  dropped_itemcode=174,825,032  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=176  kept=175,560  dropped_itemcode=175,824,440  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=177  kept=176,421  dropped_itemcode=176,823,579  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=178  kept=178,034  dropped_itemcode=177,821,966  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=179  kept=179,780  dropped_itemcode=178,820,220  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=180  kept=180,537  dropped_itemcode=179,819,463  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=181  kept=181,378  dropped_itemcode=180,818,622  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=182  kept=182,136  dropped_itemcode=181,817,864  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=183  kept=182,718  dropped_itemcode=182,817,282  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=184  kept=183,265  dropped_itemcode=183,816,735  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=185  kept=185,162  dropped_itemcode=184,814,838  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=186  kept=186,180  dropped_itemcode=185,813,820  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=187  kept=186,762  dropped_itemcode=186,813,238  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=188  kept=187,293  dropped_itemcode=187,812,707  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=189  kept=187,832  dropped_itemcode=188,812,168  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=190  kept=188,400  dropped_itemcode=189,811,600  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=191  kept=188,974  dropped_itemcode=190,811,026  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=192  kept=189,662  dropped_itemcode=191,810,338  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=193  kept=190,281  dropped_itemcode=192,809,719  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=194  kept=191,312  dropped_itemcode=193,808,688  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=195  kept=192,377  dropped_itemcode=194,807,623  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=196  kept=194,175  dropped_itemcode=195,805,825  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=197  kept=195,936  dropped_itemcode=196,804,064  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=198  kept=196,935  dropped_itemcode=197,803,065  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=199  kept=198,135  dropped_itemcode=198,801,865  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=200  kept=199,442  dropped_itemcode=199,800,558  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=201  kept=200,374  dropped_itemcode=200,799,626  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=202  kept=201,353  dropped_itemcode=201,798,647  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=203  kept=202,509  dropped_itemcode=202,797,491  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=204  kept=203,311  dropped_itemcode=203,796,689  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=205  kept=204,207  dropped_itemcode=204,795,793  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=206  kept=205,255  dropped_itemcode=205,794,745  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=207  kept=206,359  dropped_itemcode=206,793,641  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=208  kept=207,428  dropped_itemcode=207,792,572  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=209  kept=208,264  dropped_itemcode=208,791,736  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=210  kept=209,117  dropped_itemcode=209,790,883  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=211  kept=210,150  dropped_itemcode=210,789,850  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=212  kept=211,377  dropped_itemcode=211,788,623  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=213  kept=212,426  dropped_itemcode=212,787,574  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=214  kept=213,914  dropped_itemcode=213,786,086  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=215  kept=214,819  dropped_itemcode=214,785,181  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=216  kept=216,037  dropped_itemcode=215,783,963  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=217  kept=217,324  dropped_itemcode=216,782,676  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=218  kept=218,344  dropped_itemcode=217,781,656  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=219  kept=219,354  dropped_itemcode=218,780,646  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=220  kept=220,324  dropped_itemcode=219,779,676  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=221  kept=221,385  dropped_itemcode=220,778,615  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=222  kept=222,346  dropped_itemcode=221,777,654  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=223  kept=223,384  dropped_itemcode=222,776,616  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=224  kept=224,235  dropped_itemcode=223,775,765  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=225  kept=225,006  dropped_itemcode=224,774,994  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=226  kept=225,788  dropped_itemcode=225,774,212  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=227  kept=226,788  dropped_itemcode=226,773,212  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=228  kept=227,580  dropped_itemcode=227,772,420  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=229  kept=229,020  dropped_itemcode=228,770,980  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=230  kept=230,098  dropped_itemcode=229,769,902  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=231  kept=230,960  dropped_itemcode=230,769,040  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=232  kept=232,163  dropped_itemcode=231,767,837  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=233  kept=233,169  dropped_itemcode=232,766,831  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=234  kept=234,459  dropped_itemcode=233,765,541  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=235  kept=235,542  dropped_itemcode=234,764,458  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=236  kept=236,541  dropped_itemcode=235,763,459  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=237  kept=237,369  dropped_itemcode=236,762,631  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=238  kept=238,218  dropped_itemcode=237,761,782  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=239  kept=239,199  dropped_itemcode=238,760,801  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=240  kept=240,177  dropped_itemcode=239,759,823  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=241  kept=241,069  dropped_itemcode=240,758,931  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=242  kept=241,890  dropped_itemcode=241,758,110  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=243  kept=242,680  dropped_itemcode=242,757,320  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=244  kept=243,357  dropped_itemcode=243,756,643  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=245  kept=244,016  dropped_itemcode=244,755,984  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=246  kept=244,743  dropped_itemcode=245,755,257  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=247  kept=245,578  dropped_itemcode=246,754,422  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=248  kept=246,770  dropped_itemcode=247,753,230  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=249  kept=247,851  dropped_itemcode=248,752,149  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=250  kept=248,620  dropped_itemcode=249,751,380  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=251  kept=249,570  dropped_itemcode=250,750,430  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=252  kept=250,972  dropped_itemcode=251,749,028  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=253  kept=252,037  dropped_itemcode=252,747,963  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=254  kept=252,927  dropped_itemcode=253,747,073  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=255  kept=253,891  dropped_itemcode=254,746,109  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=256  kept=254,756  dropped_itemcode=255,745,244  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=257  kept=255,616  dropped_itemcode=256,744,384  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=258  kept=256,364  dropped_itemcode=257,743,636  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=259  kept=257,165  dropped_itemcode=258,742,835  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=260  kept=257,908  dropped_itemcode=259,742,092  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=261  kept=258,687  dropped_itemcode=260,741,313  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=262  kept=259,401  dropped_itemcode=261,740,599  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=263  kept=260,070  dropped_itemcode=262,739,930  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=264  kept=260,757  dropped_itemcode=263,739,243  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=265  kept=261,451  dropped_itemcode=264,738,549  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=266  kept=262,156  dropped_itemcode=265,737,844  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=267  kept=262,923  dropped_itemcode=266,737,077  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=268  kept=263,699  dropped_itemcode=267,736,301  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=269  kept=264,543  dropped_itemcode=268,735,457  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=270  kept=265,286  dropped_itemcode=269,734,714  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=271  kept=266,059  dropped_itemcode=270,733,941  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=272  kept=266,863  dropped_itemcode=271,733,137  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=273  kept=267,558  dropped_itemcode=272,732,442  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=274  kept=268,345  dropped_itemcode=273,731,655  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=275  kept=269,045  dropped_itemcode=274,730,955  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=276  kept=269,801  dropped_itemcode=275,730,199  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=277  kept=270,543  dropped_itemcode=276,729,457  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=278  kept=271,302  dropped_itemcode=277,728,698  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=279  kept=272,135  dropped_itemcode=278,727,865  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=280  kept=273,144  dropped_itemcode=279,726,856  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=281  kept=274,297  dropped_itemcode=280,725,703  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=282  kept=275,295  dropped_itemcode=281,724,705  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=283  kept=276,620  dropped_itemcode=282,723,380  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=284  kept=277,819  dropped_itemcode=283,722,181  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=285  kept=279,288  dropped_itemcode=284,720,712  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=286  kept=280,943  dropped_itemcode=285,719,057  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=287  kept=283,525  dropped_itemcode=286,716,475  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=288  kept=284,495  dropped_itemcode=287,715,505  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=289  kept=285,497  dropped_itemcode=288,714,503  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=290  kept=286,434  dropped_itemcode=289,713,566  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=291  kept=287,439  dropped_itemcode=290,712,561  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=292  kept=288,144  dropped_itemcode=291,711,856  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=293  kept=288,957  dropped_itemcode=292,711,043  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=294  kept=289,735  dropped_itemcode=293,710,265  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=295  kept=290,674  dropped_itemcode=294,709,326  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=296  kept=291,634  dropped_itemcode=295,708,366  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=297  kept=293,274  dropped_itemcode=296,706,726  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=298  kept=296,041  dropped_itemcode=297,703,959  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=299  kept=303,182  dropped_itemcode=298,696,818  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=300  kept=303,729  dropped_itemcode=299,696,271  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=301  kept=304,248  dropped_itemcode=300,695,752  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=302  kept=304,813  dropped_itemcode=301,695,187  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=303  kept=305,350  dropped_itemcode=302,694,650  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=304  kept=305,881  dropped_itemcode=303,694,119  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=305  kept=306,406  dropped_itemcode=304,693,594  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=306  kept=306,903  dropped_itemcode=305,693,097  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=307  kept=307,767  dropped_itemcode=306,692,233  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=308  kept=308,885  dropped_itemcode=307,691,115  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=309  kept=309,925  dropped_itemcode=308,690,075  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=310  kept=310,961  dropped_itemcode=309,689,039  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=311  kept=312,080  dropped_itemcode=310,687,920  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=312  kept=313,339  dropped_itemcode=311,686,661  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=313  kept=314,033  dropped_itemcode=312,685,967  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=314  kept=315,073  dropped_itemcode=313,684,927  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=315  kept=315,656  dropped_itemcode=314,684,344  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=316  kept=316,123  dropped_itemcode=315,683,877  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=317  kept=316,621  dropped_itemcode=316,683,379  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=318  kept=317,167  dropped_itemcode=317,682,833  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=319  kept=317,688  dropped_itemcode=318,682,312  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=320  kept=318,182  dropped_itemcode=319,681,818  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=321  kept=318,715  dropped_itemcode=320,681,285  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=322  kept=319,239  dropped_itemcode=321,680,761  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=323  kept=319,757  dropped_itemcode=322,680,243  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=324  kept=320,318  dropped_itemcode=323,679,682  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=325  kept=320,827  dropped_itemcode=324,679,173  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=326  kept=321,358  dropped_itemcode=325,678,642  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=327  kept=321,909  dropped_itemcode=326,678,091  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=328  kept=322,445  dropped_itemcode=327,677,555  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=329  kept=323,058  dropped_itemcode=328,676,942  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=330  kept=323,611  dropped_itemcode=329,676,389  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=331  kept=324,111  dropped_itemcode=330,675,889  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=332  kept=324,701  dropped_itemcode=331,675,299  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=333  kept=325,208  dropped_itemcode=332,674,792  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=334  kept=325,723  dropped_itemcode=333,674,277  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=335  kept=326,258  dropped_itemcode=334,673,742  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=336  kept=326,761  dropped_itemcode=335,673,239  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=337  kept=327,275  dropped_itemcode=336,672,725  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=338  kept=327,844  dropped_itemcode=337,672,156  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=339  kept=328,383  dropped_itemcode=338,671,617  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=340  kept=328,979  dropped_itemcode=339,671,021  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=341  kept=329,557  dropped_itemcode=340,670,443  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=342  kept=330,359  dropped_itemcode=341,669,641  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=343  kept=331,192  dropped_itemcode=342,668,808  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=344  kept=332,132  dropped_itemcode=343,667,868  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=345  kept=333,657  dropped_itemcode=344,666,343  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=346  kept=335,356  dropped_itemcode=345,664,644  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=347  kept=336,297  dropped_itemcode=346,663,703  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=348  kept=337,207  dropped_itemcode=347,662,793  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=349  kept=338,225  dropped_itemcode=348,661,775  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=350  kept=339,246  dropped_itemcode=349,660,754  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=351  kept=340,075  dropped_itemcode=350,659,925  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=352  kept=341,042  dropped_itemcode=351,658,958  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=353  kept=341,818  dropped_itemcode=352,658,182  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=354  kept=343,201  dropped_itemcode=353,656,799  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=355  kept=344,288  dropped_itemcode=354,655,712  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=356  kept=345,134  dropped_itemcode=355,654,866  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=357  kept=345,997  dropped_itemcode=356,654,003  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=358  kept=346,812  dropped_itemcode=357,653,188  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=359  kept=347,406  dropped_itemcode=358,652,594  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=360  kept=348,001  dropped_itemcode=359,651,999  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=361  kept=348,619  dropped_itemcode=360,651,381  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=362  kept=349,533  dropped_itemcode=361,650,467  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=363  kept=350,761  dropped_itemcode=362,649,239  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=364  kept=352,224  dropped_itemcode=363,647,776  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=365  kept=353,185  dropped_itemcode=364,646,815  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=366  kept=354,049  dropped_itemcode=365,645,951  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=367  kept=354,980  dropped_itemcode=366,645,020  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=368  kept=355,847  dropped_itemcode=367,644,153  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=369  kept=356,593  dropped_itemcode=368,643,407  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=370  kept=357,658  dropped_itemcode=369,642,342  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=371  kept=359,262  dropped_itemcode=370,640,738  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=372  kept=360,367  dropped_itemcode=371,639,633  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=373  kept=361,382  dropped_itemcode=372,638,618  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=374  kept=361,952  dropped_itemcode=373,638,048  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=375  kept=362,683  dropped_itemcode=374,637,317  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=376  kept=363,360  dropped_itemcode=375,636,640  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=377  kept=364,035  dropped_itemcode=376,635,965  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=378  kept=364,729  dropped_itemcode=377,635,271  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=379  kept=365,492  dropped_itemcode=378,634,508  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=380  kept=366,469  dropped_itemcode=379,633,531  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=381  kept=367,635  dropped_itemcode=380,632,365  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=382  kept=368,979  dropped_itemcode=381,631,021  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=383  kept=369,499  dropped_itemcode=382,630,501  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=384  kept=369,970  dropped_itemcode=383,630,030  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=385  kept=370,467  dropped_itemcode=384,629,533  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=386  kept=370,938  dropped_itemcode=385,629,062  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=387  kept=371,462  dropped_itemcode=386,628,538  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=388  kept=371,959  dropped_itemcode=387,628,041  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=389  kept=372,498  dropped_itemcode=388,627,502  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=390  kept=373,023  dropped_itemcode=389,626,977  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=391  kept=373,537  dropped_itemcode=390,626,463  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=392  kept=374,002  dropped_itemcode=391,625,998  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=393  kept=374,449  dropped_itemcode=392,625,551  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=394  kept=374,929  dropped_itemcode=393,625,071  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=395  kept=375,465  dropped_itemcode=394,624,535  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=396  kept=375,936  dropped_itemcode=395,624,064  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=397  kept=376,521  dropped_itemcode=396,623,479  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=398  kept=377,095  dropped_itemcode=397,622,905  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=399  kept=377,574  dropped_itemcode=398,622,426  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=400  kept=377,984  dropped_itemcode=399,622,016  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=401  kept=378,519  dropped_itemcode=400,621,481  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=402  kept=378,964  dropped_itemcode=401,621,036  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=403  kept=379,417  dropped_itemcode=402,620,583  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=404  kept=379,911  dropped_itemcode=403,620,089  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=405  kept=380,340  dropped_itemcode=404,619,660  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=406  kept=380,835  dropped_itemcode=405,619,165  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=407  kept=381,339  dropped_itemcode=406,618,661  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=408  kept=381,798  dropped_itemcode=407,618,202  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=409  kept=382,324  dropped_itemcode=408,617,676  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=410  kept=382,734  dropped_itemcode=409,617,266  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=411  kept=383,245  dropped_itemcode=410,616,755  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=412  kept=383,851  dropped_itemcode=411,616,149  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=413  kept=384,930  dropped_itemcode=412,615,070  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=414  kept=385,443  dropped_itemcode=413,614,557  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=415  kept=385,920  dropped_itemcode=414,614,080  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=416  kept=386,418  dropped_itemcode=415,613,582  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=417  kept=386,844  dropped_itemcode=416,613,156  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=418  kept=387,261  dropped_itemcode=417,612,739  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=419  kept=387,815  dropped_itemcode=418,612,185  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=420  kept=388,291  dropped_itemcode=419,611,709  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=421  kept=388,807  dropped_itemcode=420,611,193  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=422  kept=389,502  dropped_itemcode=421,610,498  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=423  kept=389,938  dropped_itemcode=422,610,062  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=424  kept=390,409  dropped_itemcode=423,609,591  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=425  kept=390,973  dropped_itemcode=424,609,027  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=426  kept=391,560  dropped_itemcode=425,608,440  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=427  kept=392,108  dropped_itemcode=426,607,892  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=428  kept=392,613  dropped_itemcode=427,607,387  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=429  kept=393,067  dropped_itemcode=428,606,933  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=430  kept=393,583  dropped_itemcode=429,606,417  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=431  kept=394,055  dropped_itemcode=430,605,945  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=432  kept=394,493  dropped_itemcode=431,605,507  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=433  kept=394,928  dropped_itemcode=432,605,072  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=434  kept=395,427  dropped_itemcode=433,604,573  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=435  kept=395,851  dropped_itemcode=434,604,149  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=436  kept=396,340  dropped_itemcode=435,603,660  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=437  kept=396,804  dropped_itemcode=436,603,196  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=438  kept=397,328  dropped_itemcode=437,602,672  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=439  kept=398,026  dropped_itemcode=438,601,974  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=440  kept=398,503  dropped_itemcode=439,601,497  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=441  kept=398,996  dropped_itemcode=440,601,004  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=442  kept=399,511  dropped_itemcode=441,600,489  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=443  kept=400,068  dropped_itemcode=442,599,932  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=444  kept=400,660  dropped_itemcode=443,599,340  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=445  kept=401,191  dropped_itemcode=444,598,809  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=446  kept=401,758  dropped_itemcode=445,598,242  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=447  kept=402,278  dropped_itemcode=446,597,722  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=448  kept=402,829  dropped_itemcode=447,597,171  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=449  kept=403,427  dropped_itemcode=448,596,573  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=450  kept=403,936  dropped_itemcode=449,596,064  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=451  kept=404,480  dropped_itemcode=450,595,520  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=452  kept=405,006  dropped_itemcode=451,594,994  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=453  kept=405,685  dropped_itemcode=452,594,315  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=454  kept=406,650  dropped_itemcode=453,593,350  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=455  kept=407,297  dropped_itemcode=454,592,703  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=456  kept=407,828  dropped_itemcode=455,592,172  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=457  kept=408,358  dropped_itemcode=456,591,642  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=458  kept=408,884  dropped_itemcode=457,591,116  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=459  kept=409,427  dropped_itemcode=458,590,573  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=460  kept=410,033  dropped_itemcode=459,589,967  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=461  kept=410,592  dropped_itemcode=460,589,408  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=462  kept=411,106  dropped_itemcode=461,588,894  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=463  kept=411,669  dropped_itemcode=462,588,331  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=464  kept=412,194  dropped_itemcode=463,587,806  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=465  kept=412,683  dropped_itemcode=464,587,317  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=466  kept=413,177  dropped_itemcode=465,586,823  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=467  kept=414,425  dropped_itemcode=466,585,575  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=468  kept=414,977  dropped_itemcode=467,585,023  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=469  kept=415,497  dropped_itemcode=468,584,503  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=470  kept=416,998  dropped_itemcode=469,583,002  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=471  kept=418,119  dropped_itemcode=470,581,881  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=472  kept=418,845  dropped_itemcode=471,581,155  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=473  kept=419,481  dropped_itemcode=472,580,519  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=474  kept=420,221  dropped_itemcode=473,579,779  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=475  kept=420,922  dropped_itemcode=474,579,078  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=476  kept=422,343  dropped_itemcode=475,577,657  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=477  kept=422,955  dropped_itemcode=476,577,045  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=478  kept=424,452  dropped_itemcode=477,575,548  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=479  kept=425,319  dropped_itemcode=478,574,681  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=480  kept=426,332  dropped_itemcode=479,573,668  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=481  kept=427,458  dropped_itemcode=480,572,542  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=482  kept=428,325  dropped_itemcode=481,571,675  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=483  kept=429,313  dropped_itemcode=482,570,687  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=484  kept=430,022  dropped_itemcode=483,569,978  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=485  kept=430,639  dropped_itemcode=484,569,361  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=486  kept=431,297  dropped_itemcode=485,568,703  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=487  kept=431,986  dropped_itemcode=486,568,014  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=488  kept=432,920  dropped_itemcode=487,567,080  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=489  kept=433,602  dropped_itemcode=488,566,398  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=490  kept=434,259  dropped_itemcode=489,565,741  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=491  kept=434,913  dropped_itemcode=490,565,087  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=492  kept=435,866  dropped_itemcode=491,564,134  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=493  kept=436,503  dropped_itemcode=492,563,497  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=494  kept=437,092  dropped_itemcode=493,562,908  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=495  kept=437,790  dropped_itemcode=494,562,210  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=496  kept=438,448  dropped_itemcode=495,561,552  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=497  kept=439,005  dropped_itemcode=496,560,995  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=498  kept=439,615  dropped_itemcode=497,560,385  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=499  kept=440,222  dropped_itemcode=498,559,778  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=500  kept=440,860  dropped_itemcode=499,559,140  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=501  kept=441,531  dropped_itemcode=500,558,469  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=502  kept=442,113  dropped_itemcode=501,557,887  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=503  kept=442,786  dropped_itemcode=502,557,214  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=504  kept=443,332  dropped_itemcode=503,556,668  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=505  kept=443,985  dropped_itemcode=504,556,015  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=506  kept=444,729  dropped_itemcode=505,555,271  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=507  kept=445,622  dropped_itemcode=506,554,378  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=508  kept=446,964  dropped_itemcode=507,553,036  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=509  kept=447,609  dropped_itemcode=508,552,391  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=510  kept=448,341  dropped_itemcode=509,551,659  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=511  kept=449,029  dropped_itemcode=510,550,971  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=512  kept=449,907  dropped_itemcode=511,550,093  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=513  kept=450,835  dropped_itemcode=512,549,165  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=514  kept=452,002  dropped_itemcode=513,547,998  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=515  kept=453,147  dropped_itemcode=514,546,853  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=516  kept=453,963  dropped_itemcode=515,546,037  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=517  kept=454,614  dropped_itemcode=516,545,386  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=518  kept=455,263  dropped_itemcode=517,544,737  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=519  kept=455,946  dropped_itemcode=518,544,054  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=520  kept=456,629  dropped_itemcode=519,543,371  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=521  kept=457,297  dropped_itemcode=520,542,703  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=522  kept=457,924  dropped_itemcode=521,542,076  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=523  kept=458,530  dropped_itemcode=522,541,470  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=524  kept=459,128  dropped_itemcode=523,540,872  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=525  kept=459,696  dropped_itemcode=524,540,304  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=526  kept=460,281  dropped_itemcode=525,539,719  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=527  kept=460,903  dropped_itemcode=526,539,097  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=528  kept=461,505  dropped_itemcode=527,538,495  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=529  kept=462,109  dropped_itemcode=528,537,891  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=530  kept=462,720  dropped_itemcode=529,537,280  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=531  kept=463,328  dropped_itemcode=530,536,672  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=532  kept=463,967  dropped_itemcode=531,536,033  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=533  kept=464,554  dropped_itemcode=532,535,446  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=534  kept=465,146  dropped_itemcode=533,534,854  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=535  kept=466,872  dropped_itemcode=534,533,128  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=536  kept=467,488  dropped_itemcode=535,532,512  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=537  kept=468,363  dropped_itemcode=536,531,637  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=538  kept=470,096  dropped_itemcode=537,529,904  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=539  kept=471,196  dropped_itemcode=538,528,804  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=540  kept=472,722  dropped_itemcode=539,527,278  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=541  kept=474,201  dropped_itemcode=540,525,799  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=542  kept=475,132  dropped_itemcode=541,524,868  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=543  kept=476,164  dropped_itemcode=542,523,836  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=544  kept=477,113  dropped_itemcode=543,522,887  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=545  kept=478,098  dropped_itemcode=544,521,902  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=546  kept=479,312  dropped_itemcode=545,520,688  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=547  kept=480,534  dropped_itemcode=546,519,466  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=548  kept=481,611  dropped_itemcode=547,518,389  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=549  kept=482,518  dropped_itemcode=548,517,482  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=550  kept=483,513  dropped_itemcode=549,516,487  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=551  kept=484,711  dropped_itemcode=550,515,289  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=552  kept=485,758  dropped_itemcode=551,514,242  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=553  kept=487,075  dropped_itemcode=552,512,925  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=554  kept=488,358  dropped_itemcode=553,511,642  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=555  kept=489,676  dropped_itemcode=554,510,324  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=556  kept=490,633  dropped_itemcode=555,509,367  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=557  kept=492,123  dropped_itemcode=556,507,877  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=558  kept=492,803  dropped_itemcode=557,507,197  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=559  kept=493,895  dropped_itemcode=558,506,105  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=560  kept=494,909  dropped_itemcode=559,505,091  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=561  kept=495,485  dropped_itemcode=560,504,515  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=562  kept=495,971  dropped_itemcode=561,504,029  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=563  kept=497,427  dropped_itemcode=562,502,573  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=564  kept=498,695  dropped_itemcode=563,501,305  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=565  kept=499,971  dropped_itemcode=564,500,029  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=566  kept=500,767  dropped_itemcode=565,499,233  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=567  kept=501,399  dropped_itemcode=566,498,601  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=568  kept=502,295  dropped_itemcode=567,497,705  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=569  kept=502,935  dropped_itemcode=568,497,065  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=570  kept=503,814  dropped_itemcode=569,496,186  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=571  kept=504,547  dropped_itemcode=570,495,453  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=572  kept=505,425  dropped_itemcode=571,494,575  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=573  kept=506,416  dropped_itemcode=572,493,584  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=574  kept=507,836  dropped_itemcode=573,492,164  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=575  kept=508,718  dropped_itemcode=574,491,282  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=576  kept=509,429  dropped_itemcode=575,490,571  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=577  kept=510,103  dropped_itemcode=576,489,897  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=578  kept=510,813  dropped_itemcode=577,489,187  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=579  kept=511,666  dropped_itemcode=578,488,334  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=580  kept=512,911  dropped_itemcode=579,487,089  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=581  kept=513,899  dropped_itemcode=580,486,101  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=582  kept=514,685  dropped_itemcode=581,485,315  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=583  kept=516,001  dropped_itemcode=582,483,999  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=584  kept=520,358  dropped_itemcode=583,479,642  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=585  kept=523,130  dropped_itemcode=584,476,870  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=586  kept=524,376  dropped_itemcode=585,475,624  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=587  kept=525,290  dropped_itemcode=586,474,710  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=588  kept=526,147  dropped_itemcode=587,473,853  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=589  kept=527,803  dropped_itemcode=588,472,197  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=590  kept=528,401  dropped_itemcode=589,471,599  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=591  kept=529,851  dropped_itemcode=590,470,149  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=592  kept=530,760  dropped_itemcode=591,469,240  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=593  kept=531,710  dropped_itemcode=592,468,290  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=594  kept=532,933  dropped_itemcode=593,467,067  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=595  kept=533,741  dropped_itemcode=594,466,259  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=596  kept=534,522  dropped_itemcode=595,465,478  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=597  kept=535,287  dropped_itemcode=596,464,713  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=598  kept=536,050  dropped_itemcode=597,463,950  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=599  kept=536,795  dropped_itemcode=598,463,205  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=600  kept=537,547  dropped_itemcode=599,462,453  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=601  kept=538,259  dropped_itemcode=600,461,741  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=602  kept=538,979  dropped_itemcode=601,461,021  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=603  kept=539,789  dropped_itemcode=602,460,211  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=604  kept=540,823  dropped_itemcode=603,459,177  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=605  kept=541,572  dropped_itemcode=604,458,428  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=606  kept=542,333  dropped_itemcode=605,457,667  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=607  kept=543,111  dropped_itemcode=606,456,889  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=608  kept=543,822  dropped_itemcode=607,456,178  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=609  kept=544,585  dropped_itemcode=608,455,415  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=610  kept=545,338  dropped_itemcode=609,454,662  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=611  kept=546,108  dropped_itemcode=610,453,892  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=612  kept=547,310  dropped_itemcode=611,452,690  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=613  kept=548,391  dropped_itemcode=612,451,609  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=614  kept=549,842  dropped_itemcode=613,450,158  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=615  kept=551,369  dropped_itemcode=614,448,631  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=616  kept=552,570  dropped_itemcode=615,447,430  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=617  kept=553,494  dropped_itemcode=616,446,506  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=618  kept=554,518  dropped_itemcode=617,445,482  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=619  kept=555,675  dropped_itemcode=618,444,325  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=620  kept=557,040  dropped_itemcode=619,442,960  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=621  kept=558,223  dropped_itemcode=620,441,777  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=622  kept=559,410  dropped_itemcode=621,440,590  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=623  kept=560,322  dropped_itemcode=622,439,678  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=624  kept=561,386  dropped_itemcode=623,438,614  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=625  kept=562,386  dropped_itemcode=624,437,614  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=626  kept=563,457  dropped_itemcode=625,436,543  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=627  kept=564,970  dropped_itemcode=626,435,030  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=628  kept=565,678  dropped_itemcode=627,434,322  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=629  kept=566,526  dropped_itemcode=628,433,474  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=630  kept=567,491  dropped_itemcode=629,432,509  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=631  kept=568,921  dropped_itemcode=630,431,079  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=632  kept=570,013  dropped_itemcode=631,429,987  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=633  kept=571,443  dropped_itemcode=632,428,557  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=634  kept=572,426  dropped_itemcode=633,427,574  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=635  kept=573,503  dropped_itemcode=634,426,497  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=636  kept=574,660  dropped_itemcode=635,425,340  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=637  kept=575,591  dropped_itemcode=636,424,409  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=638  kept=576,772  dropped_itemcode=637,423,228  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=639  kept=577,648  dropped_itemcode=638,422,352  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=640  kept=578,701  dropped_itemcode=639,421,299  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=641  kept=579,522  dropped_itemcode=640,420,478  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=642  kept=580,431  dropped_itemcode=641,419,569  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=643  kept=581,555  dropped_itemcode=642,418,445  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=644  kept=582,435  dropped_itemcode=643,417,565  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=645  kept=583,390  dropped_itemcode=644,416,610  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=646  kept=584,279  dropped_itemcode=645,415,721  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=647  kept=585,237  dropped_itemcode=646,414,763  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=648  kept=586,157  dropped_itemcode=647,413,843  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=649  kept=586,957  dropped_itemcode=648,413,043  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=650  kept=587,755  dropped_itemcode=649,412,245  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=651  kept=588,682  dropped_itemcode=650,411,318  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=652  kept=589,402  dropped_itemcode=651,410,598  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=653  kept=590,313  dropped_itemcode=652,409,687  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=654  kept=591,430  dropped_itemcode=653,408,570  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=655  kept=592,534  dropped_itemcode=654,407,466  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=656  kept=593,415  dropped_itemcode=655,406,585  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=657  kept=594,049  dropped_itemcode=656,405,951  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=658  kept=594,697  dropped_itemcode=657,405,303  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=659  kept=595,395  dropped_itemcode=658,404,605  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=660  kept=596,159  dropped_itemcode=659,403,841  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=661  kept=596,967  dropped_itemcode=660,403,033  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=662  kept=597,851  dropped_itemcode=661,402,149  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=663  kept=600,038  dropped_itemcode=662,399,962  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=664  kept=600,993  dropped_itemcode=663,399,007  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=665  kept=601,534  dropped_itemcode=664,398,466  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=666  kept=602,054  dropped_itemcode=665,397,946  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=667  kept=602,599  dropped_itemcode=666,397,401  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=668  kept=603,105  dropped_itemcode=667,396,895  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=669  kept=603,657  dropped_itemcode=668,396,343  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=670  kept=604,162  dropped_itemcode=669,395,838  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=671  kept=604,732  dropped_itemcode=670,395,268  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=672  kept=605,211  dropped_itemcode=671,394,789  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=673  kept=605,705  dropped_itemcode=672,394,295  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=674  kept=606,261  dropped_itemcode=673,393,739  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=675  kept=606,749  dropped_itemcode=674,393,251  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=676  kept=607,300  dropped_itemcode=675,392,700  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=677  kept=607,800  dropped_itemcode=676,392,200  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=678  kept=608,294  dropped_itemcode=677,391,706  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=679  kept=608,811  dropped_itemcode=678,391,189  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=680  kept=609,391  dropped_itemcode=679,390,609  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=681  kept=609,933  dropped_itemcode=680,390,067  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=682  kept=610,469  dropped_itemcode=681,389,531  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=683  kept=610,993  dropped_itemcode=682,389,007  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=684  kept=611,545  dropped_itemcode=683,388,455  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=685  kept=612,069  dropped_itemcode=684,387,931  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=686  kept=612,614  dropped_itemcode=685,387,386  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=687  kept=613,171  dropped_itemcode=686,386,829  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=688  kept=613,715  dropped_itemcode=687,386,285  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=689  kept=614,283  dropped_itemcode=688,385,717  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=690  kept=614,805  dropped_itemcode=689,385,195  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=691  kept=615,373  dropped_itemcode=690,384,627  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=692  kept=615,927  dropped_itemcode=691,384,073  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=693  kept=616,484  dropped_itemcode=692,383,516  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=694  kept=617,234  dropped_itemcode=693,382,766  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=695  kept=618,130  dropped_itemcode=694,381,870  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=696  kept=618,843  dropped_itemcode=695,381,157  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=697  kept=619,545  dropped_itemcode=696,380,455  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=698  kept=620,304  dropped_itemcode=697,379,696  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=699  kept=621,070  dropped_itemcode=698,378,930  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=700  kept=621,786  dropped_itemcode=699,378,214  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=701  kept=622,625  dropped_itemcode=700,377,375  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=702  kept=623,325  dropped_itemcode=701,376,675  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=703  kept=624,175  dropped_itemcode=702,375,825  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=704  kept=625,837  dropped_itemcode=703,374,163  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=705  kept=626,586  dropped_itemcode=704,373,414  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=706  kept=627,345  dropped_itemcode=705,372,655  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=707  kept=628,017  dropped_itemcode=706,371,983  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=708  kept=628,748  dropped_itemcode=707,371,252  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=709  kept=629,610  dropped_itemcode=708,370,390  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=710  kept=630,267  dropped_itemcode=709,369,733  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=711  kept=630,898  dropped_itemcode=710,369,102  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=712  kept=631,626  dropped_itemcode=711,368,374  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=713  kept=632,307  dropped_itemcode=712,367,693  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=714  kept=633,606  dropped_itemcode=713,366,394  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=715  kept=634,326  dropped_itemcode=714,365,674  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=716  kept=634,861  dropped_itemcode=715,365,139  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=717  kept=635,397  dropped_itemcode=716,364,603  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=718  kept=636,027  dropped_itemcode=717,363,973  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=719  kept=636,757  dropped_itemcode=718,363,243  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=720  kept=637,535  dropped_itemcode=719,362,465  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=721  kept=638,229  dropped_itemcode=720,361,771  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=722  kept=638,997  dropped_itemcode=721,361,003  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=723  kept=640,696  dropped_itemcode=722,359,304  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=724  kept=642,783  dropped_itemcode=723,357,217  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=725  kept=643,676  dropped_itemcode=724,356,324  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=726  kept=644,589  dropped_itemcode=725,355,411  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=727  kept=645,588  dropped_itemcode=726,354,412  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=728  kept=646,610  dropped_itemcode=727,353,390  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=729  kept=647,655  dropped_itemcode=728,352,345  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=730  kept=648,763  dropped_itemcode=729,351,237  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=731  kept=649,853  dropped_itemcode=730,350,147  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=732  kept=650,800  dropped_itemcode=731,349,200  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=733  kept=651,819  dropped_itemcode=732,348,181  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=734  kept=653,025  dropped_itemcode=733,346,975  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=735  kept=653,852  dropped_itemcode=734,346,148  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=736  kept=654,850  dropped_itemcode=735,345,150  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=737  kept=656,097  dropped_itemcode=736,343,903  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=738  kept=657,221  dropped_itemcode=737,342,779  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=739  kept=658,424  dropped_itemcode=738,341,576  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=740  kept=659,531  dropped_itemcode=739,340,469  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=741  kept=660,797  dropped_itemcode=740,339,203  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=742  kept=661,968  dropped_itemcode=741,338,032  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=743  kept=663,945  dropped_itemcode=742,336,055  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=744  kept=664,846  dropped_itemcode=743,335,154  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=745  kept=666,041  dropped_itemcode=744,333,959  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=746  kept=667,082  dropped_itemcode=745,332,918  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=747  kept=668,185  dropped_itemcode=746,331,815  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=748  kept=669,122  dropped_itemcode=747,330,878  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=749  kept=670,240  dropped_itemcode=748,329,760  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=750  kept=671,557  dropped_itemcode=749,328,443  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=751  kept=673,063  dropped_itemcode=750,326,937  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=752  kept=674,437  dropped_itemcode=751,325,563  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=753  kept=675,580  dropped_itemcode=752,324,420  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=754  kept=676,790  dropped_itemcode=753,323,210  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=755  kept=677,982  dropped_itemcode=754,322,018  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=756  kept=679,254  dropped_itemcode=755,320,746  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=757  kept=680,477  dropped_itemcode=756,319,523  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=758  kept=681,695  dropped_itemcode=757,318,305  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=759  kept=682,797  dropped_itemcode=758,317,203  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=760  kept=683,986  dropped_itemcode=759,316,014  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=761  kept=685,221  dropped_itemcode=760,314,779  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=762  kept=686,573  dropped_itemcode=761,313,427  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=763  kept=687,948  dropped_itemcode=762,312,052  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=764  kept=689,260  dropped_itemcode=763,310,740  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=765  kept=690,701  dropped_itemcode=764,309,299  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=766  kept=692,073  dropped_itemcode=765,307,927  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=767  kept=692,984  dropped_itemcode=766,307,016  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=768  kept=694,060  dropped_itemcode=767,305,940  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=769  kept=695,336  dropped_itemcode=768,304,664  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=770  kept=696,437  dropped_itemcode=769,303,563  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=771  kept=697,822  dropped_itemcode=770,302,178  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=772  kept=699,070  dropped_itemcode=771,300,930  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=773  kept=700,463  dropped_itemcode=772,299,537  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=774  kept=701,790  dropped_itemcode=773,298,210  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=775  kept=703,117  dropped_itemcode=774,296,883  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=776  kept=704,449  dropped_itemcode=775,295,551  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=777  kept=705,764  dropped_itemcode=776,294,236  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=778  kept=707,072  dropped_itemcode=777,292,928  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=779  kept=708,341  dropped_itemcode=778,291,659  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=780  kept=709,524  dropped_itemcode=779,290,476  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=781  kept=711,041  dropped_itemcode=780,288,959  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=782  kept=712,619  dropped_itemcode=781,287,381  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=783  kept=713,893  dropped_itemcode=782,286,107  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=784  kept=715,208  dropped_itemcode=783,284,792  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=785  kept=716,890  dropped_itemcode=784,283,110  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=786  kept=720,922  dropped_itemcode=785,279,078  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=787  kept=722,334  dropped_itemcode=786,277,666  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=788  kept=723,500  dropped_itemcode=787,276,500  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=789  kept=724,813  dropped_itemcode=788,275,187  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=790  kept=726,449  dropped_itemcode=789,273,551  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=791  kept=728,139  dropped_itemcode=790,271,861  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=792  kept=729,499  dropped_itemcode=791,270,501  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=793  kept=731,478  dropped_itemcode=792,268,522  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=794  kept=732,636  dropped_itemcode=793,267,364  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=795  kept=734,068  dropped_itemcode=794,265,932  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=796  kept=735,380  dropped_itemcode=795,264,620  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=797  kept=736,549  dropped_itemcode=796,263,451  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=798  kept=738,537  dropped_itemcode=797,261,463  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=799  kept=740,380  dropped_itemcode=798,259,620  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=800  kept=741,654  dropped_itemcode=799,258,346  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=801  kept=743,584  dropped_itemcode=800,256,416  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=802  kept=746,765  dropped_itemcode=801,253,235  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=803  kept=748,056  dropped_itemcode=802,251,944  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=804  kept=749,453  dropped_itemcode=803,250,547  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=805  kept=750,730  dropped_itemcode=804,249,270  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=806  kept=752,010  dropped_itemcode=805,247,990  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=807  kept=753,516  dropped_itemcode=806,246,484  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=808  kept=754,990  dropped_itemcode=807,245,010  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=809  kept=757,146  dropped_itemcode=808,242,854  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=810  kept=758,763  dropped_itemcode=809,241,237  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=811  kept=761,068  dropped_itemcode=810,238,932  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=812  kept=762,831  dropped_itemcode=811,237,169  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=813  kept=764,029  dropped_itemcode=812,235,971  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=814  kept=765,359  dropped_itemcode=813,234,641  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=815  kept=766,819  dropped_itemcode=814,233,181  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=816  kept=768,139  dropped_itemcode=815,231,861  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=817  kept=768,860  dropped_itemcode=816,231,140  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=818  kept=769,575  dropped_itemcode=817,230,425  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=819  kept=770,456  dropped_itemcode=818,229,544  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=820  kept=771,341  dropped_itemcode=819,228,659  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=821  kept=772,063  dropped_itemcode=820,227,937  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=822  kept=772,838  dropped_itemcode=821,227,162  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=823  kept=773,951  dropped_itemcode=822,226,049  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=824  kept=774,762  dropped_itemcode=823,225,238  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=825  kept=775,654  dropped_itemcode=824,224,346  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=826  kept=776,500  dropped_itemcode=825,223,500  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=827  kept=777,269  dropped_itemcode=826,222,731  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=828  kept=778,190  dropped_itemcode=827,221,810  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=829  kept=779,191  dropped_itemcode=828,220,809  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=830  kept=780,014  dropped_itemcode=829,219,986  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=831  kept=780,751  dropped_itemcode=830,219,249  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=832  kept=781,511  dropped_itemcode=831,218,489  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=833  kept=782,189  dropped_itemcode=832,217,811  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=834  kept=783,074  dropped_itemcode=833,216,926  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=835  kept=783,957  dropped_itemcode=834,216,043  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=836  kept=784,928  dropped_itemcode=835,215,072  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=837  kept=785,761  dropped_itemcode=836,214,239  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=838  kept=786,884  dropped_itemcode=837,213,116  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=839  kept=787,907  dropped_itemcode=838,212,093  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=840  kept=789,183  dropped_itemcode=839,210,817  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=841  kept=790,459  dropped_itemcode=840,209,541  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=842  kept=791,379  dropped_itemcode=841,208,621  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=843  kept=792,135  dropped_itemcode=842,207,865  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=844  kept=793,089  dropped_itemcode=843,206,911  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=845  kept=794,065  dropped_itemcode=844,205,935  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=846  kept=795,163  dropped_itemcode=845,204,837  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=847  kept=796,174  dropped_itemcode=846,203,826  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=848  kept=797,111  dropped_itemcode=847,202,889  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=849  kept=798,084  dropped_itemcode=848,201,916  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=850  kept=799,037  dropped_itemcode=849,200,963  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=851  kept=799,978  dropped_itemcode=850,200,022  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=852  kept=800,900  dropped_itemcode=851,199,100  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=853  kept=801,964  dropped_itemcode=852,198,036  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=854  kept=803,050  dropped_itemcode=853,196,950  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=855  kept=803,859  dropped_itemcode=854,196,141  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=856  kept=804,981  dropped_itemcode=855,195,019  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=857  kept=806,074  dropped_itemcode=856,193,926  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=858  kept=807,113  dropped_itemcode=857,192,887  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=859  kept=808,440  dropped_itemcode=858,191,560  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=860  kept=809,479  dropped_itemcode=859,190,521  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=861  kept=810,539  dropped_itemcode=860,189,461  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=862  kept=811,628  dropped_itemcode=861,188,372  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=863  kept=812,943  dropped_itemcode=862,187,057  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=864  kept=816,631  dropped_itemcode=863,183,369  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=865  kept=818,203  dropped_itemcode=864,181,797  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=866  kept=818,865  dropped_itemcode=865,181,135  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=867  kept=819,726  dropped_itemcode=866,180,274  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=868  kept=820,612  dropped_itemcode=867,179,388  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=869  kept=821,416  dropped_itemcode=868,178,584  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=870  kept=822,297  dropped_itemcode=869,177,703  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=871  kept=823,184  dropped_itemcode=870,176,816  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=872  kept=824,010  dropped_itemcode=871,175,990  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=873  kept=824,802  dropped_itemcode=872,175,198  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=874  kept=825,587  dropped_itemcode=873,174,413  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=875  kept=826,478  dropped_itemcode=874,173,522  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=876  kept=827,470  dropped_itemcode=875,172,530  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=877  kept=828,494  dropped_itemcode=876,171,506  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=878  kept=830,032  dropped_itemcode=877,169,968  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=879  kept=830,972  dropped_itemcode=878,169,028  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=880  kept=831,762  dropped_itemcode=879,168,238  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=881  kept=832,668  dropped_itemcode=880,167,332  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=882  kept=833,581  dropped_itemcode=881,166,419  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=883  kept=834,345  dropped_itemcode=882,165,655  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=884  kept=835,248  dropped_itemcode=883,164,752  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=885  kept=836,116  dropped_itemcode=884,163,884  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=886  kept=836,921  dropped_itemcode=885,163,079  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=887  kept=837,871  dropped_itemcode=886,162,129  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=888  kept=838,714  dropped_itemcode=887,161,286  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=889  kept=839,507  dropped_itemcode=888,160,493  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=890  kept=840,739  dropped_itemcode=889,159,261  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=891  kept=841,606  dropped_itemcode=890,158,394  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=892  kept=842,401  dropped_itemcode=891,157,599  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=893  kept=843,195  dropped_itemcode=892,156,805  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=894  kept=844,044  dropped_itemcode=893,155,956  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=895  kept=845,029  dropped_itemcode=894,154,971  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=896  kept=845,863  dropped_itemcode=895,154,137  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=897  kept=846,660  dropped_itemcode=896,153,340  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=898  kept=847,412  dropped_itemcode=897,152,588  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=899  kept=848,341  dropped_itemcode=898,151,659  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=900  kept=849,215  dropped_itemcode=899,150,785  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=901  kept=849,929  dropped_itemcode=900,150,071  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=902  kept=851,041  dropped_itemcode=901,148,959  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=903  kept=851,916  dropped_itemcode=902,148,084  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=904  kept=852,936  dropped_itemcode=903,147,064  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=905  kept=854,391  dropped_itemcode=904,145,609  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=906  kept=856,028  dropped_itemcode=905,143,972  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=907  kept=857,477  dropped_itemcode=906,142,523  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=908  kept=859,030  dropped_itemcode=907,140,970  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=909  kept=860,436  dropped_itemcode=908,139,564  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=910  kept=861,904  dropped_itemcode=909,138,096  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=911  kept=863,535  dropped_itemcode=910,136,465  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=912  kept=866,591  dropped_itemcode=911,133,409  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=913  kept=870,108  dropped_itemcode=912,129,892  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=914  kept=874,328  dropped_itemcode=913,125,672  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=915  kept=877,780  dropped_itemcode=914,122,220  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=916  kept=882,041  dropped_itemcode=915,117,959  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=917  kept=886,729  dropped_itemcode=916,113,271  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=918  kept=892,644  dropped_itemcode=917,107,356  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=919  kept=896,346  dropped_itemcode=918,103,654  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=920  kept=898,052  dropped_itemcode=919,101,948  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=921  kept=899,895  dropped_itemcode=920,100,105  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=922  kept=901,322  dropped_itemcode=921,098,678  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=923  kept=908,076  dropped_itemcode=922,091,924  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=924  kept=912,590  dropped_itemcode=923,087,410  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=925  kept=917,435  dropped_itemcode=924,082,565  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=926  kept=918,381  dropped_itemcode=925,081,619  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=927  kept=919,750  dropped_itemcode=926,080,250  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=928  kept=920,899  dropped_itemcode=927,079,101  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=929  kept=921,822  dropped_itemcode=928,078,178  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=930  kept=922,761  dropped_itemcode=929,077,239  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=931  kept=923,684  dropped_itemcode=930,076,316  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=932  kept=924,745  dropped_itemcode=931,075,255  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=933  kept=925,780  dropped_itemcode=932,074,220  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=934  kept=926,811  dropped_itemcode=933,073,189  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=935  kept=928,000  dropped_itemcode=934,072,000  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=936  kept=928,982  dropped_itemcode=935,071,018  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=937  kept=930,026  dropped_itemcode=936,069,974  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=938  kept=931,252  dropped_itemcode=937,068,748  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=939  kept=932,373  dropped_itemcode=938,067,627  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=940  kept=933,551  dropped_itemcode=939,066,449  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=941  kept=934,630  dropped_itemcode=940,065,370  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=942  kept=935,885  dropped_itemcode=941,064,115  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=943  kept=936,849  dropped_itemcode=942,063,151  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=944  kept=937,904  dropped_itemcode=943,062,096  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=945  kept=939,136  dropped_itemcode=944,060,864  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=946  kept=940,204  dropped_itemcode=945,059,796  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=947  kept=941,374  dropped_itemcode=946,058,626  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=948  kept=942,538  dropped_itemcode=947,057,462  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=949  kept=943,665  dropped_itemcode=948,056,335  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=950  kept=944,683  dropped_itemcode=949,055,317  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=951  kept=945,812  dropped_itemcode=950,054,188  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=952  kept=946,863  dropped_itemcode=951,053,137  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=953  kept=948,076  dropped_itemcode=952,051,924  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=954  kept=949,061  dropped_itemcode=953,050,939  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=955  kept=950,270  dropped_itemcode=954,049,730  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=956  kept=951,514  dropped_itemcode=955,048,486  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=957  kept=952,497  dropped_itemcode=956,047,503  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=958  kept=953,781  dropped_itemcode=957,046,219  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=959  kept=955,256  dropped_itemcode=958,044,744  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=960  kept=956,875  dropped_itemcode=959,043,125  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=961  kept=958,206  dropped_itemcode=960,041,794  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=962  kept=959,635  dropped_itemcode=961,040,365  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=963  kept=960,870  dropped_itemcode=962,039,130  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=964  kept=962,337  dropped_itemcode=963,037,663  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=965  kept=963,639  dropped_itemcode=964,036,361  dropped_freq=0\n",
      "[extract] B_relevant_raw.txt  chunk=966  kept=964,684  dropped_itemcode=964,574,800  dropped_freq=0\n",
      "[extract] moved final file to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/B_relevant_raw.txt\n",
      "[extract] DONE: wrote 964,684 rows to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/B_relevant_raw.txt\n",
      "[extract] C_relevant_raw.txt  chunk=1  kept=232,317  dropped_itemcode=749,886  dropped_freq=110,993\n",
      "[extract] C_relevant_raw.txt  chunk=2  kept=365,774  dropped_itemcode=1,557,647  dropped_freq=451,546\n",
      "[extract] C_relevant_raw.txt  chunk=3  kept=500,765  dropped_itemcode=2,344,999  dropped_freq=841,325\n",
      "[extract] C_relevant_raw.txt  chunk=4  kept=644,823  dropped_itemcode=3,107,975  dropped_freq=1,237,232\n",
      "[extract] C_relevant_raw.txt  chunk=5  kept=788,214  dropped_itemcode=3,868,514  dropped_freq=1,646,359\n",
      "[extract] C_relevant_raw.txt  chunk=6  kept=926,600  dropped_itemcode=4,638,598  dropped_freq=2,053,234\n",
      "[extract] C_relevant_raw.txt  chunk=7  kept=1,063,336  dropped_itemcode=5,420,362  dropped_freq=2,438,928\n",
      "[extract] C_relevant_raw.txt  chunk=8  kept=1,214,620  dropped_itemcode=6,177,727  dropped_freq=2,826,832\n",
      "[extract] C_relevant_raw.txt  chunk=9  kept=1,368,210  dropped_itemcode=6,929,136  dropped_freq=3,221,081\n",
      "[extract] C_relevant_raw.txt  chunk=10  kept=1,522,483  dropped_itemcode=7,697,821  dropped_freq=3,567,480\n",
      "[extract] C_relevant_raw.txt  chunk=11  kept=1,715,767  dropped_itemcode=8,455,100  dropped_freq=3,810,481\n",
      "[extract] C_relevant_raw.txt  chunk=12  kept=1,905,111  dropped_itemcode=9,212,038  dropped_freq=4,057,985\n",
      "[extract] C_relevant_raw.txt  chunk=13  kept=2,149,415  dropped_itemcode=9,924,421  dropped_freq=4,251,253\n",
      "[extract] C_relevant_raw.txt  chunk=14  kept=2,345,695  dropped_itemcode=10,663,658  dropped_freq=4,582,760\n",
      "[extract] C_relevant_raw.txt  chunk=15  kept=2,539,362  dropped_itemcode=11,401,190  dropped_freq=4,939,271\n",
      "[extract] C_relevant_raw.txt  chunk=16  kept=2,705,821  dropped_itemcode=12,159,106  dropped_freq=5,316,087\n",
      "[extract] C_relevant_raw.txt  chunk=17  kept=2,869,738  dropped_itemcode=12,909,263  dropped_freq=5,679,804\n",
      "[extract] C_relevant_raw.txt  chunk=18  kept=3,074,919  dropped_itemcode=13,607,241  dropped_freq=6,026,946\n",
      "[extract] C_relevant_raw.txt  chunk=19  kept=3,281,866  dropped_itemcode=14,304,980  dropped_freq=6,356,480\n",
      "[extract] C_relevant_raw.txt  chunk=20  kept=3,487,227  dropped_itemcode=15,001,166  dropped_freq=6,697,153\n",
      "[extract] C_relevant_raw.txt  chunk=21  kept=3,694,711  dropped_itemcode=15,689,031  dropped_freq=7,059,903\n",
      "[extract] C_relevant_raw.txt  chunk=22  kept=3,859,729  dropped_itemcode=16,426,045  dropped_freq=7,452,070\n",
      "[extract] C_relevant_raw.txt  chunk=23  kept=4,017,589  dropped_itemcode=17,167,545  dropped_freq=7,859,440\n",
      "[extract] C_relevant_raw.txt  chunk=24  kept=4,195,528  dropped_itemcode=17,893,843  dropped_freq=8,246,401\n",
      "[extract] C_relevant_raw.txt  chunk=25  kept=4,369,420  dropped_itemcode=18,648,764  dropped_freq=8,583,811\n",
      "[extract] C_relevant_raw.txt  chunk=26  kept=4,549,424  dropped_itemcode=19,374,794  dropped_freq=8,952,712\n",
      "[extract] C_relevant_raw.txt  chunk=27  kept=4,697,915  dropped_itemcode=20,137,927  dropped_freq=9,353,189\n",
      "[extract] C_relevant_raw.txt  chunk=28  kept=4,875,368  dropped_itemcode=20,879,662  dropped_freq=9,713,076\n",
      "[extract] C_relevant_raw.txt  chunk=29  kept=5,070,033  dropped_itemcode=21,589,310  dropped_freq=10,077,533\n",
      "[extract] C_relevant_raw.txt  chunk=30  kept=5,250,003  dropped_itemcode=22,350,180  dropped_freq=10,358,770\n",
      "[extract] C_relevant_raw.txt  chunk=31  kept=5,391,983  dropped_itemcode=23,154,866  dropped_freq=10,663,188\n",
      "[extract] C_relevant_raw.txt  chunk=32  kept=5,547,759  dropped_itemcode=23,907,889  dropped_freq=11,035,889\n",
      "[extract] C_relevant_raw.txt  chunk=33  kept=5,668,225  dropped_itemcode=24,719,610  dropped_freq=11,412,021\n",
      "[extract] C_relevant_raw.txt  chunk=34  kept=5,782,452  dropped_itemcode=25,541,021  dropped_freq=11,804,550\n",
      "[extract] C_relevant_raw.txt  chunk=35  kept=5,987,534  dropped_itemcode=26,286,003  dropped_freq=12,024,735\n",
      "[extract] C_relevant_raw.txt  chunk=36  kept=6,234,148  dropped_itemcode=27,012,260  dropped_freq=12,138,343\n",
      "[extract] C_relevant_raw.txt  chunk=37  kept=6,396,325  dropped_itemcode=27,756,523  dropped_freq=12,523,558\n",
      "[extract] C_relevant_raw.txt  chunk=38  kept=6,554,589  dropped_itemcode=28,506,849  dropped_freq=12,907,993\n",
      "[extract] C_relevant_raw.txt  chunk=39  kept=6,720,621  dropped_itemcode=29,249,683  dropped_freq=13,286,440\n",
      "[extract] C_relevant_raw.txt  chunk=40  kept=6,905,099  dropped_itemcode=29,987,393  dropped_freq=13,599,932\n",
      "[extract] C_relevant_raw.txt  chunk=41  kept=7,083,017  dropped_itemcode=30,723,710  dropped_freq=13,945,659\n",
      "[extract] C_relevant_raw.txt  chunk=42  kept=7,253,866  dropped_itemcode=31,472,005  dropped_freq=14,291,895\n",
      "[extract] C_relevant_raw.txt  chunk=43  kept=7,437,600  dropped_itemcode=32,205,767  dropped_freq=14,617,334\n",
      "[extract] C_relevant_raw.txt  chunk=44  kept=7,621,683  dropped_itemcode=32,938,325  dropped_freq=14,943,313\n",
      "[extract] C_relevant_raw.txt  chunk=45  kept=7,804,846  dropped_itemcode=33,692,055  dropped_freq=15,223,402\n",
      "[extract] C_relevant_raw.txt  chunk=46  kept=7,977,277  dropped_itemcode=34,460,300  dropped_freq=15,512,754\n",
      "[extract] C_relevant_raw.txt  chunk=47  kept=8,161,630  dropped_itemcode=35,199,339  dropped_freq=15,816,542\n",
      "[extract] C_relevant_raw.txt  chunk=48  kept=8,328,849  dropped_itemcode=35,943,580  dropped_freq=16,172,231\n",
      "[extract] C_relevant_raw.txt  chunk=49  kept=8,418,018  dropped_itemcode=36,356,472  dropped_freq=16,360,567\n",
      "[extract] moved final file to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/C_relevant_raw.txt\n",
      "[extract] DONE: wrote 8,418,018 rows to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/C_relevant_raw.txt\n",
      "[extract] D_relevant_raw.txt  chunk=1  kept=91,312  dropped_itemcode=883,473  dropped_freq=124,358\n",
      "[extract] D_relevant_raw.txt  chunk=2  kept=184,434  dropped_itemcode=1,789,293  dropped_freq=131,607\n",
      "[extract] D_relevant_raw.txt  chunk=3  kept=266,920  dropped_itemcode=2,670,517  dropped_freq=357,618\n",
      "[extract] D_relevant_raw.txt  chunk=4  kept=351,960  dropped_itemcode=3,527,373  dropped_freq=651,771\n",
      "[extract] D_relevant_raw.txt  chunk=5  kept=430,885  dropped_itemcode=4,387,748  dropped_freq=990,757\n",
      "[extract] D_relevant_raw.txt  chunk=6  kept=503,705  dropped_itemcode=5,263,134  dropped_freq=1,298,194\n",
      "[extract] D_relevant_raw.txt  chunk=7  kept=578,142  dropped_itemcode=6,137,362  dropped_freq=1,598,975\n",
      "[extract] D_relevant_raw.txt  chunk=8  kept=642,488  dropped_itemcode=7,021,317  dropped_freq=1,979,196\n",
      "[extract] D_relevant_raw.txt  chunk=9  kept=708,328  dropped_itemcode=7,904,747  dropped_freq=2,345,987\n",
      "[extract] D_relevant_raw.txt  chunk=10  kept=778,676  dropped_itemcode=8,781,890  dropped_freq=2,687,562\n",
      "[extract] D_relevant_raw.txt  chunk=11  kept=850,380  dropped_itemcode=9,656,631  dropped_freq=3,015,774\n",
      "[extract] D_relevant_raw.txt  chunk=12  kept=921,964  dropped_itemcode=10,531,916  dropped_freq=3,347,201\n",
      "[extract] D_relevant_raw.txt  chunk=13  kept=994,387  dropped_itemcode=11,408,556  dropped_freq=3,663,334\n",
      "[extract] D_relevant_raw.txt  chunk=14  kept=1,073,221  dropped_itemcode=12,276,919  dropped_freq=3,975,744\n",
      "[extract] D_relevant_raw.txt  chunk=15  kept=1,151,773  dropped_itemcode=13,148,600  dropped_freq=4,275,010\n",
      "[extract] D_relevant_raw.txt  chunk=16  kept=1,222,535  dropped_itemcode=14,030,499  dropped_freq=4,582,625\n",
      "[extract] D_relevant_raw.txt  chunk=17  kept=1,291,606  dropped_itemcode=14,916,567  dropped_freq=4,886,570\n",
      "[extract] D_relevant_raw.txt  chunk=18  kept=1,361,283  dropped_itemcode=15,798,570  dropped_freq=5,207,356\n",
      "[extract] D_relevant_raw.txt  chunk=19  kept=1,431,474  dropped_itemcode=16,680,529  dropped_freq=5,524,090\n",
      "[extract] D_relevant_raw.txt  chunk=20  kept=1,499,911  dropped_itemcode=17,563,260  dropped_freq=5,851,701\n",
      "[extract] D_relevant_raw.txt  chunk=21  kept=1,583,049  dropped_itemcode=18,438,716  dropped_freq=6,107,409\n",
      "[extract] D_relevant_raw.txt  chunk=22  kept=1,670,547  dropped_itemcode=19,308,984  dropped_freq=6,338,384\n",
      "[extract] D_relevant_raw.txt  chunk=23  kept=1,773,201  dropped_itemcode=20,179,134  dropped_freq=6,474,199\n",
      "[extract] D_relevant_raw.txt  chunk=24  kept=1,858,097  dropped_itemcode=21,057,101  dropped_freq=6,691,734\n",
      "[extract] D_relevant_raw.txt  chunk=25  kept=1,936,336  dropped_itemcode=21,939,334  dropped_freq=6,946,370\n",
      "[extract] D_relevant_raw.txt  chunk=26  kept=2,014,972  dropped_itemcode=22,841,470  dropped_freq=7,073,511\n",
      "[extract] D_relevant_raw.txt  chunk=27  kept=2,101,414  dropped_itemcode=23,733,411  dropped_freq=7,203,661\n",
      "[extract] D_relevant_raw.txt  chunk=28  kept=2,172,440  dropped_itemcode=24,616,355  dropped_freq=7,510,289\n",
      "[extract] D_relevant_raw.txt  chunk=29  kept=2,242,299  dropped_itemcode=25,499,432  dropped_freq=7,818,370\n",
      "[extract] D_relevant_raw.txt  chunk=30  kept=2,311,864  dropped_itemcode=26,384,056  dropped_freq=8,121,120\n",
      "[extract] D_relevant_raw.txt  chunk=31  kept=2,380,283  dropped_itemcode=27,267,620  dropped_freq=8,433,914\n",
      "[extract] D_relevant_raw.txt  chunk=32  kept=2,447,998  dropped_itemcode=28,150,717  dropped_freq=8,754,482\n",
      "[extract] D_relevant_raw.txt  chunk=33  kept=2,527,735  dropped_itemcode=29,011,147  dropped_freq=9,086,525\n",
      "[extract] D_relevant_raw.txt  chunk=34  kept=2,601,857  dropped_itemcode=29,886,141  dropped_freq=9,406,532\n",
      "[extract] D_relevant_raw.txt  chunk=35  kept=2,676,333  dropped_itemcode=30,763,934  dropped_freq=9,712,487\n",
      "[extract] D_relevant_raw.txt  chunk=36  kept=2,747,449  dropped_itemcode=31,646,052  dropped_freq=10,010,194\n",
      "[extract] D_relevant_raw.txt  chunk=37  kept=2,808,848  dropped_itemcode=32,540,288  dropped_freq=10,310,414\n",
      "[extract] D_relevant_raw.txt  chunk=38  kept=2,871,081  dropped_itemcode=33,438,036  dropped_freq=10,583,614\n",
      "[extract] D_relevant_raw.txt  chunk=39  kept=2,932,475  dropped_itemcode=34,335,322  dropped_freq=10,864,819\n",
      "[extract] D_relevant_raw.txt  chunk=40  kept=2,995,849  dropped_itemcode=35,232,443  dropped_freq=11,132,096\n",
      "[extract] D_relevant_raw.txt  chunk=41  kept=3,058,562  dropped_itemcode=36,126,678  dropped_freq=11,422,610\n",
      "[extract] D_relevant_raw.txt  chunk=42  kept=3,120,993  dropped_itemcode=37,019,097  dropped_freq=11,726,073\n",
      "[extract] D_relevant_raw.txt  chunk=43  kept=3,183,440  dropped_itemcode=37,911,344  dropped_freq=12,030,564\n",
      "[extract] D_relevant_raw.txt  chunk=44  kept=3,245,219  dropped_itemcode=38,804,147  dropped_freq=12,339,490\n",
      "[extract] D_relevant_raw.txt  chunk=45  kept=3,313,412  dropped_itemcode=39,689,339  dropped_freq=12,663,996\n",
      "[extract] D_relevant_raw.txt  chunk=46  kept=3,382,286  dropped_itemcode=40,570,610  dropped_freq=12,999,148\n",
      "[extract] D_relevant_raw.txt  chunk=47  kept=3,449,341  dropped_itemcode=41,454,256  dropped_freq=13,336,157\n",
      "[extract] D_relevant_raw.txt  chunk=48  kept=3,514,552  dropped_itemcode=42,339,408  dropped_freq=13,681,459\n",
      "[extract] D_relevant_raw.txt  chunk=49  kept=3,581,191  dropped_itemcode=43,224,377  dropped_freq=14,013,129\n",
      "[extract] D_relevant_raw.txt  chunk=50  kept=3,645,765  dropped_itemcode=44,110,945  dropped_freq=14,337,393\n",
      "[extract] D_relevant_raw.txt  chunk=51  kept=3,711,418  dropped_itemcode=44,995,401  dropped_freq=14,668,424\n",
      "[extract] D_relevant_raw.txt  chunk=52  kept=3,794,787  dropped_itemcode=45,863,924  dropped_freq=14,930,560\n",
      "[extract] D_relevant_raw.txt  chunk=53  kept=3,866,712  dropped_itemcode=46,754,464  dropped_freq=15,177,665\n",
      "[extract] D_relevant_raw.txt  chunk=54  kept=3,932,227  dropped_itemcode=47,639,642  dropped_freq=15,519,953\n",
      "[extract] D_relevant_raw.txt  chunk=55  kept=4,009,611  dropped_itemcode=48,503,274  dropped_freq=15,859,610\n",
      "[extract] D_relevant_raw.txt  chunk=56  kept=4,078,505  dropped_itemcode=49,383,083  dropped_freq=16,202,796\n",
      "[extract] D_relevant_raw.txt  chunk=57  kept=4,146,139  dropped_itemcode=50,269,124  dropped_freq=16,525,551\n",
      "[extract] D_relevant_raw.txt  chunk=58  kept=4,216,034  dropped_itemcode=51,155,487  dropped_freq=16,826,589\n",
      "[extract] D_relevant_raw.txt  chunk=59  kept=4,284,802  dropped_itemcode=52,050,502  dropped_freq=17,079,469\n",
      "[extract] D_relevant_raw.txt  chunk=60  kept=4,349,342  dropped_itemcode=52,942,541  dropped_freq=17,390,768\n",
      "[extract] D_relevant_raw.txt  chunk=61  kept=4,417,469  dropped_itemcode=53,833,332  dropped_freq=17,683,343\n",
      "[extract] D_relevant_raw.txt  chunk=62  kept=4,493,616  dropped_itemcode=54,727,800  dropped_freq=17,883,506\n",
      "[extract] D_relevant_raw.txt  chunk=63  kept=4,575,979  dropped_itemcode=55,588,363  dropped_freq=18,196,347\n",
      "[extract] D_relevant_raw.txt  chunk=64  kept=4,661,419  dropped_itemcode=56,442,558  dropped_freq=18,512,920\n",
      "[extract] D_relevant_raw.txt  chunk=65  kept=4,749,737  dropped_itemcode=57,324,347  dropped_freq=18,709,051\n",
      "[extract] D_relevant_raw.txt  chunk=66  kept=4,821,164  dropped_itemcode=58,209,464  dropped_freq=19,008,281\n",
      "[extract] D_relevant_raw.txt  chunk=67  kept=4,890,379  dropped_itemcode=59,093,969  dropped_freq=19,326,943\n",
      "[extract] D_relevant_raw.txt  chunk=68  kept=4,964,086  dropped_itemcode=59,969,356  dropped_freq=19,646,913\n",
      "[extract] D_relevant_raw.txt  chunk=69  kept=5,065,264  dropped_itemcode=60,794,457  dropped_freq=19,949,969\n",
      "[extract] D_relevant_raw.txt  chunk=70  kept=5,155,578  dropped_itemcode=61,634,084  dropped_freq=20,276,151\n",
      "[extract] D_relevant_raw.txt  chunk=71  kept=5,226,757  dropped_itemcode=62,511,570  dropped_freq=20,608,799\n",
      "[extract] D_relevant_raw.txt  chunk=72  kept=5,305,122  dropped_itemcode=63,412,175  dropped_freq=20,750,816\n",
      "[extract] D_relevant_raw.txt  chunk=73  kept=5,391,093  dropped_itemcode=64,321,523  dropped_freq=20,778,721\n",
      "[extract] D_relevant_raw.txt  chunk=74  kept=5,477,877  dropped_itemcode=65,229,965  dropped_freq=20,805,620\n",
      "[extract] D_relevant_raw.txt  chunk=75  kept=5,552,289  dropped_itemcode=66,114,098  dropped_freq=21,040,277\n",
      "[extract] D_relevant_raw.txt  chunk=76  kept=5,623,747  dropped_itemcode=66,989,164  dropped_freq=21,345,703\n",
      "[extract] D_relevant_raw.txt  chunk=77  kept=5,697,591  dropped_itemcode=67,860,829  dropped_freq=21,656,620\n",
      "[extract] D_relevant_raw.txt  chunk=78  kept=5,766,755  dropped_itemcode=68,739,449  dropped_freq=21,976,985\n",
      "[extract] D_relevant_raw.txt  chunk=79  kept=5,840,625  dropped_itemcode=69,610,725  dropped_freq=22,289,619\n",
      "[extract] D_relevant_raw.txt  chunk=80  kept=5,913,345  dropped_itemcode=70,488,390  dropped_freq=22,591,038\n",
      "[extract] D_relevant_raw.txt  chunk=81  kept=5,979,911  dropped_itemcode=71,374,053  dropped_freq=22,870,828\n",
      "[extract] D_relevant_raw.txt  chunk=82  kept=6,047,963  dropped_itemcode=72,258,200  dropped_freq=23,141,500\n",
      "[extract] D_relevant_raw.txt  chunk=83  kept=6,113,970  dropped_itemcode=73,145,091  dropped_freq=23,414,239\n",
      "[extract] D_relevant_raw.txt  chunk=84  kept=6,178,675  dropped_itemcode=74,033,324  dropped_freq=23,710,331\n",
      "[extract] D_relevant_raw.txt  chunk=85  kept=6,245,840  dropped_itemcode=74,917,464  dropped_freq=23,994,869\n",
      "[extract] D_relevant_raw.txt  chunk=86  kept=6,314,237  dropped_itemcode=75,800,665  dropped_freq=24,276,538\n",
      "[extract] D_relevant_raw.txt  chunk=87  kept=6,386,052  dropped_itemcode=76,677,235  dropped_freq=24,563,115\n",
      "[extract] D_relevant_raw.txt  chunk=88  kept=6,454,012  dropped_itemcode=77,559,084  dropped_freq=24,855,476\n",
      "[extract] D_relevant_raw.txt  chunk=89  kept=6,522,059  dropped_itemcode=78,441,520  dropped_freq=25,146,544\n",
      "[extract] D_relevant_raw.txt  chunk=90  kept=6,590,536  dropped_itemcode=79,323,305  dropped_freq=25,436,229\n",
      "[extract] D_relevant_raw.txt  chunk=91  kept=6,659,452  dropped_itemcode=80,211,281  dropped_freq=25,671,120\n",
      "[extract] D_relevant_raw.txt  chunk=92  kept=6,738,024  dropped_itemcode=81,087,758  dropped_freq=25,919,425\n",
      "[extract] D_relevant_raw.txt  chunk=93  kept=6,818,197  dropped_itemcode=81,952,375  dropped_freq=26,221,631\n",
      "[extract] D_relevant_raw.txt  chunk=94  kept=6,891,129  dropped_itemcode=82,842,153  dropped_freq=26,440,750\n",
      "[extract] D_relevant_raw.txt  chunk=95  kept=6,960,356  dropped_itemcode=83,719,683  dropped_freq=26,745,783\n",
      "[extract] D_relevant_raw.txt  chunk=96  kept=7,030,636  dropped_itemcode=84,595,262  dropped_freq=27,052,011\n",
      "[extract] D_relevant_raw.txt  chunk=97  kept=7,102,443  dropped_itemcode=85,468,097  dropped_freq=27,362,733\n",
      "[extract] D_relevant_raw.txt  chunk=98  kept=7,162,436  dropped_itemcode=86,179,228  dropped_freq=27,592,636\n",
      "[extract] moved final file to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/D_relevant_raw.txt\n",
      "[extract] DONE: wrote 7,162,436 rows to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/D_relevant_raw.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell performs the first main extraction step for the pipeline:\n",
    "#   - It defines `extract_relevant_rows_only`, a chunk-based reader that:\n",
    "#       * streams large input text files row by row in chunks to control memory usage\n",
    "#       * assigns column names depending on whether the file is \"A_like\" or \"B_like\"\n",
    "#       * converts the ItemCode column to integers and keeps only rows whose\n",
    "#         ItemCode is contained in a precomputed set of relevant item codes\n",
    "#       * optionally filters out rows with certain Frequency values (E, L, R, U)\n",
    "#         for A_like files. This is a key filtering transformation that reduces\n",
    "#         the dataset to only relevant data points.\n",
    "#       * writes the filtered subset to an output file, appending chunk by chunk\n",
    "#         via `save_append_txt`.\n",
    "#       * keeps counters of how many rows are retained and how many are dropped\n",
    "#         due to item code or frequency filtering to provide transparency on\n",
    "#         the impact of the filtering.\n",
    "#       * optionally writes to a local temporary file first and then moves it\n",
    "#         to the final Google Drive location for faster I/O.\n",
    "#   - It then calls this function four times to produce A/B/C/D \"relevant_raw\"\n",
    "#     files, each based on a different input source and relevant item set:\n",
    "#       * A: fundamentals dataset (A_like schema, frequency filtering enabled)\n",
    "#       * B: current dataset (B_like schema, no frequency filtering)\n",
    "#       * C: calendar dataset (A_like schema, frequency filtering enabled)\n",
    "#       * D: metadata dataset (A_like schema, frequency filtering enabled)\n",
    "# =============================================================================\n",
    "\n",
    "# ---------- STEP 1 — Extract relevant rows for A, B, C, D ----------\n",
    "\n",
    "import gc, shutil  # gc for manual garbage collection, shutil for file moving utilities\n",
    "\n",
    "\n",
    "def extract_relevant_rows_only(\n",
    "    input_path: str,\n",
    "    schema: str,\n",
    "    out_dir: str,\n",
    "    out_filename: str,\n",
    "    relevant_itemcodes: set[int],\n",
    "    drop_freq: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    schema: \"A_like\" (ID,PIT Date,Frequency,FiscalPeriod,ItemCode,Value)\n",
    "            \"B_like\" (ID,PIT Date,ItemCode,Value)\n",
    "    drop_freq: if True, drop Frequency in {\"E\",\"L\",\"R\",\"U\"} (use for A_like datasets)\n",
    "    \"\"\"\n",
    "    # Ensure out_dir is a Path object and create the directory if needed\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Construct the final output file path\n",
    "    final_path = out_dir / out_filename\n",
    "\n",
    "    # Decide whether to use a local temporary path for faster writes\n",
    "    if USE_LOCAL_TMP:\n",
    "        # Temporary partial file path in local storage\n",
    "        part_path = LOCAL_TMP_DIR / (out_filename + \".part\")\n",
    "        # Remove any existing temporary file from previous runs\n",
    "        if part_path.exists():\n",
    "            part_path.unlink()\n",
    "        # Remove any existing final file to avoid appending to old data\n",
    "        if final_path.exists():\n",
    "            final_path.unlink()\n",
    "        # Write output initially to the temporary path\n",
    "        write_path = part_path\n",
    "    else:\n",
    "        # If not using local temp, remove existing final file if present\n",
    "        if final_path.exists():\n",
    "            final_path.unlink()\n",
    "        # Write output directly to the final path\n",
    "        write_path = final_path\n",
    "\n",
    "    # Select appropriate column name layout based on schema\n",
    "    if schema == \"A_like\":\n",
    "        # A_like layout: includes Frequency and FiscalPeriod\n",
    "        column_names = [\"ID\", \"PIT Date\", \"Frequency\", \"FiscalPeriod\", \"ItemCode\", \"Value\"]\n",
    "    else:\n",
    "        # B_like layout: only ID, PIT Date, ItemCode, Value\n",
    "        column_names = [\"ID\", \"PIT Date\", \"ItemCode\", \"Value\"]\n",
    "\n",
    "    # Track whether a header has already been written to the output file\n",
    "    wrote_header = False\n",
    "\n",
    "    # Counters for auditing how many rows are kept or dropped by filters\n",
    "    total_kept = 0\n",
    "    total_dropped_itemcode = 0\n",
    "    total_dropped_frequency = 0\n",
    "    chunk_idx = 0\n",
    "\n",
    "    # Stream the input file in chunks to limit memory usage\n",
    "    for chunk in pd.read_csv(\n",
    "        input_path,\n",
    "        sep=\"|\",              # Input data is pipe-delimited\n",
    "        header=None,          # No header row in the input file\n",
    "        names=column_names,   # Assign explicit column names\n",
    "        dtype=str,            # Read all columns as strings initially\n",
    "        engine=\"c\",           # Use faster C parser\n",
    "        on_bad_lines=\"skip\",  # Skip lines that cannot be parsed correctly\n",
    "        chunksize=CHUNK_SIZE, # Number of rows per chunk\n",
    "        encoding=\"latin1\"     # Encoding for extended characters\n",
    "    ):\n",
    "        # Increment chunk index for progress reporting\n",
    "        chunk_idx += 1\n",
    "\n",
    "        # Convert ItemCode column to nullable integer type for filtering\n",
    "        itemc = pd.to_numeric(chunk[\"ItemCode\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        # Mask indicating rows whose ItemCode is in the relevant item set\n",
    "        mask_item = itemc.isin(relevant_itemcodes)\n",
    "\n",
    "        # If we are dropping certain frequency codes and Frequency column exists\n",
    "        if drop_freq and (\"Frequency\" in chunk.columns):\n",
    "            # Keep frequencies that are not in the exclusion set {E, L, R, U}\n",
    "            mask_freq = ~chunk[\"Frequency\"].isin([\"E\", \"L\", \"R\", \"U\"])\n",
    "\n",
    "            # Final mask: rows that satisfy both itemcode and frequency conditions\n",
    "            mask_final = mask_item & mask_freq\n",
    "\n",
    "            # Count how many rows were dropped purely due to excluded frequencies\n",
    "            total_dropped_frequency += int((~mask_freq).sum())\n",
    "        else:\n",
    "            # If not filtering by frequency, only use the item code mask\n",
    "            mask_final = mask_item\n",
    "\n",
    "        # Apply the final combined mask to filter the chunk down to relevant rows\n",
    "        subset = chunk[mask_final]\n",
    "\n",
    "        # Count how many rows were dropped due to item code irrelevance\n",
    "        total_dropped_itemcode += int((~mask_item).sum())\n",
    "\n",
    "        # If there are any relevant rows in this chunk, append them to the output file\n",
    "        if not subset.empty:\n",
    "            # Write filtered rows, appending to the target file\n",
    "            save_append_txt(subset, str(write_path), write_header=not wrote_header)\n",
    "            # After the first write, ensure header is not written again\n",
    "            wrote_header = True\n",
    "            # Update kept row counter\n",
    "            total_kept += len(subset)\n",
    "\n",
    "        # Explicitly delete large DataFrames to free memory\n",
    "        del chunk, subset\n",
    "        gc.collect()\n",
    "\n",
    "        # Print progress information for monitoring extraction\n",
    "        print(\n",
    "            f\"[extract] {out_filename}  chunk={chunk_idx:,}  kept={total_kept:,}  \"\n",
    "            f\"dropped_itemcode={total_dropped_itemcode:,}  dropped_freq={total_dropped_frequency:,}\"\n",
    "        )\n",
    "\n",
    "    # If using local temporary storage and the partial file exists, move it to final location\n",
    "    if USE_LOCAL_TMP and write_path.exists():\n",
    "        shutil.move(str(write_path), str(final_path))\n",
    "        print(f\"[extract] moved final file to {final_path}\")\n",
    "\n",
    "    # Final summary of how many rows were written to the output file\n",
    "    print(f\"[extract] DONE: wrote {total_kept:,} rows to {final_path}\")\n",
    "    return str(final_path)\n",
    "\n",
    "\n",
    "# Run STEP 1 for all datasets using the precomputed relevant item sets\n",
    "A_RELEVANT_PATH = extract_relevant_rows_only(\n",
    "    Fundamentals_file_path,\n",
    "    schema=\"A_like\",\n",
    "    out_dir=Temp_file_path_GO,\n",
    "    out_filename=\"A_relevant_raw.txt\",\n",
    "    relevant_itemcodes=RELEVANT_ITEMCODES_A,\n",
    "    drop_freq=True\n",
    ")\n",
    "\n",
    "B_RELEVANT_PATH = extract_relevant_rows_only(\n",
    "    Current_file_path,\n",
    "    schema=\"B_like\",\n",
    "    out_dir=Temp_file_path_GO,\n",
    "    out_filename=\"B_relevant_raw.txt\",\n",
    "    relevant_itemcodes=RELEVANT_ITEMCODES_B,\n",
    "    drop_freq=False\n",
    ")\n",
    "\n",
    "C_RELEVANT_PATH = extract_relevant_rows_only(\n",
    "    Calendar_file_path,\n",
    "    schema=\"A_like\",\n",
    "    out_dir=Temp_file_path_GO,\n",
    "    out_filename=\"C_relevant_raw.txt\",\n",
    "    relevant_itemcodes=RELEVANT_ITEMCODES_C,\n",
    "    drop_freq=True\n",
    ")\n",
    "\n",
    "D_RELEVANT_PATH = extract_relevant_rows_only(\n",
    "    Meta_file_path,\n",
    "    schema=\"A_like\",\n",
    "    out_dir=Temp_file_path_GO,\n",
    "    out_filename=\"D_relevant_raw.txt\",\n",
    "    relevant_itemcodes=RELEVANT_ITEMCODES_D,\n",
    "    drop_freq=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kxIfNOAy0ve"
   },
   "source": [
    "### Clean + Coerce + Require Fields -> Final Clean Files + Audit CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45449,
     "status": "ok",
     "timestamp": 1764877734223,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "s_hgKRErrtmB",
    "outputId": "73c9931b-87ac-4e93-821c-444c85ee633f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[clean:Fundamentals_clean] moved final file to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Fundamentals_clean.txt\n",
      "[OK] All rows accounted for for Fundamentals_clean: clean=224547306, errors=5308192, malformed=0\n",
      "[clean:Current_clean] moved final file to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Current_clean.txt\n",
      "[OK] All rows accounted for for Current_clean: clean=964684, errors=0, malformed=0\n",
      "[clean:Calendar_clean] moved final file to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Calendar_clean.txt\n",
      "[OK] All rows accounted for for Calendar_clean: clean=8418018, errors=0, malformed=0\n",
      "[clean:Meta_clean] moved final file to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Meta_clean.txt\n",
      "[OK] All rows accounted for for Meta_clean: clean=7162436, errors=0, malformed=0\n",
      "[OK] Wrote audit summary CSV to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Audit_Summary_all.csv\n",
      "Clean files:\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Fundamentals_clean.txt\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Current_clean.txt\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Calendar_clean.txt\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Meta_clean.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell implements STEP 2 of the pipeline: cleaning, type coercion, and\n",
    "# strict separation of clean vs. error rows for all datasets (A, B, C, D).\n",
    "#\n",
    "# Core behavior and transformations:\n",
    "#   - Reads the STEP 1 \"relevant_raw\" files line by line, manually splitting\n",
    "#     each row into columns and detecting malformed rows based on column count.\n",
    "#   - For well-formed rows:\n",
    "#       * Applies `standardize_strings` to normalize string columns while\n",
    "#         preserving the ID column. This includes trimming whitespace and\n",
    "#         converting empty strings to missing values.\n",
    "#       * Applies `coerce_types_schema` to enforce dataset-specific schemas\n",
    "#         (\"A_num\", \"A_str\", \"B\") by converting types of ID, PIT Date,\n",
    "#         ItemCode, Frequency, FiscalPeriod, and Value.\n",
    "#   - If `REQUIRE_VALUE_AND_DATE` is True:\n",
    "#       * Classifies rows as \"clean\" only if both Value and PIT Date are\n",
    "#         present and non-missing after coercion.\n",
    "#       * All other rows, including malformed lines, preexisting missing\n",
    "#         values, or values that become missing after coercion, are written\n",
    "#         to an error file with detailed ErrorReason fields and, where\n",
    "#         applicable, original uncoerced Value/PIT Date for debugging.\n",
    "#   - Every successfully read line is written to exactly one of:\n",
    "#       * {final_basename}.txt        -> clean rows\n",
    "#       * {final_basename}_errors.txt -> non-clean rows and malformed rows\n",
    "#   - PIT Date is normalized to the compact 'YYYY-MM-DD' format in both\n",
    "#     clean and error outputs.\n",
    "#   - For each dataset (Fundamentals, Current, Calendar, Meta), this cell\n",
    "#     runs the cleaning function, collects audit metrics (new NAs from\n",
    "#     standardization/coercion, trimming changes, totals), and writes a\n",
    "#     combined audit summary CSV to disk.\n",
    "# =============================================================================\n",
    "\n",
    "import gc, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_coerce_and_require(input_path: str, schema: str, out_dir: str, final_basename: str):\n",
    "    \"\"\"\n",
    "    schema:\n",
    "      - \"A_num\" -> Value numeric, has Frequency/FiscalPeriod\n",
    "      - \"A_str\" -> Value string, has Frequency/FiscalPeriod\n",
    "      - \"B\"     -> Value string, no Frequency/FiscalPeriod\n",
    "\n",
    "    Behavior:\n",
    "      - Reads input line-by-line, manually detects malformed lines (bad column count).\n",
    "      - Applies standardize_strings + coerce_types_schema (no row drops).\n",
    "      - If REQUIRE_VALUE_AND_DATE:\n",
    "           Clean rows:   Value != NA AND PIT Date != NA after coercion\n",
    "           Error rows:   all others (incl. malformed, preexisting missing, coercion-induced NAs)\n",
    "      - Every successfully read line is written to EXACTLY ONE of:\n",
    "           {final_basename}.txt          (clean rows)\n",
    "           {final_basename}_errors.txt   (all non-clean rows, with ErrorReason)\n",
    "      - No {final_basename}_dropped_* side files anymore.\n",
    "\n",
    "    Returns:\n",
    "      (final_clean_path, totals_dict, na_std_dict, na_coerce_dict, trim_changes_dict)\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Define expected columns based on schema\n",
    "    if schema in (\"A_num\", \"A_str\"):\n",
    "        # Schema with Frequency and FiscalPeriod\n",
    "        column_names = [\"ID\", \"PIT Date\", \"Frequency\", \"FiscalPeriod\", \"ItemCode\", \"Value\"]\n",
    "    else:  # \"B\"\n",
    "        # Simpler schema without Frequency/FiscalPeriod\n",
    "        column_names = [\"ID\", \"PIT Date\", \"ItemCode\", \"Value\"]\n",
    "\n",
    "    n_cols = len(column_names)\n",
    "\n",
    "    # Construct output paths for clean and error files\n",
    "    final_path  = out_dir / f\"{final_basename}.txt\"\n",
    "    errors_path = out_dir / f\"{final_basename}_errors.txt\"\n",
    "\n",
    "    # Prepare clean writer target, potentially via a local temporary path\n",
    "    if USE_LOCAL_TMP:\n",
    "        part_path = LOCAL_TMP_DIR / (final_path.name + \".part\")\n",
    "        # Remove any existing partial file from previous runs\n",
    "        if part_path.exists():\n",
    "            part_path.unlink()\n",
    "        # Remove existing final file to avoid appending to stale data\n",
    "        if final_path.exists():\n",
    "            final_path.unlink()\n",
    "        clean_write_path = part_path\n",
    "    else:\n",
    "        # Direct write to final path, removing previous file if any\n",
    "        if final_path.exists():\n",
    "            final_path.unlink()\n",
    "        clean_write_path = final_path\n",
    "\n",
    "    # Remove any existing error file to avoid mixing runs\n",
    "    if errors_path.exists():\n",
    "        errors_path.unlink()\n",
    "\n",
    "    # Header flags to ensure header is written exactly once per file\n",
    "    clean_header_written = False\n",
    "    err_header_written = False\n",
    "\n",
    "    # Counters to track row statistics for auditing\n",
    "    totals = {\n",
    "        \"rows_in\": 0,           # number of well-formed rows processed\n",
    "        \"malformed_rows\": 0,    # rows with wrong column count\n",
    "        \"rows_clean\": 0,        # rows written to clean file\n",
    "        \"rows_error\": 0         # rows written to error file (incl. malformed)\n",
    "    }\n",
    "\n",
    "    # Dictionaries to aggregate NA increases and trimming changes over all chunks\n",
    "    na_std, na_coerce, trimmed = {}, {}, {}\n",
    "\n",
    "    def _acc(dst: dict, delta: dict):\n",
    "        \"\"\"\n",
    "        Accumulate counts from delta into dst, summing values per key.\n",
    "        \"\"\"\n",
    "        for k, v in (delta or {}).items():\n",
    "            dst[k] = dst.get(k, 0) + int(v)\n",
    "\n",
    "    # Small helper to append a DataFrame to a target file (clean or error)\n",
    "    def _append_df(df: pd.DataFrame, path: Path, header_flag_attr: str):\n",
    "        nonlocal clean_header_written, err_header_written\n",
    "        if df.empty:\n",
    "            return\n",
    "\n",
    "        # Decide whether to write header based on file type and previous writes\n",
    "        if header_flag_attr == \"clean\":\n",
    "            write_header = not clean_header_written\n",
    "            clean_header_written = True\n",
    "        else:\n",
    "            write_header = not err_header_written\n",
    "            err_header_written = True\n",
    "\n",
    "        # Append DataFrame to target CSV with consistent formatting\n",
    "        df.to_csv(\n",
    "            path,\n",
    "            index=False,\n",
    "            sep=OUTPUT_SEP,\n",
    "            mode=\"a\",\n",
    "            header=write_header,\n",
    "            lineterminator=\"\\n\",\n",
    "            date_format=\"%Y-%m-%d\"\n",
    "        )\n",
    "\n",
    "    # Buffer to collect well-formed rows before processing them as a chunk\n",
    "    buffer_rows = []   # list of lists, each of length n_cols\n",
    "\n",
    "    def process_buffer():\n",
    "        \"\"\"\n",
    "        Process the current buffer_rows as one chunk: standardize, coerce, and\n",
    "        split rows into clean vs. error groups based on required fields.\n",
    "        \"\"\"\n",
    "        nonlocal buffer_rows\n",
    "        if not buffer_rows:\n",
    "            return\n",
    "\n",
    "        # Convert buffer into a DataFrame using the known column names\n",
    "        df = pd.DataFrame(buffer_rows, columns=column_names)\n",
    "        totals[\"rows_in\"] += len(df)\n",
    "\n",
    "        # 1) Standardize string columns (ID preserved, other string columns trimmed)\n",
    "        b_std = df.copy(deep=False)\n",
    "        df = standardize_strings(df)\n",
    "        _acc(na_std, _na_increase(b_std, df))\n",
    "        _acc(trimmed, _trim_changes(b_std, df))\n",
    "\n",
    "        # 2) Coerce types based on schema (ID, PIT Date, ItemCode, etc.)\n",
    "        b_co = df.copy(deep=False)\n",
    "        df = coerce_types_schema(df, schema=schema)\n",
    "        _acc(na_coerce, _na_increase(b_co, df))\n",
    "\n",
    "        # 3) Classify rows as clean or error, depending on required fields\n",
    "        if REQUIRE_VALUE_AND_DATE:\n",
    "            has_value = \"Value\" in df.columns\n",
    "\n",
    "            # Track missingness for Value and PIT Date before coercion\n",
    "            pre_missing_val   = (b_co[\"Value\"].isna()    if has_value else pd.Series(False, index=df.index))\n",
    "            pre_missing_date  = (b_co[\"PIT Date\"].isna() if \"PIT Date\" in b_co.columns else pd.Series(False, index=df.index))\n",
    "\n",
    "            # Track missingness for Value and PIT Date after coercion\n",
    "            post_missing_val  = (df[\"Value\"].isna()    if has_value else pd.Series(False, index=df.index))\n",
    "            post_missing_date = (df[\"PIT Date\"].isna() if \"PIT Date\" in df.columns else pd.Series(False, index=df.index))\n",
    "\n",
    "            # Rows failing the required Value/PIT Date constraint after coercion\n",
    "            bad_req = post_missing_val | post_missing_date\n",
    "\n",
    "            # Initialize a string-based error reason for each row\n",
    "            err_reason = pd.Series(\"\", index=df.index, dtype=\"string\")\n",
    "\n",
    "            if has_value:\n",
    "                # Preexisting missing Value\n",
    "                mask = pre_missing_val & post_missing_val\n",
    "                err_reason[mask] = err_reason[mask] + \"Value_missing_preexisting;\"\n",
    "                # Value that became missing due to coercion\n",
    "                mask = (~pre_missing_val) & post_missing_val\n",
    "                err_reason[mask] = err_reason[mask] + \"Value_became_NA_after_coerce;\"\n",
    "\n",
    "            if \"PIT Date\" in df.columns:\n",
    "                # Preexisting missing PIT Date\n",
    "                mask = pre_missing_date & post_missing_date\n",
    "                err_reason[mask] = err_reason[mask] + \"PITDate_missing_preexisting;\"\n",
    "                # PIT Date that became missing due to coercion\n",
    "                mask = (~pre_missing_date) & post_missing_date\n",
    "                err_reason[mask] = err_reason[mask] + \"PITDate_became_NA_after_coerce;\"\n",
    "\n",
    "            # Default label for rows failing requirements without a more specific reason\n",
    "            err_reason[(bad_req) & (err_reason == \"\")] = \"Missing_required_fields\"\n",
    "\n",
    "            # Build explicit clean and error masks\n",
    "            error_mask = bad_req\n",
    "            clean_mask = ~error_mask\n",
    "\n",
    "            # Split into clean and error chunks\n",
    "            clean_chunk = df[clean_mask].copy()\n",
    "            error_chunk = df[error_mask].copy()\n",
    "\n",
    "            # Attach diagnostic columns to error rows only\n",
    "            if not error_chunk.empty:\n",
    "                # Trim trailing semicolons from concatenated reasons\n",
    "                error_chunk[\"ErrorReason\"] = err_reason[error_mask].str.rstrip(\";\")\n",
    "                if has_value:\n",
    "                    # Preserve original string representation of Value\n",
    "                    error_chunk[\"OriginalValue\"] = b_co.loc[error_mask, \"Value\"].astype(\"string\")\n",
    "                if \"PIT Date\" in b_co.columns:\n",
    "                    # Preserve original PIT Date as string\n",
    "                    error_chunk[\"OriginalPITDate\"] = b_co.loc[error_mask, \"PIT Date\"].astype(\"string\")\n",
    "\n",
    "        else:\n",
    "            # If no required-field enforcement, all rows are considered clean\n",
    "            clean_chunk = df.copy()\n",
    "            error_chunk = df.iloc[0:0].copy()  # empty DataFrame for errors\n",
    "\n",
    "        # 4) Normalize PIT Date format in both clean and error outputs\n",
    "        for out_df in (clean_chunk, error_chunk):\n",
    "            if not out_df.empty and \"PIT Date\" in out_df.columns:\n",
    "                d = pd.to_datetime(out_df[\"PIT Date\"], errors=\"coerce\", utc=False, format=\"mixed\")\n",
    "                out_df[\"PIT Date\"] = d.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # 5) Write clean and error chunks to their respective files\n",
    "        if not clean_chunk.empty:\n",
    "            _append_df(clean_chunk, clean_write_path, header_flag_attr=\"clean\")\n",
    "            totals[\"rows_clean\"] += len(clean_chunk)\n",
    "\n",
    "        if not error_chunk.empty:\n",
    "            _append_df(error_chunk, errors_path, header_flag_attr=\"err\")\n",
    "            totals[\"rows_error\"] += len(error_chunk)\n",
    "\n",
    "        # Clear the buffer and trigger garbage collection\n",
    "        buffer_rows = []\n",
    "        gc.collect()\n",
    "\n",
    "    # --------- STREAM INPUT & COLLECT MALFORMED + GOOD ROWS ---------\n",
    "\n",
    "    with open(input_path, encoding=\"latin1\") as f_in:\n",
    "        # Read and discard the first line as header (structure derived from schema)\n",
    "        header_line = f_in.readline()\n",
    "        if not header_line:\n",
    "            print(f\"[WARN] Empty input file: {input_path}\")\n",
    "        # Subsequent lines are data lines\n",
    "\n",
    "        for line_no, line in enumerate(f_in, start=2):\n",
    "            # Remove trailing newline character\n",
    "            raw = line.rstrip(\"\\n\")\n",
    "            # Split line into fields using the global output separator\n",
    "            parts = raw.split(OUTPUT_SEP)\n",
    "\n",
    "            # If column count does not match expectation, treat row as malformed\n",
    "            if len(parts) != n_cols:\n",
    "                totals[\"malformed_rows\"] += 1\n",
    "\n",
    "                # Construct a minimal row aligned to our schema, padding missing fields with empty strings\n",
    "                row_dict = {col: (parts[i] if i < len(parts) else \"\") for i, col in enumerate(column_names)}\n",
    "                row_dict[\"ErrorReason\"] = f\"MalformedRow_wrong_column_count_expected_{n_cols}_got_{len(parts)}\"\n",
    "                row_dict[\"RawLine\"] = raw\n",
    "\n",
    "                # Write malformed row directly to the error file\n",
    "                err_df = pd.DataFrame([row_dict])\n",
    "                _append_df(err_df, errors_path, header_flag_attr=\"err\")\n",
    "                totals[\"rows_error\"] += 1\n",
    "                continue\n",
    "\n",
    "            # Well-formed row: add to buffer for further processing\n",
    "            buffer_rows.append(parts)\n",
    "\n",
    "            # If buffer reaches chunk size, process it as a unit\n",
    "            if len(buffer_rows) >= CHUNK_SIZE:\n",
    "                process_buffer()\n",
    "\n",
    "        # Process any remaining rows left in the buffer\n",
    "        process_buffer()\n",
    "\n",
    "    # If local temp storage is used, move the fully written file to its final location\n",
    "    if USE_LOCAL_TMP and clean_write_path.exists():\n",
    "        shutil.move(str(clean_write_path), str(final_path))\n",
    "        print(f\"[clean:{final_basename}] moved final file to {final_path}\")\n",
    "\n",
    "    # ---- Sanity check: ensure all visible rows are accounted for ----\n",
    "    total_visible = totals[\"rows_clean\"] + totals[\"rows_error\"]\n",
    "    if total_visible != (totals[\"rows_in\"] + totals[\"malformed_rows\"]):\n",
    "        print(\n",
    "            \"[WARN] Row mismatch in clean_coerce_and_require: \"\n",
    "            f\"rows_in={totals['rows_in']} + malformed={totals['malformed_rows']} \"\n",
    "            f\"!= clean({totals['rows_clean']}) + error({totals['rows_error']})\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"[OK] All rows accounted for for {final_basename}: \"\n",
    "            f\"clean={totals['rows_clean']}, errors={totals['rows_error']}, \"\n",
    "            f\"malformed={totals['malformed_rows']}\"\n",
    "        )\n",
    "\n",
    "    # Return final clean file path and audit dictionaries\n",
    "    return str(final_path), totals, na_std, na_coerce, trimmed\n",
    "\n",
    "\n",
    "# ---------- Run STEP 2 for all datasets ----------\n",
    "\n",
    "# Clean, coerce, and classify rows for Fundamentals (A_num schema)\n",
    "A_SINGLE_PATH, A_TOTALS, A_NA_STD, A_NA_COERCE, A_TRIM = clean_coerce_and_require(\n",
    "    A_RELEVANT_PATH, schema=\"A_num\", out_dir=Temp_file_path_GO, final_basename=\"Fundamentals_clean\"\n",
    ")\n",
    "\n",
    "# Clean, coerce, and classify rows for Current (B schema)\n",
    "B_SINGLE_PATH, B_TOTALS, B_NA_STD, B_NA_COERCE, B_TRIM = clean_coerce_and_require(\n",
    "    B_RELEVANT_PATH, schema=\"B\",     out_dir=Temp_file_path_GO, final_basename=\"Current_clean\"\n",
    ")\n",
    "\n",
    "# Clean, coerce, and classify rows for Calendar (A_str schema)\n",
    "C_SINGLE_PATH, C_TOTALS, C_NA_STD, C_NA_COERCE, C_TRIM = clean_coerce_and_require(\n",
    "    C_RELEVANT_PATH, schema=\"A_str\", out_dir=Temp_file_path_GO, final_basename=\"Calendar_clean\"\n",
    ")\n",
    "\n",
    "# Clean, coerce, and classify rows for Meta (A_str schema)\n",
    "D_SINGLE_PATH, D_TOTALS, D_NA_STD, D_NA_COERCE, D_TRIM = clean_coerce_and_require(\n",
    "    D_RELEVANT_PATH, schema=\"A_str\", out_dir=Temp_file_path_GO, final_basename=\"Meta_clean\"\n",
    ")\n",
    "\n",
    "# ---------- Audit summary aggregation ----------\n",
    "\n",
    "def _totals_to_rows(ds, totals):\n",
    "    \"\"\"\n",
    "    Convert a totals dictionary into a list of row dicts for the audit summary.\n",
    "    \"\"\"\n",
    "    return [{\"dataset\": ds, \"section\": \"totals\", \"metric\": k, \"value\": v} for k, v in totals.items()]\n",
    "\n",
    "def _dict_to_rows(ds, section, d):\n",
    "    \"\"\"\n",
    "    Convert a generic dictionary into sorted row dicts for the audit summary.\n",
    "    \"\"\"\n",
    "    return [{\"dataset\": ds, \"section\": section, \"metric\": k, \"value\": v} for k, v in sorted((d or {}).items())]\n",
    "\n",
    "# Collect all audit entries for each dataset and category into a single list\n",
    "rows = []\n",
    "rows += _totals_to_rows(\"Fundamentals\", A_TOTALS) \\\n",
    "     + _dict_to_rows(\"Fundamentals\", \"new_NA_from_standardize\", A_NA_STD) \\\n",
    "     + _dict_to_rows(\"Fundamentals\", \"new_NA_from_coercion\", A_NA_COERCE) \\\n",
    "     + _dict_to_rows(\"Fundamentals\", \"string_trim_changes\", A_TRIM)\n",
    "\n",
    "rows += _totals_to_rows(\"Current\", B_TOTALS) \\\n",
    "     + _dict_to_rows(\"Current\", \"new_NA_from_standardize\", B_NA_STD) \\\n",
    "     + _dict_to_rows(\"Current\", \"new_NA_from_coercion\", B_NA_COERCE) \\\n",
    "     + _dict_to_rows(\"Current\", \"string_trim_changes\", B_TRIM)\n",
    "\n",
    "rows += _totals_to_rows(\"Calendar\", C_TOTALS) \\\n",
    "     + _dict_to_rows(\"Calendar\", \"new_NA_from_standardize\", C_NA_STD) \\\n",
    "     + _dict_to_rows(\"Calendar\", \"new_NA_from_coercion\", C_NA_COERCE) \\\n",
    "     + _dict_to_rows(\"Calendar\", \"string_trim_changes\", C_TRIM)\n",
    "\n",
    "rows += _totals_to_rows(\"Meta\", D_TOTALS) \\\n",
    "     + _dict_to_rows(\"Meta\", \"new_NA_from_standardize\", D_NA_STD) \\\n",
    "     + _dict_to_rows(\"Meta\", \"new_NA_from_coercion\", D_NA_COERCE) \\\n",
    "     + _dict_to_rows(\"Meta\", \"string_trim_changes\", D_TRIM)\n",
    "\n",
    "# Build a consolidated audit summary DataFrame with a fixed column order\n",
    "audit_summary_df = pd.DataFrame(rows, columns=[\"dataset\", \"section\", \"metric\", \"value\"])\n",
    "\n",
    "# Persist the audit summary as a CSV in the general overview temp directory\n",
    "AUDIT_SUMMARY_CSV = Path(Temp_file_path_GO) / \"Audit_Summary_all.csv\"\n",
    "audit_summary_df.to_csv(AUDIT_SUMMARY_CSV, index=False)\n",
    "print(\"[OK] Wrote audit summary CSV to:\", AUDIT_SUMMARY_CSV)\n",
    "\n",
    "# Print paths to the clean files for quick reference\n",
    "print(\"Clean files:\")\n",
    "print(\"  -\", A_SINGLE_PATH)\n",
    "print(\"  -\", B_SINGLE_PATH)\n",
    "print(\"  -\", C_SINGLE_PATH)\n",
    "print(\"  -\", D_SINGLE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlB7pqtvkNOJ"
   },
   "source": [
    "### Overview = unique IDs per Year (from Dataset A's FiscalPeriod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13488,
     "status": "ok",
     "timestamp": 1764877747716,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "myEPpO45kQoZ",
    "outputId": "952c1161-3d17-4944-d74b-76533add2605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved overviews:\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Fundamentals_clean_overview.txt\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Fundamentals_firms_per_year.txt\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Current_clean_overview.txt\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Calendar_clean_overview.txt\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Calendar_firms_per_year.txt\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Meta_clean_overview.txt\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Meta_firms_per_year.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell computes dataset-level overviews for all four cleaned datasets:\n",
    "# Fundamentals, Current, Calendar, and Meta.\n",
    "#\n",
    "# Main transformations and derived metrics:\n",
    "#   - For A-like datasets (Fundamentals, Calendar, Meta):\n",
    "#       * Stream the cleaned file in chunks to handle large data efficiently.\n",
    "#       * Track total row count and per-column missing value counts.\n",
    "#       * Collect the set of unique firm IDs and unique (firm, FiscalPeriod)\n",
    "#         pairs, giving a sense of panel structure and coverage.\n",
    "#       * Derive \"firm-year\" counts by extracting year information from\n",
    "#         FiscalPeriod using a regex and deduplicating (ID, Year) combinations.\n",
    "#       * Compute minimum and maximum PIT Date values across the entire file.\n",
    "#       * Compute minimum and maximum FiscalPeriod values across non-missing rows.\n",
    "#       * Aggregate Frequency value counts to understand the distribution of\n",
    "#         reporting frequencies.\n",
    "#       * Return:\n",
    "#           1) a compact overview table of totals, missing counts, date ranges,\n",
    "#              FiscalPeriod ranges, and frequency counts\n",
    "#           2) a per-year table with the number of unique firms per year.\n",
    "#\n",
    "#   - For B-like datasets (Current):\n",
    "#       * Stream the cleaned file in chunks.\n",
    "#       * Track total row count, missing values per column, and unique IDs.\n",
    "#       * Compute the PIT Date range (min/max) across the whole dataset.\n",
    "#       * Return a compact overview table similar to A-like, but without\n",
    "#         frequency and FiscalPeriod information.\n",
    "#\n",
    "#   - All computed overviews are written as pipe-separated text files to the\n",
    "#     general overview temp directory for later inspection and diagnostics.\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "\n",
    "def overview_A_like(clean_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Initialize aggregators and tracking structures for A-like datasets\n",
    "    total_rows = 0\n",
    "    missing = {}\n",
    "    unique_ids = set()\n",
    "    freq_counts = {}\n",
    "    firm_year_pairs = set()\n",
    "    per_year_counts = {}\n",
    "    min_pit, max_pit, min_fp, max_fp = None, None, None, None\n",
    "\n",
    "    # Regex to extract 4-digit year from FiscalPeriod strings\n",
    "    year_re = re.compile(r\"(\\d{4})\")\n",
    "\n",
    "    # Columns expected in A-like cleaned files\n",
    "    usecols = [\"ID\", \"PIT Date\", \"Frequency\", \"FiscalPeriod\", \"ItemCode\", \"Value\"]\n",
    "\n",
    "    # Stream the dataset in chunks to limit memory usage\n",
    "    for ch in pd.read_csv(\n",
    "        clean_path,\n",
    "        sep=OUTPUT_SEP,\n",
    "        usecols=usecols,\n",
    "        dtype=str,\n",
    "        engine=\"c\",\n",
    "        on_bad_lines=\"skip\",\n",
    "        chunksize=1_000_000,\n",
    "        low_memory=False,\n",
    "        encoding=\"latin1\"\n",
    "    ):\n",
    "        # Count total number of rows processed\n",
    "        total_rows += len(ch)\n",
    "\n",
    "        # Accumulate missing-value counts per column for this chunk\n",
    "        m = ch.isna().sum()\n",
    "        for c, v in m.items():\n",
    "            missing[c] = missing.get(c, 0) + int(v)\n",
    "\n",
    "        # Update the set of unique firm IDs (excluding missing values)\n",
    "        unique_ids.update(ch[\"ID\"].dropna().astype(\"string\").unique().tolist())\n",
    "\n",
    "        # If FiscalPeriod is present, derive firm-year structures and coverage\n",
    "        if \"FiscalPeriod\" in ch.columns:\n",
    "            # Build (ID, FiscalPeriod) pairs and add to global set for uniqueness\n",
    "            pairs = ch[[\"ID\", \"FiscalPeriod\"]].dropna().astype(\"string\").drop_duplicates()\n",
    "            for rid, fp in pairs.itertuples(index=False):\n",
    "                firm_year_pairs.add((rid, fp))\n",
    "\n",
    "            # Extract a 4-digit year from FiscalPeriod for firm-year counts\n",
    "            yrs = ch[\"FiscalPeriod\"].astype(\"string\").str.extract(year_re, expand=False)\n",
    "            tmp = pd.DataFrame({\"ID\": ch[\"ID\"].astype(\"string\"), \"Year\": yrs})\n",
    "\n",
    "            # Keep non-missing (ID, Year) combinations and drop duplicates\n",
    "            tmp = tmp.dropna(subset=[\"ID\", \"Year\"]).drop_duplicates(subset=[\"ID\", \"Year\"])\n",
    "\n",
    "            # Count occurrences per year within this chunk\n",
    "            counts = tmp[\"Year\"].value_counts()\n",
    "            for y, cnt in counts.items():\n",
    "                per_year_counts[y] = per_year_counts.get(y, 0) + int(cnt)\n",
    "\n",
    "            # Track minimum and maximum FiscalPeriod (string comparison)\n",
    "            fp_nonnull = ch[\"FiscalPeriod\"].dropna().astype(\"string\")\n",
    "            if len(fp_nonnull):\n",
    "                min_fp = fp_nonnull.min() if min_fp is None else min(min_fp, fp_nonnull.min())\n",
    "                max_fp = fp_nonnull.max() if max_fp is None else max(max_fp, fp_nonnull.max())\n",
    "\n",
    "        # Convert PIT Date to datetime and update global min/max PIT Date\n",
    "        d = pd.to_datetime(ch[\"PIT Date\"], errors=\"coerce\", utc=False, format=\"mixed\")\n",
    "        if d.notna().any():\n",
    "            dmin, dmax = d.min(skipna=True), d.max(skipna=True)\n",
    "            if pd.notna(dmin):\n",
    "                min_pit = dmin if min_pit is None else min(min_pit, dmin)\n",
    "            if pd.notna(dmax):\n",
    "                max_pit = dmax if max_pit is None else max(max_pit, dmax)\n",
    "\n",
    "        # Count Frequency occurrences in this chunk and update global frequency counts\n",
    "        fq = ch[\"Frequency\"].astype(\"string\").dropna()\n",
    "        vc = fq.value_counts()\n",
    "        for k, v in vc.items():\n",
    "            freq_counts[str(k)] = freq_counts.get(str(k), 0) + int(v)\n",
    "\n",
    "        # Explicitly delete chunk reference to help garbage collection\n",
    "        del ch\n",
    "\n",
    "    # Build per-year summary DataFrame (Year vs number of unique IDs)\n",
    "    fpy = (\n",
    "        pd.DataFrame(sorted(per_year_counts.items()), columns=[\"Year\", \"n_unique_IDs\"])\n",
    "        if per_year_counts else\n",
    "        pd.DataFrame(columns=[\"Year\", \"n_unique_IDs\"])\n",
    "    )\n",
    "\n",
    "    # Sum over all per-year firm counts to get a grand total\n",
    "    grand_total_firms_per_year = int(fpy[\"n_unique_IDs\"].sum()) if not fpy.empty else 0\n",
    "\n",
    "    # Assemble summary metrics into a list of rows\n",
    "    rows = []\n",
    "    rows += [\n",
    "        (\"totals\", \"total_rows\", total_rows),\n",
    "        (\"totals\", \"unique_IDs\", len(unique_ids)),\n",
    "        (\"totals\", \"firm_years_unique\", len(firm_year_pairs)),\n",
    "        (\"totals\", \"firms_per_year_SUM\", grand_total_firms_per_year),\n",
    "    ]\n",
    "    rows += [(\"missing\", c, int(v)) for c, v in sorted(missing.items())]\n",
    "    rows += [\n",
    "        (\"dates\", \"PIT_Date_min\", None if min_pit is None else min_pit.strftime(\"%Y-%m-%d\")),\n",
    "        (\"dates\", \"PIT_Date_max\", None if max_pit is None else max_pit.strftime(\"%Y-%m-%d\")),\n",
    "        (\"fiscalperiod\", \"FiscalPeriod_min\", min_fp),\n",
    "        (\"fiscalperiod\", \"FiscalPeriod_max\", max_fp),\n",
    "    ]\n",
    "    rows += [(\"frequency_rows\", k, int(v)) for k, v in sorted(freq_counts.items())]\n",
    "\n",
    "    # Return overview table and per-year firm-count table\n",
    "    return pd.DataFrame(rows, columns=[\"section\", \"metric\", \"value\"]), fpy\n",
    "\n",
    "\n",
    "def overview_B_like(clean_path: str) -> pd.DataFrame:\n",
    "    # Initialize aggregators and tracking structures for B-like datasets\n",
    "    total_rows = 0\n",
    "    missing = {}\n",
    "    unique_ids = set()\n",
    "    min_pit, max_pit = None, None\n",
    "\n",
    "    # Columns expected in B-like cleaned files\n",
    "    usecols = [\"ID\", \"PIT Date\", \"ItemCode\", \"Value\"]\n",
    "\n",
    "    # Stream the dataset in chunks to limit memory usage\n",
    "    for ch in pd.read_csv(\n",
    "        clean_path,\n",
    "        sep=OUTPUT_SEP,\n",
    "        usecols=usecols,\n",
    "        dtype=str,\n",
    "        engine=\"c\",\n",
    "        on_bad_lines=\"skip\",\n",
    "        chunksize=1_000_000,\n",
    "        low_memory=False,\n",
    "        encoding=\"latin1\"\n",
    "    ):\n",
    "        # Count total rows seen in this chunk\n",
    "        total_rows += len(ch)\n",
    "\n",
    "        # Aggregate missing value counts per column\n",
    "        m = ch.isna().sum()\n",
    "        for c, v in m.items():\n",
    "            missing[c] = missing.get(c, 0) + int(v)\n",
    "\n",
    "        # Update unique ID set, ignoring missing IDs\n",
    "        unique_ids.update(ch[\"ID\"].dropna().astype(\"string\").unique().tolist())\n",
    "\n",
    "        # Convert PIT Date and update global min/max PIT Date\n",
    "        d = pd.to_datetime(ch[\"PIT Date\"], errors=\"coerce\", utc=False, format=\"mixed\")\n",
    "        if d.notna().any():\n",
    "            dmin, dmax = d.min(skipna=True), d.max(skipna=True)\n",
    "            if pd.notna(dmin):\n",
    "                min_pit = dmin if min_pit is None else min(min_pit, dmin)\n",
    "            if pd.notna(dmax):\n",
    "                max_pit = dmax if max_pit is None else max(max_pit, dmax)\n",
    "\n",
    "        # Explicitly delete chunk reference to help garbage collection\n",
    "        del ch\n",
    "\n",
    "    # Build summary rows for B-like overview\n",
    "    rows = [\n",
    "        (\"totals\", \"total_rows\", total_rows),\n",
    "        (\"totals\", \"unique_IDs\", len(unique_ids)),\n",
    "        (\"dates\", \"PIT_Date_min\", None if min_pit is None else min_pit.strftime(\"%Y-%m-%d\")),\n",
    "        (\"dates\", \"PIT_Date_max\", None if max_pit is None else max_pit.strftime(\"%Y-%m-%d\")),\n",
    "    ]\n",
    "    rows += [(\"missing\", c, int(v)) for c, v in sorted(missing.items())]\n",
    "\n",
    "    # Return overview table as DataFrame\n",
    "    return pd.DataFrame(rows, columns=[\"section\", \"metric\", \"value\"])\n",
    "\n",
    "\n",
    "# Compute overviews for each cleaned dataset using the paths from the cleaning step\n",
    "over_A_df, over_A_fpy = overview_A_like(A_SINGLE_PATH)        # Fundamentals_clean (A-like schema)\n",
    "over_B_df              = overview_B_like(B_SINGLE_PATH)       # Current_clean (B-like schema)\n",
    "over_C_df, over_C_fpy = overview_A_like(C_SINGLE_PATH)        # Calendar_clean (A-like schema)\n",
    "over_D_df, over_D_fpy = overview_A_like(D_SINGLE_PATH)        # Meta_clean (A-like schema)\n",
    "\n",
    "\n",
    "# Construct output paths for the overview and firms-per-year files\n",
    "A_OV_PATH  = Path(Temp_file_path_GO) / \"Fundamentals_clean_overview.txt\"\n",
    "A_FPY_PATH = Path(Temp_file_path_GO) / \"Fundamentals_firms_per_year.txt\"\n",
    "B_OV_PATH  = Path(Temp_file_path_GO) / \"Current_clean_overview.txt\"\n",
    "C_OV_PATH  = Path(Temp_file_path_GO) / \"Calendar_clean_overview.txt\"\n",
    "C_FPY_PATH = Path(Temp_file_path_GO) / \"Calendar_firms_per_year.txt\"\n",
    "D_OV_PATH  = Path(Temp_file_path_GO) / \"Meta_clean_overview.txt\"\n",
    "D_FPY_PATH = Path(Temp_file_path_GO) / \"Meta_firms_per_year.txt\"\n",
    "\n",
    "# Save all overviews to pipe-separated text files for downstream inspection\n",
    "over_A_df.to_csv(A_OV_PATH, index=False, sep=OUTPUT_SEP)\n",
    "over_A_fpy.to_csv(A_FPY_PATH, index=False, sep=OUTPUT_SEP)\n",
    "over_B_df.to_csv(B_OV_PATH, index=False, sep=OUTPUT_SEP)\n",
    "over_C_df.to_csv(C_OV_PATH, index=False, sep=OUTPUT_SEP)\n",
    "over_C_fpy.to_csv(C_FPY_PATH, index=False, sep=OUTPUT_SEP)\n",
    "over_D_df.to_csv(D_OV_PATH, index=False, sep=OUTPUT_SEP)\n",
    "over_D_fpy.to_csv(D_FPY_PATH, index=False, sep=OUTPUT_SEP)\n",
    "\n",
    "# Print a short summary of where the overview files were written\n",
    "print(\"[OK] Saved overviews:\")\n",
    "print(\"  -\", A_OV_PATH)\n",
    "print(\"  -\", A_FPY_PATH)\n",
    "print(\"  -\", B_OV_PATH)\n",
    "print(\"  -\", C_OV_PATH)\n",
    "print(\"  -\", C_FPY_PATH)\n",
    "print(\"  -\", D_OV_PATH)\n",
    "print(\"  -\", D_FPY_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL6aDeQTkZ2G"
   },
   "source": [
    "### Summaries grouped by (ID, ItemCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9979,
     "status": "ok",
     "timestamp": 1764877757691,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "MjBJDRXfkaKP",
    "outputId": "a397f701-5cb7-4fb6-ac96-7090c9518113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved grouped summaries:\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Fundamentals_grouped_by_ID_ItemCode.txt\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Current_grouped_by_ID_ItemCode.txt\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Calendar_grouped_by_ID_ItemCode.txt\n",
      "  - /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Meta_grouped_by_ID_ItemCode.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell builds grouped summaries at the (ID, ItemCode) level for each of the\n",
    "# four cleaned datasets (Fundamentals, Current, Calendar, Meta).\n",
    "#\n",
    "# Main filtering and transformation logic:\n",
    "#   - Reads each cleaned file in chunks to handle large data efficiently.\n",
    "#   - For every chunk:\n",
    "#       * Parses PIT Date into datetime for temporal aggregation.\n",
    "#       * Converts ItemCode to a nullable integer type for stable grouping.\n",
    "#       * Depending on the schema:\n",
    "#           - \"A_num\" (numeric Value, e.g. Fundamentals):\n",
    "#               · Converts Value to numeric and groups by (ID, ItemCode).\n",
    "#               · Computes per-group statistics:\n",
    "#                   n_rows, n_valid_value, min/max/mean/std of Value,\n",
    "#                   first_date and last_date (min/max PIT Date).\n",
    "#           - \"A_str\" / \"B\" (string Value, e.g. Current, Calendar, Meta):\n",
    "#               · Treats Value as string and groups by (ID, ItemCode).\n",
    "#               · Computes per-group statistics:\n",
    "#                   n_rows, n_nonnull_value, first_date, last_date.\n",
    "#       * Each chunk-level grouped result is appended to a list.\n",
    "#   - After all chunks:\n",
    "#       * Concatenates all chunk-level group results.\n",
    "#       * Re-aggregates at (ID, ItemCode) to combine across chunks:\n",
    "#           - Numeric schema (\"A_num\"): sums counts, recomputes global min/max\n",
    "#             for value and dates, and averages mean/std across chunks.\n",
    "#           - String schema (\"A_str\"/\"B\"): sums counts and recomputes min/max dates.\n",
    "#       * Normalizes first_date and last_date into 'YYYY-MM-DD' string format.\n",
    "#   - Finally, it runs this summarization for all four datasets and writes the\n",
    "#     grouped summaries to pipe-separated text files, each containing one row\n",
    "#     per (ID, ItemCode) combination with the corresponding aggregation metrics.\n",
    "# =============================================================================\n",
    "\n",
    "def summarize_from_clean(single_path: str, schema: str) -> pd.DataFrame:\n",
    "    # List to collect per-chunk grouped DataFrames\n",
    "    frames = []\n",
    "\n",
    "    # Columns required from the clean file for aggregation\n",
    "    usecols = [\"ID\", \"ItemCode\", \"Value\", \"PIT Date\"]\n",
    "\n",
    "    # Stream the cleaned file in chunks to control memory usage\n",
    "    for ch in pd.read_csv(\n",
    "        single_path,\n",
    "        sep=OUTPUT_SEP,\n",
    "        usecols=usecols,\n",
    "        dtype=str,\n",
    "        engine=\"c\",\n",
    "        on_bad_lines=\"skip\",\n",
    "        chunksize=1_000_000,\n",
    "        low_memory=False,\n",
    "        encoding=\"latin1\"\n",
    "    ):\n",
    "        # Parse PIT Date column to datetime for temporal aggregations\n",
    "        d = pd.to_datetime(ch[\"PIT Date\"], errors=\"coerce\", utc=False, format=\"mixed\")\n",
    "\n",
    "        if schema in (\"A_num\",):\n",
    "            # For numeric schema: convert Value to numeric for statistical aggregation\n",
    "            v = pd.to_numeric(ch[\"Value\"], errors=\"coerce\")\n",
    "\n",
    "            # Build a DataFrame with converted columns for grouping\n",
    "            g = (\n",
    "                pd.DataFrame({\n",
    "                    \"ID\": ch[\"ID\"].astype(\"string\"),\n",
    "                    \"ItemCode\": pd.to_numeric(ch[\"ItemCode\"], errors=\"coerce\").astype(\"Int64\"),\n",
    "                    \"Value\": v,\n",
    "                    \"PIT Date\": d,\n",
    "                })\n",
    "                # Group by (ID, ItemCode), keeping rows with possible missing keys\n",
    "                .groupby([\"ID\", \"ItemCode\"], dropna=False)\n",
    "                # Compute per-group numeric statistics and date range\n",
    "                .agg(\n",
    "                    n_rows=(\"Value\", \"size\"),         # Total rows in group\n",
    "                    n_valid_value=(\"Value\", \"count\"), # Number of non-NA Value entries\n",
    "                    min_value=(\"Value\", \"min\"),       # Minimum Value\n",
    "                    max_value=(\"Value\", \"max\"),       # Maximum Value\n",
    "                    mean_value=(\"Value\", \"mean\"),     # Mean Value\n",
    "                    std_value=(\"Value\", \"std\"),       # Standard deviation of Value\n",
    "                    first_date=(\"PIT Date\", \"min\"),   # Earliest PIT Date\n",
    "                    last_date=(\"PIT Date\", \"max\"),    # Latest PIT Date\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "        else:\n",
    "            # For string schema: keep Value as string, focus on counts and date range\n",
    "            g = (\n",
    "                pd.DataFrame({\n",
    "                    \"ID\": ch[\"ID\"].astype(\"string\"),\n",
    "                    \"ItemCode\": pd.to_numeric(ch[\"ItemCode\"], errors=\"coerce\").astype(\"Int64\"),\n",
    "                    \"Value\": ch[\"Value\"].astype(\"string\"),\n",
    "                    \"PIT Date\": d,\n",
    "                })\n",
    "                # Group by (ID, ItemCode), keeping rows with possible missing keys\n",
    "                .groupby([\"ID\", \"ItemCode\"], dropna=False)\n",
    "                # Compute per-group counts and date range\n",
    "                .agg(\n",
    "                    n_rows=(\"Value\", \"size\"),          # Total rows in group\n",
    "                    n_nonnull_value=(\"Value\", \"count\"),# Number of non-NA Value entries\n",
    "                    first_date=(\"PIT Date\", \"min\"),    # Earliest PIT Date\n",
    "                    last_date=(\"PIT Date\", \"max\"),     # Latest PIT Date\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "        # Append per-chunk grouped result to the list\n",
    "        frames.append(g)\n",
    "        # Clean up chunk-level objects to free memory\n",
    "        del ch, g\n",
    "\n",
    "    # If no frames were produced (e.g., empty file), return an empty structure\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=[\"ID\", \"ItemCode\"])\n",
    "\n",
    "    # Concatenate per-chunk grouped DataFrames into a single DataFrame\n",
    "    allg = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    if schema in (\"A_num\",):\n",
    "        # For numeric schema, aggregate again across chunks at (ID, ItemCode) level\n",
    "        out = (\n",
    "            allg.groupby([\"ID\", \"ItemCode\"], dropna=False)\n",
    "                .agg(\n",
    "                    n_rows=(\"n_rows\", \"sum\"),              # Sum of rows across chunks\n",
    "                    n_valid_value=(\"n_valid_value\", \"sum\"),# Sum of valid Value counts\n",
    "                    min_value=(\"min_value\", \"min\"),        # Global minimum Value\n",
    "                    max_value=(\"max_value\", \"max\"),        # Global maximum Value\n",
    "                    first_date=(\"first_date\", \"min\"),      # Earliest observed PIT Date\n",
    "                    last_date=(\"last_date\", \"max\"),        # Latest observed PIT Date\n",
    "                    mean_value=(\"mean_value\", \"mean\"),     # Average of chunk-level means\n",
    "                    std_value=(\"std_value\", \"mean\"),       # Average of chunk-level std devs\n",
    "                )\n",
    "                .reset_index()\n",
    "        )\n",
    "    else:\n",
    "        # For string schema, aggregate counts and date ranges across chunks\n",
    "        out = (\n",
    "            allg.groupby([\"ID\", \"ItemCode\"], dropna=False)\n",
    "                .agg(\n",
    "                    n_rows=(\"n_rows\", \"sum\"),                  # Sum of rows across chunks\n",
    "                    n_nonnull_value=(\"n_nonnull_value\", \"sum\"),# Sum of non-null Value counts\n",
    "                    first_date=(\"first_date\", \"min\"),          # Earliest observed PIT Date\n",
    "                    last_date=(\"last_date\", \"max\"),            # Latest observed PIT Date\n",
    "                )\n",
    "                .reset_index()\n",
    "        )\n",
    "\n",
    "    # Normalize first_date and last_date to 'YYYY-MM-DD' string format if present\n",
    "    if \"first_date\" in out.columns:\n",
    "        out[\"first_date\"] = pd.to_datetime(out[\"first_date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "    if \"last_date\" in out.columns:\n",
    "        out[\"last_date\"] = pd.to_datetime(out[\"last_date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Return the final grouped summary DataFrame\n",
    "    return out\n",
    "\n",
    "\n",
    "# Build grouped summaries for all four datasets using the cleaned file paths\n",
    "groupA = summarize_from_clean(A_SINGLE_PATH, schema=\"A_num\")  # Fundamentals (numeric Value)\n",
    "groupB = summarize_from_clean(B_SINGLE_PATH, schema=\"B\")      # Current (string Value)\n",
    "groupC = summarize_from_clean(C_SINGLE_PATH, schema=\"A_str\")  # Calendar (string Value)\n",
    "groupD = summarize_from_clean(D_SINGLE_PATH, schema=\"A_str\")  # Meta (string Value)\n",
    "\n",
    "# Define output paths for the grouped summaries\n",
    "outA = Path(Temp_file_path_GO) / \"Fundamentals_grouped_by_ID_ItemCode.txt\"\n",
    "outB = Path(Temp_file_path_GO) / \"Current_grouped_by_ID_ItemCode.txt\"\n",
    "outC = Path(Temp_file_path_GO) / \"Calendar_grouped_by_ID_ItemCode.txt\"\n",
    "outD = Path(Temp_file_path_GO) / \"Meta_grouped_by_ID_ItemCode.txt\"\n",
    "\n",
    "# Write grouped summaries to pipe-separated text files with Unix line endings\n",
    "groupA.to_csv(outA, index=False, sep=OUTPUT_SEP, lineterminator=\"\\n\")\n",
    "groupB.to_csv(outB, index=False, sep=OUTPUT_SEP, lineterminator=\"\\n\")\n",
    "groupC.to_csv(outC, index=False, sep=OUTPUT_SEP, lineterminator=\"\\n\")\n",
    "groupD.to_csv(outD, index=False, sep=OUTPUT_SEP, lineterminator=\"\\n\")\n",
    "\n",
    "# Print a short status report listing the generated summary files\n",
    "print(\"[OK] Saved grouped summaries:\")\n",
    "print(\"  -\", outA)\n",
    "print(\"  -\", outB)\n",
    "print(\"  -\", outC)\n",
    "print(\"  -\", outD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXiX0-HIrUoe"
   },
   "source": [
    "### Save Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16352,
     "status": "ok",
     "timestamp": 1764877774057,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "HDPRGdALrdiJ",
    "outputId": "5f57a70b-c9f4-4dc7-aa83-fbbf0b810dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[subsets:Fundamentals] reading -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Fundamentals_clean.txt\n",
      "[subsets:Fundamentals] chunk=1  total_rows_in_clean=1,000,000  total_rows_in_subsets=1,000,000\n",
      "[subsets:Fundamentals] chunk=2  total_rows_in_clean=2,000,000  total_rows_in_subsets=2,000,000\n",
      "[subsets:Fundamentals] chunk=3  total_rows_in_clean=3,000,000  total_rows_in_subsets=3,000,000\n",
      "[subsets:Fundamentals] chunk=4  total_rows_in_clean=4,000,000  total_rows_in_subsets=4,000,000\n",
      "[subsets:Fundamentals] chunk=5  total_rows_in_clean=5,000,000  total_rows_in_subsets=5,000,000\n",
      "[subsets:Fundamentals] chunk=6  total_rows_in_clean=6,000,000  total_rows_in_subsets=6,000,000\n",
      "[subsets:Fundamentals] chunk=7  total_rows_in_clean=7,000,000  total_rows_in_subsets=7,000,000\n",
      "[subsets:Fundamentals] chunk=8  total_rows_in_clean=8,000,000  total_rows_in_subsets=8,000,000\n",
      "[subsets:Fundamentals] chunk=9  total_rows_in_clean=9,000,000  total_rows_in_subsets=9,000,000\n",
      "[subsets:Fundamentals] chunk=10  total_rows_in_clean=10,000,000  total_rows_in_subsets=10,000,000\n",
      "[subsets:Fundamentals] chunk=11  total_rows_in_clean=11,000,000  total_rows_in_subsets=11,000,000\n",
      "[subsets:Fundamentals] chunk=12  total_rows_in_clean=12,000,000  total_rows_in_subsets=12,000,000\n",
      "[subsets:Fundamentals] chunk=13  total_rows_in_clean=13,000,000  total_rows_in_subsets=13,000,000\n",
      "[subsets:Fundamentals] chunk=14  total_rows_in_clean=14,000,000  total_rows_in_subsets=14,000,000\n",
      "[subsets:Fundamentals] chunk=15  total_rows_in_clean=15,000,000  total_rows_in_subsets=15,000,000\n",
      "[subsets:Fundamentals] chunk=16  total_rows_in_clean=16,000,000  total_rows_in_subsets=16,000,000\n",
      "[subsets:Fundamentals] chunk=17  total_rows_in_clean=17,000,000  total_rows_in_subsets=17,000,000\n",
      "[subsets:Fundamentals] chunk=18  total_rows_in_clean=18,000,000  total_rows_in_subsets=18,000,000\n",
      "[subsets:Fundamentals] chunk=19  total_rows_in_clean=19,000,000  total_rows_in_subsets=19,000,000\n",
      "[subsets:Fundamentals] chunk=20  total_rows_in_clean=20,000,000  total_rows_in_subsets=20,000,000\n",
      "[subsets:Fundamentals] chunk=21  total_rows_in_clean=21,000,000  total_rows_in_subsets=21,000,000\n",
      "[subsets:Fundamentals] chunk=22  total_rows_in_clean=22,000,000  total_rows_in_subsets=22,000,000\n",
      "[subsets:Fundamentals] chunk=23  total_rows_in_clean=23,000,000  total_rows_in_subsets=23,000,000\n",
      "[subsets:Fundamentals] chunk=24  total_rows_in_clean=24,000,000  total_rows_in_subsets=24,000,000\n",
      "[subsets:Fundamentals] chunk=25  total_rows_in_clean=25,000,000  total_rows_in_subsets=25,000,000\n",
      "[subsets:Fundamentals] chunk=26  total_rows_in_clean=26,000,000  total_rows_in_subsets=26,000,000\n",
      "[subsets:Fundamentals] chunk=27  total_rows_in_clean=27,000,000  total_rows_in_subsets=27,000,000\n",
      "[subsets:Fundamentals] chunk=28  total_rows_in_clean=28,000,000  total_rows_in_subsets=28,000,000\n",
      "[subsets:Fundamentals] chunk=29  total_rows_in_clean=29,000,000  total_rows_in_subsets=29,000,000\n",
      "[subsets:Fundamentals] chunk=30  total_rows_in_clean=30,000,000  total_rows_in_subsets=30,000,000\n",
      "[subsets:Fundamentals] chunk=31  total_rows_in_clean=31,000,000  total_rows_in_subsets=31,000,000\n",
      "[subsets:Fundamentals] chunk=32  total_rows_in_clean=32,000,000  total_rows_in_subsets=32,000,000\n",
      "[subsets:Fundamentals] chunk=33  total_rows_in_clean=33,000,000  total_rows_in_subsets=33,000,000\n",
      "[subsets:Fundamentals] chunk=34  total_rows_in_clean=34,000,000  total_rows_in_subsets=34,000,000\n",
      "[subsets:Fundamentals] chunk=35  total_rows_in_clean=35,000,000  total_rows_in_subsets=35,000,000\n",
      "[subsets:Fundamentals] chunk=36  total_rows_in_clean=36,000,000  total_rows_in_subsets=36,000,000\n",
      "[subsets:Fundamentals] chunk=37  total_rows_in_clean=37,000,000  total_rows_in_subsets=37,000,000\n",
      "[subsets:Fundamentals] chunk=38  total_rows_in_clean=38,000,000  total_rows_in_subsets=38,000,000\n",
      "[subsets:Fundamentals] chunk=39  total_rows_in_clean=39,000,000  total_rows_in_subsets=39,000,000\n",
      "[subsets:Fundamentals] chunk=40  total_rows_in_clean=40,000,000  total_rows_in_subsets=40,000,000\n",
      "[subsets:Fundamentals] chunk=41  total_rows_in_clean=41,000,000  total_rows_in_subsets=41,000,000\n",
      "[subsets:Fundamentals] chunk=42  total_rows_in_clean=42,000,000  total_rows_in_subsets=42,000,000\n",
      "[subsets:Fundamentals] chunk=43  total_rows_in_clean=43,000,000  total_rows_in_subsets=43,000,000\n",
      "[subsets:Fundamentals] chunk=44  total_rows_in_clean=44,000,000  total_rows_in_subsets=44,000,000\n",
      "[subsets:Fundamentals] chunk=45  total_rows_in_clean=45,000,000  total_rows_in_subsets=45,000,000\n",
      "[subsets:Fundamentals] chunk=46  total_rows_in_clean=46,000,000  total_rows_in_subsets=46,000,000\n",
      "[subsets:Fundamentals] chunk=47  total_rows_in_clean=47,000,000  total_rows_in_subsets=47,000,000\n",
      "[subsets:Fundamentals] chunk=48  total_rows_in_clean=48,000,000  total_rows_in_subsets=48,000,000\n",
      "[subsets:Fundamentals] chunk=49  total_rows_in_clean=49,000,000  total_rows_in_subsets=49,000,000\n",
      "[subsets:Fundamentals] chunk=50  total_rows_in_clean=50,000,000  total_rows_in_subsets=50,000,000\n",
      "[subsets:Fundamentals] chunk=51  total_rows_in_clean=51,000,000  total_rows_in_subsets=51,000,000\n",
      "[subsets:Fundamentals] chunk=52  total_rows_in_clean=52,000,000  total_rows_in_subsets=52,000,000\n",
      "[subsets:Fundamentals] chunk=53  total_rows_in_clean=53,000,000  total_rows_in_subsets=53,000,000\n",
      "[subsets:Fundamentals] chunk=54  total_rows_in_clean=54,000,000  total_rows_in_subsets=54,000,000\n",
      "[subsets:Fundamentals] chunk=55  total_rows_in_clean=55,000,000  total_rows_in_subsets=55,000,000\n",
      "[subsets:Fundamentals] chunk=56  total_rows_in_clean=56,000,000  total_rows_in_subsets=56,000,000\n",
      "[subsets:Fundamentals] chunk=57  total_rows_in_clean=57,000,000  total_rows_in_subsets=57,000,000\n",
      "[subsets:Fundamentals] chunk=58  total_rows_in_clean=58,000,000  total_rows_in_subsets=58,000,000\n",
      "[subsets:Fundamentals] chunk=59  total_rows_in_clean=59,000,000  total_rows_in_subsets=59,000,000\n",
      "[subsets:Fundamentals] chunk=60  total_rows_in_clean=60,000,000  total_rows_in_subsets=60,000,000\n",
      "[subsets:Fundamentals] chunk=61  total_rows_in_clean=61,000,000  total_rows_in_subsets=61,000,000\n",
      "[subsets:Fundamentals] chunk=62  total_rows_in_clean=62,000,000  total_rows_in_subsets=62,000,000\n",
      "[subsets:Fundamentals] chunk=63  total_rows_in_clean=63,000,000  total_rows_in_subsets=63,000,000\n",
      "[subsets:Fundamentals] chunk=64  total_rows_in_clean=64,000,000  total_rows_in_subsets=64,000,000\n",
      "[subsets:Fundamentals] chunk=65  total_rows_in_clean=65,000,000  total_rows_in_subsets=65,000,000\n",
      "[subsets:Fundamentals] chunk=66  total_rows_in_clean=66,000,000  total_rows_in_subsets=66,000,000\n",
      "[subsets:Fundamentals] chunk=67  total_rows_in_clean=67,000,000  total_rows_in_subsets=67,000,000\n",
      "[subsets:Fundamentals] chunk=68  total_rows_in_clean=68,000,000  total_rows_in_subsets=68,000,000\n",
      "[subsets:Fundamentals] chunk=69  total_rows_in_clean=69,000,000  total_rows_in_subsets=69,000,000\n",
      "[subsets:Fundamentals] chunk=70  total_rows_in_clean=70,000,000  total_rows_in_subsets=70,000,000\n",
      "[subsets:Fundamentals] chunk=71  total_rows_in_clean=71,000,000  total_rows_in_subsets=71,000,000\n",
      "[subsets:Fundamentals] chunk=72  total_rows_in_clean=72,000,000  total_rows_in_subsets=72,000,000\n",
      "[subsets:Fundamentals] chunk=73  total_rows_in_clean=73,000,000  total_rows_in_subsets=73,000,000\n",
      "[subsets:Fundamentals] chunk=74  total_rows_in_clean=74,000,000  total_rows_in_subsets=74,000,000\n",
      "[subsets:Fundamentals] chunk=75  total_rows_in_clean=75,000,000  total_rows_in_subsets=75,000,000\n",
      "[subsets:Fundamentals] chunk=76  total_rows_in_clean=76,000,000  total_rows_in_subsets=76,000,000\n",
      "[subsets:Fundamentals] chunk=77  total_rows_in_clean=77,000,000  total_rows_in_subsets=77,000,000\n",
      "[subsets:Fundamentals] chunk=78  total_rows_in_clean=78,000,000  total_rows_in_subsets=78,000,000\n",
      "[subsets:Fundamentals] chunk=79  total_rows_in_clean=79,000,000  total_rows_in_subsets=79,000,000\n",
      "[subsets:Fundamentals] chunk=80  total_rows_in_clean=80,000,000  total_rows_in_subsets=80,000,000\n",
      "[subsets:Fundamentals] chunk=81  total_rows_in_clean=81,000,000  total_rows_in_subsets=81,000,000\n",
      "[subsets:Fundamentals] chunk=82  total_rows_in_clean=82,000,000  total_rows_in_subsets=82,000,000\n",
      "[subsets:Fundamentals] chunk=83  total_rows_in_clean=83,000,000  total_rows_in_subsets=83,000,000\n",
      "[subsets:Fundamentals] chunk=84  total_rows_in_clean=84,000,000  total_rows_in_subsets=84,000,000\n",
      "[subsets:Fundamentals] chunk=85  total_rows_in_clean=85,000,000  total_rows_in_subsets=85,000,000\n",
      "[subsets:Fundamentals] chunk=86  total_rows_in_clean=86,000,000  total_rows_in_subsets=86,000,000\n",
      "[subsets:Fundamentals] chunk=87  total_rows_in_clean=87,000,000  total_rows_in_subsets=87,000,000\n",
      "[subsets:Fundamentals] chunk=88  total_rows_in_clean=88,000,000  total_rows_in_subsets=88,000,000\n",
      "[subsets:Fundamentals] chunk=89  total_rows_in_clean=89,000,000  total_rows_in_subsets=89,000,000\n",
      "[subsets:Fundamentals] chunk=90  total_rows_in_clean=90,000,000  total_rows_in_subsets=90,000,000\n",
      "[subsets:Fundamentals] chunk=91  total_rows_in_clean=91,000,000  total_rows_in_subsets=91,000,000\n",
      "[subsets:Fundamentals] chunk=92  total_rows_in_clean=92,000,000  total_rows_in_subsets=92,000,000\n",
      "[subsets:Fundamentals] chunk=93  total_rows_in_clean=93,000,000  total_rows_in_subsets=93,000,000\n",
      "[subsets:Fundamentals] chunk=94  total_rows_in_clean=94,000,000  total_rows_in_subsets=94,000,000\n",
      "[subsets:Fundamentals] chunk=95  total_rows_in_clean=95,000,000  total_rows_in_subsets=95,000,000\n",
      "[subsets:Fundamentals] chunk=96  total_rows_in_clean=96,000,000  total_rows_in_subsets=96,000,000\n",
      "[subsets:Fundamentals] chunk=97  total_rows_in_clean=97,000,000  total_rows_in_subsets=97,000,000\n",
      "[subsets:Fundamentals] chunk=98  total_rows_in_clean=98,000,000  total_rows_in_subsets=98,000,000\n",
      "[subsets:Fundamentals] chunk=99  total_rows_in_clean=99,000,000  total_rows_in_subsets=99,000,000\n",
      "[subsets:Fundamentals] chunk=100  total_rows_in_clean=100,000,000  total_rows_in_subsets=100,000,000\n",
      "[subsets:Fundamentals] chunk=101  total_rows_in_clean=101,000,000  total_rows_in_subsets=101,000,000\n",
      "[subsets:Fundamentals] chunk=102  total_rows_in_clean=102,000,000  total_rows_in_subsets=102,000,000\n",
      "[subsets:Fundamentals] chunk=103  total_rows_in_clean=103,000,000  total_rows_in_subsets=103,000,000\n",
      "[subsets:Fundamentals] chunk=104  total_rows_in_clean=104,000,000  total_rows_in_subsets=104,000,000\n",
      "[subsets:Fundamentals] chunk=105  total_rows_in_clean=105,000,000  total_rows_in_subsets=105,000,000\n",
      "[subsets:Fundamentals] chunk=106  total_rows_in_clean=106,000,000  total_rows_in_subsets=106,000,000\n",
      "[subsets:Fundamentals] chunk=107  total_rows_in_clean=107,000,000  total_rows_in_subsets=107,000,000\n",
      "[subsets:Fundamentals] chunk=108  total_rows_in_clean=108,000,000  total_rows_in_subsets=108,000,000\n",
      "[subsets:Fundamentals] chunk=109  total_rows_in_clean=109,000,000  total_rows_in_subsets=109,000,000\n",
      "[subsets:Fundamentals] chunk=110  total_rows_in_clean=110,000,000  total_rows_in_subsets=110,000,000\n",
      "[subsets:Fundamentals] chunk=111  total_rows_in_clean=111,000,000  total_rows_in_subsets=111,000,000\n",
      "[subsets:Fundamentals] chunk=112  total_rows_in_clean=112,000,000  total_rows_in_subsets=112,000,000\n",
      "[subsets:Fundamentals] chunk=113  total_rows_in_clean=113,000,000  total_rows_in_subsets=113,000,000\n",
      "[subsets:Fundamentals] chunk=114  total_rows_in_clean=114,000,000  total_rows_in_subsets=114,000,000\n",
      "[subsets:Fundamentals] chunk=115  total_rows_in_clean=115,000,000  total_rows_in_subsets=115,000,000\n",
      "[subsets:Fundamentals] chunk=116  total_rows_in_clean=116,000,000  total_rows_in_subsets=116,000,000\n",
      "[subsets:Fundamentals] chunk=117  total_rows_in_clean=117,000,000  total_rows_in_subsets=117,000,000\n",
      "[subsets:Fundamentals] chunk=118  total_rows_in_clean=118,000,000  total_rows_in_subsets=118,000,000\n",
      "[subsets:Fundamentals] chunk=119  total_rows_in_clean=119,000,000  total_rows_in_subsets=119,000,000\n",
      "[subsets:Fundamentals] chunk=120  total_rows_in_clean=120,000,000  total_rows_in_subsets=120,000,000\n",
      "[subsets:Fundamentals] chunk=121  total_rows_in_clean=121,000,000  total_rows_in_subsets=121,000,000\n",
      "[subsets:Fundamentals] chunk=122  total_rows_in_clean=122,000,000  total_rows_in_subsets=122,000,000\n",
      "[subsets:Fundamentals] chunk=123  total_rows_in_clean=123,000,000  total_rows_in_subsets=123,000,000\n",
      "[subsets:Fundamentals] chunk=124  total_rows_in_clean=124,000,000  total_rows_in_subsets=124,000,000\n",
      "[subsets:Fundamentals] chunk=125  total_rows_in_clean=125,000,000  total_rows_in_subsets=125,000,000\n",
      "[subsets:Fundamentals] chunk=126  total_rows_in_clean=126,000,000  total_rows_in_subsets=126,000,000\n",
      "[subsets:Fundamentals] chunk=127  total_rows_in_clean=127,000,000  total_rows_in_subsets=127,000,000\n",
      "[subsets:Fundamentals] chunk=128  total_rows_in_clean=128,000,000  total_rows_in_subsets=128,000,000\n",
      "[subsets:Fundamentals] chunk=129  total_rows_in_clean=129,000,000  total_rows_in_subsets=129,000,000\n",
      "[subsets:Fundamentals] chunk=130  total_rows_in_clean=130,000,000  total_rows_in_subsets=130,000,000\n",
      "[subsets:Fundamentals] chunk=131  total_rows_in_clean=131,000,000  total_rows_in_subsets=131,000,000\n",
      "[subsets:Fundamentals] chunk=132  total_rows_in_clean=132,000,000  total_rows_in_subsets=132,000,000\n",
      "[subsets:Fundamentals] chunk=133  total_rows_in_clean=133,000,000  total_rows_in_subsets=133,000,000\n",
      "[subsets:Fundamentals] chunk=134  total_rows_in_clean=134,000,000  total_rows_in_subsets=134,000,000\n",
      "[subsets:Fundamentals] chunk=135  total_rows_in_clean=135,000,000  total_rows_in_subsets=135,000,000\n",
      "[subsets:Fundamentals] chunk=136  total_rows_in_clean=136,000,000  total_rows_in_subsets=136,000,000\n",
      "[subsets:Fundamentals] chunk=137  total_rows_in_clean=137,000,000  total_rows_in_subsets=137,000,000\n",
      "[subsets:Fundamentals] chunk=138  total_rows_in_clean=138,000,000  total_rows_in_subsets=138,000,000\n",
      "[subsets:Fundamentals] chunk=139  total_rows_in_clean=139,000,000  total_rows_in_subsets=139,000,000\n",
      "[subsets:Fundamentals] chunk=140  total_rows_in_clean=140,000,000  total_rows_in_subsets=140,000,000\n",
      "[subsets:Fundamentals] chunk=141  total_rows_in_clean=141,000,000  total_rows_in_subsets=141,000,000\n",
      "[subsets:Fundamentals] chunk=142  total_rows_in_clean=142,000,000  total_rows_in_subsets=142,000,000\n",
      "[subsets:Fundamentals] chunk=143  total_rows_in_clean=143,000,000  total_rows_in_subsets=143,000,000\n",
      "[subsets:Fundamentals] chunk=144  total_rows_in_clean=144,000,000  total_rows_in_subsets=144,000,000\n",
      "[subsets:Fundamentals] chunk=145  total_rows_in_clean=145,000,000  total_rows_in_subsets=145,000,000\n",
      "[subsets:Fundamentals] chunk=146  total_rows_in_clean=146,000,000  total_rows_in_subsets=146,000,000\n",
      "[subsets:Fundamentals] chunk=147  total_rows_in_clean=147,000,000  total_rows_in_subsets=147,000,000\n",
      "[subsets:Fundamentals] chunk=148  total_rows_in_clean=148,000,000  total_rows_in_subsets=148,000,000\n",
      "[subsets:Fundamentals] chunk=149  total_rows_in_clean=149,000,000  total_rows_in_subsets=149,000,000\n",
      "[subsets:Fundamentals] chunk=150  total_rows_in_clean=150,000,000  total_rows_in_subsets=150,000,000\n",
      "[subsets:Fundamentals] chunk=151  total_rows_in_clean=151,000,000  total_rows_in_subsets=151,000,000\n",
      "[subsets:Fundamentals] chunk=152  total_rows_in_clean=152,000,000  total_rows_in_subsets=152,000,000\n",
      "[subsets:Fundamentals] chunk=153  total_rows_in_clean=153,000,000  total_rows_in_subsets=153,000,000\n",
      "[subsets:Fundamentals] chunk=154  total_rows_in_clean=154,000,000  total_rows_in_subsets=154,000,000\n",
      "[subsets:Fundamentals] chunk=155  total_rows_in_clean=155,000,000  total_rows_in_subsets=155,000,000\n",
      "[subsets:Fundamentals] chunk=156  total_rows_in_clean=156,000,000  total_rows_in_subsets=156,000,000\n",
      "[subsets:Fundamentals] chunk=157  total_rows_in_clean=157,000,000  total_rows_in_subsets=157,000,000\n",
      "[subsets:Fundamentals] chunk=158  total_rows_in_clean=158,000,000  total_rows_in_subsets=158,000,000\n",
      "[subsets:Fundamentals] chunk=159  total_rows_in_clean=159,000,000  total_rows_in_subsets=159,000,000\n",
      "[subsets:Fundamentals] chunk=160  total_rows_in_clean=160,000,000  total_rows_in_subsets=160,000,000\n",
      "[subsets:Fundamentals] chunk=161  total_rows_in_clean=161,000,000  total_rows_in_subsets=161,000,000\n",
      "[subsets:Fundamentals] chunk=162  total_rows_in_clean=162,000,000  total_rows_in_subsets=162,000,000\n",
      "[subsets:Fundamentals] chunk=163  total_rows_in_clean=163,000,000  total_rows_in_subsets=163,000,000\n",
      "[subsets:Fundamentals] chunk=164  total_rows_in_clean=164,000,000  total_rows_in_subsets=164,000,000\n",
      "[subsets:Fundamentals] chunk=165  total_rows_in_clean=165,000,000  total_rows_in_subsets=165,000,000\n",
      "[subsets:Fundamentals] chunk=166  total_rows_in_clean=166,000,000  total_rows_in_subsets=166,000,000\n",
      "[subsets:Fundamentals] chunk=167  total_rows_in_clean=167,000,000  total_rows_in_subsets=167,000,000\n",
      "[subsets:Fundamentals] chunk=168  total_rows_in_clean=168,000,000  total_rows_in_subsets=168,000,000\n",
      "[subsets:Fundamentals] chunk=169  total_rows_in_clean=169,000,000  total_rows_in_subsets=169,000,000\n",
      "[subsets:Fundamentals] chunk=170  total_rows_in_clean=170,000,000  total_rows_in_subsets=170,000,000\n",
      "[subsets:Fundamentals] chunk=171  total_rows_in_clean=171,000,000  total_rows_in_subsets=171,000,000\n",
      "[subsets:Fundamentals] chunk=172  total_rows_in_clean=172,000,000  total_rows_in_subsets=172,000,000\n",
      "[subsets:Fundamentals] chunk=173  total_rows_in_clean=173,000,000  total_rows_in_subsets=173,000,000\n",
      "[subsets:Fundamentals] chunk=174  total_rows_in_clean=174,000,000  total_rows_in_subsets=174,000,000\n",
      "[subsets:Fundamentals] chunk=175  total_rows_in_clean=175,000,000  total_rows_in_subsets=175,000,000\n",
      "[subsets:Fundamentals] chunk=176  total_rows_in_clean=176,000,000  total_rows_in_subsets=176,000,000\n",
      "[subsets:Fundamentals] chunk=177  total_rows_in_clean=177,000,000  total_rows_in_subsets=177,000,000\n",
      "[subsets:Fundamentals] chunk=178  total_rows_in_clean=178,000,000  total_rows_in_subsets=178,000,000\n",
      "[subsets:Fundamentals] chunk=179  total_rows_in_clean=179,000,000  total_rows_in_subsets=179,000,000\n",
      "[subsets:Fundamentals] chunk=180  total_rows_in_clean=180,000,000  total_rows_in_subsets=180,000,000\n",
      "[subsets:Fundamentals] chunk=181  total_rows_in_clean=181,000,000  total_rows_in_subsets=181,000,000\n",
      "[subsets:Fundamentals] chunk=182  total_rows_in_clean=182,000,000  total_rows_in_subsets=182,000,000\n",
      "[subsets:Fundamentals] chunk=183  total_rows_in_clean=183,000,000  total_rows_in_subsets=183,000,000\n",
      "[subsets:Fundamentals] chunk=184  total_rows_in_clean=184,000,000  total_rows_in_subsets=184,000,000\n",
      "[subsets:Fundamentals] chunk=185  total_rows_in_clean=185,000,000  total_rows_in_subsets=185,000,000\n",
      "[subsets:Fundamentals] chunk=186  total_rows_in_clean=186,000,000  total_rows_in_subsets=186,000,000\n",
      "[subsets:Fundamentals] chunk=187  total_rows_in_clean=187,000,000  total_rows_in_subsets=187,000,000\n",
      "[subsets:Fundamentals] chunk=188  total_rows_in_clean=188,000,000  total_rows_in_subsets=188,000,000\n",
      "[subsets:Fundamentals] chunk=189  total_rows_in_clean=189,000,000  total_rows_in_subsets=189,000,000\n",
      "[subsets:Fundamentals] chunk=190  total_rows_in_clean=190,000,000  total_rows_in_subsets=190,000,000\n",
      "[subsets:Fundamentals] chunk=191  total_rows_in_clean=191,000,000  total_rows_in_subsets=191,000,000\n",
      "[subsets:Fundamentals] chunk=192  total_rows_in_clean=192,000,000  total_rows_in_subsets=192,000,000\n",
      "[subsets:Fundamentals] chunk=193  total_rows_in_clean=193,000,000  total_rows_in_subsets=193,000,000\n",
      "[subsets:Fundamentals] chunk=194  total_rows_in_clean=194,000,000  total_rows_in_subsets=194,000,000\n",
      "[subsets:Fundamentals] chunk=195  total_rows_in_clean=195,000,000  total_rows_in_subsets=195,000,000\n",
      "[subsets:Fundamentals] chunk=196  total_rows_in_clean=196,000,000  total_rows_in_subsets=196,000,000\n",
      "[subsets:Fundamentals] chunk=197  total_rows_in_clean=197,000,000  total_rows_in_subsets=197,000,000\n",
      "[subsets:Fundamentals] chunk=198  total_rows_in_clean=198,000,000  total_rows_in_subsets=198,000,000\n",
      "[subsets:Fundamentals] chunk=199  total_rows_in_clean=199,000,000  total_rows_in_subsets=199,000,000\n",
      "[subsets:Fundamentals] chunk=200  total_rows_in_clean=200,000,000  total_rows_in_subsets=200,000,000\n",
      "[subsets:Fundamentals] chunk=201  total_rows_in_clean=201,000,000  total_rows_in_subsets=201,000,000\n",
      "[subsets:Fundamentals] chunk=202  total_rows_in_clean=202,000,000  total_rows_in_subsets=202,000,000\n",
      "[subsets:Fundamentals] chunk=203  total_rows_in_clean=203,000,000  total_rows_in_subsets=203,000,000\n",
      "[subsets:Fundamentals] chunk=204  total_rows_in_clean=204,000,000  total_rows_in_subsets=204,000,000\n",
      "[subsets:Fundamentals] chunk=205  total_rows_in_clean=205,000,000  total_rows_in_subsets=205,000,000\n",
      "[subsets:Fundamentals] chunk=206  total_rows_in_clean=206,000,000  total_rows_in_subsets=206,000,000\n",
      "[subsets:Fundamentals] chunk=207  total_rows_in_clean=207,000,000  total_rows_in_subsets=207,000,000\n",
      "[subsets:Fundamentals] chunk=208  total_rows_in_clean=208,000,000  total_rows_in_subsets=208,000,000\n",
      "[subsets:Fundamentals] chunk=209  total_rows_in_clean=209,000,000  total_rows_in_subsets=209,000,000\n",
      "[subsets:Fundamentals] chunk=210  total_rows_in_clean=210,000,000  total_rows_in_subsets=210,000,000\n",
      "[subsets:Fundamentals] chunk=211  total_rows_in_clean=211,000,000  total_rows_in_subsets=211,000,000\n",
      "[subsets:Fundamentals] chunk=212  total_rows_in_clean=212,000,000  total_rows_in_subsets=212,000,000\n",
      "[subsets:Fundamentals] chunk=213  total_rows_in_clean=213,000,000  total_rows_in_subsets=213,000,000\n",
      "[subsets:Fundamentals] chunk=214  total_rows_in_clean=214,000,000  total_rows_in_subsets=214,000,000\n",
      "[subsets:Fundamentals] chunk=215  total_rows_in_clean=215,000,000  total_rows_in_subsets=215,000,000\n",
      "[subsets:Fundamentals] chunk=216  total_rows_in_clean=216,000,000  total_rows_in_subsets=216,000,000\n",
      "[subsets:Fundamentals] chunk=217  total_rows_in_clean=217,000,000  total_rows_in_subsets=217,000,000\n",
      "[subsets:Fundamentals] chunk=218  total_rows_in_clean=218,000,000  total_rows_in_subsets=218,000,000\n",
      "[subsets:Fundamentals] chunk=219  total_rows_in_clean=219,000,000  total_rows_in_subsets=219,000,000\n",
      "[subsets:Fundamentals] chunk=220  total_rows_in_clean=220,000,000  total_rows_in_subsets=220,000,000\n",
      "[subsets:Fundamentals] chunk=221  total_rows_in_clean=221,000,000  total_rows_in_subsets=221,000,000\n",
      "[subsets:Fundamentals] chunk=222  total_rows_in_clean=222,000,000  total_rows_in_subsets=222,000,000\n",
      "[subsets:Fundamentals] chunk=223  total_rows_in_clean=223,000,000  total_rows_in_subsets=223,000,000\n",
      "[subsets:Fundamentals] chunk=224  total_rows_in_clean=224,000,000  total_rows_in_subsets=224,000,000\n",
      "[subsets:Fundamentals] chunk=225  total_rows_in_clean=224,547,306  total_rows_in_subsets=224,547,306\n",
      "[subsets:Fundamentals] validation: sum(subset rows) == rows in clean file -> OK\n",
      "[subsets:Current] reading -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Current_clean.txt\n",
      "[subsets:Current] chunk=1  total_rows_in_clean=964,684  total_rows_in_subsets=964,684\n",
      "[subsets:Current] validation: sum(subset rows) == rows in clean file -> OK\n",
      "[subsets:Calendar] reading -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Calendar_clean.txt\n",
      "[subsets:Calendar] chunk=1  total_rows_in_clean=1,000,000  total_rows_in_subsets=1,000,000\n",
      "[subsets:Calendar] chunk=2  total_rows_in_clean=2,000,000  total_rows_in_subsets=2,000,000\n",
      "[subsets:Calendar] chunk=3  total_rows_in_clean=3,000,000  total_rows_in_subsets=3,000,000\n",
      "[subsets:Calendar] chunk=4  total_rows_in_clean=4,000,000  total_rows_in_subsets=4,000,000\n",
      "[subsets:Calendar] chunk=5  total_rows_in_clean=5,000,000  total_rows_in_subsets=5,000,000\n",
      "[subsets:Calendar] chunk=6  total_rows_in_clean=6,000,000  total_rows_in_subsets=6,000,000\n",
      "[subsets:Calendar] chunk=7  total_rows_in_clean=7,000,000  total_rows_in_subsets=7,000,000\n",
      "[subsets:Calendar] chunk=8  total_rows_in_clean=8,000,000  total_rows_in_subsets=8,000,000\n",
      "[subsets:Calendar] chunk=9  total_rows_in_clean=8,418,018  total_rows_in_subsets=8,418,018\n",
      "[subsets:Calendar] validation: sum(subset rows) == rows in clean file -> OK\n",
      "[subsets:Meta] reading -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Meta_clean.txt\n",
      "[subsets:Meta] chunk=1  total_rows_in_clean=1,000,000  total_rows_in_subsets=1,000,000\n",
      "[subsets:Meta] chunk=2  total_rows_in_clean=2,000,000  total_rows_in_subsets=2,000,000\n",
      "[subsets:Meta] chunk=3  total_rows_in_clean=3,000,000  total_rows_in_subsets=3,000,000\n",
      "[subsets:Meta] chunk=4  total_rows_in_clean=4,000,000  total_rows_in_subsets=4,000,000\n",
      "[subsets:Meta] chunk=5  total_rows_in_clean=5,000,000  total_rows_in_subsets=5,000,000\n",
      "[subsets:Meta] chunk=6  total_rows_in_clean=6,000,000  total_rows_in_subsets=6,000,000\n",
      "[subsets:Meta] chunk=7  total_rows_in_clean=7,000,000  total_rows_in_subsets=7,000,000\n",
      "[subsets:Meta] chunk=8  total_rows_in_clean=7,162,436  total_rows_in_subsets=7,162,436\n",
      "[subsets:Meta] validation: sum(subset rows) == rows in clean file -> OK\n",
      "\n",
      "Subset build results (flat folder):\n",
      "  Fundamentals: OK\n",
      "  Current     : OK\n",
      "  Calendar    : OK\n",
      "  Meta        : OK\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell creates per-ItemCode subset files from each of the four cleaned\n",
    "# datasets (Fundamentals, Current, Calendar, Meta) and validates that the\n",
    "# subset rows sum back to the original row counts.\n",
    "#\n",
    "# This version overwrites existing subset files on first write within a run,\n",
    "# instead of raising a RuntimeError, when ALLOW_APPEND_ON_EXIST=False.\n",
    "# =============================================================================\n",
    "\n",
    "ALLOW_APPEND_ON_EXIST = False  # Overwrite existing subset files by default\n",
    "\n",
    "subset_dir = Path(Subset_file_path)\n",
    "for p in subset_dir.glob(\"subset_*.txt\"):\n",
    "    os.remove(p)\n",
    "\n",
    "def _format_itemcode_for_filename(code_val) -> str:\n",
    "    \"\"\"\n",
    "    For filenames only:\n",
    "      - If ItemCode is exactly 4 digits -> prefix a '0' (e.g., 2048 -> '02048').\n",
    "      - Otherwise, keep numeric digits as-is.\n",
    "      - Non-numeric codes -> return None (skip).\n",
    "    \"\"\"\n",
    "    if pd.isna(code_val):\n",
    "        return None\n",
    "    try:\n",
    "        s = str(int(code_val))\n",
    "    except Exception:\n",
    "        return None\n",
    "    return \"0\" + s if len(s) == 4 else s\n",
    "\n",
    "\n",
    "def _write_subset(df: pd.DataFrame, out_path: Path, wrote_headers: set):\n",
    "    \"\"\"\n",
    "    Writes df to its subset file.\n",
    "\n",
    "    Behavior:\n",
    "      - First write to this file *in this run*:\n",
    "          * If file exists:\n",
    "              - ALLOW_APPEND_ON_EXIST = False -> OVERWRITE file (mode='w', write header)\n",
    "              - ALLOW_APPEND_ON_EXIST = True  -> APPEND (mode='a', no header)\n",
    "          * If file does not exist:\n",
    "              - Create new file (mode='w', header)\n",
    "      - Later writes in this run:\n",
    "          - Always append without header.\n",
    "    \"\"\"\n",
    "    fname = out_path.name\n",
    "    first_touch = (fname not in wrote_headers)\n",
    "\n",
    "    if first_touch:\n",
    "        if out_path.exists():\n",
    "            if ALLOW_APPEND_ON_EXIST:\n",
    "                mode = \"a\"\n",
    "                write_header = False\n",
    "            else:\n",
    "                # OVERWRITE existing file\n",
    "                mode = \"w\"\n",
    "                write_header = True\n",
    "        else:\n",
    "            mode = \"w\"\n",
    "            write_header = True\n",
    "\n",
    "        wrote_headers.add(fname)\n",
    "\n",
    "    else:\n",
    "        mode = \"a\"\n",
    "        write_header = False\n",
    "\n",
    "    df.to_csv(\n",
    "        out_path,\n",
    "        index=False,\n",
    "        sep=OUTPUT_SEP,\n",
    "        mode=mode,\n",
    "        header=write_header,\n",
    "        lineterminator=\"\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_subsets_for_dataset_flat(clean_path: str, dataset_label: str, schema: str, subset_base_dir: str):\n",
    "    \"\"\"\n",
    "    Builds one file per ItemCode from a clean dataset.\n",
    "    \"\"\"\n",
    "    print(f\"[subsets:{dataset_label}] reading -> {clean_path}\")\n",
    "\n",
    "    subset_dir = Path(subset_base_dir)\n",
    "    subset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if schema in (\"A_num\", \"A_str\"):\n",
    "        usecols = [\"ID\", \"PIT Date\", \"Frequency\", \"FiscalPeriod\", \"ItemCode\", \"Value\"]\n",
    "    else:\n",
    "        usecols = [\"ID\", \"PIT Date\", \"ItemCode\", \"Value\"]\n",
    "\n",
    "    total_rows_in_clean = 0\n",
    "    total_rows_written  = 0\n",
    "    wrote_headers = set()\n",
    "    chunk_idx = 0\n",
    "\n",
    "    for ch in pd.read_csv(\n",
    "        clean_path,\n",
    "        sep=OUTPUT_SEP,\n",
    "        usecols=usecols,\n",
    "        dtype=str,\n",
    "        engine=\"c\",\n",
    "        on_bad_lines=\"skip\",\n",
    "        chunksize=1_000_000,\n",
    "        low_memory=False,\n",
    "        encoding=\"latin1\"\n",
    "    ):\n",
    "        chunk_idx += 1\n",
    "        n_chunk = len(ch)\n",
    "        total_rows_in_clean += n_chunk\n",
    "\n",
    "        if \"PIT Date\" in ch.columns:\n",
    "            d = pd.to_datetime(ch[\"PIT Date\"], errors=\"coerce\", utc=False, format=\"mixed\")\n",
    "            ch = ch.copy()\n",
    "            ch[\"PIT Date\"] = d.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        item_numeric = pd.to_numeric(ch[\"ItemCode\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        ch = ch[item_numeric.notna()].copy()\n",
    "        if ch.empty:\n",
    "            print(f\"[subsets:{dataset_label}] chunk {chunk_idx:,}: no valid ItemCode rows, skipping\")\n",
    "            continue\n",
    "\n",
    "        ch[\"__ic__\"] = item_numeric[item_numeric.notna()]\n",
    "\n",
    "        for ic_val, df_ic in ch.groupby(\"__ic__\", sort=False, dropna=True):\n",
    "            code_str = _format_itemcode_for_filename(ic_val)\n",
    "            if not code_str:\n",
    "                continue\n",
    "\n",
    "            out_path = Path(Subset_file_path) / f\"subset_{code_str}.txt\"\n",
    "\n",
    "            _write_subset(\n",
    "                df_ic.drop(columns=\"__ic__\", errors=\"ignore\"),\n",
    "                out_path,\n",
    "                wrote_headers\n",
    "            )\n",
    "            total_rows_written += len(df_ic)\n",
    "\n",
    "        del ch\n",
    "        gc.collect()\n",
    "\n",
    "        print(\n",
    "            f\"[subsets:{dataset_label}] chunk={chunk_idx:,}  \"\n",
    "            f\"total_rows_in_clean={total_rows_in_clean:,}  \"\n",
    "            f\"total_rows_in_subsets={total_rows_written:,}\"\n",
    "        )\n",
    "\n",
    "    ok = (total_rows_in_clean == total_rows_written)\n",
    "    status = \"OK\" if ok else \"MISMATCH\"\n",
    "    print(f\"[subsets:{dataset_label}] validation: sum(subset rows) == rows in clean file -> {status}\")\n",
    "    if not ok:\n",
    "        print(f\"  rows_in_clean={total_rows_in_clean:,}, rows_in_subsets={total_rows_written:,}\")\n",
    "\n",
    "    return ok\n",
    "\n",
    "\n",
    "# Require that clean file paths exist\n",
    "assert all(name in globals() for name in [\"A_SINGLE_PATH\", \"B_SINGLE_PATH\", \"C_SINGLE_PATH\", \"D_SINGLE_PATH\"]), \\\n",
    "    \"Missing clean file paths A_SINGLE_PATH/B_SINGLE_PATH/C_SINGLE_PATH/D_SINGLE_PATH.\"\n",
    "\n",
    "# Build subsets for all four clean datasets\n",
    "okA = create_subsets_for_dataset_flat(A_SINGLE_PATH, \"Fundamentals\", \"A_num\", Subset_file_path)\n",
    "okB = create_subsets_for_dataset_flat(B_SINGLE_PATH, \"Current\",      \"B\",     Subset_file_path)\n",
    "okC = create_subsets_for_dataset_flat(C_SINGLE_PATH, \"Calendar\",     \"A_str\", Subset_file_path)\n",
    "okD = create_subsets_for_dataset_flat(D_SINGLE_PATH, \"Meta\",         \"A_str\", Subset_file_path)\n",
    "\n",
    "print(\"\\nSubset build results (flat folder):\")\n",
    "print(\"  Fundamentals:\", \"OK\" if okA else \"MISMATCH\")\n",
    "print(\"  Current     :\", \"OK\" if okB else \"MISMATCH\")\n",
    "print(\"  Calendar    :\", \"OK\" if okC else \"MISMATCH\")\n",
    "print(\"  Meta        :\", \"OK\" if okD else \"MISMATCH\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "re_o1kiWkoxp"
   },
   "source": [
    "### Final Check on Removed Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1764877774870,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "fmvTzoADktjR",
    "outputId": "060d6611-6941-418e-fb83-2fe09f85183d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== AUDIT: Fundamentals ===========\n",
      "Relevant : /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/A_relevant_raw.txt\n",
      "Clean    : /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Fundamentals_clean.txt\n",
      "Errors   : /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Fundamentals_clean_errors.txt\n",
      "\n",
      "[A] ROW CHECK (clean + errors vs relevant)\n",
      "Rows Relevant : 229855498\n",
      "Rows Clean    : 224547306\n",
      "Rows Errors   : 5308192\n",
      "Rows Explained: 229855498\n",
      "Difference    : 0\n",
      "[OK] All rows accounted for: clean + errors == relevant\n",
      "\n",
      "[B] UNIQUE 'Value' ENTRIES IN ERRORS\n",
      "Unique non-empty Value entries: 0\n",
      "\n",
      "[C] UNIQUE 'OriginalValue' ENTRIES IN ERRORS\n",
      "Unique non-empty OriginalValue entries: 1\n",
      "  n\n",
      "\n",
      "[D] TOP 20 ITEMCODES BY ERROR ROWS\n",
      "  3273: 253028\n",
      "  2652: 227161\n",
      "  5202: 189514\n",
      "  3051: 182258\n",
      "  3451: 169624\n",
      "  1250: 167469\n",
      "  3251: 165963\n",
      "  1001: 145255\n",
      "  3426: 142845\n",
      "  2256: 141608\n",
      "  3066: 138575\n",
      "  1051: 138149\n",
      "  1551: 136511\n",
      "  1706: 132036\n",
      "  1451: 131602\n",
      "  1151: 128217\n",
      "  2501: 125291\n",
      "  3501: 122574\n",
      "  2999: 121818\n",
      "  3351: 121585\n",
      "\n",
      "[E] TOP 20 COUNTRY CODES BY ERROR ROWS (from ID[2–4])\n",
      "  840: 1312588\n",
      "  392: 404885\n",
      "  410: 359134\n",
      "  124: 288351\n",
      "  156: 262827\n",
      "  356: 236031\n",
      "  826: 186673\n",
      "  643: 183378\n",
      "  760: 173496\n",
      "  458: 154306\n",
      "  704: 112782\n",
      "  344: 111565\n",
      "  036: 111187\n",
      "  752: 104713\n",
      "  702: 87084\n",
      "  796: 77143\n",
      "  280: 76852\n",
      "  250: 68517\n",
      "  376: 50649\n",
      "  380: 49429\n",
      "\n",
      "=========== AUDIT: Current ===========\n",
      "Relevant : /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/B_relevant_raw.txt\n",
      "Clean    : /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Current_clean.txt\n",
      "Errors   : /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Current_clean_errors.txt\n",
      "\n",
      "[A] ROW CHECK (clean + errors vs relevant)\n",
      "Rows Relevant : 964684\n",
      "Rows Clean    : 964684\n",
      "Rows Errors   : 0\n",
      "Rows Explained: 964684\n",
      "Difference    : 0\n",
      "[OK] All rows accounted for: clean + errors == relevant\n",
      "\n",
      "[INFO] No errors found or errors file missing; skipping error-detail analysis.\n",
      "\n",
      "=========== AUDIT: Calendar ===========\n",
      "Relevant : /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/C_relevant_raw.txt\n",
      "Clean    : /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Calendar_clean.txt\n",
      "Errors   : /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Calendar_clean_errors.txt\n",
      "\n",
      "[A] ROW CHECK (clean + errors vs relevant)\n",
      "Rows Relevant : 8418018\n",
      "Rows Clean    : 8418018\n",
      "Rows Errors   : 0\n",
      "Rows Explained: 8418018\n",
      "Difference    : 0\n",
      "[OK] All rows accounted for: clean + errors == relevant\n",
      "\n",
      "[INFO] No errors found or errors file missing; skipping error-detail analysis.\n",
      "\n",
      "=========== AUDIT: Meta ===========\n",
      "Relevant : /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/D_relevant_raw.txt\n",
      "Clean    : /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Meta_clean.txt\n",
      "Errors   : /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/Meta_clean_errors.txt\n",
      "\n",
      "[A] ROW CHECK (clean + errors vs relevant)\n",
      "Rows Relevant : 7162436\n",
      "Rows Clean    : 7162436\n",
      "Rows Errors   : 0\n",
      "Rows Explained: 7162436\n",
      "Difference    : 0\n",
      "[OK] All rows accounted for: clean + errors == relevant\n",
      "\n",
      "[INFO] No errors found or errors file missing; skipping error-detail analysis.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell implements an audit routine for the two-file setup (clean + errors)\n",
    "# of the pipeline. For each of the four datasets (Fundamentals, Current,\n",
    "# Calendar, Meta), it:\n",
    "#\n",
    "#   - Counts data rows in the relevant, clean, and error files (excluding\n",
    "#     header rows) and checks that:\n",
    "#       rows_clean + rows_errors == rows_relevant\n",
    "#     This validates that each row from the \"relevant_raw\" input is either\n",
    "#     cleaned or classified as an error, with no silent drops.\n",
    "#\n",
    "#   - If an errors file exists and contains rows, it scans the error file\n",
    "#     in chunks and derives several diagnostic summaries:\n",
    "#       * The set size of unique 'Value' entries appearing in error rows.\n",
    "#       * The set size of unique 'OriginalValue' entries (if that column\n",
    "#         exists), which captures pre-coercion values.\n",
    "#       * The most frequent ItemCodes among error rows, highlighting which\n",
    "#         items drive most of the issues.\n",
    "#       * The most frequent country codes extracted from positions 2–4 of\n",
    "#         the ID string, indicating which regions have higher error rates.\n",
    "#\n",
    "#   - It prints human-readable summaries (row counts and top offenders) and\n",
    "#     returns a structured dictionary with key results for potential programmatic\n",
    "#     inspection, including row counts, uniqueness counts, and the top-n\n",
    "#     ItemCodes and countries.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# ======================\n",
    "# CONFIG\n",
    "# ======================\n",
    "\n",
    "OUTPUT_SEP = \"|\"           # Delimiter used in the pipeline's text files\n",
    "CHUNK_SIZE = 1_000_000     # Chunk size to safely read large CSVs in memory\n",
    "\n",
    "\n",
    "# ======================\n",
    "# HELPERS\n",
    "# ======================\n",
    "\n",
    "def count_data_rows(path: Path) -> int:\n",
    "    \"\"\"\n",
    "    Count data rows (excluding header). Returns 0 if file doesn't exist.\n",
    "    \"\"\"\n",
    "    # If the file does not exist, there are no data rows\n",
    "    if not path.exists():\n",
    "        return 0\n",
    "\n",
    "    # Count total lines in the file\n",
    "    with open(path, encoding=\"latin1\") as f:\n",
    "        total = sum(1 for _ in f)\n",
    "\n",
    "    # Subtract one for the header line, but never return negative values\n",
    "    return max(total - 1, 0)\n",
    "\n",
    "\n",
    "def get_country_code(id_val: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract country code as chars 2–4 (1-based) from alphanumeric ID.\n",
    "    If ID too short or missing, return ''.\n",
    "    \"\"\"\n",
    "    # Ensure the ID is a string; otherwise, return empty code\n",
    "    if not isinstance(id_val, str):\n",
    "        return \"\"\n",
    "    # If ID has at least 4 characters, return positions 2–4; else return empty\n",
    "    if len(id_val) >= 4:\n",
    "        return id_val[1:4]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def audit_new_pipeline(\n",
    "    temp_dir: str,\n",
    "    relevant_filename: str,\n",
    "    clean_filename: str,\n",
    "    errors_filename: str,\n",
    "    label: str,\n",
    "    top_n: int = 20,\n",
    "):\n",
    "    \"\"\"\n",
    "    For the 2-file setup (clean + errors):\n",
    "\n",
    "    - Check: rows_clean + rows_errors == rows_relevant\n",
    "    - Report:\n",
    "        * Unique 'Value' in errors\n",
    "        * Unique 'OriginalValue' in errors (if present)\n",
    "        * Top ItemCodes by error count\n",
    "        * Top country codes by error count (from ID[2–4])\n",
    "    \"\"\"\n",
    "\n",
    "    # Base directory where all intermediate files of this dataset are stored\n",
    "    base = Path(temp_dir)\n",
    "\n",
    "    # Construct full paths to relevant, clean, and errors files\n",
    "    relevant_path = base / relevant_filename\n",
    "    clean_path    = base / clean_filename\n",
    "    errors_path   = base / errors_filename\n",
    "\n",
    "    # High-level context output\n",
    "    print(f\"\\n=========== AUDIT: {label} ===========\")\n",
    "    print(\"Relevant :\", relevant_path)\n",
    "    print(\"Clean    :\", clean_path)\n",
    "    print(\"Errors   :\", errors_path)\n",
    "\n",
    "    # ----- A) Row check -----\n",
    "    # Count rows in each of the three files, excluding headers\n",
    "    n_rel   = count_data_rows(relevant_path)\n",
    "    n_clean = count_data_rows(clean_path)\n",
    "    n_err   = count_data_rows(errors_path)\n",
    "\n",
    "    print(\"\\n[A] ROW CHECK (clean + errors vs relevant)\")\n",
    "    print(f\"Rows Relevant : {n_rel}\")\n",
    "    print(f\"Rows Clean    : {n_clean}\")\n",
    "    print(f\"Rows Errors   : {n_err}\")\n",
    "\n",
    "    # Total rows explained by clean + error outputs\n",
    "    explained = n_clean + n_err\n",
    "    # Difference between relevant rows and explained rows\n",
    "    diff = n_rel - explained\n",
    "\n",
    "    print(f\"Rows Explained: {explained}\")\n",
    "    print(f\"Difference    : {diff}\")\n",
    "\n",
    "    # Interpret the row-balance condition and warn if mismatch exists\n",
    "    if n_rel > 0 and diff == 0:\n",
    "        print(\"[OK] All rows accounted for: clean + errors == relevant\")\n",
    "    elif n_rel == 0:\n",
    "        print(\"[WARN] Relevant file seems empty or missing.\")\n",
    "    else:\n",
    "        print(\"[WARN] Mismatch: investigate pipeline / filenames.\")\n",
    "\n",
    "    # If there are no error rows or the error file is absent, skip detailed analysis\n",
    "    if not errors_path.exists() or n_err == 0:\n",
    "        print(\"\\n[INFO] No errors found or errors file missing; skipping error-detail analysis.\")\n",
    "        return {\n",
    "            \"rows\": {\n",
    "                \"relevant\": n_rel,\n",
    "                \"clean\": n_clean,\n",
    "                \"errors\": n_err,\n",
    "                \"explained\": explained,\n",
    "                \"diff\": diff,\n",
    "            },\n",
    "            \"error_value_unique_count\": 0,\n",
    "            \"error_original_value_unique_count\": 0,\n",
    "            \"top_itemcodes\": [],\n",
    "            \"top_countries\": [],\n",
    "        }\n",
    "\n",
    "    # ----- B) Scan errors file (chunked) -----\n",
    "    # Sets to capture unique string representations of problematic values\n",
    "    value_uniques = set()\n",
    "    original_value_uniques = set()\n",
    "\n",
    "    # Counters for ItemCodes and country codes observed in error rows\n",
    "    item_counter = Counter()\n",
    "    country_counter = Counter()\n",
    "\n",
    "    # Read the error file in chunks to handle large files\n",
    "    for chunk in pd.read_csv(\n",
    "        errors_path,\n",
    "        sep=OUTPUT_SEP,\n",
    "        dtype=str,\n",
    "        engine=\"c\",\n",
    "        encoding=\"latin1\",\n",
    "        chunksize=CHUNK_SIZE,\n",
    "        on_bad_lines=\"skip\"\n",
    "    ):\n",
    "        # Unique Value entries observed in the error rows\n",
    "        if \"Value\" in chunk.columns:\n",
    "            vals = chunk[\"Value\"].dropna().astype(str)\n",
    "            value_uniques.update(vals.unique())\n",
    "\n",
    "        # Unique OriginalValue entries if column is present (before coercion)\n",
    "        if \"OriginalValue\" in chunk.columns:\n",
    "            ovals = chunk[\"OriginalValue\"].dropna().astype(str)\n",
    "            original_value_uniques.update(ovals.unique())\n",
    "\n",
    "        # ItemCode distribution: count occurrences of each ItemCode in errors\n",
    "        if \"ItemCode\" in chunk.columns:\n",
    "            ic = chunk[\"ItemCode\"].dropna().astype(str)\n",
    "            item_counter.update(ic)\n",
    "\n",
    "        # Country distribution: derive country code from ID[2–4] for each error row\n",
    "        if \"ID\" in chunk.columns:\n",
    "            ids = chunk[\"ID\"].dropna().astype(str)\n",
    "            countries = [get_country_code(x) for x in ids if len(x) >= 4]\n",
    "            country_counter.update(countries)\n",
    "\n",
    "    # ----- C) Print summaries -----\n",
    "\n",
    "    print(\"\\n[B] UNIQUE 'Value' ENTRIES IN ERRORS\")\n",
    "    print(f\"Unique non-empty Value entries: {len(value_uniques)}\")\n",
    "    for v in list(value_uniques)[:20]:\n",
    "        print(f\"  {v}\")\n",
    "\n",
    "    print(\"\\n[C] UNIQUE 'OriginalValue' ENTRIES IN ERRORS\")\n",
    "    print(f\"Unique non-empty OriginalValue entries: {len(original_value_uniques)}\")\n",
    "    for v in list(original_value_uniques)[:20]:\n",
    "        print(f\"  {v}\")\n",
    "\n",
    "    print(f\"\\n[D] TOP {top_n} ITEMCODES BY ERROR ROWS\")\n",
    "    if item_counter:\n",
    "        for item, cnt in item_counter.most_common(top_n):\n",
    "            print(f\"  {item}: {cnt}\")\n",
    "    else:\n",
    "        print(\"  No ItemCode column or no error rows with ItemCode.\")\n",
    "\n",
    "    print(f\"\\n[E] TOP {top_n} COUNTRY CODES BY ERROR ROWS (from ID[2–4])\")\n",
    "    if country_counter:\n",
    "        for cc, cnt in country_counter.most_common(top_n):\n",
    "            print(f\"  {cc}: {cnt}\")\n",
    "    else:\n",
    "        print(\"  No valid country codes derived from ID in errors.\")\n",
    "\n",
    "    # Return a structured summary of the audit results\n",
    "    return {\n",
    "        \"rows\": {\n",
    "            \"relevant\": n_rel,\n",
    "            \"clean\": n_clean,\n",
    "            \"errors\": n_err,\n",
    "            \"explained\": explained,\n",
    "            \"diff\": diff,\n",
    "        },\n",
    "        \"error_value_unique_count\": len(value_uniques),\n",
    "        \"error_original_value_unique_count\": len(original_value_uniques),\n",
    "        \"top_itemcodes\": item_counter.most_common(top_n),\n",
    "        \"top_countries\": country_counter.most_common(top_n),\n",
    "        \"paths\": {\n",
    "            \"relevant\": str(relevant_path),\n",
    "            \"clean\": str(clean_path),\n",
    "            \"errors\": str(errors_path),\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# ======================\n",
    "# RUN AUDIT FOR ALL 4 DATASETS\n",
    "# ======================\n",
    "\n",
    "# Run audit on Fundamentals dataset and collect summary\n",
    "fundamentals_result = audit_new_pipeline(\n",
    "    Temp_file_path_GO,\n",
    "    \"A_relevant_raw.txt\",\n",
    "    \"Fundamentals_clean.txt\",\n",
    "    \"Fundamentals_clean_errors.txt\",\n",
    "    label=\"Fundamentals\"\n",
    ")\n",
    "\n",
    "# Run audit on Current dataset and collect summary\n",
    "current_result = audit_new_pipeline(\n",
    "    Temp_file_path_GO,\n",
    "    \"B_relevant_raw.txt\",\n",
    "    \"Current_clean.txt\",\n",
    "    \"Current_clean_errors.txt\",\n",
    "    label=\"Current\"\n",
    ")\n",
    "\n",
    "# Run audit on Calendar dataset and collect summary\n",
    "calendar_result = audit_new_pipeline(\n",
    "    Temp_file_path_GO,\n",
    "    \"C_relevant_raw.txt\",\n",
    "    \"Calendar_clean.txt\",\n",
    "    \"Calendar_clean_errors.txt\",\n",
    "    label=\"Calendar\"\n",
    ")\n",
    "\n",
    "# Run audit on Meta dataset and collect summary\n",
    "meta_result = audit_new_pipeline(\n",
    "    Temp_file_path_GO,\n",
    "    \"D_relevant_raw.txt\",\n",
    "    \"Meta_clean.txt\",\n",
    "    \"Meta_clean_errors.txt\",\n",
    "    label=\"Meta\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZIgG1pxw5mi"
   },
   "source": [
    "# 2.0. Datastream (Restart Kernel before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY4Vj34bVcfE"
   },
   "source": [
    "### Transform Constituents File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1696,
     "status": "ok",
     "timestamp": 1764875882144,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "D0XzcjWYVbHl",
    "outputId": "54204442-fcf1-499a-d99e-f51d8f27dcb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial total rows: 167649\n",
      "Initial unique IDs: 102113\n",
      "Dropped rows by reason:\n",
      "DropReason\n",
      "Duplicate_DS_ID_ID    53992\n",
      "Flag_N                10525\n",
      "C_not_EQ               6380\n",
      "Non_unique_ID          1528\n",
      "Invalid_ID_format       120\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final clean rows: 95104\n",
      "Unique IDs in clean output: 95104\n",
      "\n",
      "Error rows per NationCode:\n",
      "NationCode  ErrorRowCount\n",
      "       156           8805\n",
      "       826           7276\n",
      "       356           6946\n",
      "       124           6883\n",
      "       392           6047\n",
      "       840           5682\n",
      "       760           4536\n",
      "       036           4060\n",
      "       410           3074\n",
      "       344           2271\n",
      "       250           2196\n",
      "       280           2189\n",
      "       643           1557\n",
      "       458           1545\n",
      "       764            987\n",
      "       076            968\n",
      "        an            936\n",
      "       752            641\n",
      "       617            567\n",
      "       796            502\n",
      "       484            432\n",
      "       756            378\n",
      "       208            337\n",
      "       300            302\n",
      "       730            262\n",
      "       152            254\n",
      "       586            247\n",
      "       597            243\n",
      "       366            243\n",
      "       056            226\n",
      "       380            201\n",
      "       025            179\n",
      "       246            171\n",
      "       220            135\n",
      "       710            129\n",
      "       608             98\n",
      "       620             94\n",
      "       578             84\n",
      "       528             84\n",
      "       175             78\n",
      "       504             73\n",
      "       702             54\n",
      "       040             54\n",
      "       350             48\n",
      "       862             46\n",
      "       442             39\n",
      "       376             35\n",
      "       196             32\n",
      "       703             30\n",
      "       724             30\n",
      "       136             27\n",
      "       554             24\n",
      "       831             24\n",
      "       372             20\n",
      "       060             19\n",
      "       203             19\n",
      "       070             10\n",
      "       480              9\n",
      "       582              8\n",
      "       804              7\n",
      "       642              7\n",
      "       398              6\n",
      "       705              6\n",
      "       400              6\n",
      "       832              5\n",
      "       428              4\n",
      "       784              4\n",
      "       833              4\n",
      "       566              3\n",
      "       422              3\n",
      "       388              3\n",
      "       191              3\n",
      "       591              3\n",
      "       682              3\n",
      "       788              3\n",
      "       897              3\n",
      "       092              2\n",
      "       218              2\n",
      "       050              2\n",
      "       352              2\n",
      "       404              2\n",
      "       440              2\n",
      "       499              2\n",
      "       048              1\n",
      "       275              1\n",
      "       044              1\n",
      "       100              1\n",
      "       268              1\n",
      "       454              1\n",
      "       470              1\n",
      "       414              1\n",
      "       516              1\n",
      "       780              1\n",
      "       834              1\n",
      "       894              1\n",
      "\n",
      "Clean rows per NationCode:\n",
      "NationCode  CleanRowCount\n",
      "       840          22752\n",
      "       124           6613\n",
      "       392           6370\n",
      "       156           6252\n",
      "       356           5796\n",
      "       826           5049\n",
      "       036           3704\n",
      "       410           3629\n",
      "       760           2874\n",
      "       344           2693\n",
      "       250           1947\n",
      "       280           1926\n",
      "       458           1592\n",
      "       752           1462\n",
      "       704           1427\n",
      "       702           1047\n",
      "       643           1019\n",
      "       366            986\n",
      "       710            944\n",
      "       380            912\n",
      "       617            888\n",
      "       376            828\n",
      "       076            766\n",
      "       578            759\n",
      "       764            684\n",
      "       796            656\n",
      "       724            544\n",
      "       586            526\n",
      "       756            490\n",
      "       208            472\n",
      "       528            451\n",
      "       300            448\n",
      "       682            365\n",
      "       608            349\n",
      "       152            345\n",
      "       246            333\n",
      "       100            330\n",
      "       554            318\n",
      "       056            314\n",
      "       220            294\n",
      "       484            281\n",
      "       730            279\n",
      "       400            259\n",
      "       597            254\n",
      "       642            242\n",
      "       040            228\n",
      "       414            228\n",
      "       784            187\n",
      "       566            180\n",
      "       196            177\n",
      "       372            176\n",
      "       050            172\n",
      "       582            150\n",
      "       620            150\n",
      "       025            149\n",
      "       804            146\n",
      "       688            146\n",
      "       191            144\n",
      "       070            117\n",
      "       203            112\n",
      "       175            111\n",
      "       369            104\n",
      "       060            103\n",
      "       480             95\n",
      "       398             95\n",
      "       442             93\n",
      "       504             92\n",
      "       350             87\n",
      "       788             82\n",
      "       404             65\n",
      "       705             64\n",
      "       388             59\n",
      "       634             59\n",
      "       440             54\n",
      "       862             52\n",
      "       703             51\n",
      "       048             49\n",
      "       352             47\n",
      "       136             46\n",
      "       807             46\n",
      "       275             43\n",
      "       499             42\n",
      "       593             40\n",
      "       428             39\n",
      "       182             39\n",
      "       470             37\n",
      "       233             35\n",
      "       897             35\n",
      "       288             34\n",
      "       072             30\n",
      "       591             25\n",
      "       759             24\n",
      "       068             23\n",
      "       092             20\n",
      "       894             20\n",
      "       834             17\n",
      "       780             16\n",
      "       860             16\n",
      "       454             15\n",
      "       242             15\n",
      "       218             14\n",
      "       516             13\n",
      "       833             12\n",
      "       178             11\n",
      "       052             11\n",
      "       222             10\n",
      "       044             10\n",
      "       831             10\n",
      "       800             10\n",
      "       422              9\n",
      "       832              9\n",
      "       646              6\n",
      "       320              6\n",
      "       340              5\n",
      "       748              4\n",
      "       234              3\n",
      "       328              3\n",
      "       686              2\n",
      "       116              2\n",
      "       120              1\n",
      "       562              1\n",
      "       496              1\n",
      "       736              1\n",
      "\n",
      "Saved 72545 error rows → /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/ID_mapping_errors.txt\n",
      "Saved 95104 clean rows → /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/ID_mapping_clean.txt\n",
      "Unique IDs in clean output: 95104\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Summary of this cell\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Read the constituents CSV:\n",
    "#    - column 0 → DS_ID\n",
    "#    - column 2 (Excel \"C\") → EQ flag\n",
    "#    - column 4 (Excel \"E\") → filter column (e.g. 'N')\n",
    "#    - column 7 → ID\n",
    "# 2. Keep only rows where column C == \"EQ\"; others → error file (reason).\n",
    "# 3. Add NationCode (digits at positions 2–4 of ID) as a helper column.\n",
    "# 4. Drop all rows where column E == \"N\" (these go to the error file with reason).\n",
    "# 5. Remove duplicate (DS_ID, ID) combinations, keeping the first; dropped rows → error file.\n",
    "# 6. Drop rows whose ID does not match the required pattern ^C[a-zA-Z0-9]{8}$; → error file.\n",
    "# 7. Drop rows whose ID occurs more than once (non-unique IDs); all such rows → error file.\n",
    "# 8. Save:\n",
    "#    - Clean rows → ID_mapping_clean.txt  (without NationCode column)\n",
    "#    - All dropped rows + DropReason → ID_mapping_errors.txt  (without NationCode column)\n",
    "# 9. Print:\n",
    "#    - Initial totals and unique-ID counts\n",
    "#    - How many rows were dropped for each reason\n",
    "#    - Final clean-row count and unique-ID count\n",
    "#    - Error-row counts per NationCode\n",
    "#    - Clean-row counts per NationCode\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Define input and output paths\n",
    "# -----------------------------------------------------------------------------\n",
    "input_file = Constituents_file_path\n",
    "output_folder = Temp_file_path_GO\n",
    "\n",
    "# Create output folder if missing\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Read the CSV with fallback encoding\n",
    "#    We need:\n",
    "#      - column 0 → DS_ID\n",
    "#      - column 2 (Excel \"C\") → EQ flag\n",
    "#      - column 4 (Excel \"E\") → filter column (e.g. 'N')\n",
    "#      - column 7 → ID\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        input_file,\n",
    "        header=0,\n",
    "        usecols=[0, 2, 4, 7]\n",
    "    )\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(\n",
    "        input_file,\n",
    "        header=0,\n",
    "        usecols=[0, 2, 4, 7],\n",
    "        encoding=\"ISO-8859-1\"\n",
    "    )\n",
    "\n",
    "# Keep original column names so we can rename only what we need\n",
    "orig_cols = df.columns.tolist()\n",
    "ds_col   = orig_cols[0]   # DS_ID column (col 0)\n",
    "col_c    = orig_cols[1]   # column C (EQ flag)\n",
    "flag_col = orig_cols[2]   # column E (used for 'N' filter)\n",
    "id_col   = orig_cols[3]   # ID column (col 7)\n",
    "\n",
    "# Rename DS_ID and ID; keep the original names of column C and column E\n",
    "df = df.rename(columns={ds_col: \"DS_ID\", id_col: \"ID\"})\n",
    "# col_c and flag_col stay as they are\n",
    "\n",
    "# Store initial stats (before any filtering)\n",
    "initial_total_rows = len(df)\n",
    "initial_unique_ids = df[\"ID\"].nunique()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Add NationCode helper column (positions 2–4 of ID)\n",
    "# -----------------------------------------------------------------------------\n",
    "df[\"NationCode\"] = df[\"ID\"].astype(str).str[1:4]\n",
    "\n",
    "# List to collect all dropped rows\n",
    "error_frames = []\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. NEW: Filter on column C == \"EQ\" (keep only \"EQ\")\n",
    "# -----------------------------------------------------------------------------\n",
    "mask_c_not_eq = df[col_c].astype(str) != \"EQ\"\n",
    "dropped_c_not_eq = df[mask_c_not_eq].copy()\n",
    "if not dropped_c_not_eq.empty:\n",
    "    dropped_c_not_eq[\"DropReason\"] = \"C_not_EQ\"\n",
    "    error_frames.append(dropped_c_not_eq)\n",
    "\n",
    "df = df[~mask_c_not_eq]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Drop rows with 'N' in column E\n",
    "# -----------------------------------------------------------------------------\n",
    "mask_flag_n = df[flag_col].astype(str) == \"N\"\n",
    "dropped_flag_n = df[mask_flag_n].copy()\n",
    "if not dropped_flag_n.empty:\n",
    "    dropped_flag_n[\"DropReason\"] = \"Flag_N\"\n",
    "    error_frames.append(dropped_flag_n)\n",
    "\n",
    "df = df[~mask_flag_n]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. Remove duplicate DS_ID+ID combinations (keep first)\n",
    "# -----------------------------------------------------------------------------\n",
    "mask_dups_ds_id = df.duplicated(subset=[\"DS_ID\", \"ID\"], keep=\"first\")\n",
    "dropped_dups_ds_id = df[mask_dups_ds_id].copy()\n",
    "if not dropped_dups_ds_id.empty:\n",
    "    dropped_dups_ds_id[\"DropReason\"] = \"Duplicate_DS_ID_ID\"\n",
    "    error_frames.append(dropped_dups_ds_id)\n",
    "\n",
    "df = df[~mask_dups_ds_id]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. Validate ID pattern: must match ^C[a-zA-Z0-9]{8}$\n",
    "# -----------------------------------------------------------------------------\n",
    "pattern = re.compile(r\"^C[a-zA-Z0-9]{8}$\")\n",
    "mask_invalid = ~df[\"ID\"].astype(str).str.match(pattern)\n",
    "\n",
    "dropped_invalid = df[mask_invalid].copy()\n",
    "if not dropped_invalid.empty:\n",
    "    dropped_invalid[\"DropReason\"] = \"Invalid_ID_format\"\n",
    "    error_frames.append(dropped_invalid)\n",
    "\n",
    "df = df[~mask_invalid]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8. Drop non-unique IDs (IDs that appear more than once)\n",
    "# -----------------------------------------------------------------------------\n",
    "id_counts = df[\"ID\"].value_counts()\n",
    "nonunique_ids = id_counts[id_counts > 1].index\n",
    "\n",
    "mask_nonunique_ids = df[\"ID\"].isin(nonunique_ids)\n",
    "dropped_nonunique = df[mask_nonunique_ids].copy()\n",
    "if not dropped_nonunique.empty:\n",
    "    dropped_nonunique[\"DropReason\"] = \"Non_unique_ID\"\n",
    "    error_frames.append(dropped_nonunique)\n",
    "\n",
    "df_clean = df[~mask_nonunique_ids].copy()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9. Combine error rows\n",
    "# -----------------------------------------------------------------------------\n",
    "if error_frames:\n",
    "    df_errors = pd.concat(error_frames, ignore_index=True)\n",
    "else:\n",
    "    # Empty frame with expected columns if no errors (unlikely, but safe)\n",
    "    df_errors = pd.DataFrame(columns=list(df_clean.columns) + [\"DropReason\"])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 10. Final stats and NationCode summaries\n",
    "# -----------------------------------------------------------------------------\n",
    "final_total_rows = len(df_clean)\n",
    "final_unique_ids = df_clean[\"ID\"].nunique()\n",
    "\n",
    "print(f\"Initial total rows: {initial_total_rows}\")\n",
    "print(f\"Initial unique IDs: {initial_unique_ids}\")\n",
    "print(\"Dropped rows by reason:\")\n",
    "print(df_errors[\"DropReason\"].value_counts())\n",
    "\n",
    "print(f\"\\nFinal clean rows: {final_total_rows}\")\n",
    "print(f\"Unique IDs in clean output: {final_unique_ids}\")\n",
    "\n",
    "# Error counts per NationCode\n",
    "if not df_errors.empty:\n",
    "    print(\"\\nError rows per NationCode:\")\n",
    "    error_nat_counts = (\n",
    "        df_errors.groupby(\"NationCode\")\n",
    "        .size()\n",
    "        .reset_index(name=\"ErrorRowCount\")\n",
    "        .sort_values(by=\"ErrorRowCount\", ascending=False)\n",
    "    )\n",
    "    print(error_nat_counts.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo error rows; therefore no NationCode error breakdown.\")\n",
    "\n",
    "# Clean counts per NationCode\n",
    "if not df_clean.empty:\n",
    "    print(\"\\nClean rows per NationCode:\")\n",
    "    clean_nat_counts = (\n",
    "        df_clean.groupby(\"NationCode\")\n",
    "        .size()\n",
    "        .reset_index(name=\"CleanRowCount\")\n",
    "        .sort_values(by=\"CleanRowCount\", ascending=False)\n",
    "    )\n",
    "    print(clean_nat_counts.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo clean rows; therefore no NationCode clean breakdown.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 11. Drop helper column NationCode before saving\n",
    "# -----------------------------------------------------------------------------\n",
    "# Be defensive: only drop columns that actually exist (e.g. MAJOR might not be present)\n",
    "cols_to_drop = [col for col in [\"NationCode\", \"MAJOR\", \"TYPE\"] if col in df_clean.columns]\n",
    "df_clean_to_save = df_clean.drop(columns=cols_to_drop)\n",
    "\n",
    "cols_to_drop_errors = [col for col in [\"NationCode\", \"MAJOR\"] if col in df_errors.columns]\n",
    "df_errors_to_save = df_errors.drop(columns=cols_to_drop_errors)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 12. Write pipe-separated output files\n",
    "# -----------------------------------------------------------------------------\n",
    "output_clean_file = os.path.join(output_folder, \"ID_mapping_clean.txt\")\n",
    "output_error_file = os.path.join(output_folder, \"ID_mapping_errors.txt\")\n",
    "\n",
    "df_clean_to_save.to_csv(\n",
    "    output_clean_file,\n",
    "    sep=\"|\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "df_errors_to_save.to_csv(\n",
    "    output_error_file,\n",
    "    sep=\"|\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"\\nSaved {len(df_errors_to_save)} error rows → {output_error_file}\")\n",
    "print(f\"Saved {len(df_clean_to_save)} clean rows → {output_clean_file}\")\n",
    "print(f\"Unique IDs in clean output: {final_unique_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8OFoUUqopEE"
   },
   "source": [
    "### Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1764875883838,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "201e07a7",
    "outputId": "f02ce4b8-2b31-42d0-df63-c05b87a8aca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for standalone duplicate IDs and DS_IDs in: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/ID_mapping_clean.txt\n",
      "\n",
      "--- Checking 'ID' column for duplicates ---\n",
      "No standalone duplicate values found in the 'ID' column.\n",
      "\n",
      "--- Checking 'DS_ID' column for duplicates ---\n",
      "No standalone duplicate values found in the 'DS_ID' column.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_folder_go = Temp_file_path_GO\n",
    "mapping_file = os.path.join(base_folder_go, \"ID_mapping_clean.txt\")\n",
    "\n",
    "print(f\"Checking for standalone duplicate IDs and DS_IDs in: {mapping_file}\")\n",
    "\n",
    "try:\n",
    "    # Load the mapping file, explicitly typing columns as strings to prevent mixed types\n",
    "    mapping_df = pd.read_csv(\n",
    "        mapping_file,\n",
    "        sep=\"|\",\n",
    "        dtype={\"DS_ID\": \"string\", \"ID\": \"string\"},\n",
    "        low_memory=False\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Checking 'ID' column for duplicates ---\")\n",
    "    # Find duplicate IDs\n",
    "    duplicate_ids = mapping_df[mapping_df['ID'].duplicated(keep=False)].sort_values('ID')\n",
    "\n",
    "    if not duplicate_ids.empty:\n",
    "        print(f\"Found {len(duplicate_ids['ID'].unique())} unique IDs that appear more than once in the 'ID' column:\")\n",
    "        display(duplicate_ids[['ID']].drop_duplicates().reset_index(drop=True))\n",
    "    else:\n",
    "        print(\"No standalone duplicate values found in the 'ID' column.\")\n",
    "\n",
    "    print(\"\\n--- Checking 'DS_ID' column for duplicates ---\")\n",
    "    # Find duplicate DS_IDs\n",
    "    duplicate_ds_ids = mapping_df[mapping_df['DS_ID'].duplicated(keep=False)].sort_values('DS_ID')\n",
    "\n",
    "    if not duplicate_ds_ids.empty:\n",
    "        print(f\"Found {len(duplicate_ds_ids['DS_ID'].unique())} unique DS_IDs that appear more than once in the 'DS_ID' column:\")\n",
    "        display(duplicate_ds_ids[['DS_ID']].drop_duplicates().reset_index(drop=True))\n",
    "    else:\n",
    "        print(\"No standalone duplicate values found in the 'DS_ID' column.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {mapping_file} was not found. Please ensure the path is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJzOyKSPTju0"
   },
   "source": [
    "### Transform MarketValues and Drop Small Companies in Accordance with Bartram & Grinblatt (2021)(MV)\n",
    "\n",
    "===> Hier mit USD MVs, damit <10 Mio rausgefiltert werden. Danach is USD MV erst wieder für Übersichtstabellen relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1764689661104,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "B1qrgn1DxFmL",
    "outputId": "da727bd3-2f2e-44a6-d1d1-3b2b46f29027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.01.csv\n",
      "  Warning: 339246 MV < 10M\n",
      "  Warning: 107557 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1643017\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 339246\n",
      "    MV>=10 removed (firm excluded)        : 107557\n",
      "    Rows written                          : 1196214\n",
      "    Total removed                         : 446803\n",
      "    Check (initial-removed)               : 1196214\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.02.csv\n",
      "  Warning: 123829 MV < 10M\n",
      "  Warning: 39930 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 970197\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 123829\n",
      "    MV>=10 removed (firm excluded)        : 39930\n",
      "    Rows written                          : 806438\n",
      "    Total removed                         : 163759\n",
      "    Check (initial-removed)               : 806438\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.03.csv\n",
      "  Warning: 979253 MV < 10M\n",
      "  Warning: 130723 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1882282\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 979253\n",
      "    MV>=10 removed (firm excluded)        : 130723\n",
      "    Rows written                          : 772306\n",
      "    Total removed                         : 1109976\n",
      "    Check (initial-removed)               : 772306\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.04.csv\n",
      "  Warning: 144566 MV < 10M\n",
      "  Warning: 47411 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1538379\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 144566\n",
      "    MV>=10 removed (firm excluded)        : 47411\n",
      "    Rows written                          : 1346402\n",
      "    Total removed                         : 191977\n",
      "    Check (initial-removed)               : 1346402\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.05.csv\n",
      "  Warning: 559768 MV < 10M\n",
      "  Warning: 180424 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1836602\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 559768\n",
      "    MV>=10 removed (firm excluded)        : 180424\n",
      "    Rows written                          : 1096410\n",
      "    Total removed                         : 740192\n",
      "    Check (initial-removed)               : 1096410\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.06.csv\n",
      "  Warning: 24832 MV < 10M\n",
      "  Warning: 11383 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2742539\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 24832\n",
      "    MV>=10 removed (firm excluded)        : 11383\n",
      "    Rows written                          : 2706324\n",
      "    Total removed                         : 36215\n",
      "    Check (initial-removed)               : 2706324\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.07.csv\n",
      "  Warning: 204769 MV < 10M\n",
      "  Warning: 71127 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1933145\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 204769\n",
      "    MV>=10 removed (firm excluded)        : 71127\n",
      "    Rows written                          : 1657249\n",
      "    Total removed                         : 275896\n",
      "    Check (initial-removed)               : 1657249\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.08.csv\n",
      "  Warning: 237000 MV < 10M\n",
      "  Warning: 79836 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1101462\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 237000\n",
      "    MV>=10 removed (firm excluded)        : 79836\n",
      "    Rows written                          : 784626\n",
      "    Total removed                         : 316836\n",
      "    Check (initial-removed)               : 784626\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.09.csv\n",
      "  Warning: 47991 MV < 10M\n",
      "  Warning: 26780 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 974477\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 47991\n",
      "    MV>=10 removed (firm excluded)        : 26780\n",
      "    Rows written                          : 899706\n",
      "    Total removed                         : 74771\n",
      "    Check (initial-removed)               : 899706\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.10.csv\n",
      "  Warning: 325638 MV < 10M\n",
      "  Warning: 148513 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1984095\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 325638\n",
      "    MV>=10 removed (firm excluded)        : 148513\n",
      "    Rows written                          : 1509944\n",
      "    Total removed                         : 474151\n",
      "    Check (initial-removed)               : 1509944\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.11.csv\n",
      "  Warning: 135715 MV < 10M\n",
      "  Warning: 49299 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 838859\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 135715\n",
      "    MV>=10 removed (firm excluded)        : 49299\n",
      "    Rows written                          : 653845\n",
      "    Total removed                         : 185014\n",
      "    Check (initial-removed)               : 653845\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.12.csv\n",
      "  Warning: 165368 MV < 10M\n",
      "  Warning: 53402 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1243774\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 165368\n",
      "    MV>=10 removed (firm excluded)        : 53402\n",
      "    Rows written                          : 1025004\n",
      "    Total removed                         : 218770\n",
      "    Check (initial-removed)               : 1025004\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.13.csv\n",
      "  Warning: 214908 MV < 10M\n",
      "  Warning: 72032 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1341597\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 214908\n",
      "    MV>=10 removed (firm excluded)        : 72032\n",
      "    Rows written                          : 1054657\n",
      "    Total removed                         : 286940\n",
      "    Check (initial-removed)               : 1054657\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19911995.14.csv\n",
      "  Warning: 215834 MV < 10M\n",
      "  Warning: 59444 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1355139\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 215834\n",
      "    MV>=10 removed (firm excluded)        : 59444\n",
      "    Rows written                          : 1079861\n",
      "    Total removed                         : 275278\n",
      "    Check (initial-removed)               : 1079861\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.15.csv\n",
      "  Warning: 688983 MV < 10M\n",
      "  Warning: 172019 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2486196\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 688983\n",
      "    MV>=10 removed (firm excluded)        : 172019\n",
      "    Rows written                          : 1625194\n",
      "    Total removed                         : 861002\n",
      "    Check (initial-removed)               : 1625194\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.16.csv\n",
      "  Warning: 195905 MV < 10M\n",
      "  Warning: 80786 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1844823\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 195905\n",
      "    MV>=10 removed (firm excluded)        : 80786\n",
      "    Rows written                          : 1568132\n",
      "    Total removed                         : 276691\n",
      "    Check (initial-removed)               : 1568132\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.17.csv\n",
      "  Warning: 1178946 MV < 10M\n",
      "  Warning: 219342 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2581130\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1178946\n",
      "    MV>=10 removed (firm excluded)        : 219342\n",
      "    Rows written                          : 1182842\n",
      "    Total removed                         : 1398288\n",
      "    Check (initial-removed)               : 1182842\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.18.csv\n",
      "  Warning: 270211 MV < 10M\n",
      "  Warning: 137444 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2266717\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 270211\n",
      "    MV>=10 removed (firm excluded)        : 137444\n",
      "    Rows written                          : 1859062\n",
      "    Total removed                         : 407655\n",
      "    Check (initial-removed)               : 1859062\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.19.csv\n",
      "  Warning: 1645847 MV < 10M\n",
      "  Warning: 264927 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3019215\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1645847\n",
      "    MV>=10 removed (firm excluded)        : 264927\n",
      "    Rows written                          : 1108441\n",
      "    Total removed                         : 1910774\n",
      "    Check (initial-removed)               : 1108441\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.20.csv\n",
      "  Warning: 77512 MV < 10M\n",
      "  Warning: 69291 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3379936\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 77512\n",
      "    MV>=10 removed (firm excluded)        : 69291\n",
      "    Rows written                          : 3233133\n",
      "    Total removed                         : 146803\n",
      "    Check (initial-removed)               : 3233133\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.21.csv\n",
      "  Warning: 493994 MV < 10M\n",
      "  Warning: 350825 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2923962\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 493994\n",
      "    MV>=10 removed (firm excluded)        : 350825\n",
      "    Rows written                          : 2079143\n",
      "    Total removed                         : 844819\n",
      "    Check (initial-removed)               : 2079143\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.22.csv\n",
      "  Warning: 477755 MV < 10M\n",
      "  Warning: 139910 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1830750\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 477755\n",
      "    MV>=10 removed (firm excluded)        : 139910\n",
      "    Rows written                          : 1213085\n",
      "    Total removed                         : 617665\n",
      "    Check (initial-removed)               : 1213085\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.23.csv\n",
      "  Warning: 202570 MV < 10M\n",
      "  Warning: 124685 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1479930\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 202570\n",
      "    MV>=10 removed (firm excluded)        : 124685\n",
      "    Rows written                          : 1152675\n",
      "    Total removed                         : 327255\n",
      "    Check (initial-removed)               : 1152675\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.24.csv\n",
      "  Warning: 367682 MV < 10M\n",
      "  Warning: 131069 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2647522\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 367682\n",
      "    MV>=10 removed (firm excluded)        : 131069\n",
      "    Rows written                          : 2148771\n",
      "    Total removed                         : 498751\n",
      "    Check (initial-removed)               : 2148771\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.25.csv\n",
      "  Warning: 250670 MV < 10M\n",
      "  Warning: 99728 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1337867\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 250670\n",
      "    MV>=10 removed (firm excluded)        : 99728\n",
      "    Rows written                          : 987469\n",
      "    Total removed                         : 350398\n",
      "    Check (initial-removed)               : 987469\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.26.csv\n",
      "  Warning: 308799 MV < 10M\n",
      "  Warning: 115122 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2106921\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 308799\n",
      "    MV>=10 removed (firm excluded)        : 115122\n",
      "    Rows written                          : 1683000\n",
      "    Total removed                         : 423921\n",
      "    Check (initial-removed)               : 1683000\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.27.csv\n",
      "  Warning: 337838 MV < 10M\n",
      "  Warning: 133105 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2139569\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 337838\n",
      "    MV>=10 removed (firm excluded)        : 133105\n",
      "    Rows written                          : 1668626\n",
      "    Total removed                         : 470943\n",
      "    Check (initial-removed)               : 1668626\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19951999.28.csv\n",
      "  Warning: 347751 MV < 10M\n",
      "  Warning: 125603 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2214332\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 347751\n",
      "    MV>=10 removed (firm excluded)        : 125603\n",
      "    Rows written                          : 1740978\n",
      "    Total removed                         : 473354\n",
      "    Check (initial-removed)               : 1740978\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.29.csv\n",
      "  Warning: 1139838 MV < 10M\n",
      "  Warning: 323948 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3475752\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1139838\n",
      "    MV>=10 removed (firm excluded)        : 323948\n",
      "    Rows written                          : 2011966\n",
      "    Total removed                         : 1463786\n",
      "    Check (initial-removed)               : 2011966\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.30.csv\n",
      "  Warning: 338255 MV < 10M\n",
      "  Warning: 91202 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2605002\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 338255\n",
      "    MV>=10 removed (firm excluded)        : 91202\n",
      "    Rows written                          : 2175545\n",
      "    Total removed                         : 429457\n",
      "    Check (initial-removed)               : 2175545\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.31.csv\n",
      "  Warning: 1702851 MV < 10M\n",
      "  Warning: 276376 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3437616\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1702851\n",
      "    MV>=10 removed (firm excluded)        : 276376\n",
      "    Rows written                          : 1458389\n",
      "    Total removed                         : 1979227\n",
      "    Check (initial-removed)               : 1458389\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.32.csv\n",
      "  Warning: 521535 MV < 10M\n",
      "  Warning: 223786 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3240071\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 521535\n",
      "    MV>=10 removed (firm excluded)        : 223786\n",
      "    Rows written                          : 2494750\n",
      "    Total removed                         : 745321\n",
      "    Check (initial-removed)               : 2494750\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.33.csv\n",
      "  Warning: 1933641 MV < 10M\n",
      "  Warning: 217593 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3546033\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1933641\n",
      "    MV>=10 removed (firm excluded)        : 217593\n",
      "    Rows written                          : 1394799\n",
      "    Total removed                         : 2151234\n",
      "    Check (initial-removed)               : 1394799\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.34.csv\n",
      "  Warning: 264042 MV < 10M\n",
      "  Warning: 148724 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3980591\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 264042\n",
      "    MV>=10 removed (firm excluded)        : 148724\n",
      "    Rows written                          : 3567825\n",
      "    Total removed                         : 412766\n",
      "    Check (initial-removed)               : 3567825\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.35.csv\n",
      "  Warning: 859579 MV < 10M\n",
      "  Warning: 375290 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3906636\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 859579\n",
      "    MV>=10 removed (firm excluded)        : 375290\n",
      "    Rows written                          : 2671767\n",
      "    Total removed                         : 1234869\n",
      "    Check (initial-removed)               : 2671767\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.36.csv\n",
      "  Warning: 974410 MV < 10M\n",
      "  Warning: 180998 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2701987\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 974410\n",
      "    MV>=10 removed (firm excluded)        : 180998\n",
      "    Rows written                          : 1546579\n",
      "    Total removed                         : 1155408\n",
      "    Check (initial-removed)               : 1546579\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.37.csv\n",
      "  Warning: 342893 MV < 10M\n",
      "  Warning: 135246 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2093802\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 342893\n",
      "    MV>=10 removed (firm excluded)        : 135246\n",
      "    Rows written                          : 1615663\n",
      "    Total removed                         : 478139\n",
      "    Check (initial-removed)               : 1615663\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.38.csv\n",
      "  Warning: 665360 MV < 10M\n",
      "  Warning: 244406 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3402712\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 665360\n",
      "    MV>=10 removed (firm excluded)        : 244406\n",
      "    Rows written                          : 2492946\n",
      "    Total removed                         : 909766\n",
      "    Check (initial-removed)               : 2492946\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.39.csv\n",
      "  Warning: 639036 MV < 10M\n",
      "  Warning: 183516 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1982791\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 639036\n",
      "    MV>=10 removed (firm excluded)        : 183516\n",
      "    Rows written                          : 1160239\n",
      "    Total removed                         : 822552\n",
      "    Check (initial-removed)               : 1160239\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.40.csv\n",
      "  Warning: 758379 MV < 10M\n",
      "  Warning: 181650 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2945704\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 758379\n",
      "    MV>=10 removed (firm excluded)        : 181650\n",
      "    Rows written                          : 2005675\n",
      "    Total removed                         : 940029\n",
      "    Check (initial-removed)               : 2005675\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.41.csv\n",
      "  Warning: 769211 MV < 10M\n",
      "  Warning: 200492 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2913676\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 769211\n",
      "    MV>=10 removed (firm excluded)        : 200492\n",
      "    Rows written                          : 1943973\n",
      "    Total removed                         : 969703\n",
      "    Check (initial-removed)               : 1943973\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD19992003.42.csv\n",
      "  Warning: 816085 MV < 10M\n",
      "  Warning: 213464 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3026389\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 816085\n",
      "    MV>=10 removed (firm excluded)        : 213464\n",
      "    Rows written                          : 1996840\n",
      "    Total removed                         : 1029549\n",
      "    Check (initial-removed)               : 1996840\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.43.csv\n",
      "  Warning: 1126817 MV < 10M\n",
      "  Warning: 342116 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4514038\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1126817\n",
      "    MV>=10 removed (firm excluded)        : 342116\n",
      "    Rows written                          : 3045105\n",
      "    Total removed                         : 1468933\n",
      "    Check (initial-removed)               : 3045105\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.44.csv\n",
      "  Warning: 322580 MV < 10M\n",
      "  Warning: 86462 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3109916\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 322580\n",
      "    MV>=10 removed (firm excluded)        : 86462\n",
      "    Rows written                          : 2700874\n",
      "    Total removed                         : 409042\n",
      "    Check (initial-removed)               : 2700874\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.45.csv\n",
      "  Warning: 1408419 MV < 10M\n",
      "  Warning: 442066 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4295258\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1408419\n",
      "    MV>=10 removed (firm excluded)        : 442066\n",
      "    Rows written                          : 2444773\n",
      "    Total removed                         : 1850485\n",
      "    Check (initial-removed)               : 2444773\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.46.csv\n",
      "  Warning: 591558 MV < 10M\n",
      "  Warning: 185751 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3778726\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 591558\n",
      "    MV>=10 removed (firm excluded)        : 185751\n",
      "    Rows written                          : 3001417\n",
      "    Total removed                         : 777309\n",
      "    Check (initial-removed)               : 3001417\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.47.csv\n",
      "  Warning: 1794538 MV < 10M\n",
      "  Warning: 358857 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4337334\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1794538\n",
      "    MV>=10 removed (firm excluded)        : 358857\n",
      "    Rows written                          : 2183939\n",
      "    Total removed                         : 2153395\n",
      "    Check (initial-removed)               : 2183939\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.48.csv\n",
      "  Warning: 234188 MV < 10M\n",
      "  Warning: 68825 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4778618\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 234188\n",
      "    MV>=10 removed (firm excluded)        : 68825\n",
      "    Rows written                          : 4475605\n",
      "    Total removed                         : 303013\n",
      "    Check (initial-removed)               : 4475605\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.49.csv\n",
      "  Warning: 892440 MV < 10M\n",
      "  Warning: 394696 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5003205\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 892440\n",
      "    MV>=10 removed (firm excluded)        : 394696\n",
      "    Rows written                          : 3716069\n",
      "    Total removed                         : 1287136\n",
      "    Check (initial-removed)               : 3716069\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.50.csv\n",
      "  Warning: 423633 MV < 10M\n",
      "  Warning: 106623 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1317093\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 423633\n",
      "    MV>=10 removed (firm excluded)        : 106623\n",
      "    Rows written                          : 786837\n",
      "    Total removed                         : 530256\n",
      "    Check (initial-removed)               : 786837\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.51.csv\n",
      "  Warning: 334082 MV < 10M\n",
      "  Warning: 128475 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2823636\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 334082\n",
      "    MV>=10 removed (firm excluded)        : 128475\n",
      "    Rows written                          : 2361079\n",
      "    Total removed                         : 462557\n",
      "    Check (initial-removed)               : 2361079\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.52.csv\n",
      "  Warning: 873066 MV < 10M\n",
      "  Warning: 191578 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4361285\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 873066\n",
      "    MV>=10 removed (firm excluded)        : 191578\n",
      "    Rows written                          : 3296641\n",
      "    Total removed                         : 1064644\n",
      "    Check (initial-removed)               : 3296641\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.53.csv\n",
      "  Warning: 276942 MV < 10M\n",
      "  Warning: 61189 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1263698\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 276942\n",
      "    MV>=10 removed (firm excluded)        : 61189\n",
      "    Rows written                          : 925567\n",
      "    Total removed                         : 338131\n",
      "    Check (initial-removed)               : 925567\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.54.csv\n",
      "  Warning: 890776 MV < 10M\n",
      "  Warning: 144972 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3514545\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 890776\n",
      "    MV>=10 removed (firm excluded)        : 144972\n",
      "    Rows written                          : 2478797\n",
      "    Total removed                         : 1035748\n",
      "    Check (initial-removed)               : 2478797\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.55.csv\n",
      "  Warning: 905905 MV < 10M\n",
      "  Warning: 127339 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3288735\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 905905\n",
      "    MV>=10 removed (firm excluded)        : 127339\n",
      "    Rows written                          : 2255491\n",
      "    Total removed                         : 1033244\n",
      "    Check (initial-removed)               : 2255491\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20032007.56.csv\n",
      "  Warning: 955398 MV < 10M\n",
      "  Warning: 118504 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3386277\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 955398\n",
      "    MV>=10 removed (firm excluded)        : 118504\n",
      "    Rows written                          : 2312375\n",
      "    Total removed                         : 1073902\n",
      "    Check (initial-removed)               : 2312375\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.57.csv\n",
      "  Warning: 340025 MV < 10M\n",
      "  Warning: 151402 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1337790\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 340025\n",
      "    MV>=10 removed (firm excluded)        : 151402\n",
      "    Rows written                          : 846363\n",
      "    Total removed                         : 491427\n",
      "    Check (initial-removed)               : 846363\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.58.csv\n",
      "  Warning: 240196 MV < 10M\n",
      "  Warning: 46376 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1236658\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 240196\n",
      "    MV>=10 removed (firm excluded)        : 46376\n",
      "    Rows written                          : 950086\n",
      "    Total removed                         : 286572\n",
      "    Check (initial-removed)               : 950086\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.59.csv\n",
      "  Warning: 1986241 MV < 10M\n",
      "  Warning: 538552 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5376726\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1986241\n",
      "    MV>=10 removed (firm excluded)        : 538552\n",
      "    Rows written                          : 2851933\n",
      "    Total removed                         : 2524793\n",
      "    Check (initial-removed)               : 2851933\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.60.csv\n",
      "  Warning: 155754 MV < 10M\n",
      "  Warning: 34618 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1262827\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 155754\n",
      "    MV>=10 removed (firm excluded)        : 34618\n",
      "    Rows written                          : 1072455\n",
      "    Total removed                         : 190372\n",
      "    Check (initial-removed)               : 1072455\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.61.csv\n",
      "  Warning: 497946 MV < 10M\n",
      "  Warning: 105366 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1387096\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 497946\n",
      "    MV>=10 removed (firm excluded)        : 105366\n",
      "    Rows written                          : 783784\n",
      "    Total removed                         : 603312\n",
      "    Check (initial-removed)               : 783784\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.62.csv\n",
      "  Warning: 194047 MV < 10M\n",
      "  Warning: 69072 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1321229\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 194047\n",
      "    MV>=10 removed (firm excluded)        : 69072\n",
      "    Rows written                          : 1058110\n",
      "    Total removed                         : 263119\n",
      "    Check (initial-removed)               : 1058110\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.63.csv\n",
      "  Warning: 199655 MV < 10M\n",
      "  Warning: 80842 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1337443\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 199655\n",
      "    MV>=10 removed (firm excluded)        : 80842\n",
      "    Rows written                          : 1056946\n",
      "    Total removed                         : 280497\n",
      "    Check (initial-removed)               : 1056946\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.64.csv\n",
      "  Warning: 1116801 MV < 10M\n",
      "  Warning: 267364 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4582917\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1116801\n",
      "    MV>=10 removed (firm excluded)        : 267364\n",
      "    Rows written                          : 3198752\n",
      "    Total removed                         : 1384165\n",
      "    Check (initial-removed)               : 3198752\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.65.csv\n",
      "  Warning: 128961 MV < 10M\n",
      "  Warning: 47474 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1262429\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 128961\n",
      "    MV>=10 removed (firm excluded)        : 47474\n",
      "    Rows written                          : 1085994\n",
      "    Total removed                         : 176435\n",
      "    Check (initial-removed)               : 1085994\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.66.csv\n",
      "  Warning: 382593 MV < 10M\n",
      "  Warning: 65930 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1357503\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 382593\n",
      "    MV>=10 removed (firm excluded)        : 65930\n",
      "    Rows written                          : 908980\n",
      "    Total removed                         : 448523\n",
      "    Check (initial-removed)               : 908980\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.67.csv\n",
      "  Warning: 351660 MV < 10M\n",
      "  Warning: 63548 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1293583\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 351660\n",
      "    MV>=10 removed (firm excluded)        : 63548\n",
      "    Rows written                          : 878375\n",
      "    Total removed                         : 415208\n",
      "    Check (initial-removed)               : 878375\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.68.csv\n",
      "  Warning: 1215865 MV < 10M\n",
      "  Warning: 168071 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4053507\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1215865\n",
      "    MV>=10 removed (firm excluded)        : 168071\n",
      "    Rows written                          : 2669571\n",
      "    Total removed                         : 1383936\n",
      "    Check (initial-removed)               : 2669571\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.69.csv\n",
      "  Warning: 438374 MV < 10M\n",
      "  Warning: 41512 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1376959\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 438374\n",
      "    MV>=10 removed (firm excluded)        : 41512\n",
      "    Rows written                          : 897073\n",
      "    Total removed                         : 479886\n",
      "    Check (initial-removed)               : 897073\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20072011.70.csv\n",
      "  Warning: 465020 MV < 10M\n",
      "  Warning: 42400 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1387605\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 465020\n",
      "    MV>=10 removed (firm excluded)        : 42400\n",
      "    Rows written                          : 880185\n",
      "    Total removed                         : 507420\n",
      "    Check (initial-removed)               : 880185\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.71.csv\n",
      "  Warning: 375158 MV < 10M\n",
      "  Warning: 87047 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1348923\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 375158\n",
      "    MV>=10 removed (firm excluded)        : 87047\n",
      "    Rows written                          : 886718\n",
      "    Total removed                         : 462205\n",
      "    Check (initial-removed)               : 886718\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.72.csv\n",
      "  Warning: 206592 MV < 10M\n",
      "  Warning: 31581 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1216303\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 206592\n",
      "    MV>=10 removed (firm excluded)        : 31581\n",
      "    Rows written                          : 978130\n",
      "    Total removed                         : 238173\n",
      "    Check (initial-removed)               : 978130\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.73.csv\n",
      "  Warning: 1986205 MV < 10M\n",
      "  Warning: 255916 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4595633\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1986205\n",
      "    MV>=10 removed (firm excluded)        : 255916\n",
      "    Rows written                          : 2353512\n",
      "    Total removed                         : 2242121\n",
      "    Check (initial-removed)               : 2353512\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.74.csv\n",
      "  Warning: 565984 MV < 10M\n",
      "  Warning: 79589 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3425179\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 565984\n",
      "    MV>=10 removed (firm excluded)        : 79589\n",
      "    Rows written                          : 2779606\n",
      "    Total removed                         : 645573\n",
      "    Check (initial-removed)               : 2779606\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.75.csv\n",
      "  Warning: 1816037 MV < 10M\n",
      "  Warning: 239165 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4393094\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1816037\n",
      "    MV>=10 removed (firm excluded)        : 239165\n",
      "    Rows written                          : 2337892\n",
      "    Total removed                         : 2055202\n",
      "    Check (initial-removed)               : 2337892\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.76.csv\n",
      "  Warning: 424076 MV < 10M\n",
      "  Warning: 71416 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4219535\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 424076\n",
      "    MV>=10 removed (firm excluded)        : 71416\n",
      "    Rows written                          : 3724043\n",
      "    Total removed                         : 495492\n",
      "    Check (initial-removed)               : 3724043\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.77.csv\n",
      "  Warning: 886081 MV < 10M\n",
      "  Warning: 129525 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5109116\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 886081\n",
      "    MV>=10 removed (firm excluded)        : 129525\n",
      "    Rows written                          : 4093510\n",
      "    Total removed                         : 1015606\n",
      "    Check (initial-removed)               : 4093510\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.78.csv\n",
      "  Warning: 548025 MV < 10M\n",
      "  Warning: 76541 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1411355\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 548025\n",
      "    MV>=10 removed (firm excluded)        : 76541\n",
      "    Rows written                          : 786789\n",
      "    Total removed                         : 624566\n",
      "    Check (initial-removed)               : 786789\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.79.csv\n",
      "  Warning: 314455 MV < 10M\n",
      "  Warning: 81522 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3019899\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 314455\n",
      "    MV>=10 removed (firm excluded)        : 81522\n",
      "    Rows written                          : 2623922\n",
      "    Total removed                         : 395977\n",
      "    Check (initial-removed)               : 2623922\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.80.csv\n",
      "  Warning: 396227 MV < 10M\n",
      "  Warning: 29288 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1350979\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 396227\n",
      "    MV>=10 removed (firm excluded)        : 29288\n",
      "    Rows written                          : 925464\n",
      "    Total removed                         : 425515\n",
      "    Check (initial-removed)               : 925464\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.81.csv\n",
      "  Warning: 1025082 MV < 10M\n",
      "  Warning: 144484 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2724590\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1025082\n",
      "    MV>=10 removed (firm excluded)        : 144484\n",
      "    Rows written                          : 1555024\n",
      "    Total removed                         : 1169566\n",
      "    Check (initial-removed)               : 1555024\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.82.csv\n",
      "  Warning: 1051766 MV < 10M\n",
      "  Warning: 79347 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3354681\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1051766\n",
      "    MV>=10 removed (firm excluded)        : 79347\n",
      "    Rows written                          : 2223568\n",
      "    Total removed                         : 1131113\n",
      "    Check (initial-removed)               : 2223568\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.83.csv\n",
      "  Warning: 968040 MV < 10M\n",
      "  Warning: 43331 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2870811\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 968040\n",
      "    MV>=10 removed (firm excluded)        : 43331\n",
      "    Rows written                          : 1859440\n",
      "    Total removed                         : 1011371\n",
      "    Check (initial-removed)               : 1859440\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20112014.84.csv\n",
      "  Warning: 493685 MV < 10M\n",
      "  Warning: 18336 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1385690\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 493685\n",
      "    MV>=10 removed (firm excluded)        : 18336\n",
      "    Rows written                          : 873669\n",
      "    Total removed                         : 512021\n",
      "    Check (initial-removed)               : 873669\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.85.csv\n",
      "  Warning: 1619641 MV < 10M\n",
      "  Warning: 227081 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5047421\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1619641\n",
      "    MV>=10 removed (firm excluded)        : 227081\n",
      "    Rows written                          : 3200699\n",
      "    Total removed                         : 1846722\n",
      "    Check (initial-removed)               : 3200699\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.86.csv\n",
      "  Warning: 410055 MV < 10M\n",
      "  Warning: 40487 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4112112\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 410055\n",
      "    MV>=10 removed (firm excluded)        : 40487\n",
      "    Rows written                          : 3661570\n",
      "    Total removed                         : 450542\n",
      "    Check (initial-removed)               : 3661570\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.87.csv\n",
      "  Warning: 2243569 MV < 10M\n",
      "  Warning: 247319 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4826805\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 2243569\n",
      "    MV>=10 removed (firm excluded)        : 247319\n",
      "    Rows written                          : 2335917\n",
      "    Total removed                         : 2490888\n",
      "    Check (initial-removed)               : 2335917\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.88.csv\n",
      "  Warning: 653714 MV < 10M\n",
      "  Warning: 71908 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3806105\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 653714\n",
      "    MV>=10 removed (firm excluded)        : 71908\n",
      "    Rows written                          : 3080483\n",
      "    Total removed                         : 725622\n",
      "    Check (initial-removed)               : 3080483\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.89.csv\n",
      "  Warning: 1935702 MV < 10M\n",
      "  Warning: 208494 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4857161\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1935702\n",
      "    MV>=10 removed (firm excluded)        : 208494\n",
      "    Rows written                          : 2712965\n",
      "    Total removed                         : 2144196\n",
      "    Check (initial-removed)               : 2712965\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.90.csv\n",
      "  Warning: 430742 MV < 10M\n",
      "  Warning: 46046 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4471110\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 430742\n",
      "    MV>=10 removed (firm excluded)        : 46046\n",
      "    Rows written                          : 3994322\n",
      "    Total removed                         : 476788\n",
      "    Check (initial-removed)               : 3994322\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.91.csv\n",
      "  Warning: 1033929 MV < 10M\n",
      "  Warning: 133335 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5552461\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1033929\n",
      "    MV>=10 removed (firm excluded)        : 133335\n",
      "    Rows written                          : 4385197\n",
      "    Total removed                         : 1167264\n",
      "    Check (initial-removed)               : 4385197\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.92.csv\n",
      "  Warning: 1211930 MV < 10M\n",
      "  Warning: 168338 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4270476\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1211930\n",
      "    MV>=10 removed (firm excluded)        : 168338\n",
      "    Rows written                          : 2890208\n",
      "    Total removed                         : 1380268\n",
      "    Check (initial-removed)               : 2890208\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.93.csv\n",
      "  Warning: 400828 MV < 10M\n",
      "  Warning: 70694 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3340118\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 400828\n",
      "    MV>=10 removed (firm excluded)        : 70694\n",
      "    Rows written                          : 2868596\n",
      "    Total removed                         : 471522\n",
      "    Check (initial-removed)               : 2868596\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.94.csv\n",
      "  Warning: 1762537 MV < 10M\n",
      "  Warning: 169799 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5078450\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1762537\n",
      "    MV>=10 removed (firm excluded)        : 169799\n",
      "    Rows written                          : 3146114\n",
      "    Total removed                         : 1932336\n",
      "    Check (initial-removed)               : 3146114\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.95.csv\n",
      "  Warning: 1150006 MV < 10M\n",
      "  Warning: 155548 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3156483\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1150006\n",
      "    MV>=10 removed (firm excluded)        : 155548\n",
      "    Rows written                          : 1850929\n",
      "    Total removed                         : 1305554\n",
      "    Check (initial-removed)               : 1850929\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.96.csv\n",
      "  Warning: 1130449 MV < 10M\n",
      "  Warning: 86288 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3616416\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1130449\n",
      "    MV>=10 removed (firm excluded)        : 86288\n",
      "    Rows written                          : 2399679\n",
      "    Total removed                         : 1216737\n",
      "    Check (initial-removed)               : 2399679\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.97.csv\n",
      "  Warning: 1026068 MV < 10M\n",
      "  Warning: 33474 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2973085\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1026068\n",
      "    MV>=10 removed (firm excluded)        : 33474\n",
      "    Rows written                          : 1913543\n",
      "    Total removed                         : 1059542\n",
      "    Check (initial-removed)               : 1913543\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20142017.98.csv\n",
      "  Warning: 1050600 MV < 10M\n",
      "  Warning: 26831 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2977074\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1050600\n",
      "    MV>=10 removed (firm excluded)        : 26831\n",
      "    Rows written                          : 1899643\n",
      "    Total removed                         : 1077431\n",
      "    Check (initial-removed)               : 1899643\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.A1.csv\n",
      "  Warning: 1650735 MV < 10M\n",
      "  Warning: 212881 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5385655\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1650735\n",
      "    MV>=10 removed (firm excluded)        : 212881\n",
      "    Rows written                          : 3522039\n",
      "    Total removed                         : 1863616\n",
      "    Check (initial-removed)               : 3522039\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.A2.csv\n",
      "  Warning: 434332 MV < 10M\n",
      "  Warning: 44165 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4711500\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 434332\n",
      "    MV>=10 removed (firm excluded)        : 44165\n",
      "    Rows written                          : 4233003\n",
      "    Total removed                         : 478497\n",
      "    Check (initial-removed)               : 4233003\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.A3.csv\n",
      "  Warning: 2320362 MV < 10M\n",
      "  Warning: 272219 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5182070\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 2320362\n",
      "    MV>=10 removed (firm excluded)        : 272219\n",
      "    Rows written                          : 2589489\n",
      "    Total removed                         : 2592581\n",
      "    Check (initial-removed)               : 2589489\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.A4.csv\n",
      "  Warning: 742107 MV < 10M\n",
      "  Warning: 149680 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4327556\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 742107\n",
      "    MV>=10 removed (firm excluded)        : 149680\n",
      "    Rows written                          : 3435769\n",
      "    Total removed                         : 891787\n",
      "    Check (initial-removed)               : 3435769\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.A5.csv\n",
      "  Warning: 2363635 MV < 10M\n",
      "  Warning: 256593 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5505122\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 2363635\n",
      "    MV>=10 removed (firm excluded)        : 256593\n",
      "    Rows written                          : 2884894\n",
      "    Total removed                         : 2620228\n",
      "    Check (initial-removed)               : 2884894\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.A6.csv\n",
      "  Warning: 458924 MV < 10M\n",
      "  Warning: 44519 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4735452\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 458924\n",
      "    MV>=10 removed (firm excluded)        : 44519\n",
      "    Rows written                          : 4232009\n",
      "    Total removed                         : 503443\n",
      "    Check (initial-removed)               : 4232009\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.A7.csv\n",
      "  Warning: 1168586 MV < 10M\n",
      "  Warning: 147538 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 6033972\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1168586\n",
      "    MV>=10 removed (firm excluded)        : 147538\n",
      "    Rows written                          : 4717848\n",
      "    Total removed                         : 1316124\n",
      "    Check (initial-removed)               : 4717848\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.A8.csv\n",
      "  Warning: 1348892 MV < 10M\n",
      "  Warning: 188255 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4624580\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1348892\n",
      "    MV>=10 removed (firm excluded)        : 188255\n",
      "    Rows written                          : 3087433\n",
      "    Total removed                         : 1537147\n",
      "    Check (initial-removed)               : 3087433\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.A9.csv\n",
      "  Warning: 471752 MV < 10M\n",
      "  Warning: 92706 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3604691\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 471752\n",
      "    MV>=10 removed (firm excluded)        : 92706\n",
      "    Rows written                          : 3040233\n",
      "    Total removed                         : 564458\n",
      "    Check (initial-removed)               : 3040233\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.B1.csv\n",
      "  Warning: 1941246 MV < 10M\n",
      "  Warning: 157491 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5539997\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1941246\n",
      "    MV>=10 removed (firm excluded)        : 157491\n",
      "    Rows written                          : 3441260\n",
      "    Total removed                         : 2098737\n",
      "    Check (initial-removed)               : 3441260\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.B2.csv\n",
      "  Warning: 1233252 MV < 10M\n",
      "  Warning: 177585 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3631710\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1233252\n",
      "    MV>=10 removed (firm excluded)        : 177585\n",
      "    Rows written                          : 2220873\n",
      "    Total removed                         : 1410837\n",
      "    Check (initial-removed)               : 2220873\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.B3.csv\n",
      "  Warning: 1191380 MV < 10M\n",
      "  Warning: 81194 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3850423\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1191380\n",
      "    MV>=10 removed (firm excluded)        : 81194\n",
      "    Rows written                          : 2577849\n",
      "    Total removed                         : 1272574\n",
      "    Check (initial-removed)               : 2577849\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.B4.csv\n",
      "  Warning: 1063242 MV < 10M\n",
      "  Warning: 14886 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3021782\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1063242\n",
      "    MV>=10 removed (firm excluded)        : 14886\n",
      "    Rows written                          : 1943654\n",
      "    Total removed                         : 1078128\n",
      "    Check (initial-removed)               : 1943654\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20172020.B5.csv\n",
      "  Warning: 1086338 MV < 10M\n",
      "  Warning: 14552 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3010240\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1086338\n",
      "    MV>=10 removed (firm excluded)        : 14552\n",
      "    Rows written                          : 1909350\n",
      "    Total removed                         : 1100890\n",
      "    Check (initial-removed)               : 1909350\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.C1.csv\n",
      "  Warning: 1632042 MV < 10M\n",
      "  Warning: 223760 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5757254\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1632042\n",
      "    MV>=10 removed (firm excluded)        : 223760\n",
      "    Rows written                          : 3901452\n",
      "    Total removed                         : 1855802\n",
      "    Check (initial-removed)               : 3901452\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.C2.csv\n",
      "  Warning: 438238 MV < 10M\n",
      "  Warning: 44182 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5522714\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 438238\n",
      "    MV>=10 removed (firm excluded)        : 44182\n",
      "    Rows written                          : 5040294\n",
      "    Total removed                         : 482420\n",
      "    Check (initial-removed)               : 5040294\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.C3.csv\n",
      "  Warning: 2418380 MV < 10M\n",
      "  Warning: 309048 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5686935\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 2418380\n",
      "    MV>=10 removed (firm excluded)        : 309048\n",
      "    Rows written                          : 2959507\n",
      "    Total removed                         : 2727428\n",
      "    Check (initial-removed)               : 2959507\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.C4.csv\n",
      "  Warning: 849736 MV < 10M\n",
      "  Warning: 168343 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4734102\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 849736\n",
      "    MV>=10 removed (firm excluded)        : 168343\n",
      "    Rows written                          : 3716023\n",
      "    Total removed                         : 1018079\n",
      "    Check (initial-removed)               : 3716023\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.C5.csv\n",
      "  Warning: 2297963 MV < 10M\n",
      "  Warning: 291183 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 6063149\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 2297963\n",
      "    MV>=10 removed (firm excluded)        : 291183\n",
      "    Rows written                          : 3474003\n",
      "    Total removed                         : 2589146\n",
      "    Check (initial-removed)               : 3474003\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.C6.csv\n",
      "  Warning: 559987 MV < 10M\n",
      "  Warning: 75156 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4986052\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 559987\n",
      "    MV>=10 removed (firm excluded)        : 75156\n",
      "    Rows written                          : 4350909\n",
      "    Total removed                         : 635143\n",
      "    Check (initial-removed)               : 4350909\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.C7.csv\n",
      "  Warning: 1239867 MV < 10M\n",
      "  Warning: 147509 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 6531408\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1239867\n",
      "    MV>=10 removed (firm excluded)        : 147509\n",
      "    Rows written                          : 5144032\n",
      "    Total removed                         : 1387376\n",
      "    Check (initial-removed)               : 5144032\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.C8.csv\n",
      "  Warning: 1356987 MV < 10M\n",
      "  Warning: 211011 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4895448\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1356987\n",
      "    MV>=10 removed (firm excluded)        : 211011\n",
      "    Rows written                          : 3327450\n",
      "    Total removed                         : 1567998\n",
      "    Check (initial-removed)               : 3327450\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.C9.csv\n",
      "  Warning: 444171 MV < 10M\n",
      "  Warning: 55859 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3900954\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 444171\n",
      "    MV>=10 removed (firm excluded)        : 55859\n",
      "    Rows written                          : 3400924\n",
      "    Total removed                         : 500030\n",
      "    Check (initial-removed)               : 3400924\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.D1.csv\n",
      "  Warning: 1919897 MV < 10M\n",
      "  Warning: 187764 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 5782611\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1919897\n",
      "    MV>=10 removed (firm excluded)        : 187764\n",
      "    Rows written                          : 3674950\n",
      "    Total removed                         : 2107661\n",
      "    Check (initial-removed)               : 3674950\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.D2.csv\n",
      "  Warning: 1243649 MV < 10M\n",
      "  Warning: 276208 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4541921\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1243649\n",
      "    MV>=10 removed (firm excluded)        : 276208\n",
      "    Rows written                          : 3022064\n",
      "    Total removed                         : 1519857\n",
      "    Check (initial-removed)               : 3022064\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.D3.csv\n",
      "  Warning: 1219750 MV < 10M\n",
      "  Warning: 109764 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 4223101\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1219750\n",
      "    MV>=10 removed (firm excluded)        : 109764\n",
      "    Rows written                          : 2893587\n",
      "    Total removed                         : 1329514\n",
      "    Check (initial-removed)               : 2893587\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.D4.csv\n",
      "  Warning: 1077453 MV < 10M\n",
      "  Warning: 1092 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3019150\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1077453\n",
      "    MV>=10 removed (firm excluded)        : 1092\n",
      "    Rows written                          : 1940605\n",
      "    Total removed                         : 1078545\n",
      "    Check (initial-removed)               : 1940605\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20202023.D5.csv\n",
      "  Warning: 1094928 MV < 10M\n",
      "  Warning: 696 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 3005017\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1094928\n",
      "    MV>=10 removed (firm excluded)        : 696\n",
      "    Rows written                          : 1909393\n",
      "    Total removed                         : 1095624\n",
      "    Check (initial-removed)               : 1909393\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.E1.csv\n",
      "  Warning: 714924 MV < 10M\n",
      "  Warning: 50578 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2238975\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 714924\n",
      "    MV>=10 removed (firm excluded)        : 50578\n",
      "    Rows written                          : 1473473\n",
      "    Total removed                         : 765502\n",
      "    Check (initial-removed)               : 1473473\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.E2.csv\n",
      "  Warning: 185942 MV < 10M\n",
      "  Warning: 8585 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2243590\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 185942\n",
      "    MV>=10 removed (firm excluded)        : 8585\n",
      "    Rows written                          : 2049063\n",
      "    Total removed                         : 194527\n",
      "    Check (initial-removed)               : 2049063\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.E3.csv\n",
      "  Warning: 1068123 MV < 10M\n",
      "  Warning: 52065 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2246119\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 1068123\n",
      "    MV>=10 removed (firm excluded)        : 52065\n",
      "    Rows written                          : 1125931\n",
      "    Total removed                         : 1120188\n",
      "    Check (initial-removed)               : 1125931\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.E4.csv\n",
      "  Warning: 394933 MV < 10M\n",
      "  Warning: 41836 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1867623\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 394933\n",
      "    MV>=10 removed (firm excluded)        : 41836\n",
      "    Rows written                          : 1430854\n",
      "    Total removed                         : 436769\n",
      "    Check (initial-removed)               : 1430854\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.E5.csv\n",
      "  Warning: 852518 MV < 10M\n",
      "  Warning: 68272 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2528224\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 852518\n",
      "    MV>=10 removed (firm excluded)        : 68272\n",
      "    Rows written                          : 1607434\n",
      "    Total removed                         : 920790\n",
      "    Check (initial-removed)               : 1607434\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.E6.csv\n",
      "  Warning: 264575 MV < 10M\n",
      "  Warning: 20464 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1971769\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 264575\n",
      "    MV>=10 removed (firm excluded)        : 20464\n",
      "    Rows written                          : 1686730\n",
      "    Total removed                         : 285039\n",
      "    Check (initial-removed)               : 1686730\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.E7.csv\n",
      "  Warning: 535014 MV < 10M\n",
      "  Warning: 31261 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2609488\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 535014\n",
      "    MV>=10 removed (firm excluded)        : 31261\n",
      "    Rows written                          : 2043213\n",
      "    Total removed                         : 566275\n",
      "    Check (initial-removed)               : 2043213\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.E8.csv\n",
      "  Warning: 567068 MV < 10M\n",
      "  Warning: 36371 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1915337\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 567068\n",
      "    MV>=10 removed (firm excluded)        : 36371\n",
      "    Rows written                          : 1311898\n",
      "    Total removed                         : 603439\n",
      "    Check (initial-removed)               : 1311898\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.E9.csv\n",
      "  Warning: 195280 MV < 10M\n",
      "  Warning: 16953 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1582031\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 195280\n",
      "    MV>=10 removed (firm excluded)        : 16953\n",
      "    Rows written                          : 1369798\n",
      "    Total removed                         : 212233\n",
      "    Check (initial-removed)               : 1369798\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.F1.csv\n",
      "  Warning: 788210 MV < 10M\n",
      "  Warning: 30354 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 2225244\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 788210\n",
      "    MV>=10 removed (firm excluded)        : 30354\n",
      "    Rows written                          : 1406680\n",
      "    Total removed                         : 818564\n",
      "    Check (initial-removed)               : 1406680\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.F2.csv\n",
      "  Warning: 601796 MV < 10M\n",
      "  Warning: 47390 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1778436\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 601796\n",
      "    MV>=10 removed (firm excluded)        : 47390\n",
      "    Rows written                          : 1129250\n",
      "    Total removed                         : 649186\n",
      "    Check (initial-removed)               : 1129250\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.F3.csv\n",
      "  Warning: 516825 MV < 10M\n",
      "  Warning: 19074 MV>=10 rows removed (firm excluded)\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1624542\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 516825\n",
      "    MV>=10 removed (firm excluded)        : 19074\n",
      "    Rows written                          : 1088643\n",
      "    Total removed                         : 535899\n",
      "    Check (initial-removed)               : 1088643\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.F4.csv\n",
      "  Warning: 409563 MV < 10M\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1146717\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 409563\n",
      "    MV>=10 removed (firm excluded)        : 0\n",
      "    Rows written                          : 737154\n",
      "    Total removed                         : 409563\n",
      "    Check (initial-removed)               : 737154\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV USD/DailyMVUSD20232025.F5.csv\n",
      "  Warning: 415800 MV < 10M\n",
      "  Row accounting:\n",
      "    Initial rows                          : 1141371\n",
      "    Invalid dates removed                 : 0\n",
      "    Invalid MV removed                    : 0\n",
      "    MV < 10M removed (row-level)          : 415800\n",
      "    MV>=10 removed (firm excluded)        : 0\n",
      "    Rows written                          : 725571\n",
      "    Total removed                         : 415800\n",
      "    Check (initial-removed)               : 725571\n",
      "Done. Combined file written to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/MV_raw.txt\n",
      "All dropped rows logged to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/MV_cleaning_errors.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# PURPOSE\n",
    "# -------\n",
    "# This cell processes a folder of raw daily market value CSV files and produces\n",
    "# one standardized long-format output file, while logging every dropped row.\n",
    "#\n",
    "# It is designed to remove all market values below 10 million (MV < 10) without\n",
    "# accidentally excluding firms that are above 10 million most of the time.\n",
    "#\n",
    "# INPUT\n",
    "# -----\n",
    "# - Multiple raw CSV files in `MarketValues_file_path` (processed sequentially).\n",
    "# - Each file structure:\n",
    "#     * First row = metadata (skipped)\n",
    "#     * Next row contains date labels starting at `start_col`\n",
    "#     * Subsequent rows contain DS_ID in column 0 and market values in date columns\n",
    "#\n",
    "# PROCESSING OVERVIEW (per file)\n",
    "# ------------------------------\n",
    "# 1) Read CSV (delimiter inferred), skip first metadata row.\n",
    "# 2) Drop fully empty rows/columns.\n",
    "# 3) Reshape wide matrix (DS_ID × date columns) into long format:\n",
    "#       DS_ID | DayDate | MV\n",
    "# 4) Date cleaning:\n",
    "#       - Parse DayDate as datetime (day-first)\n",
    "#       - Drop & log unparseable dates as \"Invalid DayDate\"\n",
    "# 5) MV cleaning:\n",
    "#       - Convert MV to numeric\n",
    "#       - Drop & log non-numeric MV as \"Invalid MV\"\n",
    "# 6) Enforce minimum MV rule:\n",
    "#       - Drop & log all rows with MV < 10 as \"MV below 10M\"\n",
    "# 7) Firm eligibility decision (firm-level):\n",
    "#       - This step decides whether to keep the firm's remaining MV>=10 rows\n",
    "#         or exclude the firm entirely.\n",
    "#       - Eligibility metrics are computed using all valid MV observations\n",
    "#         (including MV<10) to avoid skew:\n",
    "#           * n_obs: number of observations for the firm in this file\n",
    "#           * frac_ge10: fraction of observations where MV >= 10\n",
    "#       - A firm is eligible if:\n",
    "#           * n_obs >= MIN_OBS\n",
    "#           * frac_ge10 >= KEEP_FRAC\n",
    "#       - Firms that do not meet these thresholds are excluded entirely, and any\n",
    "#         MV>=10 rows that would otherwise remain are logged as:\n",
    "#           \"Firm excluded by stability thresholds\"\n",
    "#\n",
    "# OUTPUT\n",
    "# ------\n",
    "# - MV_raw.txt:\n",
    "#     DS_ID|DayDate|MV   (only MV>=10 for eligible firms)\n",
    "# - MV_cleaning_errors.txt:\n",
    "#     DS_ID|DayDate|MV|Reason|SourceFile (all dropped rows)\n",
    "#\n",
    "# AUDITABILITY\n",
    "# ------------\n",
    "# Per file, prints row accounting so:\n",
    "#   initial_rows = written + invalid_dates + invalid_mv + mv_below_10 + mv_ge10_excluded_firm\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pandas.errors import ParserError\n",
    "\n",
    "# --------------------------\n",
    "# User-tunable thresholds\n",
    "# --------------------------\n",
    "KEEP_FRAC = 0.80  # keep firms that are >=10M on at least 80% of observations\n",
    "MIN_OBS = 30      # require at least 30 observations per firm in a file\n",
    "\n",
    "# Input / output configuration\n",
    "input_folder = MarketValues_file_path\n",
    "output_folder = Temp_file_path_GO\n",
    "start_col = 7\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(output_folder, \"MV_raw.txt\")\n",
    "error_file = os.path.join(output_folder, \"MV_cleaning_errors.txt\")\n",
    "\n",
    "# Remove old files to start clean\n",
    "for f in [output_file, error_file]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "\n",
    "# Create output files with headers\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"DS_ID|DayDate|MV\\n\")\n",
    "\n",
    "with open(error_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"DS_ID|DayDate|MV|Reason|SourceFile\\n\")\n",
    "\n",
    "csv_files = sorted(glob.glob(os.path.join(input_folder, \"*.csv\")))\n",
    "\n",
    "for input_file in csv_files:\n",
    "    print(f\"Processing: {input_file}\")\n",
    "    source_name = os.path.basename(input_file)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Read raw file (robust parsing)\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        df = pd.read_csv(input_file, header=None, sep=None, engine=\"python\", skiprows=1)\n",
    "    except ParserError:\n",
    "        df = pd.read_csv(\n",
    "            input_file,\n",
    "            header=None,\n",
    "            sep=None,\n",
    "            engine=\"python\",\n",
    "            skiprows=1,\n",
    "            on_bad_lines=\"skip\"\n",
    "        )\n",
    "\n",
    "    # Drop empty rows and columns\n",
    "    df = df.dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # -----------------------------------\n",
    "    # Restructure wide -> long (DS_ID, DayDate, MV)\n",
    "    # -----------------------------------\n",
    "    dates = df.iloc[0, start_col:]\n",
    "    ids = df.iloc[1:, 0]\n",
    "    values = df.iloc[1:, start_col:]\n",
    "    values.columns = dates.values\n",
    "\n",
    "    id_map = ids.to_dict()\n",
    "\n",
    "    long_df = values.stack().reset_index()\n",
    "    long_df.columns = [\"row_idx\", \"DayDate\", \"MV\"]\n",
    "    long_df[\"DS_ID\"] = long_df[\"row_idx\"].map(id_map)\n",
    "\n",
    "    # Row accounting: initial rows after reshaping (before cleaning)\n",
    "    initial_rows = len(long_df)\n",
    "\n",
    "    # Initialize counters for removals\n",
    "    invalid_daydate_count = 0\n",
    "    invalid_mv_count = 0\n",
    "    low_mv_count = 0\n",
    "    firm_excluded_mv_ge10_count = 0\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1) Date parsing\n",
    "    # ---------------------------------------------------------------------\n",
    "    long_df[\"DayDate\"] = pd.to_datetime(long_df[\"DayDate\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "    invalid_daydate = long_df[long_df[\"DayDate\"].isna()].copy()\n",
    "    invalid_daydate_count = len(invalid_daydate)\n",
    "\n",
    "    if invalid_daydate_count > 0:\n",
    "        invalid_daydate[\"Reason\"] = \"Invalid DayDate\"\n",
    "        invalid_daydate[\"SourceFile\"] = source_name\n",
    "        invalid_daydate[[\"DS_ID\", \"DayDate\", \"MV\", \"Reason\", \"SourceFile\"]].to_csv(\n",
    "            error_file, sep=\"|\", index=False, mode=\"a\", header=False\n",
    "        )\n",
    "        print(f\"  Warning: {invalid_daydate_count} invalid dates\")\n",
    "\n",
    "    # Keep only valid dates\n",
    "    long_df = long_df[long_df[\"DayDate\"].notna()].copy()\n",
    "    long_df[\"DayDate\"] = long_df[\"DayDate\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2) Market value parsing\n",
    "    # ---------------------------------------------------------------------\n",
    "    long_df[\"MV\"] = pd.to_numeric(long_df[\"MV\"], errors=\"coerce\")\n",
    "\n",
    "    invalid_mv = long_df[long_df[\"MV\"].isna()].copy()\n",
    "    invalid_mv_count = len(invalid_mv)\n",
    "\n",
    "    if invalid_mv_count > 0:\n",
    "        invalid_mv[\"Reason\"] = \"Invalid MV\"\n",
    "        invalid_mv[\"SourceFile\"] = source_name\n",
    "        invalid_mv[[\"DS_ID\", \"DayDate\", \"MV\", \"Reason\", \"SourceFile\"]].to_csv(\n",
    "            error_file, sep=\"|\", index=False, mode=\"a\", header=False\n",
    "        )\n",
    "        print(f\"  Warning: {invalid_mv_count} invalid MV values\")\n",
    "\n",
    "    # Keep only valid numeric MV\n",
    "    long_df = long_df[long_df[\"MV\"].notna()].copy()\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3) Always remove MV < 10 (row-level)\n",
    "    # ---------------------------------------------------------------------\n",
    "    low_mv = long_df[long_df[\"MV\"] < 10].copy()\n",
    "    low_mv_count = len(low_mv)\n",
    "\n",
    "    if low_mv_count > 0:\n",
    "        low_mv[\"Reason\"] = \"MV below 10M\"\n",
    "        low_mv[\"SourceFile\"] = source_name\n",
    "        low_mv[[\"DS_ID\", \"DayDate\", \"MV\", \"Reason\", \"SourceFile\"]].to_csv(\n",
    "            error_file, sep=\"|\", index=False, mode=\"a\", header=False\n",
    "        )\n",
    "        print(f\"  Warning: {low_mv_count} MV < 10M\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 4) Firm-level eligibility using n_obs and frac_ge10 ONLY\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Compute metrics on all valid MV rows (including <10) so frac_ge10 is meaningful.\n",
    "    metrics = long_df.groupby(\"DS_ID\")[\"MV\"].agg(\n",
    "        n_obs=\"size\",\n",
    "        frac_ge10=lambda x: (x >= 10).mean()\n",
    "    )\n",
    "\n",
    "    eligible_ids = metrics[\n",
    "        (metrics[\"n_obs\"] >= MIN_OBS) &\n",
    "        (metrics[\"frac_ge10\"] >= KEEP_FRAC)\n",
    "    ].index\n",
    "\n",
    "    # Log MV>=10 rows that are removed solely because the firm is excluded\n",
    "    ineligible_mv_ge10 = long_df[\n",
    "        (~long_df[\"DS_ID\"].isin(eligible_ids)) &\n",
    "        (long_df[\"MV\"] >= 10)\n",
    "    ].copy()\n",
    "    firm_excluded_mv_ge10_count = len(ineligible_mv_ge10)\n",
    "\n",
    "    if firm_excluded_mv_ge10_count > 0:\n",
    "        ineligible_mv_ge10[\"Reason\"] = \"Firm excluded by stability thresholds\"\n",
    "        ineligible_mv_ge10[\"SourceFile\"] = source_name\n",
    "        ineligible_mv_ge10[[\"DS_ID\", \"DayDate\", \"MV\", \"Reason\", \"SourceFile\"]].to_csv(\n",
    "            error_file, sep=\"|\", index=False, mode=\"a\", header=False\n",
    "        )\n",
    "        print(f\"  Warning: {firm_excluded_mv_ge10_count} MV>=10 rows removed (firm excluded)\")\n",
    "\n",
    "    # Final kept dataset: eligible firms + MV >= 10 only\n",
    "    result = long_df[\n",
    "        (long_df[\"DS_ID\"].isin(eligible_ids)) &\n",
    "        (long_df[\"MV\"] >= 10)\n",
    "    ][[\"DS_ID\", \"DayDate\", \"MV\"]].copy()\n",
    "\n",
    "    rows_written = len(result)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Row accounting reconciliation\n",
    "    # ---------------------------------------------------------------------\n",
    "    total_removed = (\n",
    "        invalid_daydate_count\n",
    "        + invalid_mv_count\n",
    "        + low_mv_count\n",
    "        + firm_excluded_mv_ge10_count\n",
    "    )\n",
    "    check_value = initial_rows - total_removed  # should equal rows_written\n",
    "\n",
    "    print(\"  Row accounting:\")\n",
    "    print(f\"    Initial rows                          : {initial_rows}\")\n",
    "    print(f\"    Invalid dates removed                 : {invalid_daydate_count}\")\n",
    "    print(f\"    Invalid MV removed                    : {invalid_mv_count}\")\n",
    "    print(f\"    MV < 10M removed (row-level)          : {low_mv_count}\")\n",
    "    print(f\"    MV>=10 removed (firm excluded)        : {firm_excluded_mv_ge10_count}\")\n",
    "    print(f\"    Rows written                          : {rows_written}\")\n",
    "    print(f\"    Total removed                         : {total_removed}\")\n",
    "    print(f\"    Check (initial-removed)               : {check_value}\")\n",
    "\n",
    "    if check_value != rows_written:\n",
    "        print(\"  WARNING: Row accounting mismatch detected (please review pipeline).\")\n",
    "\n",
    "    # Append output\n",
    "    if not result.empty:\n",
    "        result.to_csv(output_file, sep=\"|\", index=False, mode=\"a\", header=False)\n",
    "\n",
    "print(f\"Done. Combined file written to: {output_file}\")\n",
    "print(f\"All dropped rows logged to: {error_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x-IWhe7Fu3v"
   },
   "source": [
    "### Remove Delisted Comp's MVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "error",
     "timestamp": 1764689893630,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "tjL18-nTFy5e",
    "outputId": "3ceb231f-bdf0-4001-eef8-84780551da03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Cleaned MV file written to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/MV_raw_excl_delisted.txt\n",
      "Delisting log written to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/MV_delisting_audit.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "652"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell takes the standardized long-format market value file `MV_raw.txt`\n",
    "# and removes \"dead\" tails for instruments that appear to be delisted.\n",
    "#\n",
    "# Delisting detection logic (per DS_ID):\n",
    "#   - Sort observations by DayDate ascending.\n",
    "#   - Look from the end of the series backwards and compute the length of the\n",
    "#     final run of identical MV values (a constant tail).\n",
    "#   - If this final constant run is at least 20 consecutive rows, and the last\n",
    "#     MV equals the MV in this run (which it does by construction), we treat\n",
    "#     the instrument as delisted.\n",
    "#   - We then DROP ALL ROWS from the first date of this final constant run\n",
    "#     onwards (i.e., the entire flat tail including the initial 20 days).\n",
    "#\n",
    "# Outputs:\n",
    "#   - `MV_cleaned.txt`  : same structure as MV_raw (DS_ID|DayDate|MV) but with\n",
    "#                         delisted tails removed.\n",
    "#   - `MV_delisting_log.txt` : list of instruments identified as delisted with\n",
    "#                              the date from which they are excluded.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input/output folder (same as in previous cell)\n",
    "output_folder = Temp_file_path_GO\n",
    "\n",
    "# Input standardized market value file\n",
    "mv_input_file = os.path.join(output_folder, \"MV_raw.txt\")\n",
    "\n",
    "# Output files\n",
    "mv_cleaned_file = os.path.join(output_folder, \"MV_raw_excl_delisted.txt\")\n",
    "delist_log_file = os.path.join(output_folder, \"MV_delisting_audit.txt\")\n",
    "\n",
    "# Remove old outputs if they exist\n",
    "for path in [mv_cleaned_file, delist_log_file]:\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "\n",
    "# Read the standardized long-format MV file\n",
    "# DS_ID is kept as string to avoid losing leading zeros etc.\n",
    "mv_df = pd.read_csv(\n",
    "    mv_input_file,\n",
    "    sep=\"|\",\n",
    "    dtype={\"DS_ID\": str}\n",
    ")\n",
    "\n",
    "# Parse DayDate to datetime for proper sorting and comparison\n",
    "mv_df[\"DayDate\"] = pd.to_datetime(mv_df[\"DayDate\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "# Drop any rows where DayDate could not be parsed (should normally not occur)\n",
    "mv_df = mv_df[mv_df[\"DayDate\"].notna()].copy()\n",
    "\n",
    "# Create output files with headers\n",
    "with open(mv_cleaned_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    f_out.write(\"DS_ID|DayDate|MV\\n\")\n",
    "\n",
    "with open(delist_log_file, \"w\", encoding=\"utf-8\") as f_log:\n",
    "    f_log.write(\"DS_ID|DelistFromDate\\n\")\n",
    "\n",
    "# Minimum tail length (in consecutive rows) to declare delisting\n",
    "MIN_TAIL_LENGTH = 20\n",
    "\n",
    "# Process each instrument separately\n",
    "for ds_id, grp in mv_df.groupby(\"DS_ID\"):\n",
    "    g = grp.sort_values(\"DayDate\").copy()\n",
    "    mv_values = g[\"MV\"].values\n",
    "    n = len(mv_values)\n",
    "\n",
    "    # Default: keep full history\n",
    "    cutoff_date = None\n",
    "\n",
    "    if n >= MIN_TAIL_LENGTH:\n",
    "        # Identify the final run of identical MV values from the end backwards\n",
    "        last_mv = mv_values[-1]\n",
    "        tail_length = 1\n",
    "\n",
    "        for i in range(n - 2, -1, -1):\n",
    "            if mv_values[i] == last_mv:\n",
    "                tail_length += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # If the final constant tail is long enough, declare delisting\n",
    "        if tail_length >= MIN_TAIL_LENGTH:\n",
    "            # Index (within the sorted group) of the first obs in the final constant run\n",
    "            start_tail_idx = n - tail_length\n",
    "            cutoff_date = g.iloc[start_tail_idx][\"DayDate\"]\n",
    "\n",
    "    if cutoff_date is not None:\n",
    "        # Log delisting info: instrument and the date from which we drop values\n",
    "        with open(delist_log_file, \"a\", encoding=\"utf-8\") as f_log:\n",
    "            f_log.write(f\"{ds_id}|{cutoff_date.strftime('%Y-%m-%d')}\\n\")\n",
    "\n",
    "        # Exclude all rows from cutoff_date onwards (including the tail itself)\n",
    "        g_filtered = g[g[\"DayDate\"] < cutoff_date].copy()\n",
    "    else:\n",
    "        # No delisting detected: keep full history\n",
    "        g_filtered = g\n",
    "\n",
    "    # Append remaining rows for this DS_ID to cleaned output file\n",
    "    if not g_filtered.empty:\n",
    "        # Convert DayDate back to YYYY-MM-DD string format\n",
    "        g_filtered[\"DayDate\"] = g_filtered[\"DayDate\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        g_filtered[[\"DS_ID\", \"DayDate\", \"MV\"]].to_csv(\n",
    "            mv_cleaned_file,\n",
    "            sep=\"|\",\n",
    "            index=False,\n",
    "            header=False,\n",
    "            mode=\"a\"\n",
    "        )\n",
    "\n",
    "print(f\"Done. Cleaned MV file written to: {mv_cleaned_file}\")\n",
    "print(f\"Delisting log written to: {delist_log_file}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Cleanup: free memory from this cell\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "# Delete large objects\n",
    "del mv_df, g, grp, mv_values, g_filtered\n",
    "\n",
    "# If still in scope\n",
    "try:\n",
    "    del ds_id\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA9_D0JETpwo"
   },
   "source": [
    "### Transform Total Return Index (TRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Ok-oIwbzQkX0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.01.csv\n",
      "  Rows written: 1571667\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.02.csv\n",
      "  Rows written: 973685\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.03.csv\n",
      "  Rows written: 2035848\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.04.csv\n",
      "  Rows written: 1542081\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.05.csv\n",
      "  Rows written: 1839856\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.06.csv\n",
      "  Rows written: 2742617\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.07.csv\n",
      "  Rows written: 1940603\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.08.csv\n",
      "  Rows written: 1108178\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.09.csv\n",
      "  Rows written: 974787\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.10.csv\n",
      "  Rows written: 1983762\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.11.csv\n",
      "  Rows written: 839314\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.12.csv\n",
      "  Rows written: 1248998\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.13.csv\n",
      "  Rows written: 1348929\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19911995.14.csv\n",
      "  Rows written: 1360645\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.15.csv\n",
      "  Rows written: 2548794\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.16.csv\n",
      "  Rows written: 1846477\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.17.csv\n",
      "  Rows written: 2779790\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.18.csv\n",
      "  Rows written: 2270739\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.19.csv\n",
      "  Rows written: 3025332\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.20.csv\n",
      "  Rows written: 3382028\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.21.csv\n",
      "  Rows written: 2933835\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.22.csv\n",
      "  Rows written: 1837425\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.23.csv\n",
      "  Rows written: 1479061\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.24.csv\n",
      "  Rows written: 2649167\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.25.csv\n",
      "  Rows written: 1340473\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.26.csv\n",
      "  Rows written: 2121093\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.27.csv\n",
      "  Rows written: 2158307\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19951999.28.csv\n",
      "  Rows written: 2230392\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.29.csv\n",
      "  Rows written: 3559280\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.30.csv\n",
      "  Rows written: 2607869\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.31.csv\n",
      "  Rows written: 3692001\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.32.csv\n",
      "  Rows written: 3243688\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.33.csv\n",
      "  Rows written: 3554440\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.34.csv\n",
      "  Rows written: 3982883\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.35.csv\n",
      "  Rows written: 3930822\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.36.csv\n",
      "  Rows written: 2705072\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.37.csv\n",
      "  Rows written: 2092630\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.38.csv\n",
      "  Rows written: 3405796\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.39.csv\n",
      "  Rows written: 1988201\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.40.csv\n",
      "  Rows written: 2964317\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.41.csv\n",
      "  Rows written: 2944315\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD19992003.42.csv\n",
      "  Rows written: 3045233\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.43.csv\n",
      "  Rows written: 4617932\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.44.csv\n",
      "  Rows written: 3112009\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.45.csv\n",
      "  Rows written: 4594265\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.46.csv\n",
      "  Rows written: 3782619\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.47.csv\n",
      "  Rows written: 4349281\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.48.csv\n",
      "  Rows written: 4780677\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.49.csv\n",
      "  Rows written: 5037550\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.50.csv\n",
      "  Rows written: 3432603\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.51.csv\n",
      "  Rows written: 2823372\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.52.csv\n",
      "  Rows written: 4365116\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.53.csv\n",
      "  Rows written: 2516331\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.54.csv\n",
      "  Rows written: 3535715\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.55.csv\n",
      "  Rows written: 3320573\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20032007.56.csv\n",
      "  Rows written: 3409488\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.57.csv\n",
      "  Rows written: 5998120\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.58.csv\n",
      "  Rows written: 3997632\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.59.csv\n",
      "  Rows written: 5707133\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.60.csv\n",
      "  Rows written: 4250840\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.61.csv\n",
      "  Rows written: 5294008\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.62.csv\n",
      "  Rows written: 5406496\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.63.csv\n",
      "  Rows written: 6149510\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.64.csv\n",
      "  Rows written: 4593467\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.65.csv\n",
      "  Rows written: 3488718\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.66.csv\n",
      "  Rows written: 5646335\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.67.csv\n",
      "  Rows written: 3125638\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.68.csv\n",
      "  Rows written: 4078059\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.69.csv\n",
      "  Rows written: 3663564\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20072011.70.csv\n",
      "  Rows written: 3703975\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.71.csv\n",
      "  Rows written: 4879980\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.72.csv\n",
      "  Rows written: 3700604\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.73.csv\n",
      "  Rows written: 4895055\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.74.csv\n",
      "  Rows written: 3429883\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.75.csv\n",
      "  Rows written: 4399683\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.76.csv\n",
      "  Rows written: 4217183\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.77.csv\n",
      "  Rows written: 5127840\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.78.csv\n",
      "  Rows written: 3930676\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.79.csv\n",
      "  Rows written: 3021480\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.80.csv\n",
      "  Rows written: 4700313\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.81.csv\n",
      "  Rows written: 2729607\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.82.csv\n",
      "  Rows written: 3372974\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.83.csv\n",
      "  Rows written: 2894331\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20112014.84.csv\n",
      "  Rows written: 2910938\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.85.csv\n",
      "  Rows written: 5176181\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.86.csv\n",
      "  Rows written: 4112373\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.87.csv\n",
      "  Rows written: 5130556\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.88.csv\n",
      "  Rows written: 3810803\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.89.csv\n",
      "  Rows written: 4860845\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.90.csv\n",
      "  Rows written: 4468761\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.91.csv\n",
      "  Rows written: 5571174\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.92.csv\n",
      "  Rows written: 4280753\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.93.csv\n",
      "  Rows written: 3343250\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.94.csv\n",
      "  Rows written: 5083409\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.95.csv\n",
      "  Rows written: 3161891\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.96.csv\n",
      "  Rows written: 3633106\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.97.csv\n",
      "  Rows written: 2996575\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20142017.98.csv\n",
      "  Rows written: 2995517\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.A1.csv\n",
      "  Rows written: 5523215\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.A2.csv\n",
      "  Rows written: 4711500\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.A3.csv\n",
      "  Rows written: 5487410\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.A4.csv\n",
      "  Rows written: 4332266\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.A5.csv\n",
      "  Rows written: 5501985\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.A6.csv\n",
      "  Rows written: 4735848\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.A7.csv\n",
      "  Rows written: 6051069\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.A8.csv\n",
      "  Rows written: 4634785\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.A9.csv\n",
      "  Rows written: 3606717\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.B1.csv\n",
      "  Rows written: 5545313\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.B2.csv\n",
      "  Rows written: 3637209\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.B3.csv\n",
      "  Rows written: 3868240\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.B4.csv\n",
      "  Rows written: 3045332\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20172020.B5.csv\n",
      "  Rows written: 3029362\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.C1.csv\n",
      "  Rows written: 5905783\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.C2.csv\n",
      "  Rows written: 5523248\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.C3.csv\n",
      "  Rows written: 5995357\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.C4.csv\n",
      "  Rows written: 4738279\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.C5.csv\n",
      "  Rows written: 6056623\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.C6.csv\n",
      "  Rows written: 4986834\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.C7.csv\n",
      "  Rows written: 6548612\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.C8.csv\n",
      "  Rows written: 4905564\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.C9.csv\n",
      "  Rows written: 3901154\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.D1.csv\n",
      "  Rows written: 5788085\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.D2.csv\n",
      "  Rows written: 4549699\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.D3.csv\n",
      "  Rows written: 4240171\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.D4.csv\n",
      "  Rows written: 3042610\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20202023.D5.csv\n",
      "  Rows written: 3024567\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.E1.csv\n",
      "  Rows written: 2293262\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.E2.csv\n",
      "  Rows written: 2280073\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.E3.csv\n",
      "  Rows written: 2401506\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.E4.csv\n",
      "  Rows written: 1867471\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.E5.csv\n",
      "  Rows written: 2524221\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.E6.csv\n",
      "  Rows written: 1958622\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.E7.csv\n",
      "  Rows written: 2610468\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.E8.csv\n",
      "  Rows written: 1983972\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.E9.csv\n",
      "  Rows written: 1568756\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.F1.csv\n",
      "  Rows written: 2223572\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.F2.csv\n",
      "  Rows written: 1908842\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.F3.csv\n",
      "  Rows written: 2360026\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.F4.csv\n",
      "  Rows written: 2033159\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily Returns USD/DailyReturnsUSD20232025.F5.csv\n",
      "  Rows written: 2020649\n",
      "Done. Combined file written to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/TRI_raw.txt\n",
      "Dropped rows with reasons logged in: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/TRI_cleaning_errors.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell standardizes a collection of raw daily TRI CSV files into a single\n",
    "# long-format text file while explicitly logging all dropped rows:\n",
    "#\n",
    "#   - Reads all TRI CSV files from a given folder one by one to keep memory\n",
    "#     usage controlled.\n",
    "#   - Skips the first metadata row, infers the separator for each CSV, and\n",
    "#     removes fully empty rows and columns.\n",
    "#   - Interprets the first remaining row (from start_col onward) as date labels,\n",
    "#     and the following rows as IDs with TRI values across those date columns.\n",
    "#   - Reshapes the wide date matrix into long format with columns:\n",
    "#         DS_ID, DayDate, TRI\n",
    "#   - Cleans and filters:\n",
    "#       * Parses DayDate as datetime (day-first). Rows where the date fails to\n",
    "#         parse are written to TRI_cleaning_errors.txt with reason\n",
    "#         \"Invalid DayDate\".\n",
    "#       * Converts TRI values to numeric. Rows with non-numeric TRI are written\n",
    "#         to the same error file with reason \"Invalid TRI\".\n",
    "#       * Valid rows keep a normalized DayDate in 'YYYY-MM-DD' format.\n",
    "#   - Appends all valid rows into TRI_raw.txt, using a pipe separator.\n",
    "#   - Appends all invalid rows into TRI_cleaning_errors.txt, including the\n",
    "#     originating source file name and a reason for filtering.\n",
    "# =============================================================================\n",
    "\n",
    "import os                       # Filesystem operations (paths, existence checks, removals)\n",
    "import glob                     # Pattern-based file listing for CSV files\n",
    "import pandas as pd             # DataFrame handling and CSV parsing\n",
    "from pandas.errors import ParserError  # Specific exception type for CSV parsing issues\n",
    "\n",
    "# Input folder containing all raw CSV files\n",
    "input_folder = DailyTotalReturns_file_path\n",
    "\n",
    "# Output folder where the combined result will be written\n",
    "output_folder = Temp_file_path_GO\n",
    "\n",
    "# Column index (0-based) where the date columns begin (column H)\n",
    "start_col = 7\n",
    "\n",
    "# Ensure the output directory exists (create if missing)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Path of the combined output file\n",
    "output_file = os.path.join(output_folder, \"TRI_raw.txt\")\n",
    "\n",
    "# Path for the cleaning error log\n",
    "error_file = os.path.join(output_folder, \"TRI_cleaning_errors.txt\")\n",
    "\n",
    "# Remove old output files if they already exist so this run starts fresh\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "if os.path.exists(error_file):\n",
    "    os.remove(error_file)\n",
    "\n",
    "# Create the output file with header describing the standardized columns\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"DS_ID|DayDate|TRI\\n\")\n",
    "\n",
    "# Create the error log file with header describing diagnostic columns\n",
    "with open(error_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"DS_ID|DayDate|TRI|Reason|SourceFile\\n\")\n",
    "\n",
    "# Collect all CSV files in sorted order for deterministic processing\n",
    "csv_files = sorted(glob.glob(os.path.join(input_folder, \"*.csv\")))\n",
    "\n",
    "# Main loop: process one file at a time to limit memory usage\n",
    "for input_file in csv_files:\n",
    "    # Log which file is being processed\n",
    "    print(f\"Processing: {input_file}\")\n",
    "    # Extract just the file name to store as the SourceFile in logs\n",
    "    basename = os.path.basename(input_file)\n",
    "\n",
    "    try:\n",
    "        # Attempt to read the CSV, skipping the first metadata row\n",
    "        # sep=None lets pandas infer the delimiter dynamically\n",
    "        df = pd.read_csv(\n",
    "            input_file,\n",
    "            header=None,\n",
    "            sep=None,\n",
    "            engine=\"python\",\n",
    "            skiprows=1\n",
    "        )\n",
    "    except ParserError:\n",
    "        # If parsing fails due to malformed lines, retry while skipping bad lines\n",
    "        df = pd.read_csv(\n",
    "            input_file,\n",
    "            header=None,\n",
    "            sep=None,\n",
    "            engine=\"python\",\n",
    "            skiprows=1,\n",
    "            on_bad_lines=\"skip\"\n",
    "        )\n",
    "\n",
    "    # Remove fully empty rows\n",
    "    df = df.dropna(how=\"all\")\n",
    "    # Remove fully empty columns\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Extract dates (header from start_col onward), IDs (first column), and TRI values matrix\n",
    "    dates = df.iloc[0, start_col:]      # First row after metadata contains date labels from start_col\n",
    "    ids = df.iloc[1:, 0]                # First column (excluding the date row) holds DS_ID values\n",
    "    values = df.iloc[1:, start_col:]    # Remaining rows and columns hold TRI values\n",
    "\n",
    "    # Assign date strings as column names on the values DataFrame\n",
    "    values.columns = dates.values\n",
    "\n",
    "    # Map row indices in values to the corresponding DS_IDs from ids\n",
    "    id_map = ids.to_dict()\n",
    "\n",
    "    # Convert from wide (IDs x dates) to long format:\n",
    "    #   stack -> MultiIndex (row_idx, DayDate) with TRI as values\n",
    "    long_df = values.stack().reset_index()\n",
    "    long_df.columns = [\"row_idx\", \"DayDate\", \"TRI\"]\n",
    "\n",
    "    # Insert DS_ID column using the row index mapping\n",
    "    long_df[\"DS_ID\"] = long_df[\"row_idx\"].map(id_map)\n",
    "\n",
    "    # Record originating file for each row (used in error logging)\n",
    "    long_df[\"SourceFile\"] = basename\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Validate DayDate: keep invalid rows separately with reason\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse DayDate strings as datetime in day-first format (e.g., DD/MM/YYYY)\n",
    "    parsed_dates = pd.to_datetime(\n",
    "        long_df[\"DayDate\"],\n",
    "        errors=\"coerce\",\n",
    "        dayfirst=True\n",
    "    )\n",
    "    # Boolean mask of rows where date parsing failed (NaT)\n",
    "    invalid_date_mask = parsed_dates.isna()\n",
    "    # Subset of rows with invalid dates\n",
    "    invalid_daydate = long_df[invalid_date_mask].copy()\n",
    "\n",
    "    if not invalid_daydate.empty:\n",
    "        # Label invalid date rows with a descriptive reason\n",
    "        invalid_daydate[\"Reason\"] = \"Invalid DayDate\"\n",
    "        # Select and order the columns to match the error file header\n",
    "        invalid_daydate[[\"DS_ID\", \"DayDate\", \"TRI\", \"Reason\", \"SourceFile\"]].to_csv(\n",
    "            error_file,\n",
    "            sep=\"|\",\n",
    "            index=False,\n",
    "            mode=\"a\",\n",
    "            header=False\n",
    "        )\n",
    "        # Report count of invalid-date rows for this file\n",
    "        print(f\"  Warning: {len(invalid_daydate)} invalid date entries detected in this file.\")\n",
    "\n",
    "    # Keep only rows with successfully parsed dates for further processing\n",
    "    valid_df = long_df[~invalid_date_mask].copy()\n",
    "    # Attach the parsed datetime values for valid rows\n",
    "    valid_df[\"DayDate\"] = parsed_dates[~invalid_date_mask]\n",
    "\n",
    "    # Standardize the date representation to 'YYYY-MM-DD' strings\n",
    "    valid_df[\"DayDate\"] = valid_df[\"DayDate\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Validate TRI: must be numeric; log invalid rows with reason\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert TRI values to numeric; non-convertible entries become NaN\n",
    "    tri_numeric = pd.to_numeric(valid_df[\"TRI\"], errors=\"coerce\")\n",
    "    # Mask of rows where TRI is invalid (NaN after coercion)\n",
    "    invalid_tri_mask = tri_numeric.isna()\n",
    "    # Rows with invalid TRI values\n",
    "    invalid_tri = valid_df[invalid_tri_mask].copy()\n",
    "\n",
    "    if not invalid_tri.empty:\n",
    "        # Label these rows with reason indicating TRI parsing issues\n",
    "        invalid_tri[\"Reason\"] = \"Invalid TRI\"\n",
    "        # Save invalid TRI rows to the error log file\n",
    "        invalid_tri[[\"DS_ID\", \"DayDate\", \"TRI\", \"Reason\", \"SourceFile\"]].to_csv(\n",
    "            error_file,\n",
    "            sep=\"|\",\n",
    "            index=False,\n",
    "            mode=\"a\",\n",
    "            header=False\n",
    "        )\n",
    "        # Report how many invalid TRI entries were found for this file\n",
    "        print(f\"  Warning: {len(invalid_tri)} invalid TRI entries detected in this file.\")\n",
    "\n",
    "    # Keep only rows with valid numeric TRI values\n",
    "    clean_df = valid_df[~invalid_tri_mask].copy()\n",
    "    # Assign the numeric TRI series to the clean DataFrame\n",
    "    clean_df[\"TRI\"] = tri_numeric[~invalid_tri_mask]\n",
    "\n",
    "    # Select output columns for the standardized long-format result\n",
    "    result = clean_df[[\"DS_ID\", \"DayDate\", \"TRI\"]]\n",
    "\n",
    "    # Report how many cleaned rows will be written for this file\n",
    "    print(f\"  Rows written: {len(result)}\")\n",
    "\n",
    "    # If there are no valid rows, skip appending to the combined output\n",
    "    if len(result) == 0:\n",
    "        continue\n",
    "\n",
    "    # Append valid rows to the combined TRI_raw output file without header\n",
    "    result.to_csv(\n",
    "        output_file,\n",
    "        sep=\"|\",\n",
    "        index=False,\n",
    "        mode=\"a\",\n",
    "        header=False\n",
    "    )\n",
    "\n",
    "# Final summary logs with paths to the combined outputs\n",
    "print(f\"Done. Combined file written to: {output_file}\")\n",
    "print(f\"Dropped rows with reasons logged in: {error_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EfiohHcGZ5z"
   },
   "source": [
    "### Remove Delisted Comp's TRIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SKPnitRxGjA3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Cleaned TRI file written to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/TRI_raw_excl_delisted.txt\n",
      "Delisting log written to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/TRI_delisting_log.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell takes the standardized long-format TRI file `TRI_raw.txt`\n",
    "# and removes \"dead\" tails for instruments that appear to be delisted.\n",
    "#\n",
    "# Delisting detection logic (per DS_ID):\n",
    "#   - Sort observations by DayDate ascending.\n",
    "#   - Look from the end of the series backwards and compute the length of the\n",
    "#     final run of identical TRI values (a constant tail).\n",
    "#   - If this final constant run is at least 20 consecutive rows, and the last\n",
    "#     TRI equals the TRI in this run (true by construction), we treat the\n",
    "#     instrument as delisted.\n",
    "#   - We then DROP ALL ROWS from the first date of this final constant run\n",
    "#     onwards (i.e., the entire flat tail including the initial 20 days).\n",
    "#\n",
    "# Outputs:\n",
    "#   - `TRI_cleaned.txt`  : same structure as TRI_raw (DS_ID|DayDate|TRI) but with\n",
    "#                          delisted tails removed.\n",
    "#   - `TRI_delisting_log.txt` : list of instruments identified as delisted with\n",
    "#                               the date from which they are excluded.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Output folder (same as in previous cells)\n",
    "output_folder = Temp_file_path_GO\n",
    "\n",
    "# Input standardized TRI file\n",
    "tri_input_file = os.path.join(output_folder, \"TRI_raw.txt\")\n",
    "\n",
    "# Output files\n",
    "tri_cleaned_file = os.path.join(output_folder, \"TRI_raw_excl_delisted.txt\")\n",
    "tri_delist_log_file = os.path.join(output_folder, \"TRI_delisting_log.txt\")\n",
    "\n",
    "# Remove old outputs if they exist\n",
    "for path in [tri_cleaned_file, tri_delist_log_file]:\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "\n",
    "# Read the standardized long-format TRI file\n",
    "# DS_ID is kept as string to avoid losing leading zeros etc.\n",
    "tri_df = pd.read_csv(\n",
    "    tri_input_file,\n",
    "    sep=\"|\",\n",
    "    dtype={\"DS_ID\": str}\n",
    ")\n",
    "\n",
    "# Parse DayDate to datetime for proper sorting and comparison\n",
    "tri_df[\"DayDate\"] = pd.to_datetime(tri_df[\"DayDate\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "# Drop any rows where DayDate could not be parsed (should normally not occur)\n",
    "tri_df = tri_df[tri_df[\"DayDate\"].notna()].copy()\n",
    "\n",
    "# Create output files with headers\n",
    "with open(tri_cleaned_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    f_out.write(\"DS_ID|DayDate|TRI\\n\")\n",
    "\n",
    "with open(tri_delist_log_file, \"w\", encoding=\"utf-8\") as f_log:\n",
    "    f_log.write(\"DS_ID|DelistFromDate\\n\")\n",
    "\n",
    "# Minimum tail length (in consecutive rows) to declare delisting\n",
    "MIN_TAIL_LENGTH = 20\n",
    "\n",
    "# Process each instrument separately\n",
    "for ds_id, grp in tri_df.groupby(\"DS_ID\"):\n",
    "    g = grp.sort_values(\"DayDate\").copy()\n",
    "    tri_values = g[\"TRI\"].values\n",
    "    n = len(tri_values)\n",
    "\n",
    "    # Default: keep full history\n",
    "    cutoff_date = None\n",
    "\n",
    "    if n >= MIN_TAIL_LENGTH:\n",
    "        # Identify the final run of identical TRI values from the end backwards\n",
    "        last_tri = tri_values[-1]\n",
    "        tail_length = 1\n",
    "\n",
    "        for i in range(n - 2, -1, -1):\n",
    "            if tri_values[i] == last_tri:\n",
    "                tail_length += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # If the final constant tail is long enough, declare delisting\n",
    "        if tail_length >= MIN_TAIL_LENGTH:\n",
    "            # Index (within the sorted group) of the first obs in the final constant run\n",
    "            start_tail_idx = n - tail_length\n",
    "            cutoff_date = g.iloc[start_tail_idx][\"DayDate\"]\n",
    "\n",
    "    if cutoff_date is not None:\n",
    "        # Log delisting info: instrument and the date from which we drop values\n",
    "        with open(tri_delist_log_file, \"a\", encoding=\"utf-8\") as f_log:\n",
    "            f_log.write(f\"{ds_id}|{cutoff_date.strftime('%Y-%m-%d')}\\n\")\n",
    "\n",
    "        # Exclude all rows from cutoff_date onwards (including the tail itself)\n",
    "        g_filtered = g[g[\"DayDate\"] < cutoff_date].copy()\n",
    "    else:\n",
    "        # No delisting detected: keep full history\n",
    "        g_filtered = g\n",
    "\n",
    "    # Append remaining rows for this DS_ID to cleaned output file\n",
    "    if not g_filtered.empty:\n",
    "        # Convert DayDate back to YYYY-MM-DD string format\n",
    "        g_filtered[\"DayDate\"] = g_filtered[\"DayDate\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        g_filtered[[\"DS_ID\", \"DayDate\", \"TRI\"]].to_csv(\n",
    "            tri_cleaned_file,\n",
    "            sep=\"|\",\n",
    "            index=False,\n",
    "            header=False,\n",
    "            mode=\"a\"\n",
    "        )\n",
    "\n",
    "print(f\"Done. Cleaned TRI file written to: {tri_cleaned_file}\")\n",
    "print(f\"Delisting log written to: {tri_delist_log_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Cleanup: free memory from this cell\n",
    "# -----------------------------\n",
    "import gc\n",
    "\n",
    "# Delete the large / important objects from this cell\n",
    "for name in [\n",
    "    \"tri_df\", \"g\", \"grp\", \"tri_values\", \"g_filtered\",\n",
    "    \"ds_id\", \"last_tri\", \"tail_length\", \"cutoff_date\"\n",
    "]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfUCAp4PXW2j"
   },
   "source": [
    "### ID Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ieYvwwMlXbAA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mapping file: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/ID_mapping_clean.txt\n",
      "Mapping columns: ['DS_ID', 'ID']\n",
      "\n",
      "=== Processing MV_raw_excl_delisted.txt in chunks of 1000000 rows ===\n",
      "  Chunk 1: 1000000 rows\n",
      "  Chunk 2: 1000000 rows\n",
      "  Chunk 3: 1000000 rows\n",
      "  Chunk 4: 1000000 rows\n",
      "  Chunk 5: 1000000 rows\n",
      "  Chunk 6: 1000000 rows\n",
      "  Chunk 7: 1000000 rows\n",
      "  Chunk 8: 1000000 rows\n",
      "  Chunk 9: 1000000 rows\n",
      "  Chunk 10: 1000000 rows\n",
      "  Chunk 11: 1000000 rows\n",
      "  Chunk 12: 1000000 rows\n",
      "  Chunk 13: 1000000 rows\n",
      "  Chunk 14: 1000000 rows\n",
      "  Chunk 15: 1000000 rows\n",
      "  Chunk 16: 1000000 rows\n",
      "  Chunk 17: 1000000 rows\n",
      "  Chunk 18: 1000000 rows\n",
      "  Chunk 19: 1000000 rows\n",
      "  Chunk 20: 1000000 rows\n",
      "  Chunk 21: 1000000 rows\n",
      "  Chunk 22: 1000000 rows\n",
      "  Chunk 23: 1000000 rows\n",
      "  Chunk 24: 1000000 rows\n",
      "  Chunk 25: 1000000 rows\n",
      "  Chunk 26: 1000000 rows\n",
      "  Chunk 27: 1000000 rows\n",
      "  Chunk 28: 1000000 rows\n",
      "  Chunk 29: 1000000 rows\n",
      "  Chunk 30: 1000000 rows\n",
      "  Chunk 31: 1000000 rows\n",
      "  Chunk 32: 1000000 rows\n",
      "  Chunk 33: 1000000 rows\n",
      "  Chunk 34: 1000000 rows\n",
      "  Chunk 35: 1000000 rows\n",
      "  Chunk 36: 1000000 rows\n",
      "  Chunk 37: 1000000 rows\n",
      "  Chunk 38: 1000000 rows\n",
      "  Chunk 39: 1000000 rows\n",
      "  Chunk 40: 1000000 rows\n",
      "  Chunk 41: 1000000 rows\n",
      "  Chunk 42: 1000000 rows\n",
      "  Chunk 43: 1000000 rows\n",
      "  Chunk 44: 1000000 rows\n",
      "  Chunk 45: 1000000 rows\n",
      "  Chunk 46: 1000000 rows\n",
      "  Chunk 47: 1000000 rows\n",
      "  Chunk 48: 1000000 rows\n",
      "  Chunk 49: 1000000 rows\n",
      "  Chunk 50: 1000000 rows\n",
      "  Chunk 51: 1000000 rows\n",
      "  Chunk 52: 1000000 rows\n",
      "  Chunk 53: 1000000 rows\n",
      "  Chunk 54: 1000000 rows\n",
      "  Chunk 55: 1000000 rows\n",
      "  Chunk 56: 1000000 rows\n",
      "  Chunk 57: 1000000 rows\n",
      "  Chunk 58: 1000000 rows\n",
      "  Chunk 59: 1000000 rows\n",
      "  Chunk 60: 1000000 rows\n",
      "  Chunk 61: 1000000 rows\n",
      "  Chunk 62: 1000000 rows\n",
      "  Chunk 63: 1000000 rows\n",
      "  Chunk 64: 1000000 rows\n",
      "  Chunk 65: 1000000 rows\n",
      "  Chunk 66: 1000000 rows\n",
      "  Chunk 67: 1000000 rows\n",
      "  Chunk 68: 1000000 rows\n",
      "  Chunk 69: 1000000 rows\n",
      "  Chunk 70: 1000000 rows\n",
      "  Chunk 71: 1000000 rows\n",
      "  Chunk 72: 1000000 rows\n",
      "  Chunk 73: 1000000 rows\n",
      "  Chunk 74: 1000000 rows\n",
      "  Chunk 75: 1000000 rows\n",
      "  Chunk 76: 1000000 rows\n",
      "  Chunk 77: 1000000 rows\n",
      "  Chunk 78: 1000000 rows\n",
      "  Chunk 79: 1000000 rows\n",
      "  Chunk 80: 1000000 rows\n",
      "  Chunk 81: 1000000 rows\n",
      "  Chunk 82: 1000000 rows\n",
      "  Chunk 83: 1000000 rows\n",
      "  Chunk 84: 1000000 rows\n",
      "  Chunk 85: 1000000 rows\n",
      "  Chunk 86: 1000000 rows\n",
      "  Chunk 87: 1000000 rows\n",
      "  Chunk 88: 1000000 rows\n",
      "  Chunk 89: 1000000 rows\n",
      "  Chunk 90: 1000000 rows\n",
      "  Chunk 91: 1000000 rows\n",
      "  Chunk 92: 1000000 rows\n",
      "  Chunk 93: 1000000 rows\n",
      "  Chunk 94: 1000000 rows\n",
      "  Chunk 95: 1000000 rows\n",
      "  Chunk 96: 1000000 rows\n",
      "  Chunk 97: 1000000 rows\n",
      "  Chunk 98: 1000000 rows\n",
      "  Chunk 99: 1000000 rows\n",
      "  Chunk 100: 1000000 rows\n",
      "  Chunk 101: 1000000 rows\n",
      "  Chunk 102: 1000000 rows\n",
      "  Chunk 103: 1000000 rows\n",
      "  Chunk 104: 1000000 rows\n",
      "  Chunk 105: 1000000 rows\n",
      "  Chunk 106: 1000000 rows\n",
      "  Chunk 107: 1000000 rows\n",
      "  Chunk 108: 1000000 rows\n",
      "  Chunk 109: 1000000 rows\n",
      "  Chunk 110: 1000000 rows\n",
      "  Chunk 111: 1000000 rows\n",
      "  Chunk 112: 1000000 rows\n",
      "  Chunk 113: 1000000 rows\n",
      "  Chunk 114: 1000000 rows\n",
      "  Chunk 115: 1000000 rows\n",
      "  Chunk 116: 1000000 rows\n",
      "  Chunk 117: 1000000 rows\n",
      "  Chunk 118: 1000000 rows\n",
      "  Chunk 119: 1000000 rows\n",
      "  Chunk 120: 1000000 rows\n",
      "  Chunk 121: 1000000 rows\n",
      "  Chunk 122: 1000000 rows\n",
      "  Chunk 123: 1000000 rows\n",
      "  Chunk 124: 1000000 rows\n",
      "  Chunk 125: 1000000 rows\n",
      "  Chunk 126: 1000000 rows\n",
      "  Chunk 127: 1000000 rows\n",
      "  Chunk 128: 1000000 rows\n",
      "  Chunk 129: 1000000 rows\n",
      "  Chunk 130: 1000000 rows\n",
      "  Chunk 131: 1000000 rows\n",
      "  Chunk 132: 1000000 rows\n",
      "  Chunk 133: 1000000 rows\n",
      "  Chunk 134: 1000000 rows\n",
      "  Chunk 135: 1000000 rows\n",
      "  Chunk 136: 1000000 rows\n",
      "  Chunk 137: 1000000 rows\n",
      "  Chunk 138: 1000000 rows\n",
      "  Chunk 139: 1000000 rows\n",
      "  Chunk 140: 1000000 rows\n",
      "  Chunk 141: 1000000 rows\n",
      "  Chunk 142: 1000000 rows\n",
      "  Chunk 143: 1000000 rows\n",
      "  Chunk 144: 1000000 rows\n",
      "  Chunk 145: 1000000 rows\n",
      "  Chunk 146: 1000000 rows\n",
      "  Chunk 147: 1000000 rows\n",
      "  Chunk 148: 1000000 rows\n",
      "  Chunk 149: 1000000 rows\n",
      "  Chunk 150: 1000000 rows\n",
      "  Chunk 151: 1000000 rows\n",
      "  Chunk 152: 1000000 rows\n",
      "  Chunk 153: 1000000 rows\n",
      "  Chunk 154: 1000000 rows\n",
      "  Chunk 155: 1000000 rows\n",
      "  Chunk 156: 1000000 rows\n",
      "  Chunk 157: 1000000 rows\n",
      "  Chunk 158: 1000000 rows\n",
      "  Chunk 159: 1000000 rows\n",
      "  Chunk 160: 1000000 rows\n",
      "  Chunk 161: 1000000 rows\n",
      "  Chunk 162: 1000000 rows\n",
      "  Chunk 163: 1000000 rows\n",
      "  Chunk 164: 1000000 rows\n",
      "  Chunk 165: 1000000 rows\n",
      "  Chunk 166: 1000000 rows\n",
      "  Chunk 167: 1000000 rows\n",
      "  Chunk 168: 1000000 rows\n",
      "  Chunk 169: 1000000 rows\n",
      "  Chunk 170: 1000000 rows\n",
      "  Chunk 171: 1000000 rows\n",
      "  Chunk 172: 1000000 rows\n",
      "  Chunk 173: 1000000 rows\n",
      "  Chunk 174: 1000000 rows\n",
      "  Chunk 175: 1000000 rows\n",
      "  Chunk 176: 1000000 rows\n",
      "  Chunk 177: 1000000 rows\n",
      "  Chunk 178: 1000000 rows\n",
      "  Chunk 179: 1000000 rows\n",
      "  Chunk 180: 1000000 rows\n",
      "  Chunk 181: 1000000 rows\n",
      "  Chunk 182: 1000000 rows\n",
      "  Chunk 183: 1000000 rows\n",
      "  Chunk 184: 1000000 rows\n",
      "  Chunk 185: 1000000 rows\n",
      "  Chunk 186: 1000000 rows\n",
      "  Chunk 187: 1000000 rows\n",
      "  Chunk 188: 1000000 rows\n",
      "  Chunk 189: 1000000 rows\n",
      "  Chunk 190: 1000000 rows\n",
      "  Chunk 191: 1000000 rows\n",
      "  Chunk 192: 1000000 rows\n",
      "  Chunk 193: 1000000 rows\n",
      "  Chunk 194: 1000000 rows\n",
      "  Chunk 195: 1000000 rows\n",
      "  Chunk 196: 1000000 rows\n",
      "  Chunk 197: 1000000 rows\n",
      "  Chunk 198: 1000000 rows\n",
      "  Chunk 199: 1000000 rows\n",
      "  Chunk 200: 1000000 rows\n",
      "  Chunk 201: 1000000 rows\n",
      "  Chunk 202: 1000000 rows\n",
      "  Chunk 203: 1000000 rows\n",
      "  Chunk 204: 1000000 rows\n",
      "  Chunk 205: 1000000 rows\n",
      "  Chunk 206: 1000000 rows\n",
      "  Chunk 207: 1000000 rows\n",
      "  Chunk 208: 1000000 rows\n",
      "  Chunk 209: 1000000 rows\n",
      "  Chunk 210: 1000000 rows\n",
      "  Chunk 211: 1000000 rows\n",
      "  Chunk 212: 1000000 rows\n",
      "  Chunk 213: 1000000 rows\n",
      "  Chunk 214: 1000000 rows\n",
      "  Chunk 215: 1000000 rows\n",
      "  Chunk 216: 1000000 rows\n",
      "  Chunk 217: 1000000 rows\n",
      "  Chunk 218: 1000000 rows\n",
      "  Chunk 219: 1000000 rows\n",
      "  Chunk 220: 1000000 rows\n",
      "  Chunk 221: 1000000 rows\n",
      "  Chunk 222: 1000000 rows\n",
      "  Chunk 223: 1000000 rows\n",
      "  Chunk 224: 1000000 rows\n",
      "  Chunk 225: 1000000 rows\n",
      "  Chunk 226: 1000000 rows\n",
      "  Chunk 227: 1000000 rows\n",
      "  Chunk 228: 1000000 rows\n",
      "  Chunk 229: 1000000 rows\n",
      "  Chunk 230: 1000000 rows\n",
      "  Chunk 231: 1000000 rows\n",
      "  Chunk 232: 1000000 rows\n",
      "  Chunk 233: 1000000 rows\n",
      "  Chunk 234: 1000000 rows\n",
      "  Chunk 235: 1000000 rows\n",
      "  Chunk 236: 1000000 rows\n",
      "  Chunk 237: 1000000 rows\n",
      "  Chunk 238: 1000000 rows\n",
      "  Chunk 239: 1000000 rows\n",
      "  Chunk 240: 1000000 rows\n",
      "  Chunk 241: 1000000 rows\n",
      "  Chunk 242: 1000000 rows\n",
      "  Chunk 243: 1000000 rows\n",
      "  Chunk 244: 1000000 rows\n",
      "  Chunk 245: 1000000 rows\n",
      "  Chunk 246: 1000000 rows\n",
      "  Chunk 247: 1000000 rows\n",
      "  Chunk 248: 1000000 rows\n",
      "  Chunk 249: 1000000 rows\n",
      "  Chunk 250: 1000000 rows\n",
      "  Chunk 251: 1000000 rows\n",
      "  Chunk 252: 1000000 rows\n",
      "  Chunk 253: 1000000 rows\n",
      "  Chunk 254: 1000000 rows\n",
      "  Chunk 255: 1000000 rows\n",
      "  Chunk 256: 1000000 rows\n",
      "  Chunk 257: 1000000 rows\n",
      "  Chunk 258: 1000000 rows\n",
      "  Chunk 259: 1000000 rows\n",
      "  Chunk 260: 1000000 rows\n",
      "  Chunk 261: 1000000 rows\n",
      "  Chunk 262: 1000000 rows\n",
      "  Chunk 263: 1000000 rows\n",
      "  Chunk 264: 1000000 rows\n",
      "  Chunk 265: 1000000 rows\n",
      "  Chunk 266: 1000000 rows\n",
      "  Chunk 267: 1000000 rows\n",
      "  Chunk 268: 1000000 rows\n",
      "  Chunk 269: 1000000 rows\n",
      "  Chunk 270: 1000000 rows\n",
      "  Chunk 271: 1000000 rows\n",
      "  Chunk 272: 1000000 rows\n",
      "  Chunk 273: 394668 rows\n",
      "\n",
      "Summary for MV_raw_excl_delisted.txt:\n",
      "  Total rows read: 272394668\n",
      "  Rows kept for MV_clean.txt: 249987171\n",
      "  Rows dropped for MV_mapping_errors.txt: 22407497\n",
      "  Dropped rows by reason:\n",
      "    No matching DS_ID in ID_mapping_clean: 22407497\n",
      "Saved clean file → /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/MV_clean.txt\n",
      "Saved error file → /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/MV_mapping_errors.txt\n",
      "\n",
      "=== Processing TRI_raw_excl_delisted.txt in chunks of 1000000 rows ===\n",
      "  Chunk 1: 1000000 rows\n",
      "  Chunk 2: 1000000 rows\n",
      "  Chunk 3: 1000000 rows\n",
      "  Chunk 4: 1000000 rows\n",
      "  Chunk 5: 1000000 rows\n",
      "  Chunk 6: 1000000 rows\n",
      "  Chunk 7: 1000000 rows\n",
      "  Chunk 8: 1000000 rows\n",
      "  Chunk 9: 1000000 rows\n",
      "  Chunk 10: 1000000 rows\n",
      "  Chunk 11: 1000000 rows\n",
      "  Chunk 12: 1000000 rows\n",
      "  Chunk 13: 1000000 rows\n",
      "  Chunk 14: 1000000 rows\n",
      "  Chunk 15: 1000000 rows\n",
      "  Chunk 16: 1000000 rows\n",
      "  Chunk 17: 1000000 rows\n",
      "  Chunk 18: 1000000 rows\n",
      "  Chunk 19: 1000000 rows\n",
      "  Chunk 20: 1000000 rows\n",
      "  Chunk 21: 1000000 rows\n",
      "  Chunk 22: 1000000 rows\n",
      "  Chunk 23: 1000000 rows\n",
      "  Chunk 24: 1000000 rows\n",
      "  Chunk 25: 1000000 rows\n",
      "  Chunk 26: 1000000 rows\n",
      "  Chunk 27: 1000000 rows\n",
      "  Chunk 28: 1000000 rows\n",
      "  Chunk 29: 1000000 rows\n",
      "  Chunk 30: 1000000 rows\n",
      "  Chunk 31: 1000000 rows\n",
      "  Chunk 32: 1000000 rows\n",
      "  Chunk 33: 1000000 rows\n",
      "  Chunk 34: 1000000 rows\n",
      "  Chunk 35: 1000000 rows\n",
      "  Chunk 36: 1000000 rows\n",
      "  Chunk 37: 1000000 rows\n",
      "  Chunk 38: 1000000 rows\n",
      "  Chunk 39: 1000000 rows\n",
      "  Chunk 40: 1000000 rows\n",
      "  Chunk 41: 1000000 rows\n",
      "  Chunk 42: 1000000 rows\n",
      "  Chunk 43: 1000000 rows\n",
      "  Chunk 44: 1000000 rows\n",
      "  Chunk 45: 1000000 rows\n",
      "  Chunk 46: 1000000 rows\n",
      "  Chunk 47: 1000000 rows\n",
      "  Chunk 48: 1000000 rows\n",
      "  Chunk 49: 1000000 rows\n",
      "  Chunk 50: 1000000 rows\n",
      "  Chunk 51: 1000000 rows\n",
      "  Chunk 52: 1000000 rows\n",
      "  Chunk 53: 1000000 rows\n",
      "  Chunk 54: 1000000 rows\n",
      "  Chunk 55: 1000000 rows\n",
      "  Chunk 56: 1000000 rows\n",
      "  Chunk 57: 1000000 rows\n",
      "  Chunk 58: 1000000 rows\n",
      "  Chunk 59: 1000000 rows\n",
      "  Chunk 60: 1000000 rows\n",
      "  Chunk 61: 1000000 rows\n",
      "  Chunk 62: 1000000 rows\n",
      "  Chunk 63: 1000000 rows\n",
      "  Chunk 64: 1000000 rows\n",
      "  Chunk 65: 1000000 rows\n",
      "  Chunk 66: 1000000 rows\n",
      "  Chunk 67: 1000000 rows\n",
      "  Chunk 68: 1000000 rows\n",
      "  Chunk 69: 1000000 rows\n",
      "  Chunk 70: 1000000 rows\n",
      "  Chunk 71: 1000000 rows\n",
      "  Chunk 72: 1000000 rows\n",
      "  Chunk 73: 1000000 rows\n",
      "  Chunk 74: 1000000 rows\n",
      "  Chunk 75: 1000000 rows\n",
      "  Chunk 76: 1000000 rows\n",
      "  Chunk 77: 1000000 rows\n",
      "  Chunk 78: 1000000 rows\n",
      "  Chunk 79: 1000000 rows\n",
      "  Chunk 80: 1000000 rows\n",
      "  Chunk 81: 1000000 rows\n",
      "  Chunk 82: 1000000 rows\n",
      "  Chunk 83: 1000000 rows\n",
      "  Chunk 84: 1000000 rows\n",
      "  Chunk 85: 1000000 rows\n",
      "  Chunk 86: 1000000 rows\n",
      "  Chunk 87: 1000000 rows\n",
      "  Chunk 88: 1000000 rows\n",
      "  Chunk 89: 1000000 rows\n",
      "  Chunk 90: 1000000 rows\n",
      "  Chunk 91: 1000000 rows\n",
      "  Chunk 92: 1000000 rows\n",
      "  Chunk 93: 1000000 rows\n",
      "  Chunk 94: 1000000 rows\n",
      "  Chunk 95: 1000000 rows\n",
      "  Chunk 96: 1000000 rows\n",
      "  Chunk 97: 1000000 rows\n",
      "  Chunk 98: 1000000 rows\n",
      "  Chunk 99: 1000000 rows\n",
      "  Chunk 100: 1000000 rows\n",
      "  Chunk 101: 1000000 rows\n",
      "  Chunk 102: 1000000 rows\n",
      "  Chunk 103: 1000000 rows\n",
      "  Chunk 104: 1000000 rows\n",
      "  Chunk 105: 1000000 rows\n",
      "  Chunk 106: 1000000 rows\n",
      "  Chunk 107: 1000000 rows\n",
      "  Chunk 108: 1000000 rows\n",
      "  Chunk 109: 1000000 rows\n",
      "  Chunk 110: 1000000 rows\n",
      "  Chunk 111: 1000000 rows\n",
      "  Chunk 112: 1000000 rows\n",
      "  Chunk 113: 1000000 rows\n",
      "  Chunk 114: 1000000 rows\n",
      "  Chunk 115: 1000000 rows\n",
      "  Chunk 116: 1000000 rows\n",
      "  Chunk 117: 1000000 rows\n",
      "  Chunk 118: 1000000 rows\n",
      "  Chunk 119: 1000000 rows\n",
      "  Chunk 120: 1000000 rows\n",
      "  Chunk 121: 1000000 rows\n",
      "  Chunk 122: 1000000 rows\n",
      "  Chunk 123: 1000000 rows\n",
      "  Chunk 124: 1000000 rows\n",
      "  Chunk 125: 1000000 rows\n",
      "  Chunk 126: 1000000 rows\n",
      "  Chunk 127: 1000000 rows\n",
      "  Chunk 128: 1000000 rows\n",
      "  Chunk 129: 1000000 rows\n",
      "  Chunk 130: 1000000 rows\n",
      "  Chunk 131: 1000000 rows\n",
      "  Chunk 132: 1000000 rows\n",
      "  Chunk 133: 1000000 rows\n",
      "  Chunk 134: 1000000 rows\n",
      "  Chunk 135: 1000000 rows\n",
      "  Chunk 136: 1000000 rows\n",
      "  Chunk 137: 1000000 rows\n",
      "  Chunk 138: 1000000 rows\n",
      "  Chunk 139: 1000000 rows\n",
      "  Chunk 140: 1000000 rows\n",
      "  Chunk 141: 1000000 rows\n",
      "  Chunk 142: 1000000 rows\n",
      "  Chunk 143: 1000000 rows\n",
      "  Chunk 144: 1000000 rows\n",
      "  Chunk 145: 1000000 rows\n",
      "  Chunk 146: 1000000 rows\n",
      "  Chunk 147: 1000000 rows\n",
      "  Chunk 148: 1000000 rows\n",
      "  Chunk 149: 1000000 rows\n",
      "  Chunk 150: 1000000 rows\n",
      "  Chunk 151: 1000000 rows\n",
      "  Chunk 152: 1000000 rows\n",
      "  Chunk 153: 1000000 rows\n",
      "  Chunk 154: 1000000 rows\n",
      "  Chunk 155: 1000000 rows\n",
      "  Chunk 156: 1000000 rows\n",
      "  Chunk 157: 1000000 rows\n",
      "  Chunk 158: 1000000 rows\n",
      "  Chunk 159: 1000000 rows\n",
      "  Chunk 160: 1000000 rows\n",
      "  Chunk 161: 1000000 rows\n",
      "  Chunk 162: 1000000 rows\n",
      "  Chunk 163: 1000000 rows\n",
      "  Chunk 164: 1000000 rows\n",
      "  Chunk 165: 1000000 rows\n",
      "  Chunk 166: 1000000 rows\n",
      "  Chunk 167: 1000000 rows\n",
      "  Chunk 168: 1000000 rows\n",
      "  Chunk 169: 1000000 rows\n",
      "  Chunk 170: 1000000 rows\n",
      "  Chunk 171: 1000000 rows\n",
      "  Chunk 172: 1000000 rows\n",
      "  Chunk 173: 1000000 rows\n",
      "  Chunk 174: 1000000 rows\n",
      "  Chunk 175: 1000000 rows\n",
      "  Chunk 176: 1000000 rows\n",
      "  Chunk 177: 1000000 rows\n",
      "  Chunk 178: 1000000 rows\n",
      "  Chunk 179: 1000000 rows\n",
      "  Chunk 180: 1000000 rows\n",
      "  Chunk 181: 1000000 rows\n",
      "  Chunk 182: 1000000 rows\n",
      "  Chunk 183: 1000000 rows\n",
      "  Chunk 184: 1000000 rows\n",
      "  Chunk 185: 1000000 rows\n",
      "  Chunk 186: 1000000 rows\n",
      "  Chunk 187: 1000000 rows\n",
      "  Chunk 188: 1000000 rows\n",
      "  Chunk 189: 1000000 rows\n",
      "  Chunk 190: 1000000 rows\n",
      "  Chunk 191: 1000000 rows\n",
      "  Chunk 192: 1000000 rows\n",
      "  Chunk 193: 1000000 rows\n",
      "  Chunk 194: 1000000 rows\n",
      "  Chunk 195: 1000000 rows\n",
      "  Chunk 196: 1000000 rows\n",
      "  Chunk 197: 1000000 rows\n",
      "  Chunk 198: 1000000 rows\n",
      "  Chunk 199: 1000000 rows\n",
      "  Chunk 200: 1000000 rows\n",
      "  Chunk 201: 1000000 rows\n",
      "  Chunk 202: 1000000 rows\n",
      "  Chunk 203: 1000000 rows\n",
      "  Chunk 204: 1000000 rows\n",
      "  Chunk 205: 1000000 rows\n",
      "  Chunk 206: 1000000 rows\n",
      "  Chunk 207: 1000000 rows\n",
      "  Chunk 208: 1000000 rows\n",
      "  Chunk 209: 1000000 rows\n",
      "  Chunk 210: 1000000 rows\n",
      "  Chunk 211: 1000000 rows\n",
      "  Chunk 212: 1000000 rows\n",
      "  Chunk 213: 1000000 rows\n",
      "  Chunk 214: 1000000 rows\n",
      "  Chunk 215: 1000000 rows\n",
      "  Chunk 216: 1000000 rows\n",
      "  Chunk 217: 1000000 rows\n",
      "  Chunk 218: 1000000 rows\n",
      "  Chunk 219: 1000000 rows\n",
      "  Chunk 220: 1000000 rows\n",
      "  Chunk 221: 1000000 rows\n",
      "  Chunk 222: 1000000 rows\n",
      "  Chunk 223: 1000000 rows\n",
      "  Chunk 224: 1000000 rows\n",
      "  Chunk 225: 1000000 rows\n",
      "  Chunk 226: 1000000 rows\n",
      "  Chunk 227: 1000000 rows\n",
      "  Chunk 228: 1000000 rows\n",
      "  Chunk 229: 1000000 rows\n",
      "  Chunk 230: 1000000 rows\n",
      "  Chunk 231: 1000000 rows\n",
      "  Chunk 232: 1000000 rows\n",
      "  Chunk 233: 1000000 rows\n",
      "  Chunk 234: 1000000 rows\n",
      "  Chunk 235: 1000000 rows\n",
      "  Chunk 236: 1000000 rows\n",
      "  Chunk 237: 1000000 rows\n",
      "  Chunk 238: 1000000 rows\n",
      "  Chunk 239: 1000000 rows\n",
      "  Chunk 240: 1000000 rows\n",
      "  Chunk 241: 1000000 rows\n",
      "  Chunk 242: 1000000 rows\n",
      "  Chunk 243: 1000000 rows\n",
      "  Chunk 244: 1000000 rows\n",
      "  Chunk 245: 1000000 rows\n",
      "  Chunk 246: 1000000 rows\n",
      "  Chunk 247: 1000000 rows\n",
      "  Chunk 248: 1000000 rows\n",
      "  Chunk 249: 1000000 rows\n",
      "  Chunk 250: 1000000 rows\n",
      "  Chunk 251: 1000000 rows\n",
      "  Chunk 252: 1000000 rows\n",
      "  Chunk 253: 1000000 rows\n",
      "  Chunk 254: 1000000 rows\n",
      "  Chunk 255: 1000000 rows\n",
      "  Chunk 256: 1000000 rows\n",
      "  Chunk 257: 1000000 rows\n",
      "  Chunk 258: 1000000 rows\n",
      "  Chunk 259: 1000000 rows\n",
      "  Chunk 260: 1000000 rows\n",
      "  Chunk 261: 1000000 rows\n",
      "  Chunk 262: 1000000 rows\n",
      "  Chunk 263: 1000000 rows\n",
      "  Chunk 264: 1000000 rows\n",
      "  Chunk 265: 1000000 rows\n",
      "  Chunk 266: 1000000 rows\n",
      "  Chunk 267: 1000000 rows\n",
      "  Chunk 268: 1000000 rows\n",
      "  Chunk 269: 1000000 rows\n",
      "  Chunk 270: 1000000 rows\n",
      "  Chunk 271: 1000000 rows\n",
      "  Chunk 272: 1000000 rows\n",
      "  Chunk 273: 1000000 rows\n",
      "  Chunk 274: 1000000 rows\n",
      "  Chunk 275: 1000000 rows\n",
      "  Chunk 276: 1000000 rows\n",
      "  Chunk 277: 1000000 rows\n",
      "  Chunk 278: 1000000 rows\n",
      "  Chunk 279: 1000000 rows\n",
      "  Chunk 280: 1000000 rows\n",
      "  Chunk 281: 1000000 rows\n",
      "  Chunk 282: 1000000 rows\n",
      "  Chunk 283: 1000000 rows\n",
      "  Chunk 284: 1000000 rows\n",
      "  Chunk 285: 1000000 rows\n",
      "  Chunk 286: 1000000 rows\n",
      "  Chunk 287: 1000000 rows\n",
      "  Chunk 288: 1000000 rows\n",
      "  Chunk 289: 1000000 rows\n",
      "  Chunk 290: 1000000 rows\n",
      "  Chunk 291: 1000000 rows\n",
      "  Chunk 292: 1000000 rows\n",
      "  Chunk 293: 1000000 rows\n",
      "  Chunk 294: 1000000 rows\n",
      "  Chunk 295: 1000000 rows\n",
      "  Chunk 296: 1000000 rows\n",
      "  Chunk 297: 1000000 rows\n",
      "  Chunk 298: 1000000 rows\n",
      "  Chunk 299: 1000000 rows\n",
      "  Chunk 300: 1000000 rows\n",
      "  Chunk 301: 1000000 rows\n",
      "  Chunk 302: 1000000 rows\n",
      "  Chunk 303: 1000000 rows\n",
      "  Chunk 304: 1000000 rows\n",
      "  Chunk 305: 1000000 rows\n",
      "  Chunk 306: 1000000 rows\n",
      "  Chunk 307: 1000000 rows\n",
      "  Chunk 308: 1000000 rows\n",
      "  Chunk 309: 1000000 rows\n",
      "  Chunk 310: 1000000 rows\n",
      "  Chunk 311: 1000000 rows\n",
      "  Chunk 312: 1000000 rows\n",
      "  Chunk 313: 1000000 rows\n",
      "  Chunk 314: 1000000 rows\n",
      "  Chunk 315: 1000000 rows\n",
      "  Chunk 316: 1000000 rows\n",
      "  Chunk 317: 1000000 rows\n",
      "  Chunk 318: 1000000 rows\n",
      "  Chunk 319: 1000000 rows\n",
      "  Chunk 320: 1000000 rows\n",
      "  Chunk 321: 1000000 rows\n",
      "  Chunk 322: 1000000 rows\n",
      "  Chunk 323: 1000000 rows\n",
      "  Chunk 324: 1000000 rows\n",
      "  Chunk 325: 1000000 rows\n",
      "  Chunk 326: 1000000 rows\n",
      "  Chunk 327: 1000000 rows\n",
      "  Chunk 328: 1000000 rows\n",
      "  Chunk 329: 1000000 rows\n",
      "  Chunk 330: 1000000 rows\n",
      "  Chunk 331: 1000000 rows\n",
      "  Chunk 332: 1000000 rows\n",
      "  Chunk 333: 1000000 rows\n",
      "  Chunk 334: 1000000 rows\n",
      "  Chunk 335: 1000000 rows\n",
      "  Chunk 336: 1000000 rows\n",
      "  Chunk 337: 1000000 rows\n",
      "  Chunk 338: 1000000 rows\n",
      "  Chunk 339: 1000000 rows\n",
      "  Chunk 340: 1000000 rows\n",
      "  Chunk 341: 1000000 rows\n",
      "  Chunk 342: 1000000 rows\n",
      "  Chunk 343: 1000000 rows\n",
      "  Chunk 344: 1000000 rows\n",
      "  Chunk 345: 1000000 rows\n",
      "  Chunk 346: 1000000 rows\n",
      "  Chunk 347: 1000000 rows\n",
      "  Chunk 348: 1000000 rows\n",
      "  Chunk 349: 1000000 rows\n",
      "  Chunk 350: 1000000 rows\n",
      "  Chunk 351: 1000000 rows\n",
      "  Chunk 352: 1000000 rows\n",
      "  Chunk 353: 1000000 rows\n",
      "  Chunk 354: 1000000 rows\n",
      "  Chunk 355: 1000000 rows\n",
      "  Chunk 356: 1000000 rows\n",
      "  Chunk 357: 1000000 rows\n",
      "  Chunk 358: 1000000 rows\n",
      "  Chunk 359: 1000000 rows\n",
      "  Chunk 360: 1000000 rows\n",
      "  Chunk 361: 1000000 rows\n",
      "  Chunk 362: 1000000 rows\n",
      "  Chunk 363: 1000000 rows\n",
      "  Chunk 364: 1000000 rows\n",
      "  Chunk 365: 1000000 rows\n",
      "  Chunk 366: 1000000 rows\n",
      "  Chunk 367: 1000000 rows\n",
      "  Chunk 368: 1000000 rows\n",
      "  Chunk 369: 1000000 rows\n",
      "  Chunk 370: 1000000 rows\n",
      "  Chunk 371: 1000000 rows\n",
      "  Chunk 372: 1000000 rows\n",
      "  Chunk 373: 1000000 rows\n",
      "  Chunk 374: 1000000 rows\n",
      "  Chunk 375: 1000000 rows\n",
      "  Chunk 376: 1000000 rows\n",
      "  Chunk 377: 1000000 rows\n",
      "  Chunk 378: 1000000 rows\n",
      "  Chunk 379: 1000000 rows\n",
      "  Chunk 380: 1000000 rows\n",
      "  Chunk 381: 1000000 rows\n",
      "  Chunk 382: 1000000 rows\n",
      "  Chunk 383: 1000000 rows\n",
      "  Chunk 384: 1000000 rows\n",
      "  Chunk 385: 1000000 rows\n",
      "  Chunk 386: 1000000 rows\n",
      "  Chunk 387: 1000000 rows\n",
      "  Chunk 388: 1000000 rows\n",
      "  Chunk 389: 1000000 rows\n",
      "  Chunk 390: 1000000 rows\n",
      "  Chunk 391: 1000000 rows\n",
      "  Chunk 392: 1000000 rows\n",
      "  Chunk 393: 1000000 rows\n",
      "  Chunk 394: 1000000 rows\n",
      "  Chunk 395: 1000000 rows\n",
      "  Chunk 396: 1000000 rows\n",
      "  Chunk 397: 1000000 rows\n",
      "  Chunk 398: 1000000 rows\n",
      "  Chunk 399: 1000000 rows\n",
      "  Chunk 400: 1000000 rows\n",
      "  Chunk 401: 1000000 rows\n",
      "  Chunk 402: 1000000 rows\n",
      "  Chunk 403: 1000000 rows\n",
      "  Chunk 404: 1000000 rows\n",
      "  Chunk 405: 1000000 rows\n",
      "  Chunk 406: 1000000 rows\n",
      "  Chunk 407: 1000000 rows\n",
      "  Chunk 408: 1000000 rows\n",
      "  Chunk 409: 1000000 rows\n",
      "  Chunk 410: 1000000 rows\n",
      "  Chunk 411: 1000000 rows\n",
      "  Chunk 412: 1000000 rows\n",
      "  Chunk 413: 1000000 rows\n",
      "  Chunk 414: 1000000 rows\n",
      "  Chunk 415: 1000000 rows\n",
      "  Chunk 416: 1000000 rows\n",
      "  Chunk 417: 1000000 rows\n",
      "  Chunk 418: 1000000 rows\n",
      "  Chunk 419: 1000000 rows\n",
      "  Chunk 420: 1000000 rows\n",
      "  Chunk 421: 1000000 rows\n",
      "  Chunk 422: 1000000 rows\n",
      "  Chunk 423: 1000000 rows\n",
      "  Chunk 424: 1000000 rows\n",
      "  Chunk 425: 1000000 rows\n",
      "  Chunk 426: 1000000 rows\n",
      "  Chunk 427: 1000000 rows\n",
      "  Chunk 428: 720451 rows\n",
      "\n",
      "Summary for TRI_raw_excl_delisted.txt:\n",
      "  Total rows read: 427720451\n",
      "  Rows kept for TRI_clean.txt: 395652984\n",
      "  Rows dropped for TRI_mapping_errors.txt: 32067467\n",
      "  Dropped rows by reason:\n",
      "    No matching DS_ID in ID_mapping_clean: 32067467\n",
      "Saved clean file → /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/TRI_clean.txt\n",
      "Saved error file → /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/TRI_mapping_errors.txt\n",
      "\n",
      "Done merging and cleaning MV/TRI with ID mapping.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell takes the raw market value (MV_raw_excl_delisted.txt) and total return index\n",
    "# (TRI_raw_excl_delisted.txt) files and reconciles them with a clean ID mapping table:\n",
    "#\n",
    "#   - Loads ID_mapping_clean.txt and forces DS_ID and ID to be strings to avoid\n",
    "#     mixed-type issues during merges.\n",
    "#\n",
    "#   - Defines a chunked helper function `merge_with_mapping` that:\n",
    "#       * Reads the raw MV/TRI files in chunks to control memory usage.\n",
    "#       * Left-joins each chunk with the mapping on DS_ID so that all raw rows\n",
    "#         are retained and rows failing to map can be identified.\n",
    "#       * Flags rows to be dropped based on:\n",
    "#           - Missing mapping for DS_ID (no row in the mapping table).\n",
    "#           - Empty or effectively missing ID after the merge.\n",
    "#       * Splits each chunk into:\n",
    "#           - Clean rows with valid IDs, where DS_ID is dropped from the output.\n",
    "#           - Error rows with a DropReason explaining why they were removed.\n",
    "#       * Appends clean rows to MV_clean.txt or TRI_clean.txt and error rows\n",
    "#         to MV_mapping_errors.txt or TRI_mapping_errors.txt, writing headers\n",
    "#         only once per file.\n",
    "#       * Ensures that all four output files (MV_clean.txt, TRI_clean.txt,\n",
    "#         MV_mapping_errors.txt, TRI_mapping_errors.txt) contain a single\n",
    "#         header row at the top of the file.\n",
    "#       * Accumulates counts of dropped rows by reason for reporting.\n",
    "#\n",
    "#   - Runs the merge function for both MV_raw_excl_delisted.txt and\n",
    "#     TRI_raw_excl_delisted.txt and prints a summary of total rows read, kept,\n",
    "#     and dropped, along with a breakdown of drop reasons. This step ensures\n",
    "#     that only rows with valid mapped IDs are propagated into the cleaned MV\n",
    "#     and TRI outputs, with all removals fully logged and traceable.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Base folder where all intermediate files are stored\n",
    "base_folder = Temp_file_path_GO\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Load ID mapping (force DS_ID and ID to string to avoid mixed-type issues)\n",
    "# -------------------------------------------------------------------------\n",
    "mapping_file = os.path.join(base_folder, \"ID_mapping_clean.txt\")\n",
    "\n",
    "# Read the mapping file; DS_ID and ID are explicitly typed as strings\n",
    "mapping_df = pd.read_csv(\n",
    "    mapping_file,\n",
    "    sep=\"|\",\n",
    "    dtype={\"DS_ID\": \"string\", \"ID\": \"string\"},\n",
    "    low_memory=False,\n",
    ")\n",
    "\n",
    "# Log basic information about the mapping file for traceability\n",
    "print(f\"Loaded mapping file: {mapping_file}\")\n",
    "print(\"Mapping columns:\", list(mapping_df.columns))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Helper function to merge a raw file with the mapping and clean IDs (chunked)\n",
    "# -------------------------------------------------------------------------\n",
    "def merge_with_mapping(raw_filename, clean_filename, error_filename, chunksize=1_000_000):\n",
    "    \"\"\"\n",
    "    Merge a raw DS_ID-based file with the ID mapping and produce:\n",
    "      - a cleaned file with valid IDs and DS_ID removed\n",
    "      - an error file with dropped rows and their drop reasons\n",
    "\n",
    "    The function:\n",
    "      - Processes the raw file in chunks.\n",
    "      - Performs a left merge on DS_ID.\n",
    "      - Flags rows with missing mappings or empty IDs.\n",
    "      - Writes clean and error rows to separate outputs, ensuring that each\n",
    "        output file receives its header exactly once.\n",
    "    \"\"\"\n",
    "    # Construct full paths for raw input and clean/error outputs\n",
    "    raw_path = os.path.join(base_folder, raw_filename)\n",
    "    clean_path = os.path.join(base_folder, clean_filename)\n",
    "    error_path = os.path.join(base_folder, error_filename)\n",
    "\n",
    "    # If the raw file is missing, log a warning and skip processing\n",
    "    if not os.path.exists(raw_path):\n",
    "        print(f\"WARNING: {raw_path} not found. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # High-level status log for this raw file\n",
    "    print(f\"\\n=== Processing {raw_filename} in chunks of {chunksize} rows ===\")\n",
    "\n",
    "    # Modes for writing clean and error outputs; start with write mode, then switch to append\n",
    "    clean_mode = \"w\"\n",
    "    error_mode = \"w\"\n",
    "\n",
    "    # Flags to ensure headers are only written once per output file\n",
    "    header_written_clean = False\n",
    "    header_written_error = False\n",
    "\n",
    "    # Counters for auditing how many rows are processed, kept, and dropped\n",
    "    total_rows = 0\n",
    "    total_clean = 0\n",
    "    total_error = 0\n",
    "\n",
    "    # Counter to accumulate drop reasons and their frequencies\n",
    "    drop_reason_counts = Counter()\n",
    "\n",
    "    # Read raw file in chunks to avoid loading the entire file into memory\n",
    "    for chunk_idx, chunk in enumerate(\n",
    "        pd.read_csv(\n",
    "            raw_path,\n",
    "            sep=\"|\",\n",
    "            dtype={\"DS_ID\": \"string\"},\n",
    "            low_memory=False,\n",
    "            chunksize=chunksize,\n",
    "        )\n",
    "    ):\n",
    "        # Count rows in the current chunk and add to total\n",
    "        chunk_rows = len(chunk)\n",
    "        total_rows += chunk_rows\n",
    "        print(f\"  Chunk {chunk_idx + 1}: {chunk_rows} rows\")\n",
    "\n",
    "        # Merge raw chunk with mapping on DS_ID using a left join to preserve all raw rows\n",
    "        df_merged = chunk.merge(mapping_df, on=\"DS_ID\", how=\"left\", indicator=True)\n",
    "\n",
    "        # Helper column for ID checks: normalized string representation of ID\n",
    "        df_merged[\"ID_str\"] = df_merged[\"ID\"].astype(str).str.strip()\n",
    "\n",
    "        # Initialize DropReason as empty for all rows\n",
    "        df_merged[\"DropReason\"] = \"\"\n",
    "\n",
    "        # 1) Rows with no matching DS_ID in the mapping:\n",
    "        #    - ID is NaN and merge indicator is \"left_only\"\n",
    "        mask_no_mapping = df_merged[\"ID\"].isna() & (df_merged[\"_merge\"] == \"left_only\")\n",
    "        df_merged.loc[mask_no_mapping, \"DropReason\"] = \"No matching DS_ID in ID_mapping_clean\"\n",
    "\n",
    "        # 2) Rows where ID is present in mapping but becomes empty or \"nan\" as a string\n",
    "        mask_empty_id = (df_merged[\"DropReason\"] == \"\") & (\n",
    "            (df_merged[\"ID_str\"] == \"\") | (df_merged[\"ID_str\"].str.lower() == \"nan\")\n",
    "        )\n",
    "        df_merged.loc[mask_empty_id, \"DropReason\"] = \"Empty ID after merge\"\n",
    "\n",
    "        # Split into error rows (with a DropReason) and clean rows (with no DropReason)\n",
    "        error_df = df_merged[df_merged[\"DropReason\"] != \"\"].copy()\n",
    "        clean_df = df_merged[df_merged[\"DropReason\"] == \"\"].copy()\n",
    "\n",
    "        # Drop helper and merge-indicator columns from the clean subset\n",
    "        clean_df = clean_df.drop(columns=[\"DropReason\", \"ID_str\", \"_merge\"], errors=\"ignore\")\n",
    "\n",
    "        # Drop helper columns (except DropReason) from the error subset\n",
    "        error_df = error_df.drop(columns=[\"ID_str\", \"_merge\"], errors=\"ignore\")\n",
    "\n",
    "        # Remove DS_ID from the clean output; ID now serves as the main identifier\n",
    "        if \"DS_ID\" in clean_df.columns:\n",
    "            clean_df = clean_df.drop(columns=[\"DS_ID\"])\n",
    "\n",
    "        # Update cumulative counts of clean and error rows\n",
    "        total_clean += len(clean_df)\n",
    "        total_error += len(error_df)\n",
    "\n",
    "        # If there are error rows, update the drop reason frequency counts\n",
    "        if not error_df.empty:\n",
    "            drop_reason_counts.update(error_df[\"DropReason\"].value_counts().to_dict())\n",
    "\n",
    "        # Append clean rows to the clean output file, writing header only once\n",
    "        if len(clean_df) > 0:\n",
    "            clean_df.to_csv(\n",
    "                clean_path,\n",
    "                sep=\"|\",\n",
    "                index=False,\n",
    "                mode=clean_mode,\n",
    "                header=not header_written_clean,\n",
    "            )\n",
    "            header_written_clean = True\n",
    "            clean_mode = \"a\"  # Switch to append mode after first write\n",
    "\n",
    "        # Append error rows to the error output file, writing header only once\n",
    "        if len(error_df) > 0:\n",
    "            error_df.to_csv(\n",
    "                error_path,\n",
    "                sep=\"|\",\n",
    "                index=False,\n",
    "                mode=error_mode,\n",
    "                header=not header_written_error,\n",
    "            )\n",
    "            header_written_error = True\n",
    "            error_mode = \"a\"  # Switch to append mode after first write\n",
    "\n",
    "    # Summary of processing for this raw file\n",
    "    print(f\"\\nSummary for {raw_filename}:\")\n",
    "    print(f\"  Total rows read: {total_rows}\")\n",
    "    print(f\"  Rows kept for {clean_filename}: {total_clean}\")\n",
    "    print(f\"  Rows dropped for {error_filename}: {total_error}\")\n",
    "\n",
    "    # If rows were dropped, print breakdown by drop reason\n",
    "    if total_error > 0:\n",
    "        print(\"  Dropped rows by reason:\")\n",
    "        for reason, count in drop_reason_counts.items():\n",
    "            print(f\"    {reason}: {count}\")\n",
    "\n",
    "    # Log paths of generated clean and error files\n",
    "    print(f\"Saved clean file → {clean_path}\")\n",
    "    print(f\"Saved error file → {error_path}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Run for MV_raw and TRI_raw\n",
    "# -------------------------------------------------------------------------\n",
    "merge_with_mapping(\n",
    "    raw_filename=\"MV_raw_excl_delisted.txt\",\n",
    "    clean_filename=\"MV_clean.txt\",\n",
    "    error_filename=\"MV_mapping_errors.txt\",\n",
    ")\n",
    "\n",
    "merge_with_mapping(\n",
    "    raw_filename=\"TRI_raw_excl_delisted.txt\",\n",
    "    clean_filename=\"TRI_clean.txt\",\n",
    "    error_filename=\"TRI_mapping_errors.txt\",\n",
    ")\n",
    "\n",
    "print(\"\\nDone merging and cleaning MV/TRI with ID mapping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmjLE6M6QxZE"
   },
   "source": [
    "### View Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 707,
     "status": "ok",
     "timestamp": 1764876047436,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "b22c0395",
    "outputId": "0fbec809-96b4-471c-9b9f-5c4c6e80fa36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 rows of MV_clean.txt:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DayDate</th>\n",
       "      <th>MV</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994-02-17</td>\n",
       "      <td>19.44</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1994-02-18</td>\n",
       "      <td>20.02</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1994-02-21</td>\n",
       "      <td>20.02</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994-02-22</td>\n",
       "      <td>20.02</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994-02-23</td>\n",
       "      <td>18.59</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1994-02-24</td>\n",
       "      <td>18.01</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1994-02-25</td>\n",
       "      <td>17.59</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1994-02-28</td>\n",
       "      <td>17.16</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1994-03-01</td>\n",
       "      <td>17.59</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1994-03-02</td>\n",
       "      <td>17.16</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1994-03-03</td>\n",
       "      <td>16.87</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1994-03-04</td>\n",
       "      <td>17.30</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1994-03-07</td>\n",
       "      <td>17.16</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1994-03-08</td>\n",
       "      <td>17.16</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1994-03-09</td>\n",
       "      <td>17.16</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1994-03-10</td>\n",
       "      <td>18.01</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1994-03-11</td>\n",
       "      <td>18.30</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1994-03-14</td>\n",
       "      <td>18.30</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1994-03-15</td>\n",
       "      <td>17.73</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1994-03-16</td>\n",
       "      <td>18.01</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1994-03-17</td>\n",
       "      <td>17.73</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1994-03-18</td>\n",
       "      <td>18.01</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1994-03-21</td>\n",
       "      <td>17.73</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1994-03-22</td>\n",
       "      <td>18.01</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1994-03-23</td>\n",
       "      <td>18.30</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1994-03-24</td>\n",
       "      <td>17.16</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1994-03-25</td>\n",
       "      <td>18.30</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1994-03-28</td>\n",
       "      <td>17.16</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1994-03-29</td>\n",
       "      <td>16.58</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1994-03-30</td>\n",
       "      <td>18.30</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1994-03-31</td>\n",
       "      <td>16.58</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1994-04-01</td>\n",
       "      <td>16.58</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1994-04-04</td>\n",
       "      <td>16.58</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1994-04-05</td>\n",
       "      <td>16.58</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1994-04-06</td>\n",
       "      <td>16.01</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1994-04-07</td>\n",
       "      <td>16.58</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1994-04-08</td>\n",
       "      <td>17.16</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1994-04-11</td>\n",
       "      <td>15.73</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1994-04-12</td>\n",
       "      <td>14.87</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1994-04-13</td>\n",
       "      <td>14.87</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1994-04-14</td>\n",
       "      <td>14.87</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1994-04-15</td>\n",
       "      <td>13.72</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1994-04-18</td>\n",
       "      <td>14.87</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1994-04-19</td>\n",
       "      <td>15.73</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1994-04-20</td>\n",
       "      <td>16.58</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1994-04-21</td>\n",
       "      <td>17.01</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1994-04-22</td>\n",
       "      <td>16.58</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1994-04-25</td>\n",
       "      <td>16.58</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1994-04-26</td>\n",
       "      <td>16.58</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1994-04-27</td>\n",
       "      <td>16.58</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DayDate     MV         ID\n",
       "0   1994-02-17  19.44  C840P9270\n",
       "1   1994-02-18  20.02  C840P9270\n",
       "2   1994-02-21  20.02  C840P9270\n",
       "3   1994-02-22  20.02  C840P9270\n",
       "4   1994-02-23  18.59  C840P9270\n",
       "5   1994-02-24  18.01  C840P9270\n",
       "6   1994-02-25  17.59  C840P9270\n",
       "7   1994-02-28  17.16  C840P9270\n",
       "8   1994-03-01  17.59  C840P9270\n",
       "9   1994-03-02  17.16  C840P9270\n",
       "10  1994-03-03  16.87  C840P9270\n",
       "11  1994-03-04  17.30  C840P9270\n",
       "12  1994-03-07  17.16  C840P9270\n",
       "13  1994-03-08  17.16  C840P9270\n",
       "14  1994-03-09  17.16  C840P9270\n",
       "15  1994-03-10  18.01  C840P9270\n",
       "16  1994-03-11  18.30  C840P9270\n",
       "17  1994-03-14  18.30  C840P9270\n",
       "18  1994-03-15  17.73  C840P9270\n",
       "19  1994-03-16  18.01  C840P9270\n",
       "20  1994-03-17  17.73  C840P9270\n",
       "21  1994-03-18  18.01  C840P9270\n",
       "22  1994-03-21  17.73  C840P9270\n",
       "23  1994-03-22  18.01  C840P9270\n",
       "24  1994-03-23  18.30  C840P9270\n",
       "25  1994-03-24  17.16  C840P9270\n",
       "26  1994-03-25  18.30  C840P9270\n",
       "27  1994-03-28  17.16  C840P9270\n",
       "28  1994-03-29  16.58  C840P9270\n",
       "29  1994-03-30  18.30  C840P9270\n",
       "30  1994-03-31  16.58  C840P9270\n",
       "31  1994-04-01  16.58  C840P9270\n",
       "32  1994-04-04  16.58  C840P9270\n",
       "33  1994-04-05  16.58  C840P9270\n",
       "34  1994-04-06  16.01  C840P9270\n",
       "35  1994-04-07  16.58  C840P9270\n",
       "36  1994-04-08  17.16  C840P9270\n",
       "37  1994-04-11  15.73  C840P9270\n",
       "38  1994-04-12  14.87  C840P9270\n",
       "39  1994-04-13  14.87  C840P9270\n",
       "40  1994-04-14  14.87  C840P9270\n",
       "41  1994-04-15  13.72  C840P9270\n",
       "42  1994-04-18  14.87  C840P9270\n",
       "43  1994-04-19  15.73  C840P9270\n",
       "44  1994-04-20  16.58  C840P9270\n",
       "45  1994-04-21  17.01  C840P9270\n",
       "46  1994-04-22  16.58  C840P9270\n",
       "47  1994-04-25  16.58  C840P9270\n",
       "48  1994-04-26  16.58  C840P9270\n",
       "49  1994-04-27  16.58  C840P9270"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 50 rows of TRI_clean.txt:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DayDate</th>\n",
       "      <th>TRI</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994-02-17</td>\n",
       "      <td>100.00</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1994-02-18</td>\n",
       "      <td>102.94</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1994-02-21</td>\n",
       "      <td>102.94</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1994-02-22</td>\n",
       "      <td>102.94</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994-02-23</td>\n",
       "      <td>95.59</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1994-02-24</td>\n",
       "      <td>92.65</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1994-02-25</td>\n",
       "      <td>90.44</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1994-02-28</td>\n",
       "      <td>88.24</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1994-03-01</td>\n",
       "      <td>90.44</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1994-03-02</td>\n",
       "      <td>88.24</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1994-03-03</td>\n",
       "      <td>86.76</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1994-03-04</td>\n",
       "      <td>88.97</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1994-03-07</td>\n",
       "      <td>88.24</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1994-03-08</td>\n",
       "      <td>88.24</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1994-03-09</td>\n",
       "      <td>88.24</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1994-03-10</td>\n",
       "      <td>92.65</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1994-03-11</td>\n",
       "      <td>94.12</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1994-03-14</td>\n",
       "      <td>94.12</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1994-03-15</td>\n",
       "      <td>91.18</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1994-03-16</td>\n",
       "      <td>92.65</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1994-03-17</td>\n",
       "      <td>91.18</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1994-03-18</td>\n",
       "      <td>92.65</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1994-03-21</td>\n",
       "      <td>91.18</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1994-03-22</td>\n",
       "      <td>92.65</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1994-03-23</td>\n",
       "      <td>94.12</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1994-03-24</td>\n",
       "      <td>88.24</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1994-03-25</td>\n",
       "      <td>94.12</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1994-03-28</td>\n",
       "      <td>88.24</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1994-03-29</td>\n",
       "      <td>85.29</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1994-03-30</td>\n",
       "      <td>94.12</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1994-03-31</td>\n",
       "      <td>85.29</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1994-04-01</td>\n",
       "      <td>85.29</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1994-04-04</td>\n",
       "      <td>85.29</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1994-04-05</td>\n",
       "      <td>85.29</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1994-04-06</td>\n",
       "      <td>82.35</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1994-04-07</td>\n",
       "      <td>85.29</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1994-04-08</td>\n",
       "      <td>88.24</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1994-04-11</td>\n",
       "      <td>80.88</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1994-04-12</td>\n",
       "      <td>76.47</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1994-04-13</td>\n",
       "      <td>76.47</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1994-04-14</td>\n",
       "      <td>76.47</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1994-04-15</td>\n",
       "      <td>70.59</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1994-04-18</td>\n",
       "      <td>76.47</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1994-04-19</td>\n",
       "      <td>80.88</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1994-04-20</td>\n",
       "      <td>85.29</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1994-04-21</td>\n",
       "      <td>87.50</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1994-04-22</td>\n",
       "      <td>85.29</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1994-04-25</td>\n",
       "      <td>85.29</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1994-04-26</td>\n",
       "      <td>85.29</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1994-04-27</td>\n",
       "      <td>85.29</td>\n",
       "      <td>C840P9270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DayDate     TRI         ID\n",
       "0   1994-02-17  100.00  C840P9270\n",
       "1   1994-02-18  102.94  C840P9270\n",
       "2   1994-02-21  102.94  C840P9270\n",
       "3   1994-02-22  102.94  C840P9270\n",
       "4   1994-02-23   95.59  C840P9270\n",
       "5   1994-02-24   92.65  C840P9270\n",
       "6   1994-02-25   90.44  C840P9270\n",
       "7   1994-02-28   88.24  C840P9270\n",
       "8   1994-03-01   90.44  C840P9270\n",
       "9   1994-03-02   88.24  C840P9270\n",
       "10  1994-03-03   86.76  C840P9270\n",
       "11  1994-03-04   88.97  C840P9270\n",
       "12  1994-03-07   88.24  C840P9270\n",
       "13  1994-03-08   88.24  C840P9270\n",
       "14  1994-03-09   88.24  C840P9270\n",
       "15  1994-03-10   92.65  C840P9270\n",
       "16  1994-03-11   94.12  C840P9270\n",
       "17  1994-03-14   94.12  C840P9270\n",
       "18  1994-03-15   91.18  C840P9270\n",
       "19  1994-03-16   92.65  C840P9270\n",
       "20  1994-03-17   91.18  C840P9270\n",
       "21  1994-03-18   92.65  C840P9270\n",
       "22  1994-03-21   91.18  C840P9270\n",
       "23  1994-03-22   92.65  C840P9270\n",
       "24  1994-03-23   94.12  C840P9270\n",
       "25  1994-03-24   88.24  C840P9270\n",
       "26  1994-03-25   94.12  C840P9270\n",
       "27  1994-03-28   88.24  C840P9270\n",
       "28  1994-03-29   85.29  C840P9270\n",
       "29  1994-03-30   94.12  C840P9270\n",
       "30  1994-03-31   85.29  C840P9270\n",
       "31  1994-04-01   85.29  C840P9270\n",
       "32  1994-04-04   85.29  C840P9270\n",
       "33  1994-04-05   85.29  C840P9270\n",
       "34  1994-04-06   82.35  C840P9270\n",
       "35  1994-04-07   85.29  C840P9270\n",
       "36  1994-04-08   88.24  C840P9270\n",
       "37  1994-04-11   80.88  C840P9270\n",
       "38  1994-04-12   76.47  C840P9270\n",
       "39  1994-04-13   76.47  C840P9270\n",
       "40  1994-04-14   76.47  C840P9270\n",
       "41  1994-04-15   70.59  C840P9270\n",
       "42  1994-04-18   76.47  C840P9270\n",
       "43  1994-04-19   80.88  C840P9270\n",
       "44  1994-04-20   85.29  C840P9270\n",
       "45  1994-04-21   87.50  C840P9270\n",
       "46  1994-04-22   85.29  C840P9270\n",
       "47  1994-04-25   85.29  C840P9270\n",
       "48  1994-04-26   85.29  C840P9270\n",
       "49  1994-04-27   85.29  C840P9270"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell performs a quick visual sanity check of the cleaned market value\n",
    "# and TRI datasets by reading and displaying the first 50 rows of:\n",
    "#   - MV_clean.txt (cleaned market values with mapped IDs)\n",
    "#   - TRI_clean.txt (cleaned total return index values with mapped IDs)\n",
    "#\n",
    "# Behavior:\n",
    "#   - Constructs full file paths for MV_clean.txt and TRI_clean.txt based on\n",
    "#     the Temp_file_path_GO base folder.\n",
    "#   - Attempts to read up to 50 rows from each file using a pipe separator.\n",
    "#   - If a file is present, loads its first rows into a DataFrame and displays\n",
    "#     it in a notebook-friendly format for manual inspection.\n",
    "#   - If a file is missing, prints a clear error message instead of failing.\n",
    "# This provides a lightweight way to confirm that previous cleaning and\n",
    "# merging steps produced outputs in the expected structure and format.\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Base folder where the general overview and cleaned files are stored\n",
    "base_folder_go = Temp_file_path_GO\n",
    "\n",
    "# Full paths for the cleaned MV and TRI files\n",
    "mv_clean_path = os.path.join(base_folder_go, \"MV_clean.txt\")\n",
    "tri_clean_path = os.path.join(base_folder_go, \"TRI_clean.txt\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Inspect first 50 rows of MV_clean.txt\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"First 50 rows of MV_clean.txt:\")\n",
    "try:\n",
    "    # Read the first 50 rows from the cleaned MV file (pipe-separated)\n",
    "    df_mv_head = pd.read_csv(mv_clean_path, sep=\"|\", nrows=50)\n",
    "    # Display the resulting DataFrame for visual inspection in the notebook\n",
    "    display(df_mv_head)\n",
    "except FileNotFoundError:\n",
    "    # If the MV clean file does not exist, inform the user instead of raising\n",
    "    print(f\"Error: {mv_clean_path} not found.\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Inspect first 50 rows of TRI_clean.txt\n",
    "# -------------------------------------------------------------------------\n",
    "print(\"\\nFirst 50 rows of TRI_clean.txt:\")\n",
    "try:\n",
    "    # Read the first 50 rows from the cleaned TRI file (pipe-separated)\n",
    "    df_tri_head = pd.read_csv(tri_clean_path, sep=\"|\", nrows=50)\n",
    "    # Display the resulting DataFrame for visual inspection in the notebook\n",
    "    display(df_tri_head)\n",
    "except FileNotFoundError:\n",
    "    # If the TRI clean file does not exist, inform the user instead of raising\n",
    "    print(f\"Error: {tri_clean_path} not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVs in LC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Local Currency, Check for Ambiguous Currency Links (Not Problems Detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Constituents.01.csv from /home/jovyan/work/hpool1/pseidel/test/Input...\n",
      "  - Total rows in CSV: 167649\n",
      "Loading Mapping file...\n",
      "  - Unique IDs in Mapping File: 95104\n",
      "  - Rows after matching with ID Mapping: 149033\n",
      "  - Unique Codes in Target CSV after intersection: 95104\n",
      "\n",
      "No Codes were found with multiple PCUR values.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY OF THIS SCRIPT\n",
    "# -----------------------------------------------------------------------------\n",
    "# This script identifies firms with conflicting currency data (Multiple PCURs).\n",
    "#\n",
    "# LOGIC:\n",
    "# 1. Loads a specific Target CSV file (e.g., \"Constituents.01.csv\").\n",
    "# 2. Loads the Clean ID Mapping File (\"ID_mapping_clean.txt\").\n",
    "# 3. FILTERS the Target CSV to keep only rows where the 'Code' exists in the \n",
    "#    mapping file (ensuring we only check relevant companies).\n",
    "# 4. CHECKS if any remaining 'Code' has more than one unique value in the \n",
    "#    'PCUR' (Price Currency) column.\n",
    "# 5. SAVES a report (\"MultipleCurrencyMVs.xlsx\") containing all rows for \n",
    "#    any companies found with multiple currencies, so you can inspect them.\n",
    "# =============================================================================\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "CONFIG = {\n",
    "    # 1. Name of the CSV file you want to check\n",
    "    'target_csv_name': \"Constituents.01.csv\", \n",
    "    \n",
    "    # 2. Path to the folder containing that CSV\n",
    "    'input_folder': f\"{Input_file_path}\",\n",
    "\n",
    "    # 3. Path where the output Excel will be saved\n",
    "    'output_folder': f\"{Temp_file_path_GO}\",\n",
    "\n",
    "    # 4. Path to the ID mapping file\n",
    "    'mapping_file_path': f\"{Temp_file_path_GO}/ID_mapping_clean.txt\",\n",
    "    \n",
    "    # 5. Output file name for the results\n",
    "    'output_file': 'MultipleCurrencyMVs.xlsx'\n",
    "}\n",
    "\n",
    "def check_multiple_pcur(config):\n",
    "    # Construct full path for the input CSV\n",
    "    csv_path = os.path.join(config['input_folder'], config['target_csv_name'])\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Load the Target CSV\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"Loading {config['target_csv_name']} from {config['input_folder']}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            csv_path, \n",
    "            low_memory=False, \n",
    "            encoding='ISO-8859-1' # Safe encoding for your files\n",
    "        )\n",
    "        print(f\"  - Total rows in CSV: {len(df)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {csv_path}\")\n",
    "        return\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Load the ID Mapping File\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"Loading Mapping file...\")\n",
    "    try:\n",
    "        df_map = pd.read_csv(\n",
    "            config['mapping_file_path'], \n",
    "            sep='|', \n",
    "            low_memory=False, \n",
    "            encoding='ISO-8859-1'\n",
    "        )\n",
    "        # Get valid IDs as a set of strings\n",
    "        valid_ids = set(df_map['DS_ID'].astype(str))\n",
    "        \n",
    "        # --- NEW: Print unique count in mapping file ---\n",
    "        print(f\"  - Unique IDs in Mapping File: {len(valid_ids)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading mapping file: {e}\")\n",
    "        return\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. Filter: Keep only Codes present in Mapping File\n",
    "    # ---------------------------------------------------------\n",
    "    if 'Code' not in df.columns:\n",
    "        print(\"Error: Column 'Code' not found in the CSV.\")\n",
    "        return\n",
    "\n",
    "    # Ensure Code is string for matching\n",
    "    df['Code'] = df['Code'].astype(str)\n",
    "    \n",
    "    # Filter\n",
    "    df_subset = df[df['Code'].isin(valid_ids)].copy()\n",
    "    \n",
    "    # --- NEW: Print stats after intersection ---\n",
    "    unique_codes_in_subset = df_subset['Code'].nunique()\n",
    "    print(f\"  - Rows after matching with ID Mapping: {len(df_subset)}\")\n",
    "    print(f\"  - Unique Codes in Target CSV after intersection: {unique_codes_in_subset}\")\n",
    "\n",
    "    if df_subset.empty:\n",
    "        print(\"No matching Codes found. Stopping.\")\n",
    "        return\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Check for Multiple PCURs\n",
    "    # ---------------------------------------------------------\n",
    "    if 'PCUR' not in df_subset.columns:\n",
    "        print(\"Error: Column 'PCUR' not found in the CSV.\")\n",
    "        return\n",
    "\n",
    "    # Group by Code and count unique PCUR values\n",
    "    pcur_counts = df_subset.groupby('Code')['PCUR'].nunique()\n",
    "    \n",
    "    # Filter for Codes that have > 1 unique PCUR\n",
    "    multi_pcur_codes = pcur_counts[pcur_counts > 1].index\n",
    "    \n",
    "    if len(multi_pcur_codes) > 0:\n",
    "        print(f\"\\nFOUND {len(multi_pcur_codes)} CODES WITH MULTIPLE PCURs!\")\n",
    "        \n",
    "        # Extract the specific rows for inspection\n",
    "        result_df = df_subset[df_subset['Code'].isin(multi_pcur_codes)].sort_values(by=['Code', 'PCUR'])\n",
    "        \n",
    "        # Save to Excel\n",
    "        output_path = os.path.join(config['output_folder'], config['output_file'])\n",
    "        result_df.to_excel(output_path, index=False)\n",
    "        \n",
    "        print(f\"Details saved to: {output_path}\")\n",
    "        print(\"\\nPreview of Codes with multiple PCURs:\")\n",
    "        print(result_df[['Code', 'PCUR']].drop_duplicates().head(10))\n",
    "    else:\n",
    "        print(\"\\nNo Codes were found with multiple PCUR values.\")\n",
    "\n",
    "# Run the function\n",
    "check_multiple_pcur(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Total MarketValues in Local Currency (CL_MV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.01.csv\n",
      "  Rows written: 1643017\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.02.csv\n",
      "  Rows written: 970197\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.03.csv\n",
      "  Rows written: 1882282\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.04.csv\n",
      "  Rows written: 1538379\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.05.csv\n",
      "  Rows written: 1836862\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.06.csv\n",
      "  Rows written: 2742539\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.07.csv\n",
      "  Rows written: 1933145\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.08.csv\n",
      "  Rows written: 1101462\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.09.csv\n",
      "  Rows written: 974477\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.10.csv\n",
      "  Rows written: 1986179\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.11.csv\n",
      "  Rows written: 838859\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.12.csv\n",
      "  Rows written: 1243774\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.13.csv\n",
      "  Rows written: 1341597\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19911995.14.csv\n",
      "  Rows written: 1355139\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.15.csv\n",
      "  Rows written: 2486196\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.16.csv\n",
      "  Rows written: 1844823\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.17.csv\n",
      "  Rows written: 2581130\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.18.csv\n",
      "  Rows written: 2266717\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.19.csv\n",
      "  Rows written: 3020261\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.20.csv\n",
      "  Rows written: 3379936\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.21.csv\n",
      "  Rows written: 2923962\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.22.csv\n",
      "  Rows written: 1830750\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.23.csv\n",
      "  Rows written: 1479930\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.24.csv\n",
      "  Rows written: 2649614\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.25.csv\n",
      "  Rows written: 1337867\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.26.csv\n",
      "  Rows written: 2106921\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.27.csv\n",
      "  Rows written: 2139569\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19951999.28.csv\n",
      "  Rows written: 2214332\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.29.csv\n",
      "  Rows written: 3475752\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.30.csv\n",
      "  Rows written: 2605002\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.31.csv\n",
      "  Rows written: 3437616\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.32.csv\n",
      "  Rows written: 3240071\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.33.csv\n",
      "  Rows written: 3547077\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.34.csv\n",
      "  Rows written: 3980591\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.35.csv\n",
      "  Rows written: 3906636\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.36.csv\n",
      "  Rows written: 2701987\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.37.csv\n",
      "  Rows written: 2093802\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.38.csv\n",
      "  Rows written: 3404800\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.39.csv\n",
      "  Rows written: 1982791\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.40.csv\n",
      "  Rows written: 2945704\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.41.csv\n",
      "  Rows written: 2913676\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC19992003.42.csv\n",
      "  Rows written: 3026389\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.43.csv\n",
      "  Rows written: 4514038\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.44.csv\n",
      "  Rows written: 3109916\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.45.csv\n",
      "  Rows written: 4295258\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.46.csv\n",
      "  Rows written: 3778726\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.47.csv\n",
      "  Rows written: 4338378\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.48.csv\n",
      "  Rows written: 4778618\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.49.csv\n",
      "  Rows written: 5003205\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.50.csv\n",
      "  Rows written: 3425512\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.51.csv\n",
      "  Rows written: 2823636\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.52.csv\n",
      "  Rows written: 4363373\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.53.csv\n",
      "  Rows written: 2509188\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.54.csv\n",
      "  Rows written: 3514545\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.55.csv\n",
      "  Rows written: 3288735\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20032007.56.csv\n",
      "  Rows written: 3386277\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.57.csv\n",
      "  Rows written: 5862949\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.58.csv\n",
      "  Rows written: 3996586\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.59.csv\n",
      "  Rows written: 5376726\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.60.csv\n",
      "  Rows written: 4245355\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.61.csv\n",
      "  Rows written: 5281849\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.62.csv\n",
      "  Rows written: 5403703\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.63.csv\n",
      "  Rows written: 6126081\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.64.csv\n",
      "  Rows written: 4582917\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.65.csv\n",
      "  Rows written: 3488348\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.66.csv\n",
      "  Rows written: 5641931\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.67.csv\n",
      "  Rows written: 3120302\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.68.csv\n",
      "  Rows written: 4053507\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.69.csv\n",
      "  Rows written: 3631796\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20072011.70.csv\n",
      "  Rows written: 3679940\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.71.csv\n",
      "  Rows written: 4764924\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.72.csv\n",
      "  Rows written: 3699942\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.73.csv\n",
      "  Rows written: 4595633\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.74.csv\n",
      "  Rows written: 3425179\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.75.csv\n",
      "  Rows written: 4394640\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.76.csv\n",
      "  Rows written: 4219535\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.77.csv\n",
      "  Rows written: 5109116\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.78.csv\n",
      "  Rows written: 3920472\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.79.csv\n",
      "  Rows written: 3019899\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.80.csv\n",
      "  Rows written: 4696408\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.81.csv\n",
      "  Rows written: 2724590\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.82.csv\n",
      "  Rows written: 3354681\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.83.csv\n",
      "  Rows written: 2870811\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20112014.84.csv\n",
      "  Rows written: 2892906\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.85.csv\n",
      "  Rows written: 5047421\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.86.csv\n",
      "  Rows written: 4112112\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.87.csv\n",
      "  Rows written: 4826805\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.88.csv\n",
      "  Rows written: 3806105\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.89.csv\n",
      "  Rows written: 4858727\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.90.csv\n",
      "  Rows written: 4471110\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.91.csv\n",
      "  Rows written: 5552461\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.92.csv\n",
      "  Rows written: 4270476\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.93.csv\n",
      "  Rows written: 3340118\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.94.csv\n",
      "  Rows written: 5078450\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.95.csv\n",
      "  Rows written: 3156483\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.96.csv\n",
      "  Rows written: 3616416\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.97.csv\n",
      "  Rows written: 2973085\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20142017.98.csv\n",
      "  Rows written: 2977074\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.A1.csv\n",
      "  Rows written: 5385655\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.A2.csv\n",
      "  Rows written: 4711500\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.A3.csv\n",
      "  Rows written: 5182070\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.A4.csv\n",
      "  Rows written: 4327556\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.A5.csv\n",
      "  Rows written: 5506692\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.A6.csv\n",
      "  Rows written: 4735452\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.A7.csv\n",
      "  Rows written: 6033972\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.A8.csv\n",
      "  Rows written: 4624580\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.A9.csv\n",
      "  Rows written: 3604691\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.B1.csv\n",
      "  Rows written: 5539997\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.B2.csv\n",
      "  Rows written: 3631090\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.B3.csv\n",
      "  Rows written: 3850423\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.B4.csv\n",
      "  Rows written: 3021782\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20172020.B5.csv\n",
      "  Rows written: 3010240\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.C1.csv\n",
      "  Rows written: 5757254\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.C2.csv\n",
      "  Rows written: 5522714\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.C3.csv\n",
      "  Rows written: 5686935\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.C4.csv\n",
      "  Rows written: 4734102\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.C5.csv\n",
      "  Rows written: 6064713\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.C6.csv\n",
      "  Rows written: 4986052\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.C7.csv\n",
      "  Rows written: 6531408\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.C8.csv\n",
      "  Rows written: 4895448\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.C9.csv\n",
      "  Rows written: 3900954\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.D1.csv\n",
      "  Rows written: 5782611\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.D2.csv\n",
      "  Rows written: 4541638\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.D3.csv\n",
      "  Rows written: 4223101\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.D4.csv\n",
      "  Rows written: 3019150\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20202023.D5.csv\n",
      "  Rows written: 3005017\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.E1.csv\n",
      "  Rows written: 2246534\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.E2.csv\n",
      "  Rows written: 2251190\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.E3.csv\n",
      "  Rows written: 2253709\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.E4.csv\n",
      "  Rows written: 1873961\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.E5.csv\n",
      "  Rows written: 2537558\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.E6.csv\n",
      "  Rows written: 1978485\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.E7.csv\n",
      "  Rows written: 2618387\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.E8.csv\n",
      "  Rows written: 1921823\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.E9.csv\n",
      "  Rows written: 1587440\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.F1.csv\n",
      "  Rows written: 2232743\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.F2.csv\n",
      "  Rows written: 1784424\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.F3.csv\n",
      "  Rows written: 1630012\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.F4.csv\n",
      "  Rows written: 1150578\n",
      "Processing: /home/jovyan/work/hpool1/pseidel/test/Input/Daily MV LC/DailyMVLC20232025.F5.csv\n",
      "  Rows written: 1145214\n",
      "Done. Combined file written to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/LC_MV_raw.txt\n",
      "Dropped rows with reasons logged in: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/LC_MV_cleaning_errors.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell standardizes a collection of raw daily LC_MV CSV files into a single\n",
    "# long-format text file while explicitly logging all dropped rows:\n",
    "#\n",
    "#   - Reads all LC_MV CSV files from a given folder one by one to keep memory\n",
    "#     usage controlled.\n",
    "#   - Skips the first metadata row, infers the separator for each CSV, and\n",
    "#     removes fully empty rows and columns.\n",
    "#   - Interprets the first remaining row (from start_col onward) as date labels,\n",
    "#     and the following rows as IDs with LC_MV values across those date columns.\n",
    "#   - Reshapes the wide date matrix into long format with columns:\n",
    "#         DS_ID, DayDate, LC_MV\n",
    "#   - Cleans and filters:\n",
    "#       * Parses DayDate as datetime (day-first). Rows where the date fails to\n",
    "#         parse are written to LC_MV_cleaning_errors.txt with reason\n",
    "#         \"Invalid DayDate\".\n",
    "#       * Converts LC_MV values to numeric. Rows with non-numeric TRLC_MVI are written\n",
    "#         to the same error file with reason \"Invalid LC_MV\".\n",
    "#       * Valid rows keep a normalized DayDate in 'YYYY-MM-DD' format.\n",
    "#   - Appends all valid rows into LC_MV_raw.txt, using a pipe separator.\n",
    "#   - Appends all invalid rows into LC_MV_cleaning_errors.txt, including the\n",
    "#     originating source file name and a reason for filtering.\n",
    "# =============================================================================\n",
    "\n",
    "import os                       # Filesystem operations (paths, existence checks, removals)\n",
    "import glob                     # Pattern-based file listing for CSV files\n",
    "import pandas as pd             # DataFrame handling and CSV parsing\n",
    "from pandas.errors import ParserError  # Specific exception type for CSV parsing issues\n",
    "\n",
    "# Input folder containing all raw CSV files\n",
    "input_folder = MarketValues_file_path_LC\n",
    "\n",
    "# Output folder where the combined result will be written\n",
    "output_folder = Temp_file_path_GO\n",
    "\n",
    "# Column index (0-based) where the date columns begin (column H)\n",
    "start_col = 7\n",
    "\n",
    "# Ensure the output directory exists (create if missing)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Path of the combined output file\n",
    "output_file = os.path.join(output_folder, \"LC_MV_raw.txt\")\n",
    "\n",
    "# Path for the cleaning error log\n",
    "error_file = os.path.join(output_folder, \"LC_MV_cleaning_errors.txt\")\n",
    "\n",
    "# Remove old output files if they already exist so this run starts fresh\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "if os.path.exists(error_file):\n",
    "    os.remove(error_file)\n",
    "\n",
    "# Create the output file with header describing the standardized columns\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"DS_ID|DayDate|LC_MV\\n\")\n",
    "\n",
    "# Create the error log file with header describing diagnostic columns\n",
    "with open(error_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"DS_ID|DayDate|LC_MV|Reason|SourceFile\\n\")\n",
    "\n",
    "# Collect all CSV files in sorted order for deterministic processing\n",
    "csv_files = sorted(glob.glob(os.path.join(input_folder, \"*.csv\")))\n",
    "\n",
    "# Main loop: process one file at a time to limit memory usage\n",
    "for input_file in csv_files:\n",
    "    # Log which file is being processed\n",
    "    print(f\"Processing: {input_file}\")\n",
    "    # Extract just the file name to store as the SourceFile in logs\n",
    "    basename = os.path.basename(input_file)\n",
    "\n",
    "    try:\n",
    "        # Attempt to read the CSV, skipping the first metadata row\n",
    "        # sep=None lets pandas infer the delimiter dynamically\n",
    "        df = pd.read_csv(\n",
    "            input_file,\n",
    "            header=None,\n",
    "            sep=None,\n",
    "            engine=\"python\",\n",
    "            skiprows=1\n",
    "        )\n",
    "    except ParserError:\n",
    "        # If parsing fails due to malformed lines, retry while skipping bad lines\n",
    "        df = pd.read_csv(\n",
    "            input_file,\n",
    "            header=None,\n",
    "            sep=None,\n",
    "            engine=\"python\",\n",
    "            skiprows=1,\n",
    "            on_bad_lines=\"skip\"\n",
    "        )\n",
    "\n",
    "    # Remove fully empty rows\n",
    "    df = df.dropna(how=\"all\")\n",
    "    # Remove fully empty columns\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Extract dates (header from start_col onward), IDs (first column), and LC_MV values matrix\n",
    "    dates = df.iloc[0, start_col:]      # First row after metadata contains date labels from start_col\n",
    "    ids = df.iloc[1:, 0]                # First column (excluding the date row) holds DS_ID values\n",
    "    values = df.iloc[1:, start_col:]    # Remaining rows and columns hold LC_MV values\n",
    "\n",
    "    # Assign date strings as column names on the values DataFrame\n",
    "    values.columns = dates.values\n",
    "\n",
    "    # Map row indices in values to the corresponding DS_IDs from ids\n",
    "    id_map = ids.to_dict()\n",
    "\n",
    "    # Convert from wide (IDs x dates) to long format:\n",
    "    #   stack -> MultiIndex (row_idx, DayDate) with LC_MV as values\n",
    "    long_df = values.stack().reset_index()\n",
    "    long_df.columns = [\"row_idx\", \"DayDate\", \"LC_MV\"]\n",
    "\n",
    "    # Insert DS_ID column using the row index mapping\n",
    "    long_df[\"DS_ID\"] = long_df[\"row_idx\"].map(id_map)\n",
    "\n",
    "    # Record originating file for each row (used in error logging)\n",
    "    long_df[\"SourceFile\"] = basename\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Validate DayDate: keep invalid rows separately with reason\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse DayDate strings as datetime in day-first format (e.g., DD/MM/YYYY)\n",
    "    parsed_dates = pd.to_datetime(\n",
    "        long_df[\"DayDate\"],\n",
    "        errors=\"coerce\",\n",
    "        dayfirst=True\n",
    "    )\n",
    "    # Boolean mask of rows where date parsing failed (NaT)\n",
    "    invalid_date_mask = parsed_dates.isna()\n",
    "    # Subset of rows with invalid dates\n",
    "    invalid_daydate = long_df[invalid_date_mask].copy()\n",
    "\n",
    "    if not invalid_daydate.empty:\n",
    "        # Label invalid date rows with a descriptive reason\n",
    "        invalid_daydate[\"Reason\"] = \"Invalid DayDate\"\n",
    "        # Select and order the columns to match the error file header\n",
    "        invalid_daydate[[\"DS_ID\", \"DayDate\", \"LC_MV\", \"Reason\", \"SourceFile\"]].to_csv(\n",
    "            error_file,\n",
    "            sep=\"|\",\n",
    "            index=False,\n",
    "            mode=\"a\",\n",
    "            header=False\n",
    "        )\n",
    "        # Report count of invalid-date rows for this file\n",
    "        print(f\"  Warning: {len(invalid_daydate)} invalid date entries detected in this file.\")\n",
    "\n",
    "    # Keep only rows with successfully parsed dates for further processing\n",
    "    valid_df = long_df[~invalid_date_mask].copy()\n",
    "    # Attach the parsed datetime values for valid rows\n",
    "    valid_df[\"DayDate\"] = parsed_dates[~invalid_date_mask]\n",
    "\n",
    "    # Standardize the date representation to 'YYYY-MM-DD' strings\n",
    "    valid_df[\"DayDate\"] = valid_df[\"DayDate\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Validate LC_MV: must be numeric; log invalid rows with reason\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert LC_MV values to numeric; non-convertible entries become NaN\n",
    "    tri_numeric = pd.to_numeric(valid_df[\"LC_MV\"], errors=\"coerce\")\n",
    "    # Mask of rows where LC_MV is invalid (NaN after coercion)\n",
    "    invalid_tri_mask = tri_numeric.isna()\n",
    "    # Rows with invalid LC_MV values\n",
    "    invalid_tri = valid_df[invalid_tri_mask].copy()\n",
    "\n",
    "    if not invalid_tri.empty:\n",
    "        # Label these rows with reason indicating LC_MV parsing issues\n",
    "        invalid_tri[\"Reason\"] = \"Invalid LC_MV\"\n",
    "        # Save invalid LC_MV rows to the error log file\n",
    "        invalid_tri[[\"DS_ID\", \"DayDate\", \"LC_MV\", \"Reason\", \"SourceFile\"]].to_csv(\n",
    "            error_file,\n",
    "            sep=\"|\",\n",
    "            index=False,\n",
    "            mode=\"a\",\n",
    "            header=False\n",
    "        )\n",
    "        # Report how many invalid LC_MV entries were found for this file\n",
    "        print(f\"  Warning: {len(invalid_tri)} invalid LC_MV entries detected in this file.\")\n",
    "\n",
    "    # Keep only rows with valid numeric LC_MV values\n",
    "    clean_df = valid_df[~invalid_tri_mask].copy()\n",
    "    # Assign the numeric LC_MV series to the clean DataFrame\n",
    "    clean_df[\"LC_MV\"] = tri_numeric[~invalid_tri_mask]\n",
    "\n",
    "    # Select output columns for the standardized long-format result\n",
    "    result = clean_df[[\"DS_ID\", \"DayDate\", \"LC_MV\"]]\n",
    "\n",
    "    # Report how many cleaned rows will be written for this file\n",
    "    print(f\"  Rows written: {len(result)}\")\n",
    "\n",
    "    # If there are no valid rows, skip appending to the combined output\n",
    "    if len(result) == 0:\n",
    "        continue\n",
    "\n",
    "    # Append valid rows to the combined TRI_raw output file without header\n",
    "    result.to_csv(\n",
    "        output_file,\n",
    "        sep=\"|\",\n",
    "        index=False,\n",
    "        mode=\"a\",\n",
    "        header=False\n",
    "    )\n",
    "\n",
    "# Final summary logs with paths to the combined outputs\n",
    "print(f\"Done. Combined file written to: {output_file}\")\n",
    "print(f\"Dropped rows with reasons logged in: {error_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Delisted Comp's MVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Cleaned MV file written to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/LC_MV_raw_excl_delisted.txt\n",
      "Delisting log written to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/LC_MV_delisting_audit.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell takes the standardized long-format market value file `LC_MV_raw.txt`\n",
    "# and removes \"dead\" tails for instruments that appear to be delisted.\n",
    "#\n",
    "# Delisting detection logic (per DS_ID):\n",
    "#   - Sort observations by DayDate ascending.\n",
    "#   - Look from the end of the series backwards and compute the length of the\n",
    "#     final run of identical MV values (a constant tail).\n",
    "#   - If this final constant run is at least 20 consecutive rows, and the last\n",
    "#     MV equals the MV in this run (which it does by construction), we treat\n",
    "#     the instrument as delisted.\n",
    "#   - We then DROP ALL ROWS from the first date of this final constant run\n",
    "#     onwards (i.e., the entire flat tail including the initial 20 days).\n",
    "#\n",
    "# Outputs:\n",
    "#   - `LC_MV_cleaned.txt`  : same structure as LC_MV_raw (DS_ID|DayDate|MV) but with\n",
    "#                         delisted tails removed.\n",
    "#   - `LC_MV_delisting_log.txt` : list of instruments identified as delisted with\n",
    "#                              the date from which they are excluded.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input/output folder (same as in previous cell)\n",
    "output_folder = Temp_file_path_GO\n",
    "\n",
    "# Input standardized market value file\n",
    "mv_input_file = os.path.join(output_folder, \"LC_MV_raw.txt\")\n",
    "\n",
    "# Output files\n",
    "mv_cleaned_file = os.path.join(output_folder, \"LC_MV_raw_excl_delisted.txt\")\n",
    "delist_log_file = os.path.join(output_folder, \"LC_MV_delisting_audit.txt\")\n",
    "\n",
    "# Remove old outputs if they exist\n",
    "for path in [mv_cleaned_file, delist_log_file]:\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "\n",
    "# Read the standardized long-format MV file\n",
    "# DS_ID is kept as string to avoid losing leading zeros etc.\n",
    "mv_df = pd.read_csv(\n",
    "    mv_input_file,\n",
    "    sep=\"|\",\n",
    "    dtype={\"DS_ID\": str}\n",
    ")\n",
    "\n",
    "# Parse DayDate to datetime for proper sorting and comparison\n",
    "mv_df[\"DayDate\"] = pd.to_datetime(mv_df[\"DayDate\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "# Drop any rows where DayDate could not be parsed (should normally not occur)\n",
    "mv_df = mv_df[mv_df[\"DayDate\"].notna()].copy()\n",
    "\n",
    "# Create output files with headers\n",
    "with open(mv_cleaned_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    f_out.write(\"DS_ID|DayDate|MV\\n\")\n",
    "\n",
    "with open(delist_log_file, \"w\", encoding=\"utf-8\") as f_log:\n",
    "    f_log.write(\"DS_ID|DelistFromDate\\n\")\n",
    "\n",
    "# Minimum tail length (in consecutive rows) to declare delisting\n",
    "MIN_TAIL_LENGTH = 20\n",
    "\n",
    "# Process each instrument separately\n",
    "for ds_id, grp in mv_df.groupby(\"DS_ID\"):\n",
    "    g = grp.sort_values(\"DayDate\").copy()\n",
    "    mv_values = g[\"LC_MV\"].values\n",
    "    n = len(mv_values)\n",
    "\n",
    "    # Default: keep full history\n",
    "    cutoff_date = None\n",
    "\n",
    "    if n >= MIN_TAIL_LENGTH:\n",
    "        # Identify the final run of identical MV values from the end backwards\n",
    "        last_mv = mv_values[-1]\n",
    "        tail_length = 1\n",
    "\n",
    "        for i in range(n - 2, -1, -1):\n",
    "            if mv_values[i] == last_mv:\n",
    "                tail_length += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # If the final constant tail is long enough, declare delisting\n",
    "        if tail_length >= MIN_TAIL_LENGTH:\n",
    "            # Index (within the sorted group) of the first obs in the final constant run\n",
    "            start_tail_idx = n - tail_length\n",
    "            cutoff_date = g.iloc[start_tail_idx][\"DayDate\"]\n",
    "\n",
    "    if cutoff_date is not None:\n",
    "        # Log delisting info: instrument and the date from which we drop values\n",
    "        with open(delist_log_file, \"a\", encoding=\"utf-8\") as f_log:\n",
    "            f_log.write(f\"{ds_id}|{cutoff_date.strftime('%Y-%m-%d')}\\n\")\n",
    "\n",
    "        # Exclude all rows from cutoff_date onwards (including the tail itself)\n",
    "        g_filtered = g[g[\"DayDate\"] < cutoff_date].copy()\n",
    "    else:\n",
    "        # No delisting detected: keep full history\n",
    "        g_filtered = g\n",
    "\n",
    "    # Append remaining rows for this DS_ID to cleaned output file\n",
    "    if not g_filtered.empty:\n",
    "        # Convert DayDate back to YYYY-MM-DD string format\n",
    "        g_filtered[\"DayDate\"] = g_filtered[\"DayDate\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        g_filtered[[\"DS_ID\", \"DayDate\", \"LC_MV\"]].to_csv(\n",
    "            mv_cleaned_file,\n",
    "            sep=\"|\",\n",
    "            index=False,\n",
    "            header=False,\n",
    "            mode=\"a\"\n",
    "        )\n",
    "\n",
    "print(f\"Done. Cleaned MV file written to: {mv_cleaned_file}\")\n",
    "print(f\"Delisting log written to: {delist_log_file}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Cleanup: free memory from this cell\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "# Delete large objects\n",
    "del mv_df, g, grp, mv_values, g_filtered\n",
    "\n",
    "# If still in scope\n",
    "try:\n",
    "    del ds_id\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the Local Currency of MV (1:Many Relationship as Wanted; Few Rows Lost Due to Missing PCUR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV: /home/jovyan/work/hpool1/pseidel/test/Input/Constituents.01.csv...\n",
      "  - Loaded 167649 rows. Columns extracted: Code, PCUR\n",
      "  - DEDUPLICATION: Removed 56299 duplicate Codes.\n",
      "  - Unique rows remaining: 111350\n",
      "Loading TXT: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/LC_MV_raw_excl_delisted.txt...\n",
      "  - Loaded 337375075 rows.\n",
      "\n",
      "========================================\n",
      "RELATIONSHIP ANALYSIS\n",
      "========================================\n",
      "Merge Key (CSV): 'Code'\n",
      "Merge Key (TXT): 'DS_ID'\n",
      "------------------------------\n",
      "Max duplicates in CSV (Code):  1  [Unique]\n",
      "Max duplicates in TXT (DS_ID): 17310  [Duplicates Present]\n",
      "------------------------------\n",
      "DETECTED RELATIONSHIP: One-to-Many (1:m)\n",
      "------------------------------\n",
      "Merging datasets...\n",
      "  - Rows in CSV: 111350\n",
      "  - Rows in TXT: 337375075\n",
      "  - Rows in Merged Result: 337233221\n",
      "\n",
      "Success! File saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/LC_MV_pre_clean.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY OF THIS SCRIPT\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Loads the \"Constituents\" CSV and extracts 'Code' & 'PCUR'.\n",
    "# 2. DEDUPLICATES the Constituents file so each Code appears only once.\n",
    "# 3. Loads the \"Stock Data\" TXT (pipe-separated).\n",
    "# 4. Analyzes the merge keys to confirm the relationship is now One-to-Many.\n",
    "# 5. Merges the two datasets on Code = DS_ID.\n",
    "# 6. Saves the merged result to a PIPE-SEPARATED file.\n",
    "# =============================================================================\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "CONFIG = {\n",
    "    # 1. Path to the Constituents CSV (Source of Code & PCUR)\n",
    "    'constituents_csv_path': f\"{Input_file_path}/Constituents.01.csv\",\n",
    "    \n",
    "    # 2. Path to the Daily Stock Data TXT (Pipe-separated)\n",
    "    'stock_data_txt_path': f\"{Temp_file_path_GO}/LC_MV_raw_excl_delisted.txt\",\n",
    "    \n",
    "    # 3. Output file path\n",
    "    'output_file_path': f\"{Temp_file_path_GO}/LC_MV_pre_clean.txt\",\n",
    "    \n",
    "    # 4. Join Keys\n",
    "    'merge_key_csv': 'Code',\n",
    "    'merge_key_txt': 'DS_ID'\n",
    "}\n",
    "\n",
    "def merge_and_track_relations(config):\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Load Constituents File (Left Side)\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"Loading CSV: {config['constituents_csv_path']}...\")\n",
    "    try:\n",
    "        df_csv = pd.read_csv(\n",
    "            config['constituents_csv_path'], \n",
    "            low_memory=False, \n",
    "            encoding='ISO-8859-1'\n",
    "        )\n",
    "        \n",
    "        if 'Code' not in df_csv.columns or 'PCUR' not in df_csv.columns:\n",
    "            print(f\"Error: Columns 'Code' and 'PCUR' required in CSV.\")\n",
    "            return\n",
    "\n",
    "        # Extract only relevant columns\n",
    "        df_left = df_csv[['Code', 'PCUR']].copy()\n",
    "        df_left[config['merge_key_csv']] = df_left[config['merge_key_csv']].astype(str)\n",
    "        \n",
    "        print(f\"  - Loaded {len(df_left)} rows. Columns extracted: Code, PCUR\")\n",
    "\n",
    "        # --- NEW: DEDUPLICATE ---\n",
    "        rows_before = len(df_left)\n",
    "        # Drop duplicates based on the 'Code' column, keeping the first occurrence\n",
    "        df_left = df_left.drop_duplicates(subset=[config['merge_key_csv']], keep='first')\n",
    "        rows_after = len(df_left)\n",
    "        \n",
    "        if rows_before > rows_after:\n",
    "            print(f\"  - DEDUPLICATION: Removed {rows_before - rows_after} duplicate Codes.\")\n",
    "            print(f\"  - Unique rows remaining: {rows_after}\")\n",
    "        else:\n",
    "            print(f\"  - No duplicates found in CSV.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file not found at {config['constituents_csv_path']}\")\n",
    "        return\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Load Stock Data File (Right Side)\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"Loading TXT: {config['stock_data_txt_path']}...\")\n",
    "    try:\n",
    "        df_right = pd.read_csv(\n",
    "            config['stock_data_txt_path'], \n",
    "            sep='|', \n",
    "            low_memory=False, \n",
    "            encoding='ISO-8859-1'\n",
    "        )\n",
    "        \n",
    "        if config['merge_key_txt'] not in df_right.columns:\n",
    "            print(f\"Error: Merge key '{config['merge_key_txt']}' not found in TXT file.\")\n",
    "            return\n",
    "\n",
    "        df_right[config['merge_key_txt']] = df_right[config['merge_key_txt']].astype(str)\n",
    "        print(f\"  - Loaded {len(df_right)} rows.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: TXT file not found at {config['stock_data_txt_path']}\")\n",
    "        return\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. RELATIONSHIP TRACKING / ANALYSIS\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"RELATIONSHIP ANALYSIS\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Calculate max occurrences\n",
    "    left_key_counts = df_left[config['merge_key_csv']].value_counts()\n",
    "    right_key_counts = df_right[config['merge_key_txt']].value_counts()\n",
    "\n",
    "    max_left_dups = left_key_counts.max() if not left_key_counts.empty else 0\n",
    "    max_right_dups = right_key_counts.max() if not right_key_counts.empty else 0\n",
    "\n",
    "    left_is_unique = (max_left_dups <= 1)\n",
    "    right_is_unique = (max_right_dups <= 1)\n",
    "\n",
    "    # Determine Type\n",
    "    relation_type = \"Unknown\"\n",
    "    if left_is_unique and right_is_unique:\n",
    "        relation_type = \"One-to-One (1:1)\"\n",
    "    elif left_is_unique and not right_is_unique:\n",
    "        relation_type = \"One-to-Many (1:m)\"\n",
    "    elif not left_is_unique and right_is_unique:\n",
    "        relation_type = \"Many-to-One (m:1)\"\n",
    "    else:\n",
    "        relation_type = \"Many-to-Many (m:m)\"\n",
    "\n",
    "    print(f\"Merge Key (CSV): '{config['merge_key_csv']}'\")\n",
    "    print(f\"Merge Key (TXT): '{config['merge_key_txt']}'\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Max duplicates in CSV (Code):  {max_left_dups}  [{'Unique' if left_is_unique else 'Duplicates Present'}]\")\n",
    "    print(f\"Max duplicates in TXT (DS_ID): {max_right_dups}  [{'Unique' if right_is_unique else 'Duplicates Present'}]\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"DETECTED RELATIONSHIP: {relation_type}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Merge\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Merging datasets...\")\n",
    "    \n",
    "    merged_df = pd.merge(\n",
    "        df_left,\n",
    "        df_right,\n",
    "        left_on=config['merge_key_csv'],\n",
    "        right_on=config['merge_key_txt'],\n",
    "        how='inner' \n",
    "    )\n",
    "    \n",
    "    print(f\"  - Rows in CSV: {len(df_left)}\")\n",
    "    print(f\"  - Rows in TXT: {len(df_right)}\")\n",
    "    print(f\"  - Rows in Merged Result: {len(merged_df)}\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 5. Save\n",
    "    # ---------------------------------------------------------\n",
    "    if not merged_df.empty:\n",
    "        # Create directory if it doesn't exist\n",
    "        out_dir = os.path.dirname(config['output_file_path'])\n",
    "        if out_dir and not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "            \n",
    "        # Save as pipe-separated\n",
    "        merged_df.to_csv(config['output_file_path'], index=False, sep='|')\n",
    "        print(f\"\\nSuccess! File saved to: {config['output_file_path']}\")\n",
    "    else:\n",
    "        print(\"\\nWarning: Merged dataframe is empty. No file was saved.\")\n",
    "\n",
    "# Run the function\n",
    "merge_and_track_relations(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Non-mappable IDs and Add ID Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading Mapping Dict: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/ID_mapping_clean.txt...\n",
      "   Mapped loaded. Contains 95,104 keys.\n",
      "2. Loading Stock Data: /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/LC_MV_pre_clean.txt...\n",
      "   Loaded 337,233,221 rows.\n",
      "3. Mapping IDs (Vectorized)...\n",
      "   Mapping complete. Dropped 21,994,946 unmapped rows.\n",
      "4. Saving to /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview/LC_MV_clean.txt...\n",
      "\n",
      "========================================\n",
      "FINAL STATS\n",
      "========================================\n",
      "Total Rows:   315,238,275\n",
      "Unique IDs:   85,049\n",
      "Header:        ['ID', 'PCUR', 'DayDate', 'MV']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY OF THIS SCRIPT\n",
    "# -----------------------------------------------------------------------------\n",
    "# This script filters and standardizes the pre-cleaned stock data using a\n",
    "# high-performance dictionary mapping approach (optimized for High RAM).\n",
    "#\n",
    "# LOGIC:\n",
    "# 1. Loads \"ID_mapping_clean.txt\" into a Python Dictionary (Hash Map).\n",
    "#    - Key: DS_ID, Value: ID\n",
    "# 2. Loads \"LC_MV_pre_clean.txt\" (Dataset with DS_ID, DayDate, LC_MV).\n",
    "# 3. Maps 'DS_ID' to 'ID' using the dictionary.\n",
    "#    - Fast vectorized lookup.\n",
    "#    - IDs not found in the dictionary become NaN.\n",
    "# 4. Filters out rows where ID is NaN (unmapped rows).\n",
    "# 5. Removes the old 'DS_ID' and 'Code' columns.\n",
    "# 6. Reorders columns to put 'ID' first and saves to \"LC_MV_clean.txt\".\n",
    "# =============================================================================\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "CONFIG = {\n",
    "    'input_file': f\"{Temp_file_path_GO}/LC_MV_pre_clean.txt\",\n",
    "    'mapping_file': f\"{Temp_file_path_GO}/ID_mapping_clean.txt\",\n",
    "    'output_file': f\"{Temp_file_path_GO}/LC_MV_clean.txt\",\n",
    "    'sep': '|'\n",
    "}\n",
    "\n",
    "def fast_filter_and_replace(config):\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. Load ID Mapping directly into a Dictionary\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"1. Loading Mapping Dict: {config['mapping_file']}...\")\n",
    "    \n",
    "    # Read just the two needed columns\n",
    "    df_map = pd.read_csv(\n",
    "        config['mapping_file'], \n",
    "        sep=config['sep'], \n",
    "        usecols=['ID', 'DS_ID'],\n",
    "        dtype=str\n",
    "    )\n",
    "    \n",
    "    # Convert to dictionary: { 'DS_ID_Value': 'ID_Value' }\n",
    "    # This hash map is extremely fast for lookups\n",
    "    id_mapper = dict(zip(df_map['DS_ID'], df_map['ID']))\n",
    "    \n",
    "    del df_map\n",
    "    gc.collect()\n",
    "    print(f\"   Mapped loaded. Contains {len(id_mapper):,} keys.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. Load Stock Data\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"2. Loading Stock Data: {config['input_file']}...\")\n",
    "    df_main = pd.read_csv(\n",
    "        config['input_file'], \n",
    "        sep=config['sep'], \n",
    "        low_memory=False, \n",
    "        dtype={'DS_ID': str} # Important for matching\n",
    "    )\n",
    "    initial_rows = len(df_main)\n",
    "    print(f\"   Loaded {initial_rows:,} rows.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. FAST OPERATION: MAP & DROP\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"3. Mapping IDs (Vectorized)...\")\n",
    "\n",
    "    # This single line does the lookup. \n",
    "    # If DS_ID is in the dict, it gives the ID. \n",
    "    # If NOT, it returns NaN.\n",
    "    df_main['ID'] = df_main['DS_ID'].map(id_mapper)\n",
    "\n",
    "    # Filter: Drop rows where ID is NaN (meaning DS_ID was not in map)\n",
    "    before_drop = len(df_main)\n",
    "    df_main.dropna(subset=['ID'], inplace=True)\n",
    "    rows_dropped = initial_rows - len(df_main)\n",
    "    \n",
    "    print(f\"   Mapping complete. Dropped {rows_dropped:,} unmapped rows.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Cleanup & Save\n",
    "    # ---------------------------------------------------------\n",
    "    # Drop old columns\n",
    "    cols_to_drop = ['DS_ID', 'Code']\n",
    "    df_main.drop(columns=[c for c in cols_to_drop if c in df_main.columns], inplace=True)\n",
    "\n",
    "    # Reorder columns: ID first\n",
    "    cols = ['ID'] + [c for c in df_main.columns if c != 'ID']\n",
    "    df_main = df_main[cols]\n",
    "\n",
    "    print(f\"4. Saving to {config['output_file']}...\")\n",
    "    df_main.to_csv(config['output_file'], index=False, sep=config['sep'])\n",
    "\n",
    "    # Final Stats\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"FINAL STATS\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Total Rows:   {len(df_main):,}\")\n",
    "    print(f\"Unique IDs:   {df_main['ID'].nunique():,}\")\n",
    "    print(\"Header:       \", df_main.columns.tolist())\n",
    "\n",
    "# Run\n",
    "fast_filter_and_replace(CONFIG)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqk47Y8ZH3NOiU4OWgs6cf",
   "collapsed_sections": [
    "ctXZHVs-zurM",
    "6a6_hk-ee7VZ",
    "51RgNAd-j51P",
    "hQrNim8IySIQ",
    "v5Q8pv8Yj_kc",
    "8kxIfNOAy0ve",
    "MlB7pqtvkNOJ",
    "oL6aDeQTkZ2G",
    "oXiX0-HIrUoe",
    "re_o1kiWkoxp",
    "mY4Vj34bVcfE",
    "e8OFoUUqopEE",
    "dJzOyKSPTju0",
    "DA9_D0JETpwo"
   ],
   "history_visible": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
