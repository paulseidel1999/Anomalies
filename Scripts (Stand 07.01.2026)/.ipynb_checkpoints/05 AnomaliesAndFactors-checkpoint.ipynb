{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctXZHVs-zurM"
   },
   "source": [
    "### Mount Google Drive, Import Libraries and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31335,
     "status": "ok",
     "timestamp": 1765720510380,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "DVbbtM4ztajO",
    "outputId": "9b5c735c-82b7-4533-8c15-8fe6ce9bfd81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: False\n",
      "BASE_PATH: /home/jovyan/work/hpool1/pseidel/test\n",
      "Importing numpy ...\n",
      "numpy OK\n",
      "Importing scipy ...\n",
      "scipy OK\n",
      "Importing pandas ...\n",
      "pandas OK\n",
      "Importing linearmodels ...\n",
      "linearmodels OK\n",
      "Importing xlsxwriter ...\n",
      "xlsxwriter OK\n",
      "Paths configured. Temp outputs -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview\n",
      "Example input path -> /home/jovyan/work/hpool1/pseidel/test/Input/WSFV_f_20250131.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT SETUP + PATH CONFIGURATION (SERVER / COLAB COMPATIBLE)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "from scipy.stats.mstats import winsorize\n",
    "import shutil\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) HARD SAFETY: cap native thread usage (prevents pthread_create EAGAIN)\n",
    "#    MUST be set before importing numpy / scipy / pandas\n",
    "# -----------------------------------------------------------------------------\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"BLIS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Detect environment\n",
    "# -----------------------------------------------------------------------------\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) (Colab only) Mount Google Drive\n",
    "# -----------------------------------------------------------------------------\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "else:\n",
    "    # Server base path (your target)\n",
    "    BASE_PATH = \"/home/jovyan/work/hpool1/pseidel/test\"\n",
    "\n",
    "print(\"IN_COLAB:\", IN_COLAB)\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Sanity checks: path exists + write permission\n",
    "# -----------------------------------------------------------------------------\n",
    "BASE = Path(BASE_PATH)\n",
    "if not BASE.exists():\n",
    "    raise FileNotFoundError(f\"BASE_PATH does not exist: {BASE}\")\n",
    "\n",
    "# quick write test (fails fast if you don't have permissions)\n",
    "test_file = BASE / \".write_test_tmp\"\n",
    "try:\n",
    "    test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "    test_file.unlink()\n",
    "except Exception as e:\n",
    "    raise PermissionError(f\"No write permission in {BASE}. Error: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Environment check: ensure required packages import cleanly\n",
    "# -----------------------------------------------------------------------------\n",
    "required_packages = [\"numpy\", \"scipy\", \"pandas\", \"linearmodels\", \"xlsxwriter\"]\n",
    "\n",
    "for pkg in required_packages:\n",
    "    print(f\"Importing {pkg} ...\")\n",
    "    importlib.import_module(pkg)\n",
    "    print(f\"{pkg} OK\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Base paths and input/output locations\n",
    "# -----------------------------------------------------------------------------\n",
    "Input_file_path   = str(BASE / \"Input\")\n",
    "Temp_file_path    = str(BASE / \"Temp\")\n",
    "Output_file_path  = str(BASE / \"Output\")\n",
    "\n",
    "Fundamentals_file_path = f\"{Input_file_path}/WSFV_f_20250131.txt\"\n",
    "Current_file_path      = f\"{Input_file_path}/WSCurrent_f_20250131.txt\"\n",
    "Calendar_file_path     = f\"{Input_file_path}/WSCalendarPrd_f_20250131.txt\"\n",
    "Meta_file_path         = f\"{Input_file_path}/WSMetaData_f_20250131.txt\"\n",
    "Excel_file_path        = f\"{Input_file_path}/WS PIT Table Definitions V5 with start dates.xls\"\n",
    "\n",
    "MarketValues_file_path          = f\"{Input_file_path}/Daily MV USD\"\n",
    "MarketValues_file_path_LC       = f\"{Input_file_path}/Daily MV LC\"\n",
    "DailyTotalReturns_file_path     = f\"{Input_file_path}/Daily Returns USD\"\n",
    "DailyIndexReturns_file_path     = f\"{Input_file_path}/Daily Index Returns USD\"\n",
    "Constituents_file_path          = f\"{Input_file_path}/Constituents.01.csv\"\n",
    "UniversalMatching_file_path     = f\"{Input_file_path}/Universal Matching File\"\n",
    "\n",
    "Temp_file_path_GO  = f\"{Temp_file_path}/TempGeneralOverview\"\n",
    "Temp_file_path_EoC = f\"{Temp_file_path}/TempExtractionofCharacteristics\"\n",
    "Temp_file_path_DP  = f\"{Temp_file_path}/TempDataPreparation\"\n",
    "Temp_file_path_A   = f\"{Temp_file_path}/TempAnomalies\"\n",
    "Temp_file_path_R   = f\"{Temp_file_path}/TempRegressionModel\"\n",
    "\n",
    "Relevant_items_path   = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_A = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_B = f\"{Input_file_path}/RelevantItemsB.txt\"\n",
    "Relevant_items_path_C = f\"{Input_file_path}/RelevantItemsC.txt\"\n",
    "Relevant_items_path_D = f\"{Input_file_path}/RelevantItemsD.txt\"\n",
    "\n",
    "Subset_file_path = f\"{Temp_file_path_GO}/Subsets\"\n",
    "Fundamentals_clean_file_path = f\"{Temp_file_path_GO}/Fundamentals_clean.txt\"\n",
    "Current_clean_file_path      = f\"{Temp_file_path_GO}/Current_clean.txt\"\n",
    "Calendar_clean_file_path     = f\"{Temp_file_path_GO}/Input/Calendar_clean.txt\"\n",
    "Meta_clean_file_path         = f\"{Temp_file_path_GO}/Input/Meta_clean.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Ensure required directories exist\n",
    "# -----------------------------------------------------------------------------\n",
    "Path(Output_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_GO).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_EoC).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_A).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(Subset_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Path(Calendar_clean_file_path).parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Streaming / deduplication settings\n",
    "# -----------------------------------------------------------------------------\n",
    "CHUNK_SIZE = 2_000_000\n",
    "DATE_COL = \"PIT Date\"\n",
    "DEDUP_KEYS = [\"ID\", \"ItemCode\", DATE_COL]\n",
    "\n",
    "print(\"Paths configured. Temp outputs ->\", Temp_file_path_GO)\n",
    "print(\"Example input path ->\", Fundamentals_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           754Gi       253Gi       294Gi        55Mi       215Gi       501Gi\n",
      "Swap:             0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI91nEqImRuW"
   },
   "source": [
    "# 1.0. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1I-ohwwoiaOf"
   },
   "source": [
    "### Import and Rename Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1765727712505,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "7lV6rcp-iZqs",
    "outputId": "0b44d075-5eb9-4c4b-9346-9fb5f0479ee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copied & renamed files:\n",
      "Accounts_Payable.txt\n",
      "Capital_Expenditures_Addtns_to_Fixed_Assets.txt\n",
      "Cash_Dividends_Paid___Total.txt\n",
      "Cash__Short_Term_Investments.txt\n",
      "Com_Pfd_Redeemed_Retired_Converted_Etc..txt\n",
      "Common_Equity.txt\n",
      "Cost_of_Goods_Sold_Excl_Depreciation.txt\n",
      "Current_Assets___Total.txt\n",
      "Current_Liabilities___Total.txt\n",
      "Deferred_Taxes.txt\n",
      "Depreciation_Depletion__Amortization.txt\n",
      "Disposal_of_Fixed_Assets.txt\n",
      "Earnings_Per_Share_Fiscal_Year_End.txt\n",
      "Extraordinary_Items.txt\n",
      "Funds_From_For_Other_Operating_Activities.txt\n",
      "Funds_From_Operations.txt\n",
      "Income_Taxes.txt\n",
      "Income_Taxes_Payable.txt\n",
      "Interest_Expense___Total.txt\n",
      "Inventories___Total.txt\n",
      "Investments_in_Associated_Companies.txt\n",
      "Investments_in_Sales__Direct_Financing_Leases.txt\n",
      "Long_Term_Borrowings.txt\n",
      "Long_Term_Debt.txt\n",
      "Long_Term_Receivables.txt\n",
      "Minority_Interest.txt\n",
      "Net_Cash_Flow___Financing.txt\n",
      "Net_Cash_Flow___Investing.txt\n",
      "Net_Cash_Flow___Operating_Activities.txt\n",
      "Net_Income_Before_Extra_Items_Preferred_Divs.txt\n",
      "Net_Income_Used_to_Calculate_Basic_EPS.txt\n",
      "Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt\n",
      "Net_Sales_or_Revenues.txt\n",
      "Operating_Income.txt\n",
      "Other_Assets___Total.txt\n",
      "Other_Current_Assets.txt\n",
      "Other_Current_Liabilities.txt\n",
      "Other_Investments.txt\n",
      "Other_Liabilities.txt\n",
      "Preferred_Stock.txt\n",
      "Property_Plant__Equipment___Net.txt\n",
      "ReceivablesNet.txt\n",
      "Reduction_in_Long_Term_Debt.txt\n",
      "Sales_Per_Share.txt\n",
      "Selling_General__Administrative_Expenses.txt\n",
      "Short_Term_Debt__Current_Portion_of_LT_Debt.txt\n",
      "Total_Assets.txt\n",
      "Total_Liabilities.txt\n",
      "Unspecified_Other_Loans.txt\n",
      "\n",
      "All .txt files currently in OUTPUT_DIR:\n",
      "Accounts_Payable.txt\n",
      "Capital_Expenditures_Addtns_to_Fixed_Assets.txt\n",
      "Cash_Dividends_Paid___Total.txt\n",
      "Cash__Short_Term_Investments.txt\n",
      "Com_Pfd_Redeemed_Retired_Converted_Etc..txt\n",
      "Common_Equity.txt\n",
      "Cost_of_Goods_Sold_Excl_Depreciation.txt\n",
      "Current_Assets___Total.txt\n",
      "Current_Liabilities___Total.txt\n",
      "Deferred_Taxes.txt\n",
      "Depreciation_Depletion__Amortization.txt\n",
      "Disposal_of_Fixed_Assets.txt\n",
      "DroppedFromFFMerge.txt\n",
      "Earnings_Per_Share_Fiscal_Year_End.txt\n",
      "Extraordinary_Items.txt\n",
      "FF_Benchmark_Factors_Merged_Clean.txt\n",
      "FF_Benchmark_Factors_Merged_Clean_Replication.txt\n",
      "Factors_Annual_Country.txt\n",
      "Factors_Annual_Global.txt\n",
      "Factors_Annual_Replication.txt\n",
      "Factors_Daily_Country.txt\n",
      "Factors_Daily_Global.txt\n",
      "Factors_Daily_Replication.txt\n",
      "Funds_From_For_Other_Operating_Activities.txt\n",
      "Funds_From_Operations.txt\n",
      "Income_Taxes.txt\n",
      "Income_Taxes_Payable.txt\n",
      "Interest_Expense___Total.txt\n",
      "Inventories___Total.txt\n",
      "Investments_in_Associated_Companies.txt\n",
      "Investments_in_Sales__Direct_Financing_Leases.txt\n",
      "Long_Term_Borrowings.txt\n",
      "Long_Term_Debt.txt\n",
      "Long_Term_Receivables.txt\n",
      "Minority_Interest.txt\n",
      "Net_Cash_Flow___Financing.txt\n",
      "Net_Cash_Flow___Investing.txt\n",
      "Net_Cash_Flow___Operating_Activities.txt\n",
      "Net_Income_Before_Extra_Items_Preferred_Divs.txt\n",
      "Net_Income_Used_to_Calculate_Basic_EPS.txt\n",
      "Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt\n",
      "Net_Sales_or_Revenues.txt\n",
      "Operating_Income.txt\n",
      "Other_Assets___Total.txt\n",
      "Other_Current_Assets.txt\n",
      "Other_Current_Liabilities.txt\n",
      "Other_Investments.txt\n",
      "Other_Liabilities.txt\n",
      "Preferred_Stock.txt\n",
      "Property_Plant__Equipment___Net.txt\n",
      "ReceivablesNet.txt\n",
      "Reduction_in_Long_Term_Debt.txt\n",
      "Sales_Per_Share.txt\n",
      "Selling_General__Administrative_Expenses.txt\n",
      "Short_Term_Debt__Current_Portion_of_LT_Debt.txt\n",
      "Total_Assets.txt\n",
      "Total_Liabilities.txt\n",
      "Unspecified_Other_Loans.txt\n",
      "data_Factors.txt\n",
      "processed_data_Factors.txt\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Summary:\n",
    "# This cell scans INPUT_DIR for *.txt files whose names end with\n",
    "# \"_complete_subset.txt\" and start with either \"Annualized_\" or\n",
    "# \"Mixed_\". Matching files are copied to OUTPUT_DIR and renamed to\n",
    "# \"<core>.txt\", where <core> is the middle part of the filename.\n",
    "# Existing target files are not overwritten.\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "INPUT_DIR = Path(Temp_file_path_DP)    # Directory containing source .txt files\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)    # Directory where renamed files are stored\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)  # Ensure output directory exists\n",
    "# =====================\n",
    "\n",
    "new_files = []  # List to track newly created output filenames\n",
    "\n",
    "# Iterate through all .txt files in the input directory\n",
    "for file in INPUT_DIR.glob(\"*.txt\"):\n",
    "    name = file.name          # Extract the filename\n",
    "    core = None               # Will hold the extracted core name\n",
    "\n",
    "    # Case: filename starts with \"Annualized_\" and ends with \"_complete_subset.txt\"\n",
    "    # Example: Annualized_ABC_complete_subset.txt -> ABC.txt\n",
    "    if name.startswith(\"Annualized_\") and name.endswith(\"_complete_subset.txt\"):\n",
    "        core = name[len(\"Annualized_\"):-len(\"_complete_subset.txt\")]\n",
    "\n",
    "    # Case: filename starts with \"Mixed_\" and ends with \"_complete_subset.txt\"\n",
    "    # Example: Mixed_ABC_complete_subset.txt -> ABC.txt\n",
    "    elif name.startswith(\"Mixed_\") and name.endswith(\"_complete_subset.txt\"):\n",
    "        core = name[len(\"Mixed_\"):-len(\"_complete_subset.txt\")]\n",
    "\n",
    "    # Case: filename starts with \"Special_\" and ends with \"_complete_subset.txt\"\n",
    "    # Example: Special_ABC_complete_subset.txt -> ABC.txt\n",
    "    elif name.startswith(\"Special_\") and name.endswith(\"_complete_subset.txt\"):\n",
    "        core = name[len(\"Special_\"):-len(\"_complete_subset.txt\")]\n",
    "\n",
    "    # Skip files that do not match the required pattern\n",
    "    if not core:\n",
    "        continue\n",
    "\n",
    "    # Construct the new filename: <core>.txt\n",
    "    new_name = f\"{core}.txt\"\n",
    "    dest = OUTPUT_DIR / new_name  # Full destination path\n",
    "\n",
    "    # If the destination file already exists, skip copying\n",
    "    if dest.exists():\n",
    "        print(f\"Skipping {name}: target {new_name} already exists.\")\n",
    "        continue\n",
    "\n",
    "    # Attempt to copy the file, preserving metadata\n",
    "    try:\n",
    "        shutil.copy2(file, dest)\n",
    "        new_files.append(new_name)  # Track successful copies\n",
    "    except OSError as e:\n",
    "        print(f\"[ERROR] Failed to copy {file} -> {dest}: {e}\")\n",
    "\n",
    "# Output the list of copied and renamed files\n",
    "print(\"\\nCopied & renamed files:\")\n",
    "for f in sorted(new_files):\n",
    "    print(f)\n",
    "\n",
    "# Also show all .txt files currently in the output directory\n",
    "print(\"\\nAll .txt files currently in OUTPUT_DIR:\")\n",
    "for f in sorted(p.name for p in OUTPUT_DIR.glob(\"*.txt\")):\n",
    "    print(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beVmMnSWlujq"
   },
   "source": [
    "### Check for Empty or Error Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27957,
     "status": "ok",
     "timestamp": 1765727740468,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "zVUfM7xjC0ga",
    "outputId": "ea0a5b26-65d3-4001-d344-6f07e15de5b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] DroppedFromFFMerge.txt: no 'AnnPITValue' column found.\n",
      "[WARN] FF_Benchmark_Factors_Merged_Clean.txt: no 'AnnPITValue' column found.\n",
      "[WARN] FF_Benchmark_Factors_Merged_Clean_Replication.txt: no 'AnnPITValue' column found.\n",
      "[WARN] Factors_Annual_Country.txt: no 'AnnPITValue' column found.\n",
      "[WARN] Factors_Annual_Global.txt: no 'AnnPITValue' column found.\n",
      "[WARN] Factors_Annual_Replication.txt: no 'AnnPITValue' column found.\n",
      "[WARN] Factors_Daily_Country.txt: no 'AnnPITValue' column found.\n",
      "[WARN] Factors_Daily_Global.txt: no 'AnnPITValue' column found.\n",
      "[WARN] Factors_Daily_Replication.txt: no 'AnnPITValue' column found.\n",
      "[WARN] data_Factors.txt: no 'AnnPITValue' column found.\n",
      "[WARN] processed_data_Factors.txt: no 'AnnPITValue' column found.\n",
      "\n",
      "Rows dropped due to empty 'AnnPITValue':\n",
      "Accounts_Payable.txt: 0 rows dropped\n",
      "Capital_Expenditures_Addtns_to_Fixed_Assets.txt: 589227 rows dropped\n",
      "Cash_Dividends_Paid___Total.txt: 502590 rows dropped\n",
      "Cash__Short_Term_Investments.txt: 0 rows dropped\n",
      "Com_Pfd_Redeemed_Retired_Converted_Etc..txt: 431474 rows dropped\n",
      "Common_Equity.txt: 0 rows dropped\n",
      "Cost_of_Goods_Sold_Excl_Depreciation.txt: 1652624 rows dropped\n",
      "Current_Assets___Total.txt: 0 rows dropped\n",
      "Current_Liabilities___Total.txt: 0 rows dropped\n",
      "Deferred_Taxes.txt: 0 rows dropped\n",
      "Depreciation_Depletion__Amortization.txt: 1315017 rows dropped\n",
      "Disposal_of_Fixed_Assets.txt: 425256 rows dropped\n",
      "Earnings_Per_Share_Fiscal_Year_End.txt: 2400612 rows dropped\n",
      "Extraordinary_Items.txt: 446691 rows dropped\n",
      "Funds_From_For_Other_Operating_Activities.txt: 473320 rows dropped\n",
      "Funds_From_Operations.txt: 654222 rows dropped\n",
      "Income_Taxes.txt: 1497509 rows dropped\n",
      "Income_Taxes_Payable.txt: 0 rows dropped\n",
      "Interest_Expense___Total.txt: 3939 rows dropped\n",
      "Inventories___Total.txt: 0 rows dropped\n",
      "Investments_in_Associated_Companies.txt: 0 rows dropped\n",
      "Investments_in_Sales__Direct_Financing_Leases.txt: 0 rows dropped\n",
      "Long_Term_Borrowings.txt: 362688 rows dropped\n",
      "Long_Term_Debt.txt: 0 rows dropped\n",
      "Long_Term_Receivables.txt: 0 rows dropped\n",
      "Minority_Interest.txt: 0 rows dropped\n",
      "Net_Cash_Flow___Financing.txt: 620089 rows dropped\n",
      "Net_Cash_Flow___Investing.txt: 626850 rows dropped\n",
      "Net_Cash_Flow___Operating_Activities.txt: 639985 rows dropped\n",
      "Net_Income_Before_Extra_Items_Preferred_Divs.txt: 1597774 rows dropped\n",
      "Net_Income_Used_to_Calculate_Basic_EPS.txt: 1562077 rows dropped\n",
      "Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt: 449069 rows dropped\n",
      "Net_Sales_or_Revenues.txt: 1634230 rows dropped\n",
      "Operating_Income.txt: 1662736 rows dropped\n",
      "Other_Assets___Total.txt: 0 rows dropped\n",
      "Other_Current_Assets.txt: 0 rows dropped\n",
      "Other_Current_Liabilities.txt: 0 rows dropped\n",
      "Other_Investments.txt: 0 rows dropped\n",
      "Other_Liabilities.txt: 0 rows dropped\n",
      "Preferred_Stock.txt: 0 rows dropped\n",
      "Property_Plant__Equipment___Net.txt: 0 rows dropped\n",
      "ReceivablesNet.txt: 0 rows dropped\n",
      "Reduction_in_Long_Term_Debt.txt: 397249 rows dropped\n",
      "Sales_Per_Share.txt: 0 rows dropped\n",
      "Selling_General__Administrative_Expenses.txt: 1141322 rows dropped\n",
      "Short_Term_Debt__Current_Portion_of_LT_Debt.txt: 0 rows dropped\n",
      "Total_Assets.txt: 0 rows dropped\n",
      "Total_Liabilities.txt: 0 rows dropped\n",
      "Unspecified_Other_Loans.txt: 0 rows dropped\n",
      "\n",
      "Non-numeric checks for 'AnnPITValue' column:\n",
      "Accounts_Payable.txt: OK (all numeric)\n",
      "Capital_Expenditures_Addtns_to_Fixed_Assets.txt: OK (all numeric)\n",
      "Cash_Dividends_Paid___Total.txt: OK (all numeric)\n",
      "Cash__Short_Term_Investments.txt: OK (all numeric)\n",
      "Com_Pfd_Redeemed_Retired_Converted_Etc..txt: OK (all numeric)\n",
      "Common_Equity.txt: OK (all numeric)\n",
      "Cost_of_Goods_Sold_Excl_Depreciation.txt: OK (all numeric)\n",
      "Current_Assets___Total.txt: OK (all numeric)\n",
      "Current_Liabilities___Total.txt: OK (all numeric)\n",
      "Deferred_Taxes.txt: OK (all numeric)\n",
      "Depreciation_Depletion__Amortization.txt: OK (all numeric)\n",
      "Disposal_of_Fixed_Assets.txt: OK (all numeric)\n",
      "Earnings_Per_Share_Fiscal_Year_End.txt: OK (all numeric)\n",
      "Extraordinary_Items.txt: OK (all numeric)\n",
      "Funds_From_For_Other_Operating_Activities.txt: OK (all numeric)\n",
      "Funds_From_Operations.txt: OK (all numeric)\n",
      "Income_Taxes.txt: OK (all numeric)\n",
      "Income_Taxes_Payable.txt: OK (all numeric)\n",
      "Interest_Expense___Total.txt: OK (all numeric)\n",
      "Inventories___Total.txt: OK (all numeric)\n",
      "Investments_in_Associated_Companies.txt: OK (all numeric)\n",
      "Investments_in_Sales__Direct_Financing_Leases.txt: OK (all numeric)\n",
      "Long_Term_Borrowings.txt: OK (all numeric)\n",
      "Long_Term_Debt.txt: OK (all numeric)\n",
      "Long_Term_Receivables.txt: OK (all numeric)\n",
      "Minority_Interest.txt: OK (all numeric)\n",
      "Net_Cash_Flow___Financing.txt: OK (all numeric)\n",
      "Net_Cash_Flow___Investing.txt: OK (all numeric)\n",
      "Net_Cash_Flow___Operating_Activities.txt: OK (all numeric)\n",
      "Net_Income_Before_Extra_Items_Preferred_Divs.txt: OK (all numeric)\n",
      "Net_Income_Used_to_Calculate_Basic_EPS.txt: OK (all numeric)\n",
      "Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt: OK (all numeric)\n",
      "Net_Sales_or_Revenues.txt: OK (all numeric)\n",
      "Operating_Income.txt: OK (all numeric)\n",
      "Other_Assets___Total.txt: OK (all numeric)\n",
      "Other_Current_Assets.txt: OK (all numeric)\n",
      "Other_Current_Liabilities.txt: OK (all numeric)\n",
      "Other_Investments.txt: OK (all numeric)\n",
      "Other_Liabilities.txt: OK (all numeric)\n",
      "Preferred_Stock.txt: OK (all numeric)\n",
      "Property_Plant__Equipment___Net.txt: OK (all numeric)\n",
      "ReceivablesNet.txt: OK (all numeric)\n",
      "Reduction_in_Long_Term_Debt.txt: OK (all numeric)\n",
      "Sales_Per_Share.txt: OK (all numeric)\n",
      "Selling_General__Administrative_Expenses.txt: OK (all numeric)\n",
      "Short_Term_Debt__Current_Portion_of_LT_Debt.txt: OK (all numeric)\n",
      "Total_Assets.txt: OK (all numeric)\n",
      "Total_Liabilities.txt: OK (all numeric)\n",
      "Unspecified_Other_Loans.txt: OK (all numeric)\n",
      "\n",
      "Suspiciously small non-zero 'AnnPITValue' values (per-file threshold):\n",
      "Accounts_Payable.txt: 8474 rows dropped out of 8474 suspiciously small values. Examples: {'0.001': 269, '0.002': 209, '0.004': 167, '0.006': 150, '0.009': 144, '0.005': 129, '0.007': 125, '0.003': 119, '0.008': 98, '0.0005': 34}\n",
      "Capital_Expenditures_Addtns_to_Fixed_Assets.txt: 14531 rows dropped out of 14531 suspiciously small values. Examples: {'0.002': 487, '0.004': 421, '0.003': 410, '0.005': 408, '0.001': 393, '0.006': 374, '0.008': 347, '0.007': 316, '0.009': 303, '1e-11': 22}\n",
      "Cash_Dividends_Paid___Total.txt: 3785 rows dropped out of 3785 suspiciously small values. Examples: {'0.001': 367, '0.002': 224, '0.005': 197, '0.003': 197, '0.004': 162, '0.009': 135, '0.007': 133, '0.006': 112, '0.008': 108, '2e-06': 83}\n",
      "Cash__Short_Term_Investments.txt: 19916 rows dropped out of 19916 suspiciously small values. Examples: {'0.001': 715, '0.002': 561, '0.003': 414, '0.005': 408, '0.004': 399, '0.006': 292, '0.007': 279, '0.008': 251, '0.009': 220, '1e-06': 86}\n",
      "Com_Pfd_Redeemed_Retired_Converted_Etc..txt: 4590 rows dropped out of 4590 suspiciously small values. Examples: {'0.001': 517, '0.002': 394, '0.003': 332, '0.007': 294, '0.004': 276, '0.006': 261, '0.005': 246, '0.008': 193, '0.009': 179, '2e-06': 158}\n",
      "Common_Equity.txt: 2777 rows dropped out of 2777 suspiciously small values. Examples: {'-0.006': 21, '-0.007': 18, '0.006': 18, '0.001': 17, '0.007': 17, '0.006638': 15, '0.0001': 14, '-0.005': 13, '1e-06': 13, '0.005': 11}\n",
      "Cost_of_Goods_Sold_Excl_Depreciation.txt: 5355 rows dropped out of 5355 suspiciously small values. Examples: {'0.001': 160, '0.005': 101, '0.006': 101, '0.003': 96, '0.002': 89, '0.007': 81, '0.004': 75, '0.009': 54, '0.008': 49, '0.002011': 33}\n",
      "Current_Assets___Total.txt: 8271 rows dropped out of 8271 suspiciously small values. Examples: {'0.001': 99, '0.002': 69, '0.005': 62, '0.003': 61, '1e-06': 61, '0.008': 46, '0.007': 44, '0.006': 36, '0.004': 34, '0.0001': 34}\n",
      "Current_Liabilities___Total.txt: 3454 rows dropped out of 3454 suspiciously small values. Examples: {'0.001': 74, '0.007': 69, '0.002': 64, '0.005': 51, '0.009': 49, '0.003': 45, '0.008': 31, '0.004': 31, '0.006': 29, '0.0005': 26}\n",
      "Deferred_Taxes.txt: 9251 rows dropped out of 9251 suspiciously small values. Examples: {'0.001': 438, '0.003': 359, '0.002': 323, '0.008': 318, '0.007': 305, '0.004': 294, '0.009': 279, '-0.001': 264, '-0.002': 249, '0.006': 242}\n",
      "Depreciation_Depletion__Amortization.txt: 29016 rows dropped out of 29016 suspiciously small values. Examples: {'0.001': 695, '0.002': 529, '0.005': 495, '0.004': 483, '0.003': 405, '0.009': 398, '0.006': 382, '0.007': 369, '0.008': 296, '0.004059': 43}\n",
      "Disposal_of_Fixed_Assets.txt: 31510 rows dropped out of 31510 suspiciously small values. Examples: {'0.001': 2276, '0.002': 2038, '0.003': 1830, '0.004': 1737, '0.005': 1526, '0.006': 1353, '0.007': 1250, '0.008': 1237, '0.009': 1021, '0.0005': 186}\n",
      "Earnings_Per_Share_Fiscal_Year_End.txt: 149738 suspiciously small values found, but 0 rows dropped (filter disabled for this file). Examples: {'-1e-05': 627, '0.005': 603, '1e-05': 552, '0.001': 542, '0.004': 510, '0.002': 475, '0.008': 474, '-0.005': 469, '-0.006': 388, '-0.004': 379}\n",
      "Extraordinary_Items.txt: 366 rows dropped out of 366 suspiciously small values. Examples: {'2e-06': 182, '1e-06': 34, '-1e-06': 29, '4e-06': 22, '6e-06': 14, '-3e-06': 7, '8e-06': 5, '3e-06': 4, '-0.002': 3, '-0.006': 3}\n",
      "Funds_From_For_Other_Operating_Activities.txt: 6663 rows dropped out of 6663 suspiciously small values. Examples: {'0.002': 115, '0.003': 114, '0.005': 112, '0.007': 92, '0.006': 90, '-0.005': 90, '0.009': 86, '-0.002': 85, '-0.001': 85, '0.004': 82}\n",
      "Funds_From_Operations.txt: 2487 rows dropped out of 2487 suspiciously small values. Examples: {'-0.009': 35, '0.004': 32, '-0.007': 32, '-0.005': 32, '-0.001': 32, '-0.002': 29, '-0.006': 27, '-0.008': 25, '0.002': 23, '-0.004': 22}\n",
      "Income_Taxes.txt: 20788 rows dropped out of 20788 suspiciously small values. Examples: {'0.001': 1185, '0.002': 1088, '0.003': 772, '0.004': 673, '0.0008': 623, '0.005': 617, '0.006': 558, '0.007': 469, '0.008': 446, '0.009': 425}\n",
      "Income_Taxes_Payable.txt: 14949 rows dropped out of 14949 suspiciously small values. Examples: {'0.001': 1074, '0.002': 862, '0.005': 700, '0.003': 692, '0.004': 659, '0.006': 653, '0.008': 563, '0.007': 562, '0.009': 496, '0.0008': 78}\n",
      "Interest_Expense___Total.txt: 1147 rows dropped out of 1147 suspiciously small values. Examples: {'0.001': 155, '0.002': 148, '0.003': 133, '0.004': 108, '0.005': 108, '0.006': 88, '0.008': 77, '0.009': 74, '0.007': 69, '0.000557': 9}\n",
      "Inventories___Total.txt: 10168 rows dropped out of 10168 suspiciously small values. Examples: {'0.002': 461, '0.001': 435, '0.005': 424, '0.004': 384, '0.003': 331, '0.007': 283, '0.008': 282, '0.006': 275, '0.009': 267, '1e-06': 56}\n",
      "Investments_in_Associated_Companies.txt: 14681 rows dropped out of 14681 suspiciously small values. Examples: {'0.001': 1542, '0.005': 1068, '1e-06': 1044, '0.002': 823, '0.003': 746, '0.008': 511, '0.006': 499, '0.004': 482, '0.007': 455, '2e-06': 388}\n",
      "Investments_in_Sales__Direct_Financing_Leases.txt: OK (no suspiciously small values)\n",
      "Long_Term_Borrowings.txt: 1778 rows dropped out of 1778 suspiciously small values. Examples: {'0.001': 138, '0.005': 101, '0.003': 91, '0.002': 78, '0.007': 67, '0.008': 65, '0.006': 63, '0.004': 50, '0.009': 43, '2e-06': 43}\n",
      "Long_Term_Debt.txt: 13237 rows dropped out of 13237 suspiciously small values. Examples: {'0.001': 771, '0.002': 748, '0.005': 655, '0.003': 619, '0.004': 609, '0.006': 587, '0.009': 557, '0.007': 520, '0.008': 503, '4e-06': 59}\n",
      "Long_Term_Receivables.txt: 5598 rows dropped out of 5598 suspiciously small values. Examples: {'0.001': 197, '0.005': 181, '0.003': 172, '0.002': 164, '0.008': 142, '0.006': 140, '0.004': 137, '0.009': 112, '0.007': 112, '0.0015': 50}\n",
      "Minority_Interest.txt: 26227 rows dropped out of 26227 suspiciously small values. Examples: {'0.001': 2224, '0.002': 1398, '0.003': 1071, '0.004': 901, '0.005': 842, '0.006': 707, '0.008': 678, '0.009': 651, '0.007': 612, '-0.001': 471}\n",
      "Net_Cash_Flow___Financing.txt: 5429 rows dropped out of 5429 suspiciously small values. Examples: {'-0.002': 151, '0.001': 128, '0.002': 126, '-0.001': 123, '0.006': 115, '-0.003': 114, '-0.004': 103, '-0.005': 102, '0.004': 99, '-0.009': 99}\n",
      "Net_Cash_Flow___Investing.txt: 9058 rows dropped out of 9058 suspiciously small values. Examples: {'0.002': 218, '0.003': 209, '0.005': 187, '0.001': 185, '0.004': 167, '0.006': 154, '0.007': 149, '0.008': 137, '0.009': 121, '-0.001': 91}\n",
      "Net_Cash_Flow___Operating_Activities.txt: 2963 rows dropped out of 2963 suspiciously small values. Examples: {'-0.007': 59, '-0.001': 55, '-0.008': 46, '-0.006': 42, '-0.009': 39, '-0.002': 36, '-0.004': 36, '0.001': 33, '-0.005': 32, '0.002': 29}\n",
      "Net_Income_Before_Extra_Items_Preferred_Divs.txt: 13530 suspiciously small values rounded to 0 (no rows dropped). Examples: {'0.024': 82, '0.005': 73, '0.004': 66, '0.02': 63, '0.012': 60, '0.01': 59, '0.013': 58, '0.048': 58, '0.015': 58, '0.049': 56}\n",
      "Net_Income_Used_to_Calculate_Basic_EPS.txt: 12997 suspiciously small values rounded to 0 (no rows dropped). Examples: {'0.024': 80, '0.005': 69, '0.004': 65, '0.02': 63, '0.015': 59, '0.012': 58, '-0.012': 57, '0.013': 57, '0.026': 56, '0.048': 56}\n",
      "Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt: 10647 rows dropped out of 10647 suspiciously small values. Examples: {'0.001': 1014, '0.002': 771, '0.004': 675, '0.003': 664, '0.005': 642, '0.006': 444, '0.008': 437, '0.007': 426, '0.009': 419, '4e-06': 119}\n",
      "Net_Sales_or_Revenues.txt: 67737 rows dropped out of 67737 suspiciously small values. Examples: {'0.003': 144, '0.002': 131, '0.1': 126, '0.025': 119, '0.001': 119, '0.05': 106, '0.018': 105, '0.25': 103, '0.5': 103, '0.011': 96}\n",
      "Operating_Income.txt: 3214 rows dropped out of 3214 suspiciously small values. Examples: {'-0.005': 53, '-0.006': 50, '-0.004': 40, '0.001': 38, '-0.007': 33, '-0.001': 32, '-0.002': 32, '0.002': 29, '-0.008': 29, '0.005': 28}\n",
      "Other_Assets___Total.txt: 31899 rows dropped out of 31899 suspiciously small values. Examples: {'0.001': 2704, '1e-06': 1677, '0.005': 1148, '0.002': 975, '0.003': 960, '0.004': 936, '-0.001': 856, '-1e-06': 777, '0.008': 754, '0.007': 638}\n",
      "Other_Current_Assets.txt: 85835 rows dropped out of 85835 suspiciously small values. Examples: {'0.001': 10661, '-0.001': 7221, '1e-06': 3881, '-1e-06': 3718, '1e-11': 2743, '-1e-11': 2639, '0.002': 1886, '0.003': 1336, '0.005': 1274, '0.004': 1003}\n",
      "Other_Current_Liabilities.txt: 11488 rows dropped out of 11488 suspiciously small values. Examples: {'0.001': 512, '0.002': 344, '0.004': 284, '-0.001': 283, '-1e-06': 272, '0.003': 261, '0.005': 259, '1e-06': 253, '0.009': 218, '0.008': 185}\n",
      "Other_Investments.txt: 13822 rows dropped out of 13822 suspiciously small values. Examples: {'0.001': 1408, '0.002': 1186, '0.005': 997, '0.003': 706, '0.004': 649, '0.006': 524, '0.008': 482, '1e-06': 473, '0.009': 344, '0.007': 343}\n",
      "Other_Liabilities.txt: 70633 rows dropped out of 70633 suspiciously small values. Examples: {'0.001': 21730, '-0.001': 6580, '0.002': 5617, '1e-06': 3518, '-1e-06': 2867, '1e-11': 2175, '-1e-11': 2116, '0.003': 2071, '0.004': 880, '0.008': 822}\n",
      "Preferred_Stock.txt: 19386 rows dropped out of 19386 suspiciously small values. Examples: {'0.001': 1958, '1e-06': 1289, '0.002': 931, '2e-06': 756, '0.005': 655, '0.004': 632, '0.003': 623, '0.0001': 307, '1e-05': 285, '4e-06': 238}\n",
      "Property_Plant__Equipment___Net.txt: 18223 rows dropped out of 18223 suspiciously small values. Examples: {'0.001': 630, '0.002': 425, '0.003': 356, '1e-06': 356, '0.005': 346, '0.004': 325, '0.007': 258, '0.006': 250, '0.008': 247, '0.009': 191}\n",
      "ReceivablesNet.txt: 19817 rows dropped out of 19817 suspiciously small values. Examples: {'0.001': 500, '0.002': 442, '0.005': 395, '0.004': 385, '0.003': 367, '0.006': 288, '0.007': 276, '0.008': 262, '0.009': 248, '1e-06': 54}\n",
      "Reduction_in_Long_Term_Debt.txt: 5590 rows dropped out of 5590 suspiciously small values. Examples: {'0.005': 325, '0.004': 274, '0.009': 269, '0.008': 244, '0.006': 241, '0.002': 241, '0.007': 227, '0.001': 225, '0.003': 204, '2e-06': 35}\n",
      "Sales_Per_Share.txt: 29153 suspiciously small values found, but 0 rows dropped (filter disabled for this file). Examples: {'1e-05': 508, '2e-05': 386, '3e-05': 276, '4e-05': 260, '6e-05': 222, '5e-05': 209, '9e-05': 190, '8e-05': 189, '0.00012': 183, '7e-05': 181}\n",
      "Selling_General__Administrative_Expenses.txt: 1913 rows dropped out of 1913 suspiciously small values. Examples: {'0.001': 52, '-0.008': 29, '0.004375': 25, '0.006': 25, '0.002': 23, '0.003': 23, '0.005': 21, '0.004': 19, '0.007': 17, '0.008': 12}\n",
      "Short_Term_Debt__Current_Portion_of_LT_Debt.txt: 17584 rows dropped out of 17584 suspiciously small values. Examples: {'0.005': 863, '0.002': 832, '0.001': 820, '0.004': 806, '0.003': 728, '0.007': 723, '0.006': 685, '0.008': 642, '0.009': 601, '2e-06': 56}\n",
      "Total_Assets.txt: 44293 rows dropped out of 44293 suspiciously small values. Examples: {'1e-06': 51, '0.01': 39, '0.007': 37, '0.002': 35, '0.003': 32, '0.005': 30, '0.0001': 29, '2e-06': 27, '0.001': 27, '0.313': 25}\n",
      "Total_Liabilities.txt: 105834 rows dropped out of 105834 suspiciously small values. Examples: {'0.01': 90, '0.001': 76, '0.017': 71, '0.007': 65, '0.002': 63, '0.015': 62, '0.02': 60, '0.064': 57, '0.011': 54, '0.025': 53}\n",
      "Unspecified_Other_Loans.txt: OK (no suspiciously small values)\n",
      "\n",
      "Q_lag0..Q_lag7 checks (only for files in FILES_WITH_Q_LAG_FILTER):\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# =======================================================================\n",
    "# Summary:\n",
    "# This cell processes all *.txt files in OUTPUT_DIR (no chunking).\n",
    "#\n",
    "# AnnPITValue filtering:\n",
    "#   - Rows with empty AnnPITValue are REMOVED\n",
    "#   - Non-numeric AnnPITValue values are TRACKED (but not removed)\n",
    "#   - Suspiciously small non-zero numeric values are DETECTED:\n",
    "#       * Default:            0 < |value| < 0.01\n",
    "#       * Strict files:       0 < |value| < 1.0   (all 0.x values)\n",
    "#       * Round-to-zero files 0 < |value| < 0.05  -> rounded to 0 (not removed)\n",
    "#     -> Normally, these rows are REMOVED\n",
    "#     -> EXCEPTION: for files in FILES_WITHOUT_SMALL_FILTER, they are\n",
    "#        only REPORTED but NOT REMOVED.\n",
    "#\n",
    "# Optional Q_lag0..Q_lag7 filtering (only for selected files):\n",
    "#   - Empty values are explicitly ALLOWED\n",
    "#   - Non-numeric values are TRACKED (but not removed)\n",
    "#   - Suspiciously small non-zero numeric values are DETECTED:\n",
    "#       0 < |value| < 0.01\n",
    "#     -> Normally, these rows are REMOVED\n",
    "#     -> EXCEPTION: for files in FILES_WITHOUT_SMALL_FILTER, they are\n",
    "#        only REPORTED but NOT REMOVED.\n",
    "#\n",
    "# All files are rewritten in-place using a temporary file.\n",
    "# Detailed statistics and example values are printed at the end.\n",
    "# =======================================================================\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Directory containing the files to validate\n",
    "SEP = \"|\"                             # Input files are pipe-separated\n",
    "\n",
    "# ONLY these files will receive the Q_lag0..Q_lag7 filtering\n",
    "FILES_WITH_Q_LAG_FILTER = { #### Just for the case that I add quarters back\n",
    "\n",
    "}\n",
    "\n",
    "# Files where the \"near-zero\" filter (0 < |value| < 0.01) should NOT\n",
    "# actually remove rows. For these files, we still DETECT and REPORT\n",
    "# suspiciously small values, but we DO NOT DROP them.\n",
    "FILES_WITHOUT_SMALL_FILTER = {\n",
    "    \"Earnings_Per_Share_Fiscal_Year_End.txt\",\n",
    "    \"Sales_Per_Share.txt\",\n",
    "}\n",
    "\n",
    "# Files where we want a STRICT small-value filter for AnnPITValue:\n",
    "# here, ANY non-zero 0.x value (0 < |value| < 1.0) is considered too small.\n",
    "FILES_WITH_STRICT_SMALL_FILTER = {\n",
    "    \"Total_Assets.txt\",\n",
    "    \"Total_Liabilities.txt\",\n",
    "    \"Net_Sales_or_Revenues.txt\",\n",
    "}\n",
    "\n",
    "# Files where small AnnPITValue values should be rounded to 0 instead of\n",
    "# being dropped. Threshold: 0 < |value| < 0.05.\n",
    "FILES_WITH_ROUND_SMALL_TO_ZERO = {\n",
    "    \"Net_Income_Before_Extra_Items_Preferred_Divs.txt\",\n",
    "    \"Net_Income_Used_to_Calculate_Basic_EPS.txt\",\n",
    "}\n",
    "\n",
    "# Define the Q_lag columns explicitly\n",
    "Q_LAG_COLUMNS = [f\"Q_lag{i}\" for i in range(8)]\n",
    "# =====================\n",
    "\n",
    "# ---- GLOBAL TRACKING DICTIONARIES ----\n",
    "\n",
    "rows_dropped_annpit = {}          # Rows dropped because AnnPITValue was empty\n",
    "non_numeric_in_value = {}         # Non-numeric AnnPITValue tracking\n",
    "small_magnitude_in_value = {}     # Small-magnitude AnnPITValue tracking\n",
    "\n",
    "# Q_lag statistics only for selected files\n",
    "non_numeric_in_qleg = {}          # Non-numeric Q_lag tracking\n",
    "small_magnitude_in_qleg = {}      # Small-magnitude Q_lag tracking\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "#                               MAIN LOOP\n",
    "# =======================================================================\n",
    "\n",
    "for path in sorted(OUTPUT_DIR.glob(\"*.txt\")):\n",
    "    fname = path.name\n",
    "    tmp_path = path.with_suffix(\".tmp\")\n",
    "\n",
    "    # Remove any leftover temp file from previous runs\n",
    "    if tmp_path.exists():\n",
    "        tmp_path.unlink()\n",
    "\n",
    "    # --- Load entire file (NO CHUNKING) ---\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=SEP, dtype=str)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipping {fname}: cannot read ({e})\")\n",
    "        continue\n",
    "\n",
    "    # --- Ensure AnnPITValue exists ---\n",
    "    if \"AnnPITValue\" not in df.columns:\n",
    "        print(f\"[WARN] {fname}: no 'AnnPITValue' column found.\")\n",
    "        continue\n",
    "\n",
    "    # ===================================================================\n",
    "    #                     AnnPITValue: EMPTY REMOVAL\n",
    "    # ===================================================================\n",
    "\n",
    "    # Use the original column for NaN detection, and a string version for whitespace checks\n",
    "    col_ann = df[\"AnnPITValue\"]\n",
    "    s_ann = col_ann.astype(str)\n",
    "\n",
    "    # Drop rows where AnnPITValue is NaN or only whitespace / empty\n",
    "    mask_non_empty = (~col_ann.isna()) & (s_ann.str.strip() != \"\")\n",
    "    total_dropped_empty = int(len(df) - mask_non_empty.sum())\n",
    "\n",
    "    # Keep only rows with non-empty AnnPITValue\n",
    "    df = df[mask_non_empty]\n",
    "\n",
    "    # ===================================================================\n",
    "    #               AnnPITValue: NUMERIC VALIDATION\n",
    "    # ===================================================================\n",
    "\n",
    "    s = df[\"AnnPITValue\"].fillna(\"\").astype(str).str.strip()\n",
    "    coerced = pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    # --- Track NON-NUMERIC values (DO NOT remove) ---\n",
    "    mask_nn = (s != \"\") & coerced.isna()\n",
    "    nn_count = int(mask_nn.sum())\n",
    "    nn_examples = {}\n",
    "\n",
    "    if nn_count > 0:\n",
    "        vc = s[mask_nn].value_counts()\n",
    "        for val, cnt in vc.items():\n",
    "            if len(nn_examples) < 10 or val in nn_examples:\n",
    "                nn_examples[val] = nn_examples.get(val, 0) + int(cnt)\n",
    "\n",
    "    # --- Filter / detect SUSPICIOUSLY SMALL NON-ZERO numeric values ---\n",
    "    # Default threshold:   0.01\n",
    "    # Strict files:        1.0  (so all 0.x values are treated as too small)\n",
    "    # Round-to-zero files: 0.05 (values below are set to 0, not dropped)\n",
    "    if fname in FILES_WITH_STRICT_SMALL_FILTER:\n",
    "        small_threshold = 1.0\n",
    "    elif fname in FILES_WITH_ROUND_SMALL_TO_ZERO:\n",
    "        small_threshold = 0.05\n",
    "    else:\n",
    "        small_threshold = 0.01\n",
    "\n",
    "    mask_numeric = ~coerced.isna()\n",
    "    mask_small = mask_numeric & (coerced != 0) & (coerced.abs() < small_threshold)\n",
    "\n",
    "    small_match_count = int(mask_small.sum())   # how many values match the criterion\n",
    "    small_dropped = 0                           # how many rows are actually removed\n",
    "    small_examples = {}\n",
    "\n",
    "    if small_match_count > 0:\n",
    "        vc_small = s[mask_small].value_counts()\n",
    "        for val, cnt in vc_small.items():\n",
    "            if len(small_examples) < 10 or val in small_examples:\n",
    "                small_examples[val] = small_examples.get(val, 0) + int(cnt)\n",
    "\n",
    "        if fname in FILES_WITH_ROUND_SMALL_TO_ZERO:\n",
    "            # Round small AnnPITValue entries to 0 instead of dropping rows\n",
    "            df.loc[mask_small, \"AnnPITValue\"] = \"0\"\n",
    "            small_dropped = 0\n",
    "        elif fname not in FILES_WITHOUT_SMALL_FILTER:\n",
    "            # Default behaviour: drop rows with small values\n",
    "            df = df[~mask_small]\n",
    "            small_dropped = small_match_count\n",
    "        # If the file is in FILES_WITHOUT_SMALL_FILTER, we leave df unchanged\n",
    "        # but still report the suspicious values.\n",
    "\n",
    "    # ===================================================================\n",
    "    #           OPTIONAL: Q_lag0..Q_lag7 FILTERING (SELECTED FILES)\n",
    "    # ===================================================================\n",
    "\n",
    "    qleg_non_numeric_stats = {}\n",
    "    qleg_small_stats = {}\n",
    "\n",
    "    if fname in FILES_WITH_Q_LAG_FILTER:\n",
    "        for col in Q_LAG_COLUMNS:\n",
    "\n",
    "            # Skip if column does not exist\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "\n",
    "            # Convert to clean string\n",
    "            s_q = df[col].fillna(\"\").astype(str).str.strip()\n",
    "            coerced_q = pd.to_numeric(s_q, errors=\"coerce\")\n",
    "\n",
    "            # --- Track NON-NUMERIC values (but DO NOT drop) ---\n",
    "            mask_nn_q = (s_q != \"\") & coerced_q.isna()\n",
    "            c_nn_q = int(mask_nn_q.sum())\n",
    "            nn_examples_q = {}\n",
    "\n",
    "            if c_nn_q > 0:\n",
    "                vc_nn_q = s_q[mask_nn_q].value_counts()\n",
    "                for val, cnt in vc_nn_q.items():\n",
    "                    if len(nn_examples_q) < 10 or val in nn_examples_q:\n",
    "                        nn_examples_q[val] = nn_examples_q.get(val, 0) + int(cnt)\n",
    "\n",
    "            # --- Filter / detect SUSPICIOUSLY SMALL NON-ZERO numeric values ---\n",
    "            mask_numeric_q = ~coerced_q.isna()\n",
    "            mask_small_q = (\n",
    "                mask_numeric_q\n",
    "                & (coerced_q != 0)\n",
    "                & (coerced_q.abs() < 0.01)  # Q_lag still uses the standard 0.01 threshold\n",
    "            )\n",
    "\n",
    "            c_small_match_q = int(mask_small_q.sum())   # matches\n",
    "            c_small_dropped_q = 0                       # actually dropped rows\n",
    "            small_examples_q = {}\n",
    "\n",
    "            if c_small_match_q > 0:\n",
    "                vc_small_q = s_q[mask_small_q].value_counts()\n",
    "                for val, cnt in vc_small_q.items():\n",
    "                    if len(small_examples_q) < 10 or val in small_examples_q:\n",
    "                        small_examples_q[val] = small_examples_q.get(val, 0) + int(cnt)\n",
    "\n",
    "                # Only DROP rows if this file is NOT in the exclusion list\n",
    "                if fname not in FILES_WITHOUT_SMALL_FILTER:\n",
    "                    df = df[~mask_small_q]\n",
    "                    c_small_dropped_q = c_small_match_q\n",
    "                # If the file is in FILES_WITHOUT_SMALL_FILTER, we only report.\n",
    "\n",
    "            # --- Store per-column tracking ---\n",
    "            qleg_non_numeric_stats[col] = {\n",
    "                \"count_non_numeric\": c_nn_q,\n",
    "                \"example_values\": dict(\n",
    "                    sorted(nn_examples_q.items(), key=lambda x: -x[1])[:10]\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            qleg_small_stats[col] = {\n",
    "                \"count_small_magnitude\": c_small_match_q,\n",
    "                \"count_dropped\": c_small_dropped_q,\n",
    "                \"example_values\": dict(\n",
    "                    sorted(small_examples_q.items(), key=lambda x: -x[1])[:10]\n",
    "                ),\n",
    "            }\n",
    "\n",
    "    # ===================================================================\n",
    "    #                     WRITE CLEANED FILE\n",
    "    # ===================================================================\n",
    "\n",
    "    df.to_csv(tmp_path, index=False, sep=SEP)\n",
    "    os.replace(tmp_path, path)\n",
    "\n",
    "    # ===================================================================\n",
    "    #                     STORE FILE STATISTICS\n",
    "    # ===================================================================\n",
    "\n",
    "    rows_dropped_annpit[fname] = total_dropped_empty\n",
    "\n",
    "    non_numeric_in_value[fname] = {\n",
    "        \"count_non_numeric\": nn_count,\n",
    "        \"example_values\": dict(\n",
    "            sorted(nn_examples.items(), key=lambda x: -x[1])[:10]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    small_magnitude_in_value[fname] = {\n",
    "        \"count_small_magnitude\": small_match_count,\n",
    "        \"count_dropped\": small_dropped,\n",
    "        \"example_values\": dict(\n",
    "            sorted(small_examples.items(), key=lambda x: -x[1])[:10]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    if qleg_non_numeric_stats:\n",
    "        non_numeric_in_qleg[fname] = qleg_non_numeric_stats\n",
    "        small_magnitude_in_qleg[fname] = qleg_small_stats\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "#                             FINAL REPORT\n",
    "# =======================================================================\n",
    "\n",
    "print(\"\\nRows dropped due to empty 'AnnPITValue':\")\n",
    "for name, dropped in sorted(rows_dropped_annpit.items()):\n",
    "    print(f\"{name}: {dropped} rows dropped\")\n",
    "\n",
    "print(\"\\nNon-numeric checks for 'AnnPITValue' column:\")\n",
    "for name, info in sorted(non_numeric_in_value.items()):\n",
    "    c = info[\"count_non_numeric\"]\n",
    "    if c == 0:\n",
    "        print(f\"{name}: OK (all numeric)\")\n",
    "    else:\n",
    "        print(f\"{name}: {c} non-numeric entries. Examples: {info['example_values']}\")\n",
    "\n",
    "print(\"\\nSuspiciously small non-zero 'AnnPITValue' values (per-file threshold):\")\n",
    "for name, info in sorted(small_magnitude_in_value.items()):\n",
    "    m = info[\"count_small_magnitude\"]\n",
    "    d = info[\"count_dropped\"]\n",
    "    if m == 0:\n",
    "        print(f\"{name}: OK (no suspiciously small values)\")\n",
    "    elif name in FILES_WITH_ROUND_SMALL_TO_ZERO:\n",
    "        print(\n",
    "            f\"{name}: {m} suspiciously small values rounded to 0 (no rows dropped). \"\n",
    "            f\"Examples: {info['example_values']}\"\n",
    "        )\n",
    "    elif d == 0:\n",
    "        print(\n",
    "            f\"{name}: {m} suspiciously small values found, but 0 rows dropped \"\n",
    "            f\"(filter disabled for this file). Examples: {info['example_values']}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"{name}: {d} rows dropped out of {m} suspiciously small values. \"\n",
    "            f\"Examples: {info['example_values']}\"\n",
    "        )\n",
    "\n",
    "print(\"\\nQ_lag0..Q_lag7 checks (only for files in FILES_WITH_Q_LAG_FILTER):\")\n",
    "for name in sorted(non_numeric_in_qleg.keys()):\n",
    "    print(f\"\\n{name}:\")\n",
    "    for col in Q_LAG_COLUMNS:\n",
    "        if col not in non_numeric_in_qleg[name]:\n",
    "            continue\n",
    "\n",
    "        nn_info = non_numeric_in_qleg[name][col]\n",
    "        sm_info = small_magnitude_in_qleg[name][col]\n",
    "\n",
    "        c_nn = nn_info[\"count_non_numeric\"]\n",
    "        m_sm = sm_info[\"count_small_magnitude\"]\n",
    "        d_sm = sm_info[\"count_dropped\"]\n",
    "\n",
    "        if c_nn == 0 and m_sm == 0:\n",
    "            print(f\"  {col}: OK\")\n",
    "        else:\n",
    "            print(f\"  {col}: {c_nn} non-numeric, {m_sm} suspiciously small non-zero values\")\n",
    "            if c_nn > 0:\n",
    "                print(f\"    Non-numeric examples: {nn_info['example_values']}\")\n",
    "            if m_sm > 0 and d_sm == 0:\n",
    "                print(\n",
    "                    f\"    Small-magnitude examples (no rows dropped for this file): \"\n",
    "                    f\"{sm_info['example_values']}\"\n",
    "                )\n",
    "            if d_sm > 0:\n",
    "                print(\n",
    "                    f\"    Small-magnitude examples ({d_sm} rows dropped): \"\n",
    "                    f\"{sm_info['example_values']}\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_LVUFFol4or"
   },
   "source": [
    "### Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14411,
     "status": "ok",
     "timestamp": 1765727754882,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "lo5KYnsNC3AM",
    "outputId": "0db27d5e-370f-40b9-aad3-a1d701ec70b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Capital_Expenditures_Addtns_to_Fixed_Assets.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Cash_Dividends_Paid___Total.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Com_Pfd_Redeemed_Retired_Converted_Etc..txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Cost_of_Goods_Sold_Excl_Depreciation.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Depreciation_Depletion__Amortization.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Disposal_of_Fixed_Assets.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] DroppedFromFFMerge.txt: missing columns ['CompanyName', 'AnnPITValue'] — skipped dedup.\n",
      "[WARN] Earnings_Per_Share_Fiscal_Year_End.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Extraordinary_Items.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] FF_Benchmark_Factors_Merged_Clean.txt: missing columns ['CompanyName', 'AnnPITValue'] — skipped dedup.\n",
      "[WARN] FF_Benchmark_Factors_Merged_Clean_Replication.txt: missing columns ['CompanyName', 'AnnPITValue'] — skipped dedup.\n",
      "[WARN] Factors_Annual_Country.txt: missing columns ['ID', 'CompanyName', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue', 'AnnPITValue_Period', 'PIT Date'] — skipped dedup.\n",
      "[WARN] Factors_Annual_Global.txt: missing columns ['ID', 'CompanyName', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue', 'AnnPITValue_Period', 'PIT Date'] — skipped dedup.\n",
      "[WARN] Factors_Annual_Replication.txt: missing columns ['ID', 'CompanyName', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue', 'AnnPITValue_Period', 'PIT Date'] — skipped dedup.\n",
      "[WARN] Factors_Daily_Country.txt: missing columns ['ID', 'CompanyName', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue', 'AnnPITValue_Period', 'PIT Date'] — skipped dedup.\n",
      "[WARN] Factors_Daily_Global.txt: missing columns ['ID', 'CompanyName', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue', 'AnnPITValue_Period', 'PIT Date'] — skipped dedup.\n",
      "[WARN] Factors_Daily_Replication.txt: missing columns ['ID', 'CompanyName', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue', 'AnnPITValue_Period', 'PIT Date'] — skipped dedup.\n",
      "[WARN] Funds_From_For_Other_Operating_Activities.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Funds_From_Operations.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Income_Taxes.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Interest_Expense___Total.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Long_Term_Borrowings.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Net_Cash_Flow___Financing.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Net_Cash_Flow___Investing.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Net_Cash_Flow___Operating_Activities.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Net_Income_Before_Extra_Items_Preferred_Divs.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Net_Income_Used_to_Calculate_Basic_EPS.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Net_Sales_or_Revenues.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Operating_Income.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Reduction_in_Long_Term_Debt.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Sales_Per_Share.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] Selling_General__Administrative_Expenses.txt: missing columns ['AnnPITValue_Period'] — skipped dedup.\n",
      "[WARN] data_Factors.txt: missing columns ['CompanyName', 'AnnPITValue'] — skipped dedup.\n",
      "[WARN] processed_data_Factors.txt: missing columns ['CompanyName', 'AnnPITValue'] — skipped dedup.\n",
      "\n",
      "Rows dropped due to duplicates (ID+CompanyName+HistCurrency+FiscalPeriod+AnnPITValue_Period+AnnPITValue[2dp] [+Q_lag0–Q_lag7 if present]):\n",
      "Accounts_Payable.txt: 1007103 rows dropped\n",
      "Cash__Short_Term_Investments.txt: 1183950 rows dropped\n",
      "Common_Equity.txt: 1200098 rows dropped\n",
      "Current_Assets___Total.txt: 1149875 rows dropped\n",
      "Current_Liabilities___Total.txt: 1162876 rows dropped\n",
      "Deferred_Taxes.txt: 913596 rows dropped\n",
      "Income_Taxes_Payable.txt: 598244 rows dropped\n",
      "Inventories___Total.txt: 1132213 rows dropped\n",
      "Investments_in_Associated_Companies.txt: 912701 rows dropped\n",
      "Investments_in_Sales__Direct_Financing_Leases.txt: 30 rows dropped\n",
      "Long_Term_Debt.txt: 1200482 rows dropped\n",
      "Long_Term_Receivables.txt: 109823 rows dropped\n",
      "Minority_Interest.txt: 1074731 rows dropped\n",
      "Other_Assets___Total.txt: 1206144 rows dropped\n",
      "Other_Current_Assets.txt: 1285542 rows dropped\n",
      "Other_Current_Liabilities.txt: 1358585 rows dropped\n",
      "Other_Investments.txt: 206473 rows dropped\n",
      "Other_Liabilities.txt: 1132048 rows dropped\n",
      "Preferred_Stock.txt: 1074338 rows dropped\n",
      "Property_Plant__Equipment___Net.txt: 1195234 rows dropped\n",
      "ReceivablesNet.txt: 1283899 rows dropped\n",
      "Short_Term_Debt__Current_Portion_of_LT_Debt.txt: 1210384 rows dropped\n",
      "Total_Assets.txt: 1139668 rows dropped\n",
      "Total_Liabilities.txt: 1186309 rows dropped\n",
      "Unspecified_Other_Loans.txt: 2 rows dropped\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# ============================================================================\n",
    "# Summary:\n",
    "# This cell loads each *.txt file in OUTPUT_DIR and removes duplicate rows\n",
    "# based on a key consisting of:\n",
    "#   ID, CompanyName, HistCurrency, FiscalPeriod, AnnPITValue_Period, and\n",
    "#   AnnPITValue (rounded to two decimal places), while using PIT Date to\n",
    "#   determine row ordering.\n",
    "#\n",
    "# Additionally, if the columns lag0, lag1, ..., lag7 exist in a dataset,\n",
    "# they are treated as *part of the key* as well. That means:\n",
    "#   - Rows are only considered duplicates if the base key AND all lag0–lag7\n",
    "#     values match.\n",
    "#   - If any Q_lag* value differs, the row is kept.\n",
    "#\n",
    "# If both 'Value' and 'AnnPITValue' exist, 'Value' is dropped automatically.\n",
    "#\n",
    "# NEW:\n",
    "#   - Rows with missing/empty AnnPITValue_Period are removed before dedup.\n",
    "#   - AnnPITValue_Period is included in the dedup key, so rows with the same\n",
    "#     AnnPITValue but different periods (e.g., 'A' vs 'Q4') are not treated\n",
    "#     as duplicates.\n",
    "# ============================================================================\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "KEY_COLS_BASE = [\"ID\", \"CompanyName\", \"HistCurrency\", \"FiscalPeriod\"]\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "PIT_COL = \"PIT Date\"\n",
    "LAG_COLS = [f\"Q_lag{i}\" for i in range(8)]\n",
    "# =====================\n",
    "\n",
    "dedup_dropped = {}\n",
    "\n",
    "for path in sorted(OUTPUT_DIR.glob(\"*.txt\")):\n",
    "    fname = path.name\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=SEP)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {fname}: could not load for dedup ({e})\")\n",
    "        continue\n",
    "\n",
    "    # Drop 'Value' if both exist\n",
    "    if \"Value\" in df.columns and \"AnnPITValue\" in df.columns:\n",
    "        df = df.drop(columns=[\"Value\"])\n",
    "\n",
    "    # Ensure all required columns are present\n",
    "    required_cols = KEY_COLS_BASE + [\"AnnPITValue\", PERIOD_COL, PIT_COL]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[WARN] {fname}: missing columns {missing} — skipped dedup.\")\n",
    "        del df\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    # --- NEW: drop rows with missing/empty AnnPITValue_Period ---\n",
    "    # Keep only rows where AnnPITValue_Period is not NaN and not empty/whitespace\n",
    "    period_series = df[PERIOD_COL]\n",
    "    mask_nonempty_period = period_series.notna() & (period_series.astype(str).str.strip() != \"\")\n",
    "    rows_before_period_filter = len(df)\n",
    "    df = df.loc[mask_nonempty_period].copy()\n",
    "    rows_dropped_empty_period = rows_before_period_filter - len(df)\n",
    "    if rows_dropped_empty_period > 0:\n",
    "        print(f\"{fname}: dropped {rows_dropped_empty_period} rows with empty {PERIOD_COL}\")\n",
    "\n",
    "    # If nothing remains after this filter, just overwrite the file with the empty df\n",
    "    if df.empty:\n",
    "        df.to_csv(path, index=False, sep=SEP)\n",
    "        del df\n",
    "        gc.collect()\n",
    "        dedup_dropped[fname] = 0\n",
    "        continue\n",
    "\n",
    "    # Type conversions\n",
    "    df[PIT_COL] = pd.to_datetime(df[PIT_COL], errors=\"coerce\")\n",
    "    df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    annpit_num = pd.to_numeric(df[\"AnnPITValue\"], errors=\"coerce\")\n",
    "    df[\"_AnnPIT_rounded\"] = annpit_num.round(2)\n",
    "\n",
    "    # Sorting\n",
    "    sort_cols = [\"ID\", \"HistCurrency\", PIT_COL, \"FiscalPeriod\"]\n",
    "    if PERIOD_COL in df.columns:\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "    df = df.sort_values(by=sort_cols)\n",
    "\n",
    "    # Deduplication key: include AnnPITValue_Period\n",
    "    key_cols = KEY_COLS_BASE + [PERIOD_COL, \"_AnnPIT_rounded\"]\n",
    "\n",
    "    # lag extension\n",
    "    if all(col in df.columns for col in LAG_COLS):\n",
    "        key_cols = key_cols + LAG_COLS\n",
    "\n",
    "    # Deduplication\n",
    "    dup_mask = df.duplicated(subset=key_cols, keep=\"first\")\n",
    "    dropped = int(dup_mask.sum())\n",
    "    dedup_dropped[fname] = dropped\n",
    "\n",
    "    df = df[~dup_mask].drop(columns=[\"_AnnPIT_rounded\"])\n",
    "\n",
    "    df.to_csv(path, index=False, sep=SEP)\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "# ---- FINAL SUMMARY OUTPUT ----\n",
    "print(\n",
    "    \"\\nRows dropped due to duplicates \"\n",
    "    \"(ID+CompanyName+HistCurrency+FiscalPeriod+AnnPITValue_Period+AnnPITValue[2dp]\"\n",
    "    \" [+Q_lag0–Q_lag7 if present]):\"\n",
    ")\n",
    "for name, dropped in sorted(dedup_dropped.items()):\n",
    "    print(f\"{name}: {dropped} rows dropped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MVSADPQl_Im"
   },
   "source": [
    "### Clean RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1765727755012,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "zx274XXaC3wA",
    "outputId": "a112e9cd-70d6-4ddb-9d76-e55c990c9b92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline-specific variables cleared from memory.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Summary:\n",
    "# This cell removes pipeline-related variables from the global namespace to\n",
    "# reduce memory usage and avoid unintended side effects in later cells.\n",
    "# After deleting variables that were created across the pipeline, garbage\n",
    "# collection is triggered to free memory, and a confirmation message is printed.\n",
    "# ============================================================================\n",
    "\n",
    "# List of variable names to remove from the global namespace\n",
    "to_delete = [\n",
    "    \"INPUT_DIR\", \"OUTPUT_DIR\", \"SEP\", \"CHUNK_SIZE\",\n",
    "    \"rows_dropped_annpit\", \"non_numeric_in_value\",\n",
    "    \"dedup_dropped\", \"KEY_COLS_BASE\", \"PIT_COL\",\n",
    "    \"path\", \"fname\", \"tmp_path\",\n",
    "    \"total_dropped\", \"nn_count\", \"nn_examples\",\n",
    "    \"reader\", \"first_chunk_written\", \"saw_no_annpit_warn\", \"saw_no_value_warn\",\n",
    "    \"dup_mask\", \"key_cols\", \"annpit_num\",\n",
    "]\n",
    "\n",
    "# Loop through the list and delete each variable if it exists\n",
    "for var in to_delete:\n",
    "    if var in globals():\n",
    "        del globals()[var]\n",
    "\n",
    "# Trigger garbage collection to clean up unused objects\n",
    "gc.collect()\n",
    "\n",
    "# Confirmation output\n",
    "print(\"Pipeline-specific variables cleared from memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLiYioCUmB5M"
   },
   "source": [
    "# 2.0. Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNDLh9YeGTUd"
   },
   "source": [
    "## Gather Required Inputs per Anomaly in PIT Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YA4wqdb-Gpmp"
   },
   "source": [
    "#### Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1765651378409,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "-CSZRJDpuB-o",
    "outputId": "488b5edc-b773-428e-df98-0ad9bda8817b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 13.4 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "Current_Assets___Total.txt: 876,857 unique (ID, FiscalPeriod) combinations\n",
      "Cash__Short_Term_Investments.txt: 875,450 unique (ID, FiscalPeriod) combinations\n",
      "Current_Liabilities___Total.txt: 878,053 unique (ID, FiscalPeriod) combinations\n",
      "Short_Term_Debt__Current_Portion_of_LT_Debt.txt: 852,733 unique (ID, FiscalPeriod) combinations\n",
      "Income_Taxes_Payable.txt: 559,826 unique (ID, FiscalPeriod) combinations\n",
      "Depreciation_Depletion__Amortization.txt: 806,654 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 4,218,013 rows and was built in 8.1 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/7] Merging value column 'at' ...\n",
      "    Done in 8.7 seconds. Result currently has 4,218,013 rows.\n",
      "[2/7] Merging value column 'ca' ...\n",
      "    Done in 8.8 seconds. Result currently has 4,218,013 rows.\n",
      "[3/7] Merging value column 'cce' ...\n",
      "    Done in 9.1 seconds. Result currently has 4,218,013 rows.\n",
      "[4/7] Merging value column 'cl' ...\n",
      "    Done in 8.8 seconds. Result currently has 4,218,013 rows.\n",
      "[5/7] Merging value column 'std' ...\n",
      "    Done in 8.7 seconds. Result currently has 4,218,013 rows.\n",
      "[6/7] Merging value column 'itp' ...\n",
      "    Done in 8.0 seconds. Result currently has 4,218,013 rows.\n",
      "[7/7] Merging value column 'da' ...\n",
      "    Done in 8.6 seconds. Result currently has 4,218,013 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 480,451\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - at: 1,451,245 empty values\n",
      "  - ca: 1,508,126 empty values\n",
      "  - cce: 1,712,181 empty values\n",
      "  - cl: 1,535,704 empty values\n",
      "  - std: 1,947,692 empty values\n",
      "  - itp: 3,030,583 empty values\n",
      "  - da: 1,573,484 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Acc.txt\n",
      "Total runtime: 113.6 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILES = [\n",
    "    \"Total_Assets.txt\",\n",
    "    \"Current_Assets___Total.txt\",\n",
    "    \"Cash__Short_Term_Investments.txt\",\n",
    "    \"Current_Liabilities___Total.txt\",\n",
    "    \"Short_Term_Debt__Current_Portion_of_LT_Debt.txt\",\n",
    "    \"Income_Taxes_Payable.txt\",\n",
    "    \"Depreciation_Depletion__Amortization.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"at\",\n",
    "    \"ca\",\n",
    "    \"cce\",\n",
    "    \"cl\",\n",
    "    \"std\",\n",
    "    \"itp\",\n",
    "    \"da\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "OUTPUT_FILE = \"data_Acc.txt\"\n",
    "\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lMgXNkWn1so"
   },
   "source": [
    "#### Ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1765651378912,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ViH3uGm5n5r0",
    "outputId": "a4c14f4d-56b5-4b1d-d2dd-61823e121996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 2.4 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 2,354,025 rows and was built in 1.6 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/1] Merging value column 'at' ...\n",
      "    Done in 5.0 seconds. Result currently has 2,354,025 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 871,391\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - at: 0 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Ag.txt\n",
      "Total runtime: 20.1 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Total_Assets.txt\",\n",
    "\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"at\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Ag.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9SCqWZhoHWV"
   },
   "source": [
    "#### At"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1765651379815,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "GM7QG_pSoNmh",
    "outputId": "75e6e9cf-d277-41a5-ce32-c9ac6e8fb04e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 15.0 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Sales_or_Revenues.txt: 817,863 unique (ID, FiscalPeriod) combinations\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "Cash__Short_Term_Investments.txt: 875,450 unique (ID, FiscalPeriod) combinations\n",
      "Long_Term_Debt.txt: 872,391 unique (ID, FiscalPeriod) combinations\n",
      "Minority_Interest.txt: 850,691 unique (ID, FiscalPeriod) combinations\n",
      "Preferred_Stock.txt: 861,971 unique (ID, FiscalPeriod) combinations\n",
      "Common_Equity.txt: 883,328 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 4,251,336 rows and was built in 8.8 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/7] Merging value column 'rev' ...\n",
      "    Done in 9.0 seconds. Result currently has 4,251,336 rows.\n",
      "[2/7] Merging value column 'at' ...\n",
      "    Done in 8.8 seconds. Result currently has 4,251,336 rows.\n",
      "[3/7] Merging value column 'cce' ...\n",
      "    Done in 8.6 seconds. Result currently has 4,251,336 rows.\n",
      "[4/7] Merging value column 'ltd' ...\n",
      "    Done in 9.0 seconds. Result currently has 4,251,336 rows.\n",
      "[5/7] Merging value column 'mi' ...\n",
      "    Done in 8.9 seconds. Result currently has 4,251,336 rows.\n",
      "[6/7] Merging value column 'ps' ...\n",
      "    Done in 9.4 seconds. Result currently has 4,251,336 rows.\n",
      "[7/7] Merging value column 'ce' ...\n",
      "    Done in 9.4 seconds. Result currently has 4,251,336 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 730,098\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - rev: 1,605,710 empty values\n",
      "  - at: 1,472,894 empty values\n",
      "  - cce: 1,740,892 empty values\n",
      "  - ltd: 1,926,970 empty values\n",
      "  - mi: 1,803,723 empty values\n",
      "  - ps: 1,755,157 empty values\n",
      "  - ce: 1,507,716 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_At.txt\n",
      "Total runtime: 120.6 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Sales_or_Revenues.txt\",\n",
    "    \"Total_Assets.txt\",\n",
    "    \"Cash__Short_Term_Investments.txt\",\n",
    "    \"Long_Term_Debt.txt\",\n",
    "    \"Minority_Interest.txt\",\n",
    "    \"Preferred_Stock.txt\",\n",
    "    \"Common_Equity.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"rev\",\n",
    "    \"at\",\n",
    "    \"cce\",\n",
    "    \"ltd\",\n",
    "    \"mi\",\n",
    "    \"ps\",\n",
    "    \"ce\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_At.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sZeny4spkXe"
   },
   "source": [
    "#### Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1194,
     "status": "ok",
     "timestamp": 1765651381022,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "3Oh-EWCxpojo",
    "outputId": "4a88c6db-152b-450f-f678-a8e2db818ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing finished in 14.8 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Sales_or_Revenues.txt: 817,863 unique (ID, FiscalPeriod) combinations\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "Cash__Short_Term_Investments.txt: 875,450 unique (ID, FiscalPeriod) combinations\n",
      "Long_Term_Debt.txt: 872,391 unique (ID, FiscalPeriod) combinations\n",
      "Minority_Interest.txt: 850,691 unique (ID, FiscalPeriod) combinations\n",
      "Preferred_Stock.txt: 861,971 unique (ID, FiscalPeriod) combinations\n",
      "Common_Equity.txt: 883,328 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 4,251,336 rows and was built in 8.7 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/7] Merging value column 'rev' ...\n",
      "    Done in 8.5 seconds. Result currently has 4,251,336 rows.\n",
      "[2/7] Merging value column 'at' ...\n",
      "    Done in 9.3 seconds. Result currently has 4,251,336 rows.\n",
      "[3/7] Merging value column 'cce' ...\n",
      "    Done in 9.0 seconds. Result currently has 4,251,336 rows.\n",
      "[4/7] Merging value column 'ltd' ...\n",
      "    Done in 9.2 seconds. Result currently has 4,251,336 rows.\n",
      "[5/7] Merging value column 'mi' ...\n",
      "    Done in 9.0 seconds. Result currently has 4,251,336 rows.\n",
      "[6/7] Merging value column 'ps' ...\n",
      "    Done in 9.6 seconds. Result currently has 4,251,336 rows.\n",
      "[7/7] Merging value column 'ce' ...\n",
      "    Done in 9.6 seconds. Result currently has 4,251,336 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 730,098\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - rev: 1,605,710 empty values\n",
      "  - at: 1,472,894 empty values\n",
      "  - cce: 1,740,892 empty values\n",
      "  - ltd: 1,926,970 empty values\n",
      "  - mi: 1,803,723 empty values\n",
      "  - ps: 1,755,157 empty values\n",
      "  - ce: 1,507,716 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Cat.txt\n",
      "Total runtime: 120.9 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Sales_or_Revenues.txt\",\n",
    "    \"Total_Assets.txt\",\n",
    "    \"Cash__Short_Term_Investments.txt\",\n",
    "    \"Long_Term_Debt.txt\",\n",
    "    \"Minority_Interest.txt\",\n",
    "    \"Preferred_Stock.txt\",\n",
    "    \"Common_Equity.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"rev\",\n",
    "    \"at\",\n",
    "    \"cce\",\n",
    "    \"ltd\",\n",
    "    \"mi\",\n",
    "    \"ps\",\n",
    "    \"ce\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Cat.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJrElD7tpy6D"
   },
   "source": [
    "#### Cpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1765651381578,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "El5kcVFtp08E",
    "outputId": "da5b6533-6514-4c88-9d0c-6d4dcf2f3bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 4.0 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Sales_or_Revenues.txt: 817,863 unique (ID, FiscalPeriod) combinations\n",
      "Cost_of_Goods_Sold_Excl_Depreciation.txt: 818,491 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 1,785,864 rows and was built in 5.1 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/2] Merging value column 'rev' ...\n",
      "    Done in 6.3 seconds. Result currently has 1,785,864 rows.\n",
      "[2/2] Merging value column 'cogs' ...\n",
      "    Done in 6.9 seconds. Result currently has 1,785,864 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 795,439\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - rev: 36,843 empty values\n",
      "  - cogs: 135,793 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Cpm.txt\n",
      "Total runtime: 32.1 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Sales_or_Revenues.txt\",\n",
    "    \"Cost_of_Goods_Sold_Excl_Depreciation.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"rev\",\n",
    "    \"cogs\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Cpm.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaT9qk-lqH2U"
   },
   "source": [
    "#### Ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1765651381773,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "G83c2i95HYAl",
    "outputId": "ccd9247d-738e-49f9-93cf-e3e214ecffaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 4.3 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Earnings_Per_Share_Fiscal_Year_End.txt: 840,371 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 1,888,303 rows and was built in 2.7 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/1] Merging value column 'eps' ...\n",
      "    Done in 10.6 seconds. Result currently has 1,888,303 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 840,371\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - eps: 0 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Ec.txt\n",
      "Total runtime: 26.3 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Earnings_Per_Share_Fiscal_Year_End.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"eps\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Ec.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS_NAo1V_lDt"
   },
   "source": [
    "#### Es (Yearly Adaption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1765651382080,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "zdVP_6YE_lDy",
    "outputId": "34e0e403-ce63-4ec1-b878-80ac631f592c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 4.3 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Earnings_Per_Share_Fiscal_Year_End.txt: 840,371 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 1,888,303 rows and was built in 2.8 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/1] Merging value column 'eps' ...\n",
      "    Done in 10.7 seconds. Result currently has 1,888,303 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 840,371\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - eps: 0 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Es.txt\n",
      "Total runtime: 25.2 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Earnings_Per_Share_Fiscal_Year_End.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"eps\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Es.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LdCsaYrqm2-"
   },
   "source": [
    "#### Gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1765651382605,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "j8sARlD2RKjS",
    "outputId": "99828028-5eca-4402-9737-05179512205b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 6.5 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Sales_or_Revenues.txt: 817,863 unique (ID, FiscalPeriod) combinations\n",
      "Cost_of_Goods_Sold_Excl_Depreciation.txt: 818,491 unique (ID, FiscalPeriod) combinations\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 4,139,889 rows and was built in 7.5 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/3] Merging value column 'rev' ...\n",
      "    Done in 8.7 seconds. Result currently has 4,139,889 rows.\n",
      "[2/3] Merging value column 'cogs' ...\n",
      "    Done in 9.2 seconds. Result currently has 4,139,889 rows.\n",
      "[3/3] Merging value column 'at' ...\n",
      "    Done in 8.5 seconds. Result currently has 4,139,889 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 745,149\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - rev: 1,390,825 empty values\n",
      "  - cogs: 1,566,582 empty values\n",
      "  - at: 1,785,864 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Gp.txt\n",
      "Total runtime: 64.5 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Sales_or_Revenues.txt\",\n",
    "    \"Cost_of_Goods_Sold_Excl_Depreciation.txt\",\n",
    "    \"Total_Assets.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"rev\",\n",
    "    \"cogs\",\n",
    "    \"at\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Gp.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3GqzaH4KdvB"
   },
   "source": [
    "#### Ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1765651383029,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "IcXD0kYsq8h3",
    "outputId": "a2c54793-df4f-4ac5-ef81-d074b5d94e92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 4.6 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "Inventories___Total.txt: 875,280 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 2,549,446 rows and was built in 2.7 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/2] Merging value column 'at' ...\n",
      "    Done in 5.7 seconds. Result currently has 2,549,446 rows.\n",
      "[2/2] Merging value column 'inv' ...\n",
      "    Done in 5.7 seconds. Result currently has 2,549,446 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 862,033\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - at: 58,231 empty values\n",
      "  - inv: 286,582 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Ig.txt\n",
      "Total runtime: 30.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Total_Assets.txt\",\n",
    "    \"Inventories___Total.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"at\",\n",
    "    \"inv\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Ig.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByeSFikWrNrS"
   },
   "source": [
    "#### Inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1765651383350,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "2YKB-zc8rPf2",
    "outputId": "2c7e772f-a66d-4e8f-fa9c-90dcec3ee727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 3.1 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Sales_or_Revenues.txt: 817,863 unique (ID, FiscalPeriod) combinations\n",
      "Capital_Expenditures_Addtns_to_Fixed_Assets.txt: 790,635 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 1,550,264 rows and was built in 3.8 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/2] Merging value column 'rev' ...\n",
      "    Done in 5.9 seconds. Result currently has 1,550,264 rows.\n",
      "[2/2] Merging value column 'capex' ...\n",
      "    Done in 4.7 seconds. Result currently has 1,550,264 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 770,345\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - rev: 31,207 empty values\n",
      "  - capex: 258,934 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Inv.txt\n",
      "Total runtime: 25.8 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Sales_or_Revenues.txt\",\n",
    "    \"Capital_Expenditures_Addtns_to_Fixed_Assets.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"rev\",\n",
    "    \"capex\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Inv.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymD2DOB0rjjp"
   },
   "source": [
    "#### Ltg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2622,
     "status": "ok",
     "timestamp": 1765651385974,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "-sOSVO3krmT1",
    "outputId": "d9e0d607-bde3-46d7-8e7a-ca49b4ede975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 31.3 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "Current_Assets___Total.txt: 876,857 unique (ID, FiscalPeriod) combinations\n",
      "Cash__Short_Term_Investments.txt: 875,450 unique (ID, FiscalPeriod) combinations\n",
      "Current_Liabilities___Total.txt: 878,053 unique (ID, FiscalPeriod) combinations\n",
      "Short_Term_Debt__Current_Portion_of_LT_Debt.txt: 852,733 unique (ID, FiscalPeriod) combinations\n",
      "Income_Taxes_Payable.txt: 559,826 unique (ID, FiscalPeriod) combinations\n",
      "Depreciation_Depletion__Amortization.txt: 806,654 unique (ID, FiscalPeriod) combinations\n",
      "ReceivablesNet.txt: 872,009 unique (ID, FiscalPeriod) combinations\n",
      "Inventories___Total.txt: 875,280 unique (ID, FiscalPeriod) combinations\n",
      "Other_Current_Assets.txt: 860,532 unique (ID, FiscalPeriod) combinations\n",
      "Property_Plant__Equipment___Net.txt: 871,442 unique (ID, FiscalPeriod) combinations\n",
      "Other_Assets___Total.txt: 873,733 unique (ID, FiscalPeriod) combinations\n",
      "Accounts_Payable.txt: 823,046 unique (ID, FiscalPeriod) combinations\n",
      "Other_Current_Liabilities.txt: 855,235 unique (ID, FiscalPeriod) combinations\n",
      "Other_Liabilities.txt: 857,989 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 4,891,955 rows and was built in 14.8 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/15] Merging value column 'at' ...\n",
      "    Done in 9.7 seconds. Result currently has 4,891,955 rows.\n",
      "[2/15] Merging value column 'ca' ...\n",
      "    Done in 10.3 seconds. Result currently has 4,891,955 rows.\n",
      "[3/15] Merging value column 'cce' ...\n",
      "    Done in 10.9 seconds. Result currently has 4,891,955 rows.\n",
      "[4/15] Merging value column 'cl' ...\n",
      "    Done in 10.4 seconds. Result currently has 4,891,955 rows.\n",
      "[5/15] Merging value column 'std' ...\n",
      "    Done in 10.7 seconds. Result currently has 4,891,955 rows.\n",
      "[6/15] Merging value column 'itp' ...\n",
      "    Done in 9.4 seconds. Result currently has 4,891,955 rows.\n",
      "[7/15] Merging value column 'da' ...\n",
      "    Done in 10.5 seconds. Result currently has 4,891,955 rows.\n",
      "[8/15] Merging value column 'ar' ...\n",
      "    Done in 11.7 seconds. Result currently has 4,891,955 rows.\n",
      "[9/15] Merging value column 'inv' ...\n",
      "    Done in 10.9 seconds. Result currently has 4,891,955 rows.\n",
      "[10/15] Merging value column 'oca' ...\n",
      "    Done in 11.0 seconds. Result currently has 4,891,955 rows.\n",
      "[11/15] Merging value column 'ppe' ...\n",
      "    Done in 11.9 seconds. Result currently has 4,891,955 rows.\n",
      "[12/15] Merging value column 'oa' ...\n",
      "    Done in 10.9 seconds. Result currently has 4,891,955 rows.\n",
      "[13/15] Merging value column 'ap' ...\n",
      "    Done in 10.7 seconds. Result currently has 4,891,955 rows.\n",
      "[14/15] Merging value column 'ocl' ...\n",
      "    Done in 11.9 seconds. Result currently has 4,891,955 rows.\n",
      "[15/15] Merging value column 'ol' ...\n",
      "    Done in 11.7 seconds. Result currently has 4,891,955 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 395,599\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - at: 1,675,139 empty values\n",
      "  - ca: 1,734,804 empty values\n",
      "  - cce: 1,958,807 empty values\n",
      "  - cl: 1,765,023 empty values\n",
      "  - std: 2,222,559 empty values\n",
      "  - itp: 3,477,266 empty values\n",
      "  - da: 1,709,467 empty values\n",
      "  - ar: 2,098,153 empty values\n",
      "  - inv: 1,929,501 empty values\n",
      "  - oca: 2,556,172 empty values\n",
      "  - ppe: 1,769,628 empty values\n",
      "  - oa: 2,328,504 empty values\n",
      "  - ap: 2,484,352 empty values\n",
      "  - ocl: 2,506,776 empty values\n",
      "  - ol: 2,551,098 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Ltg.txt\n",
      "Total runtime: 275.7 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Total_Assets.txt\",\n",
    "    \"Current_Assets___Total.txt\",\n",
    "    \"Cash__Short_Term_Investments.txt\",\n",
    "    \"Current_Liabilities___Total.txt\",\n",
    "    \"Short_Term_Debt__Current_Portion_of_LT_Debt.txt\",\n",
    "    \"Income_Taxes_Payable.txt\",\n",
    "    \"Depreciation_Depletion__Amortization.txt\",\n",
    "    \"ReceivablesNet.txt\",\n",
    "    \"Inventories___Total.txt\",\n",
    "    \"Other_Current_Assets.txt\",\n",
    "    \"Property_Plant__Equipment___Net.txt\",\n",
    "    \"Other_Assets___Total.txt\",\n",
    "    \"Accounts_Payable.txt\",\n",
    "    \"Other_Current_Liabilities.txt\",\n",
    "    \"Other_Liabilities.txt\",\n",
    "\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"at\",\n",
    "    \"ca\",\n",
    "    \"cce\",\n",
    "    \"cl\",\n",
    "    \"std\",\n",
    "    \"itp\",\n",
    "    \"da\",\n",
    "    \"ar\",\n",
    "    \"inv\",\n",
    "    \"oca\",\n",
    "    \"ppe\",\n",
    "    \"oa\",\n",
    "    \"ap\",\n",
    "    \"ocl\",\n",
    "    \"ol\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Ltg.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RtjOlDfsQAs"
   },
   "source": [
    "#### Nca (Special Case with IVAO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1223,
     "status": "ok",
     "timestamp": 1765651387201,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "LM2Ox3xlbn_x",
    "outputId": "3dd87d45-ace2-4a12-a45f-0f60227f526d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/2170826045.py:183: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
      "/tmp/ipykernel_847332/2170826045.py:183: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
      "/tmp/ipykernel_847332/2170826045.py:183: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
      "/tmp/ipykernel_847332/2170826045.py:183: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
      "/tmp/ipykernel_847332/2170826045.py:183: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
      "/tmp/ipykernel_847332/2170826045.py:183: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
      "/tmp/ipykernel_847332/2170826045.py:183: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
      "/tmp/ipykernel_847332/2170826045.py:183: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
      "/tmp/ipykernel_847332/2170826045.py:183: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
      "/tmp/ipykernel_847332/2170826045.py:183: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Nca.txt\n",
      "\n",
      "No optional files were missing.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods),\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations are \"complete\" under the following rule:\n",
    "    * All non-IVAO_ value columns must be non-missing.\n",
    "    * Across all IVAO_ columns, it is enough if at least ONE IVAO_ column\n",
    "      has a non-missing value (they are treated as a single logical block).\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (IVAO_ columns included, individually).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILES = [\n",
    "    \"Other_Investments.txt\",\n",
    "    \"Investments_in_Associated_Companies.txt\",\n",
    "    \"Long_Term_Receivables.txt\",\n",
    "    \"Investments_in_Sales__Direct_Financing_Leases.txt\",\n",
    "    \"Unspecified_Other_Loans.txt\",\n",
    "    \"Total_Assets.txt\",\n",
    "    \"Current_Assets___Total.txt\",\n",
    "    \"Total_Liabilities.txt\",\n",
    "    \"Short_Term_Debt__Current_Portion_of_LT_Debt.txt\",\n",
    "    \"Long_Term_Debt.txt\",\n",
    "]\n",
    "\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"IVAO_oi\",\n",
    "    \"IVAO_iac\",\n",
    "    \"IVAO_ltr\",\n",
    "    \"IVAO_isdfl\",\n",
    "    \"IVAO_uol\",\n",
    "    \"at\",\n",
    "    \"ca\",\n",
    "    \"lt\",\n",
    "    \"std\",\n",
    "    \"ltd\",\n",
    "]\n",
    "\n",
    "OPTIONAL_INPUT_FILES = {\n",
    "    \"Other_Investments.txt\",\n",
    "    \"Investments_in_Associated_Companies.txt\",\n",
    "    \"Long_Term_Receivables.txt\",\n",
    "    \"Investments_in_Sales__Direct_Financing_Leases.txt\",\n",
    "    \"Unspecified_Other_Loans.txt\",\n",
    "}\n",
    "\n",
    "OUTPUT_FILE = \"data_Nca.txt\"\n",
    "\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a single dataset from disk and prepares the columns.\n",
    "    Keeps AnnPITValue_Period if present; otherwise creates it as NA.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Reorder for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Enforce consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds the base frame with all unique identifier combinations,\n",
    "    including AnnPITValue_Period.\n",
    "    \"\"\"\n",
    "\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    base = base.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs a single as-of merge of one dataset into the base table.\n",
    "\n",
    "    Period logic:\n",
    "    - If df has non-NA AnnPITValue_Period values → period-aware:\n",
    "      grouping is (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period).\n",
    "    - If AnnPITValue_Period is all NA → period-agnostic:\n",
    "      grouping is (ID, HistCurrency, FiscalPeriod) only, so values can\n",
    "      be used for all periods.\n",
    "    \"\"\"\n",
    "\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Combine\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Ordering for forward-fill\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map({\"df\": 0, \"base\": 1}).astype(\"int8\")\n",
    "\n",
    "    # Sort keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill within group\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"]).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function that loads, merges, and writes the final dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    dfs = []\n",
    "    missing_optional_files = []\n",
    "\n",
    "    # Load all input datasets\n",
    "    for file_name, path in zip(input_files, paths):\n",
    "        if not path.exists():\n",
    "            if file_name in OPTIONAL_INPUT_FILES:\n",
    "                print(f\"Optional file missing → filled with NaN: {file_name}\")\n",
    "                missing_optional_files.append(file_name)\n",
    "                # Empty df with correct columns\n",
    "                dfs.append(pd.DataFrame(columns=BASE_COLS + [PERIOD_COL, VALUE_COL]))\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Required file missing: {path}\")\n",
    "        else:\n",
    "            dfs.append(load_dataset(path))\n",
    "\n",
    "    # Base frame\n",
    "    base = build_base_frame(dfs)\n",
    "\n",
    "    # Merge\n",
    "    result = base\n",
    "    for df, col_name in zip(dfs, value_column_names):\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "\n",
    "    # Keep identifier + period + value columns\n",
    "    result = result[BASE_COLS + [PERIOD_COL] + value_column_names]\n",
    "\n",
    "    # Write result\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "\n",
    "    if missing_optional_files:\n",
    "        print(\"\\nOptional files that were missing:\")\n",
    "        for f in missing_optional_files:\n",
    "            print(f\"  - {f}\")\n",
    "    else:\n",
    "        print(\"\\nNo optional files were missing.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdhQDQpNsvRQ"
   },
   "source": [
    "#### Noa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1765651388067,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "LpWn1lV6sypy",
    "outputId": "8d06a02e-7723-47c8-8753-825a2c8fbd86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 12.9 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "Cash__Short_Term_Investments.txt: 875,450 unique (ID, FiscalPeriod) combinations\n",
      "Long_Term_Debt.txt: 872,391 unique (ID, FiscalPeriod) combinations\n",
      "Minority_Interest.txt: 850,691 unique (ID, FiscalPeriod) combinations\n",
      "Preferred_Stock.txt: 861,971 unique (ID, FiscalPeriod) combinations\n",
      "Common_Equity.txt: 883,328 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 2,899,491 rows and was built in 5.4 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/6] Merging value column 'at' ...\n",
      "    Done in 5.8 seconds. Result currently has 2,899,491 rows.\n",
      "[2/6] Merging value column 'cce' ...\n",
      "    Done in 5.8 seconds. Result currently has 2,899,491 rows.\n",
      "[3/6] Merging value column 'ltd' ...\n",
      "    Done in 6.1 seconds. Result currently has 2,899,491 rows.\n",
      "[4/6] Merging value column 'mi' ...\n",
      "    Done in 5.9 seconds. Result currently has 2,899,491 rows.\n",
      "[5/6] Merging value column 'ps' ...\n",
      "    Done in 6.0 seconds. Result currently has 2,899,491 rows.\n",
      "[6/6] Merging value column 'ce' ...\n",
      "    Done in 6.7 seconds. Result currently has 2,899,491 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 803,539\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - at: 121,049 empty values\n",
      "  - cce: 389,047 empty values\n",
      "  - ltd: 575,125 empty values\n",
      "  - mi: 451,878 empty values\n",
      "  - ps: 403,312 empty values\n",
      "  - ce: 155,871 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Noa.txt\n",
      "Total runtime: 81.3 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Total_Assets.txt\",\n",
    "    \"Cash__Short_Term_Investments.txt\",\n",
    "    \"Long_Term_Debt.txt\",\n",
    "    \"Minority_Interest.txt\",\n",
    "    \"Preferred_Stock.txt\",\n",
    "    \"Common_Equity.txt\",\n",
    "\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"at\",\n",
    "    \"cce\",\n",
    "    \"ltd\",\n",
    "    \"mi\",\n",
    "    \"ps\",\n",
    "    \"ce\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Noa.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dl-xJ7YFlgZP"
   },
   "source": [
    "#### Nwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1765651388809,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "gSyxYMNjloFN",
    "outputId": "781b0d4c-1e91-485f-c197-064a30799db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 11.2 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "Current_Assets___Total.txt: 876,857 unique (ID, FiscalPeriod) combinations\n",
      "Cash__Short_Term_Investments.txt: 875,450 unique (ID, FiscalPeriod) combinations\n",
      "Current_Liabilities___Total.txt: 878,053 unique (ID, FiscalPeriod) combinations\n",
      "Short_Term_Debt__Current_Portion_of_LT_Debt.txt: 852,733 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 2,721,575 rows and was built in 4.6 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/5] Merging value column 'at' ...\n",
      "    Done in 5.4 seconds. Result currently has 2,721,575 rows.\n",
      "[2/5] Merging value column 'ca' ...\n",
      "    Done in 5.6 seconds. Result currently has 2,721,575 rows.\n",
      "[3/5] Merging value column 'cce' ...\n",
      "    Done in 5.7 seconds. Result currently has 2,721,575 rows.\n",
      "[4/5] Merging value column 'cl' ...\n",
      "    Done in 6.2 seconds. Result currently has 2,721,575 rows.\n",
      "[5/5] Merging value column 'std' ...\n",
      "    Done in 5.5 seconds. Result currently has 2,721,575 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 827,043\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - at: 76,939 empty values\n",
      "  - ca: 134,175 empty values\n",
      "  - cce: 334,420 empty values\n",
      "  - cl: 161,431 empty values\n",
      "  - std: 563,368 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Nwc.txt\n",
      "Total runtime: 68.6 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Total_Assets.txt\",\n",
    "    \"Current_Assets___Total.txt\",\n",
    "    \"Cash__Short_Term_Investments.txt\",\n",
    "    \"Current_Liabilities___Total.txt\",\n",
    "    \"Short_Term_Debt__Current_Portion_of_LT_Debt.txt\",\n",
    "\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"at\",\n",
    "    \"ca\",\n",
    "    \"cce\",\n",
    "    \"cl\",\n",
    "    \"std\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Nwc.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9RfF_uXtEWe"
   },
   "source": [
    "#### Ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 678,
     "status": "ok",
     "timestamp": 1765651389504,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "v2dyUah5tX4e",
    "outputId": "ae73f08b-219b-4c80-f2e2-1a95cb62e751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 5.1 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "Capital_Expenditures_Addtns_to_Fixed_Assets.txt: 790,635 unique (ID, FiscalPeriod) combinations\n",
      "Selling_General__Administrative_Expenses.txt: 721,610 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 3,890,568 rows and was built in 5.8 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/3] Merging value column 'at' ...\n",
      "    Done in 8.1 seconds. Result currently has 3,890,568 rows.\n",
      "[2/3] Merging value column 'cogs' ...\n",
      "    Done in 7.2 seconds. Result currently has 3,890,568 rows.\n",
      "[3/3] Merging value column 'sga' ...\n",
      "    Done in 7.7 seconds. Result currently has 3,890,568 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 539,786\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - at: 1,536,543 empty values\n",
      "  - cogs: 1,657,115 empty values\n",
      "  - sga: 1,899,464 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Ol.txt\n",
      "Total runtime: 56.5 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Total_Assets.txt\",\n",
    "    \"Capital_Expenditures_Addtns_to_Fixed_Assets.txt\",\n",
    "    \"Selling_General__Administrative_Expenses.txt\",\n",
    "\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"at\",\n",
    "    \"cogs\",\n",
    "    \"sga\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Ol.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibx9sNQvtqPG"
   },
   "source": [
    "#### Osc (Special Case with IFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1411,
     "status": "ok",
     "timestamp": 1765651390918,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "N3mAb2cbtuMZ",
    "outputId": "6b4ddf37-500c-4642-d975-30581545f74a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 15.4 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "Total_Liabilities.txt: 852,750 unique (ID, FiscalPeriod) combinations\n",
      "Current_Assets___Total.txt: 876,857 unique (ID, FiscalPeriod) combinations\n",
      "Current_Liabilities___Total.txt: 878,053 unique (ID, FiscalPeriod) combinations\n",
      "Net_Income_Before_Extra_Items_Preferred_Divs.txt: 842,027 unique (ID, FiscalPeriod) combinations\n",
      "Funds_From_Operations.txt: 828,759 unique (ID, FiscalPeriod) combinations\n",
      "Extraordinary_Items.txt: 683,683 unique (ID, FiscalPeriod) combinations\n",
      "Disposal_of_Fixed_Assets.txt: 648,468 unique (ID, FiscalPeriod) combinations\n",
      "Funds_From_For_Other_Operating_Activities.txt: 670,513 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 4,406,065 rows and was built in 12.1 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/9] Merging value column 'at' ...\n",
      "    Done in 9.2 seconds. Result currently has 4,406,065 rows.\n",
      "[2/9] Merging value column 'tl' ...\n",
      "    Done in 9.0 seconds. Result currently has 4,406,065 rows.\n",
      "[3/9] Merging value column 'ca' ...\n",
      "    Done in 9.3 seconds. Result currently has 4,406,065 rows.\n",
      "[4/9] Merging value column 'cl' ...\n",
      "    Done in 9.4 seconds. Result currently has 4,406,065 rows.\n",
      "[5/9] Merging value column 'ni_extra' ...\n",
      "    Done in 9.7 seconds. Result currently has 4,406,065 rows.\n",
      "[6/9] Merging value column 'IFO_ffo' ...\n",
      "    Done in 8.9 seconds. Result currently has 4,406,065 rows.\n",
      "[7/9] Merging value column 'IFO_ei' ...\n",
      "    Done in 8.3 seconds. Result currently has 4,406,065 rows.\n",
      "[8/9] Merging value column 'IFO_dofa' ...\n",
      "    Done in 8.3 seconds. Result currently has 4,406,065 rows.\n",
      "[9/9] Merging value column 'IFO_ffooa' ...\n",
      "    Done in 8.5 seconds. Result currently has 4,406,065 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 428,198\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - at: 1,883,965 empty values\n",
      "  - tl: 1,985,660 empty values\n",
      "  - ca: 1,941,397 empty values\n",
      "  - cl: 1,968,021 empty values\n",
      "  - ni_extra: 1,444,807 empty values\n",
      "  - IFO_ffo: 1,644,039 empty values\n",
      "  - IFO_ei: 2,553,942 empty values\n",
      "  - IFO_dofa: 2,585,368 empty values\n",
      "  - IFO_ffooa: 2,514,786 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Osc.txt\n",
      "Total runtime: 147.8 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods),\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILES = [\n",
    "    \"Total_Assets.txt\",\n",
    "    \"Total_Liabilities.txt\",\n",
    "    \"Current_Assets___Total.txt\",\n",
    "    \"Current_Liabilities___Total.txt\",\n",
    "    \"Net_Income_Before_Extra_Items_Preferred_Divs.txt\",\n",
    "    \"Funds_From_Operations.txt\",\n",
    "    \"Extraordinary_Items.txt\",\n",
    "    \"Disposal_of_Fixed_Assets.txt\",\n",
    "    \"Funds_From_For_Other_Operating_Activities.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"at\",\n",
    "    \"tl\",\n",
    "    \"ca\",\n",
    "    \"cl\",\n",
    "    \"ni_extra\",\n",
    "    \"IFO_ffo\",\n",
    "    \"IFO_ei\",\n",
    "    \"IFO_dofa\",\n",
    "    \"IFO_ffooa\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "OUTPUT_FILE = \"data_Osc.txt\"\n",
    "\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required base + value columns are present,\n",
    "    - keeps AnnPITValue_Period if present (or creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Reorder to a consistent layout\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns + AnnPITValue_Period from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    base = pd.concat([df[BASE_COLS + [PERIOD_COL]] for df in dfs], ignore_index=True)\n",
    "\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod) and PIT Date\n",
    "      in the base table, we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has non-NA AnnPITValue_Period values, we additionally require\n",
    "      AnnPITValue_Period to match (period-aware as-of).\n",
    "    - If df has AnnPITValue_Period all NA, we ignore the period column\n",
    "      (period-agnostic) and the values can \"fit all\" periods.\n",
    "    \"\"\"\n",
    "    # Determine whether this dataset is period-aware\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying input dataframes\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Prepare df_tmp with appropriate columns\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Ensure df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build group and sort columns\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort and forward-fill\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only base rows\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and resort\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep base identifier columns + period and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FU2KBlF4w9Sg"
   },
   "source": [
    "#### Pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1765651391351,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "kGUH83z6xBcZ",
    "outputId": "0f538fb7-d55d-4130-bee4-4c76abfaa90b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 4.1 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Sales_or_Revenues.txt: 817,863 unique (ID, FiscalPeriod) combinations\n",
      "Cost_of_Goods_Sold_Excl_Depreciation.txt: 818,491 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 1,785,864 rows and was built in 5.1 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/2] Merging value column 'rev' ...\n",
      "    Done in 6.5 seconds. Result currently has 1,785,864 rows.\n",
      "[2/2] Merging value column 'cogs' ...\n",
      "    Done in 6.8 seconds. Result currently has 1,785,864 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 795,439\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - rev: 36,843 empty values\n",
      "  - cogs: 135,793 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Pm.txt\n",
      "Total runtime: 32.6 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Sales_or_Revenues.txt\",\n",
    "    \"Cost_of_Goods_Sold_Excl_Depreciation.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"rev\",\n",
    "    \"cogs\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Pm.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPggIVBfxYKp"
   },
   "source": [
    "#### Poa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1765651391623,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "_ixI7iilxakw",
    "outputId": "1fe2f57b-509a-4e96-f82d-03bc128b13b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 3.1 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Income_Before_Extra_Items_Preferred_Divs.txt: 842,027 unique (ID, FiscalPeriod) combinations\n",
      "Net_Cash_Flow___Operating_Activities.txt: 751,599 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 1,527,513 rows and was built in 3.9 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/2] Merging value column 'ni_extra' ...\n",
      "    Done in 5.8 seconds. Result currently has 1,527,513 rows.\n",
      "[2/2] Merging value column 'opcf' ...\n",
      "    Done in 4.8 seconds. Result currently has 1,527,513 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 751,406\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - ni_extra: 12,508 empty values\n",
      "  - opcf: 237,895 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Poa.txt\n",
      "Total runtime: 26.2 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Income_Before_Extra_Items_Preferred_Divs.txt\",\n",
    "    \"Net_Cash_Flow___Operating_Activities.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"ni_extra\",\n",
    "    \"opcf\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Poa.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yy8Td9o3x5OT"
   },
   "source": [
    "#### Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1765651392049,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "9rm8lxt1x7E6",
    "outputId": "62cdaf6f-9f0a-4b72-99d8-fc55b5fff194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 4.3 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Income_Used_to_Calculate_Basic_EPS.txt: 842,036 unique (ID, FiscalPeriod) combinations\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 3,700,378 rows and was built in 4.8 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/2] Merging value column 'ni_eps' ...\n",
      "    Done in 8.0 seconds. Result currently has 3,700,378 rows.\n",
      "[2/2] Merging value column 'at' ...\n",
      "    Done in 7.6 seconds. Result currently has 3,700,378 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 793,794\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - ni_eps: 1,369,132 empty values\n",
      "  - at: 1,346,353 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Pro.txt\n",
      "Total runtime: 43.6 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Income_Used_to_Calculate_Basic_EPS.txt\",\n",
    "    \"Total_Assets.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"ni_eps\",\n",
    "    \"at\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Pro.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP_5O3nLyVP6"
   },
   "source": [
    "#### Pta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1765651392871,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "jtM10_8jybj0",
    "outputId": "c66982ab-7763-4dfe-937a-3413d05cbb9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 8.2 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Income_Before_Extra_Items_Preferred_Divs.txt: 842,027 unique (ID, FiscalPeriod) combinations\n",
      "Net_Cash_Flow___Operating_Activities.txt: 751,599 unique (ID, FiscalPeriod) combinations\n",
      "Net_Cash_Flow___Investing.txt: 745,084 unique (ID, FiscalPeriod) combinations\n",
      "Net_Cash_Flow___Financing.txt: 747,265 unique (ID, FiscalPeriod) combinations\n",
      "Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt: 740,951 unique (ID, FiscalPeriod) combinations\n",
      "Com_Pfd_Redeemed_Retired_Converted_Etc..txt: 665,679 unique (ID, FiscalPeriod) combinations\n",
      "Cash_Dividends_Paid___Total.txt: 811,535 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 1,765,455 rows and was built in 9.6 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/7] Merging value column 'ni_extra' ...\n",
      "    Done in 6.2 seconds. Result currently has 1,765,455 rows.\n",
      "[2/7] Merging value column 'opcf' ...\n",
      "    Done in 5.5 seconds. Result currently has 1,765,455 rows.\n",
      "[3/7] Merging value column 'invcf' ...\n",
      "    Done in 5.1 seconds. Result currently has 1,765,455 rows.\n",
      "[4/7] Merging value column 'fincf' ...\n",
      "    Done in 5.0 seconds. Result currently has 1,765,455 rows.\n",
      "[5/7] Merging value column 'socaps' ...\n",
      "    Done in 5.1 seconds. Result currently has 1,765,455 rows.\n",
      "[6/7] Merging value column 'pocaps' ...\n",
      "    Done in 5.0 seconds. Result currently has 1,765,455 rows.\n",
      "[7/7] Merging value column 'div' ...\n",
      "    Done in 5.4 seconds. Result currently has 1,765,455 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 612,043\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - ni_extra: 18,247 empty values\n",
      "  - opcf: 249,409 empty values\n",
      "  - invcf: 261,722 empty values\n",
      "  - fincf: 260,887 empty values\n",
      "  - socaps: 528,244 empty values\n",
      "  - pocaps: 635,834 empty values\n",
      "  - div: 256,366 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Pta.txt\n",
      "Total runtime: 72.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Income_Before_Extra_Items_Preferred_Divs.txt\",\n",
    "    \"Net_Cash_Flow___Operating_Activities.txt\",\n",
    "    \"Net_Cash_Flow___Investing.txt\",\n",
    "    \"Net_Cash_Flow___Financing.txt\",\n",
    "    \"Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt\",\n",
    "    \"Com_Pfd_Redeemed_Retired_Converted_Etc..txt\",\n",
    "    \"Cash_Dividends_Paid___Total.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"ni_extra\",\n",
    "    \"opcf\",\n",
    "    \"invcf\",\n",
    "    \"fincf\",\n",
    "    \"socaps\",\n",
    "    \"pocaps\",\n",
    "    \"div\",\n",
    "\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Pta.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee0G6keb0vx2"
   },
   "source": [
    "#### Roe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 807,
     "status": "ok",
     "timestamp": 1765651393704,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "5eE_A4Y90zJU",
    "outputId": "e51aee6d-98b2-49b8-adef-757f55e1274e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 5.7 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Income_Used_to_Calculate_Basic_EPS.txt: 842,036 unique (ID, FiscalPeriod) combinations\n",
      "Common_Equity.txt: 883,328 unique (ID, FiscalPeriod) combinations\n",
      "Deferred_Taxes.txt: 750,382 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 3,785,314 rows and was built in 5.3 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/3] Merging value column 'ni_eps' ...\n",
      "    Done in 7.8 seconds. Result currently has 3,785,314 rows.\n",
      "[2/3] Merging value column 'ce' ...\n",
      "    Done in 8.1 seconds. Result currently has 3,785,314 rows.\n",
      "[3/3] Merging value column 'dt' ...\n",
      "    Done in 7.3 seconds. Result currently has 3,785,314 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 680,276\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - ni_eps: 1,396,981 empty values\n",
      "  - ce: 1,398,836 empty values\n",
      "  - dt: 2,103,123 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Roe.txt\n",
      "Total runtime: 56.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Income_Used_to_Calculate_Basic_EPS.txt\",\n",
    "    \"Common_Equity.txt\",\n",
    "    \"Deferred_Taxes.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"ni_eps\",\n",
    "    \"ce\",\n",
    "    \"dt\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Roe.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFOGWXQu2dLr"
   },
   "source": [
    "#### Rs (Yearly Adaption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1765651393958,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "_LO4HL6O8_HW",
    "outputId": "4c02d964-edd3-46fe-f8c1-fbac84aabfa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 1.9 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Sales_or_Revenues.txt: 817,863 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 1,351,845 rows and was built in 1.5 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/1] Merging value column 'rev' ...\n",
      "    Done in 5.6 seconds. Result currently has 1,351,845 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 817,863\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - rev: 0 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Rs.txt\n",
      "Total runtime: 15.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Sales_or_Revenues.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"rev\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Rs.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2dXuVHw7m9P"
   },
   "source": [
    "#### Sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8451,
     "status": "ok",
     "timestamp": 1765651402413,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "NtdiiyrF7prZ",
    "outputId": "53333b18-e0d7-4c4a-ce48-d0213ee262e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 4.1 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Common_Equity.txt: 883,328 unique (ID, FiscalPeriod) combinations\n",
      "Deferred_Taxes.txt: 750,382 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 2,438,961 rows and was built in 2.4 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/2] Merging value column 'ce' ...\n",
      "    Done in 5.6 seconds. Result currently has 2,438,961 rows.\n",
      "[2/2] Merging value column 'dt' ...\n",
      "    Done in 4.8 seconds. Result currently has 2,438,961 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 746,347\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - ce: 52,483 empty values\n",
      "  - dt: 756,770 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Sg.txt\n",
      "Total runtime: 29.9 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Common_Equity.txt\",\n",
    "    \"Deferred_Taxes.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"ce\",\n",
    "    \"dt\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Sg.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuLx5Umh8Hgh"
   },
   "source": [
    "#### Sli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 791,
     "status": "ok",
     "timestamp": 1765651403663,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Rz2aFwyH8R15",
    "outputId": "b6d0e0cb-4af6-4431-c8ea-4b9aee9e0885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 4.2 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Sales_or_Revenues.txt: 817,863 unique (ID, FiscalPeriod) combinations\n",
      "Inventories___Total.txt: 875,280 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 3,538,149 rows and was built in 4.9 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/2] Merging value column 'rev' ...\n",
      "    Done in 7.7 seconds. Result currently has 3,538,149 rows.\n",
      "[2/2] Merging value column 'inv' ...\n",
      "    Done in 7.2 seconds. Result currently has 3,538,149 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 799,132\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - rev: 1,262,930 empty values\n",
      "  - inv: 1,351,845 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Sli.txt\n",
      "Total runtime: 39.3 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Sales_or_Revenues.txt\",\n",
    "    \"Inventories___Total.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"rev\",\n",
    "    \"inv\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Sli.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6Ju2Ehq8tjC"
   },
   "source": [
    "#### Slx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1765651403916,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "FdrOPaww8vOL",
    "outputId": "6c0db40f-42bc-4595-e2d9-1c61733bc2dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 3.7 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Sales_or_Revenues.txt: 817,863 unique (ID, FiscalPeriod) combinations\n",
      "Selling_General__Administrative_Expenses.txt: 721,610 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 1,694,448 rows and was built in 4.7 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/2] Merging value column 'rev' ...\n",
      "    Done in 6.3 seconds. Result currently has 1,694,448 rows.\n",
      "[2/2] Merging value column 'sga' ...\n",
      "    Done in 6.0 seconds. Result currently has 1,694,448 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 699,730\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - rev: 37,706 empty values\n",
      "  - sga: 402,105 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Slx.txt\n",
      "Total runtime: 29.6 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Sales_or_Revenues.txt\",\n",
    "    \"Selling_General__Administrative_Expenses.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"rev\",\n",
    "    \"sga\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Slx.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6BMDicV9HnF"
   },
   "source": [
    "#### Tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1765651404226,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "sUn4qqbC9JtY",
    "outputId": "e6ad95d7-9d5d-4180-bfbc-bbaf828ce2f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 3.6 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Income_Taxes.txt: 819,430 unique (ID, FiscalPeriod) combinations\n",
      "Net_Income_Before_Extra_Items_Preferred_Divs.txt: 842,027 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 1,451,441 rows and was built in 4.3 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/2] Merging value column 'itx' ...\n",
      "    Done in 5.2 seconds. Result currently has 1,451,441 rows.\n",
      "[2/2] Merging value column 'ni_extra' ...\n",
      "    Done in 5.7 seconds. Result currently has 1,451,441 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 819,235\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - itx: 98,894 empty values\n",
      "  - ni_extra: 11,540 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Tx.txt\n",
      "Total runtime: 26.6 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Income_Taxes.txt\",\n",
    "    \"Net_Income_Before_Extra_Items_Preferred_Divs.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"itx\",\n",
    "    \"ni_extra\",\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Tx.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SrElfeZ9gNA"
   },
   "source": [
    "#### Txf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1765651404567,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "GzdAGRYm9iGh",
    "outputId": "7d12bf25-107f-43d8-9792-e1285e932b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 7.0 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt: 740,951 unique (ID, FiscalPeriod) combinations\n",
      "Com_Pfd_Redeemed_Retired_Converted_Etc..txt: 665,679 unique (ID, FiscalPeriod) combinations\n",
      "Cash_Dividends_Paid___Total.txt: 811,535 unique (ID, FiscalPeriod) combinations\n",
      "Long_Term_Borrowings.txt: 633,163 unique (ID, FiscalPeriod) combinations\n",
      "Reduction_in_Long_Term_Debt.txt: 658,636 unique (ID, FiscalPeriod) combinations\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 3,798,877 rows and was built in 7.8 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/6] Merging value column 'socaps' ...\n",
      "    Done in 6.6 seconds. Result currently has 3,798,877 rows.\n",
      "[2/6] Merging value column 'pocaps' ...\n",
      "    Done in 6.7 seconds. Result currently has 3,798,877 rows.\n",
      "[3/6] Merging value column 'div' ...\n",
      "    Done in 7.0 seconds. Result currently has 3,798,877 rows.\n",
      "[4/6] Merging value column 'diss' ...\n",
      "    Done in 6.8 seconds. Result currently has 3,798,877 rows.\n",
      "[5/6] Merging value column 'dred' ...\n",
      "    Done in 6.7 seconds. Result currently has 3,798,877 rows.\n",
      "[6/6] Merging value column 'at' ...\n",
      "    Done in 8.9 seconds. Result currently has 3,798,877 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 369,256\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - socaps: 2,030,125 empty values\n",
      "  - pocaps: 2,174,639 empty values\n",
      "  - div: 1,557,166 empty values\n",
      "  - diss: 2,312,633 empty values\n",
      "  - dred: 2,201,996 empty values\n",
      "  - at: 1,444,852 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Txf.txt\n",
      "Total runtime: 85.3 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "# Folder where all input files are stored and where the output will be written.\n",
    "# Temp_file_path_A must already be defined in your notebook, for example:\n",
    "# Temp_file_path_A = f\"{BASE_PATH}/Temp/TempAnomalies\"\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Field separator in all input and output text files\n",
    "SEP = \"|\"\n",
    "\n",
    "# Input files (1..N). They are all expected to be located in OUTPUT_DIR.\n",
    "INPUT_FILES = [\n",
    "    \"Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt\",\n",
    "    \"Com_Pfd_Redeemed_Retired_Converted_Etc..txt\",\n",
    "    \"Cash_Dividends_Paid___Total.txt\",\n",
    "    \"Long_Term_Borrowings.txt\",\n",
    "    \"Reduction_in_Long_Term_Debt.txt\",\n",
    "    \"Total_Assets.txt\",\n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "# Names for the value columns corresponding to INPUT_FILES (same length as INPUT_FILES)\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"socaps\",\n",
    "    \"pocaps\",\n",
    "    \"div\",\n",
    "    \"diss\",\n",
    "    \"dred\",\n",
    "    \"at\",\n",
    "\n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "# Name of the final output file (will be written to OUTPUT_DIR)\n",
    "OUTPUT_FILE = \"data_Txf.txt\"\n",
    "\n",
    "# Column names in the input files (assumed to be identical in all files)\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9OgqUzrIkpt"
   },
   "source": [
    "## Calculate \"In-between-variables\" from Data (e.g. Working Capital for Acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSJg6cgGIvA1"
   },
   "source": [
    "### Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141,
     "status": "ok",
     "timestamp": 1765651726983,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "3g0GmNy9IuEB",
    "outputId": "6404e13f-7f9c-4e27-99b7-da91b47a9d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'wc' created based on formula:\n",
      "    ca - cce - cl + std + itp - da\n",
      "Rows used (all required columns non-null): 632091\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Acc.txt\n",
      "            ca         cce          cl         std        itp         da  \\\n",
      "0   748.140365  167.849993  663.515421   54.137880  10.582409  38.094417   \n",
      "1          NaN         NaN         NaN         NaN        NaN  38.094417   \n",
      "2   864.906741  320.816779  693.189519   31.500444  12.508193  68.245694   \n",
      "3          NaN         NaN         NaN         NaN        NaN  68.245694   \n",
      "4   975.879591  167.728265  773.728435  104.964605   8.751102  89.546780   \n",
      "5          NaN         NaN         NaN         NaN        NaN  89.546780   \n",
      "6   762.389782  110.705844  852.251409  364.770602   9.041866  98.504938   \n",
      "7          NaN         NaN         NaN         NaN        NaN  98.504938   \n",
      "8   665.184633   70.212302  841.844876  293.363860   9.454759  84.007512   \n",
      "9          NaN         NaN         NaN         NaN        NaN  84.007512   \n",
      "10  615.713580   40.206606  876.621681  354.393108   6.770931  76.498936   \n",
      "11         NaN         NaN         NaN         NaN        NaN  76.498936   \n",
      "12         NaN         NaN         NaN         NaN        NaN  63.755268   \n",
      "13  486.157681   45.182074  978.028694  427.774122   0.608464  54.829899   \n",
      "14         NaN         NaN         NaN         NaN        NaN  54.829899   \n",
      "15  433.210510   18.238665  777.957861  196.429136        NaN  85.965301   \n",
      "16  433.210510   18.238665  777.957861  196.429136   4.638663  57.277725   \n",
      "17         NaN         NaN         NaN         NaN        NaN  85.965301   \n",
      "18         NaN         NaN         NaN         NaN        NaN  57.277725   \n",
      "19    0.031517         NaN    0.019879         NaN        NaN        NaN   \n",
      "\n",
      "            wc  \n",
      "0   -56.599177  \n",
      "1          NaN  \n",
      "2  -173.336614  \n",
      "3          NaN  \n",
      "4    58.591818  \n",
      "5          NaN  \n",
      "6    74.740059  \n",
      "7          NaN  \n",
      "8   -28.061438  \n",
      "9          NaN  \n",
      "10  -16.449604  \n",
      "11         NaN  \n",
      "12         NaN  \n",
      "13 -163.500400  \n",
      "14         NaN  \n",
      "15         NaN  \n",
      "16 -219.195942  \n",
      "17         NaN  \n",
      "18         NaN  \n",
      "19         NaN  \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Acc.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"wc\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"ca - cce - cl + std + itp - da\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"ca\", \"cce\", \"cl\", \"std\", \"itp\", \"da\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n40a8yHIVMR6"
   },
   "source": [
    "### At"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spmfdwMZVT0K"
   },
   "source": [
    "#### Oa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1765651727089,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "R4LySaHEVT0L",
    "outputId": "6589abd8-6352-41f5-d71d-12ea94747ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'oa' created based on formula:\n",
      "    at - cce\n",
      "Rows used (all required columns non-null): 2474750\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_At.txt\n",
      "             at         cce           oa\n",
      "0   1084.355949  167.849993   916.505956\n",
      "1           NaN         NaN          NaN\n",
      "2   1267.313578  320.816779   946.496799\n",
      "3           NaN         NaN          NaN\n",
      "4   1460.696865  167.728265  1292.968600\n",
      "5           NaN         NaN          NaN\n",
      "6   1304.537890  110.705844  1193.832046\n",
      "7           NaN         NaN          NaN\n",
      "8   1265.820386   70.212302  1195.608084\n",
      "9           NaN         NaN          NaN\n",
      "10  1225.163797   40.206606  1184.957191\n",
      "11          NaN         NaN          NaN\n",
      "12  1102.461138   45.182074  1057.279064\n",
      "13          NaN         NaN          NaN\n",
      "14   945.993798   18.238665   927.755133\n",
      "15          NaN         NaN          NaN\n",
      "16          NaN         NaN          NaN\n",
      "17          NaN         NaN          NaN\n",
      "18          NaN         NaN          NaN\n",
      "19    23.439270    0.346747    23.092523\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_At.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"oa\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"at - cce\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"at\", \"cce\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pjty6RWLVT0N"
   },
   "source": [
    "#### Ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1765651727440,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "zJTNtnw4VT0O",
    "outputId": "4bc3f768-c59e-419b-fbf9-6f2ed629f18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'ol' created based on formula:\n",
      "    at - ltd - mi - ps - ce\n",
      "Rows used (all required columns non-null): 1811546\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_At.txt\n",
      "             at         ltd        mi   ps          ce          ol\n",
      "0   1084.355949   10.096319  0.000000  0.0  378.411011  695.848619\n",
      "1           NaN         NaN       NaN  NaN         NaN         NaN\n",
      "2   1267.313578  106.116741  0.117886  0.0  434.756779  726.322172\n",
      "3           NaN         NaN       NaN  NaN         NaN         NaN\n",
      "4   1460.696865  130.770231       NaN  0.0  518.204722         NaN\n",
      "5           NaN         NaN       NaN  NaN         NaN         NaN\n",
      "6   1304.537890   32.376442       NaN  0.0  386.142780         NaN\n",
      "7           NaN         NaN       NaN  NaN         NaN         NaN\n",
      "8   1265.820386    0.124015       NaN  0.0  387.109014         NaN\n",
      "9           NaN         NaN       NaN  NaN         NaN         NaN\n",
      "10  1225.163797   27.130200       NaN  0.0  292.572607         NaN\n",
      "11          NaN         NaN       NaN  NaN         NaN         NaN\n",
      "12  1102.461138   18.295417       NaN  0.0   72.674506         NaN\n",
      "13          NaN         NaN       NaN  NaN         NaN         NaN\n",
      "14   945.993798   15.555081  0.000000  0.0  112.352977  818.085740\n",
      "15          NaN         NaN       NaN  NaN         NaN         NaN\n",
      "16          NaN         NaN       NaN  NaN         NaN         NaN\n",
      "17          NaN    0.042286  0.000000  NaN    0.129111         NaN\n",
      "18          NaN    0.167289  0.000000  NaN    0.613125         NaN\n",
      "19    23.439270    3.415455  0.000000  NaN   16.638718         NaN\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_At.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"ol\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"at - ltd - mi - ps - ce\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"at\", \"ltd\", \"mi\", \"ps\", \"ce\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peLARLfKV7Ve"
   },
   "source": [
    "#### Noa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1765651727889,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "IrB-BuDdV7Vf",
    "outputId": "dac0bf3e-a175-472d-e8ec-0b92f877f259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'noa' created based on formula:\n",
      "    oa - ol\n",
      "Rows used (all required columns non-null): 1676871\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_At.txt\n",
      "             oa          ol         noa\n",
      "0    916.505956  695.848619  220.657337\n",
      "1           NaN         NaN         NaN\n",
      "2    946.496799  726.322172  220.174627\n",
      "3           NaN         NaN         NaN\n",
      "4   1292.968600         NaN         NaN\n",
      "5           NaN         NaN         NaN\n",
      "6   1193.832046         NaN         NaN\n",
      "7           NaN         NaN         NaN\n",
      "8   1195.608084         NaN         NaN\n",
      "9           NaN         NaN         NaN\n",
      "10  1184.957191         NaN         NaN\n",
      "11          NaN         NaN         NaN\n",
      "12  1057.279064         NaN         NaN\n",
      "13          NaN         NaN         NaN\n",
      "14   927.755133  818.085740  109.669393\n",
      "15          NaN         NaN         NaN\n",
      "16          NaN         NaN         NaN\n",
      "17          NaN         NaN         NaN\n",
      "18          NaN         NaN         NaN\n",
      "19    23.092523         NaN         NaN\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_At.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"noa\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"oa - ol\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"oa\", \"ol\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cdh6V_CXYjy"
   },
   "source": [
    "### Cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VV_GHlnXali"
   },
   "source": [
    "#### Oa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1765651728295,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "xlnVW4q3Xalk",
    "outputId": "b97d0d3a-4a0f-4168-fe41-d14b8cba29ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'oa' created based on formula:\n",
      "    at - cce\n",
      "Rows used (all required columns non-null): 2474750\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Cat.txt\n",
      "             at         cce           oa\n",
      "0   1084.355949  167.849993   916.505956\n",
      "1           NaN         NaN          NaN\n",
      "2   1267.313578  320.816779   946.496799\n",
      "3           NaN         NaN          NaN\n",
      "4   1460.696865  167.728265  1292.968600\n",
      "5           NaN         NaN          NaN\n",
      "6   1304.537890  110.705844  1193.832046\n",
      "7           NaN         NaN          NaN\n",
      "8   1265.820386   70.212302  1195.608084\n",
      "9           NaN         NaN          NaN\n",
      "10  1225.163797   40.206606  1184.957191\n",
      "11          NaN         NaN          NaN\n",
      "12  1102.461138   45.182074  1057.279064\n",
      "13          NaN         NaN          NaN\n",
      "14   945.993798   18.238665   927.755133\n",
      "15          NaN         NaN          NaN\n",
      "16          NaN         NaN          NaN\n",
      "17          NaN         NaN          NaN\n",
      "18          NaN         NaN          NaN\n",
      "19    23.439270    0.346747    23.092523\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Cat.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"oa\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"at - cce\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"at\", \"cce\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oujAtKVjXall"
   },
   "source": [
    "#### Ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1765651728729,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Ky1wK0FdXall",
    "outputId": "eb1f90ab-2d29-4d14-9e5d-809a0ea0c1a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'ol' created based on formula:\n",
      "    at - ltd - mi - ps - ce\n",
      "Rows used (all required columns non-null): 1811546\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Cat.txt\n",
      "             at         ltd        mi   ps          ce          ol\n",
      "0   1084.355949   10.096319  0.000000  0.0  378.411011  695.848619\n",
      "1           NaN         NaN       NaN  NaN         NaN         NaN\n",
      "2   1267.313578  106.116741  0.117886  0.0  434.756779  726.322172\n",
      "3           NaN         NaN       NaN  NaN         NaN         NaN\n",
      "4   1460.696865  130.770231       NaN  0.0  518.204722         NaN\n",
      "5           NaN         NaN       NaN  NaN         NaN         NaN\n",
      "6   1304.537890   32.376442       NaN  0.0  386.142780         NaN\n",
      "7           NaN         NaN       NaN  NaN         NaN         NaN\n",
      "8   1265.820386    0.124015       NaN  0.0  387.109014         NaN\n",
      "9           NaN         NaN       NaN  NaN         NaN         NaN\n",
      "10  1225.163797   27.130200       NaN  0.0  292.572607         NaN\n",
      "11          NaN         NaN       NaN  NaN         NaN         NaN\n",
      "12  1102.461138   18.295417       NaN  0.0   72.674506         NaN\n",
      "13          NaN         NaN       NaN  NaN         NaN         NaN\n",
      "14   945.993798   15.555081  0.000000  0.0  112.352977  818.085740\n",
      "15          NaN         NaN       NaN  NaN         NaN         NaN\n",
      "16          NaN         NaN       NaN  NaN         NaN         NaN\n",
      "17          NaN    0.042286  0.000000  NaN    0.129111         NaN\n",
      "18          NaN    0.167289  0.000000  NaN    0.613125         NaN\n",
      "19    23.439270    3.415455  0.000000  NaN   16.638718         NaN\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Cat.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"ol\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"at - ltd - mi - ps - ce\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"at\", \"ltd\", \"mi\", \"ps\", \"ce\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4KzZV73Xalm"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7t777qpXalm"
   },
   "source": [
    "#### Noa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1765651729239,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "G5jzsKpqXalm",
    "outputId": "456de58a-c63c-4747-a350-a3aee8a1792b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'noa' created based on formula:\n",
      "    oa - ol\n",
      "Rows used (all required columns non-null): 1676871\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Cat.txt\n",
      "             oa          ol         noa\n",
      "0    916.505956  695.848619  220.657337\n",
      "1           NaN         NaN         NaN\n",
      "2    946.496799  726.322172  220.174627\n",
      "3           NaN         NaN         NaN\n",
      "4   1292.968600         NaN         NaN\n",
      "5           NaN         NaN         NaN\n",
      "6   1193.832046         NaN         NaN\n",
      "7           NaN         NaN         NaN\n",
      "8   1195.608084         NaN         NaN\n",
      "9           NaN         NaN         NaN\n",
      "10  1184.957191         NaN         NaN\n",
      "11          NaN         NaN         NaN\n",
      "12  1057.279064         NaN         NaN\n",
      "13          NaN         NaN         NaN\n",
      "14   927.755133  818.085740  109.669393\n",
      "15          NaN         NaN         NaN\n",
      "16          NaN         NaN         NaN\n",
      "17          NaN         NaN         NaN\n",
      "18          NaN         NaN         NaN\n",
      "19    23.092523         NaN         NaN\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Cat.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"noa\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"oa - ol\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"oa\", \"ol\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIT0NremXpir"
   },
   "source": [
    "### Cpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1765651729389,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "kmlhUdSwXsdN",
    "outputId": "a541621e-b2c6-4e64-cdf2-daac1759f4eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'pm' created based on formula:\n",
      "    (rev - cogs) / rev\n",
      "Rows used (all required columns non-null): 1613228\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Cpm.txt\n",
      "            rev         cogs        pm\n",
      "0   1707.334371  1415.675687  0.170827\n",
      "1   1921.517802  1621.318491  0.156230\n",
      "2   2263.705199  1968.910031  0.130227\n",
      "3   1447.331160  1270.982588  0.121844\n",
      "4   1217.952697  1007.403728  0.172871\n",
      "5    693.936079   578.488211  0.166367\n",
      "6    693.936079   512.071639  0.262077\n",
      "7    609.279252   453.554285  0.255589\n",
      "8    559.817319          NaN       NaN\n",
      "9    501.495166   294.514572  0.412727\n",
      "10          NaN     0.056176       NaN\n",
      "11          NaN     0.339360       NaN\n",
      "12     8.063120     6.484731  0.195754\n",
      "13   212.478635   197.252834  0.071658\n",
      "14   341.614996   330.298911  0.033125\n",
      "15   466.963321   343.942030  0.263450\n",
      "16   473.429402   353.821975  0.252640\n",
      "17   528.266494   392.873539  0.256297\n",
      "18   590.330351   435.289043  0.262635\n",
      "19   567.494357   429.207531  0.243680\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Cpm.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"pm\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(rev - cogs) / rev\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"rev\", \"cogs\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbGeEnwfUl7y"
   },
   "source": [
    "### Inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1765651729493,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ZmCnTuYRUpE7",
    "outputId": "f7e87b61-ba38-445c-9668-fab76f880e86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'ce' created based on formula:\n",
      "    capex / rev\n",
      "Rows used (all required columns non-null): 1260123\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Inv.txt\n",
      "         capex          rev        ce\n",
      "0    24.714956  1707.334371  0.014476\n",
      "1    63.338554  1921.517802  0.032963\n",
      "2    85.214171  2263.705199  0.037644\n",
      "3   106.865080  1447.331160  0.073836\n",
      "4    53.811107  1217.952697  0.044182\n",
      "5    45.147594   693.936079  0.065060\n",
      "6    46.740995   609.279252  0.076715\n",
      "7          NaN   559.817319       NaN\n",
      "8    23.600276   501.495166  0.047060\n",
      "9          NaN     8.063120       NaN\n",
      "10         NaN   212.478635       NaN\n",
      "11    2.984188   341.614996  0.008736\n",
      "12   12.138323   466.963321  0.025994\n",
      "13   21.196735   473.429402  0.044773\n",
      "14   17.974979   528.266494  0.034026\n",
      "15   37.822982   590.330351  0.064071\n",
      "16   24.328241   567.494357  0.042870\n",
      "17   33.391428   600.805773  0.055578\n",
      "18   41.963815   672.248976  0.062423\n",
      "19   41.963815   672.248976  0.062423\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Inv.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"ce\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"capex / rev\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"capex\", \"rev\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10h4B-beVKZi"
   },
   "source": [
    "### Ltg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-t5PG_BADyJa"
   },
   "source": [
    "#### Noa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 917,
     "status": "ok",
     "timestamp": 1765651730411,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "iRD7KIS2VN1h",
    "outputId": "4a60d0d3-67f6-4c46-af00-4263b9f3aa99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'noa' created based on formula:\n",
      "    (ar + inv + oca + ppe + oa - ap - ocl - ol) / at\n",
      "Rows used (all required columns non-null): 1111788\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Ltg.txt\n",
      "            ar         inv        oca         ppe         oa          ap  \\\n",
      "0   341.028066  237.409600   1.852706  247.049311  34.493544  380.421036   \n",
      "1          NaN         NaN        NaN         NaN        NaN         NaN   \n",
      "2   329.248663  201.182443  13.658856  334.155739  45.444753  369.240530   \n",
      "3          NaN         NaN        NaN         NaN        NaN         NaN   \n",
      "4   490.270995  289.925405  27.954926  391.012073  59.092809  401.581395   \n",
      "5          NaN         NaN        NaN         NaN        NaN         NaN   \n",
      "6   390.538840  223.628192  37.516906  466.898780  27.082113  228.545754   \n",
      "7          NaN         NaN        NaN         NaN        NaN         NaN   \n",
      "8   377.209917  177.096926  40.665488  418.603235  57.544877  264.087772   \n",
      "9          NaN         NaN        NaN         NaN        NaN         NaN   \n",
      "10  361.610866  173.896930  39.999178  462.791818  53.211953  218.467531   \n",
      "11         NaN         NaN        NaN         NaN        NaN         NaN   \n",
      "12         NaN         NaN        NaN         NaN        NaN         NaN   \n",
      "13  236.148348  204.671283   0.155976  446.735511  56.178105  334.376797   \n",
      "14         NaN         NaN        NaN         NaN        NaN         NaN   \n",
      "15         NaN         NaN        NaN  401.654420        NaN         NaN   \n",
      "16  251.819555  158.117346   5.034944  401.654420  33.483882  384.499236   \n",
      "17         NaN         NaN        NaN         NaN        NaN         NaN   \n",
      "18         NaN         NaN        NaN         NaN        NaN         NaN   \n",
      "19         NaN    0.020612        NaN    0.158328        NaN         NaN   \n",
      "\n",
      "           ocl        ol           at       noa  \n",
      "0   192.428709  0.018000  1084.355949  0.266486  \n",
      "1          NaN       NaN          NaN       NaN  \n",
      "2   249.834966  0.018000  1267.313578  0.240349  \n",
      "3          NaN       NaN          NaN       NaN  \n",
      "4   240.967714  0.018000  1460.696865  0.421504  \n",
      "5          NaN       NaN          NaN       NaN  \n",
      "6   236.361064  1.735377  1304.537890  0.520508  \n",
      "7          NaN       NaN          NaN       NaN  \n",
      "8   263.192178  3.187839  1265.820386  0.427116  \n",
      "9          NaN       NaN          NaN       NaN  \n",
      "10  286.822744  0.347800  1225.163797  0.478199  \n",
      "11         NaN       NaN          NaN       NaN  \n",
      "12         NaN       NaN          NaN       NaN  \n",
      "13  207.046239  0.273000  1102.461138  0.364814  \n",
      "14         NaN       NaN          NaN       NaN  \n",
      "15         NaN       NaN   945.993798       NaN  \n",
      "16  180.754431  0.123672   945.993798  0.300988  \n",
      "17         NaN       NaN          NaN       NaN  \n",
      "18         NaN       NaN          NaN       NaN  \n",
      "19         NaN       NaN          NaN       NaN  \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Ltg.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"noa\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(ar + inv + oca + ppe + oa - ap - ocl - ol) / at\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"ar\", \"inv\", \"oca\", \"ppe\", \"oa\", \"ap\", \"ocl\", \"ol\", \"at\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qW9qTRmDD1px"
   },
   "source": [
    "#### Wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1765651730751,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "6B6N5tCPDcRX",
    "outputId": "b027817f-f956-4219-b804-c9cc29e43cfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'wc' created based on formula:\n",
      "    ca - cce - cl + std + itp - da\n",
      "Rows used (all required columns non-null): 805508\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Ltg.txt\n",
      "            ca         cce          cl         std        itp         da  \\\n",
      "0   748.140365  167.849993  663.515421   54.137880  10.582409  38.094417   \n",
      "1          NaN         NaN         NaN         NaN        NaN  38.094417   \n",
      "2   864.906741  320.816779  693.189519   31.500444  12.508193  68.245694   \n",
      "3          NaN         NaN         NaN         NaN        NaN  68.245694   \n",
      "4   975.879591  167.728265  773.728435  104.964605   8.751102  89.546780   \n",
      "5          NaN         NaN         NaN         NaN        NaN  89.546780   \n",
      "6   762.389782  110.705844  852.251409  364.770602   9.041866  98.504938   \n",
      "7          NaN         NaN         NaN         NaN        NaN  98.504938   \n",
      "8   665.184633   70.212302  841.844876  293.363860   9.454759  84.007512   \n",
      "9          NaN         NaN         NaN         NaN        NaN  84.007512   \n",
      "10  615.713580   40.206606  876.621681  354.393108   6.770931  76.498936   \n",
      "11         NaN         NaN         NaN         NaN        NaN  76.498936   \n",
      "12         NaN         NaN         NaN         NaN        NaN  63.755268   \n",
      "13  486.157681   45.182074  978.028694  427.774122   0.608464  54.829899   \n",
      "14         NaN         NaN         NaN         NaN        NaN  54.829899   \n",
      "15  433.210510   18.238665  777.957861  196.429136        NaN  85.965301   \n",
      "16  433.210510   18.238665  777.957861  196.429136   4.638663  57.277725   \n",
      "17         NaN         NaN         NaN         NaN        NaN  85.965301   \n",
      "18         NaN         NaN         NaN         NaN        NaN  57.277725   \n",
      "19    0.031517         NaN    0.019879         NaN        NaN        NaN   \n",
      "\n",
      "            wc  \n",
      "0   -56.599177  \n",
      "1          NaN  \n",
      "2  -173.336614  \n",
      "3          NaN  \n",
      "4    58.591818  \n",
      "5          NaN  \n",
      "6    74.740059  \n",
      "7          NaN  \n",
      "8   -28.061438  \n",
      "9          NaN  \n",
      "10  -16.449604  \n",
      "11         NaN  \n",
      "12         NaN  \n",
      "13 -163.500400  \n",
      "14         NaN  \n",
      "15         NaN  \n",
      "16 -219.195942  \n",
      "17         NaN  \n",
      "18         NaN  \n",
      "19         NaN  \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Ltg.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"wc\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"ca - cce - cl + std + itp - da\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"ca\", \"cce\", \"cl\", \"std\", \"itp\", \"da\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8duQiheEBE8"
   },
   "source": [
    "### Nca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bb7Y8QOFOvp"
   },
   "source": [
    "#### Ivao (Special Case for Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1765651731054,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "nvrEHFdPEDFh",
    "outputId": "a87cb5dd-06da-42b4-c6b9-fb0a4af34991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'ivao' created based on formula (NaN treated as 0):\n",
      "    IVAO_oi + IVAO_iac + IVAO_ltr + IVAO_isdfl + IVAO_uol\n",
      "Rows processed: 3108973\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Nca.txt\n",
      "      IVAO_oi   IVAO_iac   IVAO_ltr  IVAO_isdfl  IVAO_uol        ivao\n",
      "0         NaN  52.290086   2.382643         NaN       NaN   54.672729\n",
      "1         NaN  12.353737  10.452608         NaN       NaN   22.806345\n",
      "2         NaN  15.324648  19.387744         NaN       NaN   34.712392\n",
      "3         NaN  14.287399  33.879816         NaN       NaN   48.167215\n",
      "4         NaN  74.116172  50.371469         NaN       NaN  124.487641\n",
      "5         NaN  65.308715  28.137731         NaN       NaN   93.446446\n",
      "6         NaN  73.291530  40.098311         NaN       NaN  113.389841\n",
      "7         NaN        NaN        NaN         NaN       NaN    0.000000\n",
      "8         NaN  43.957836  33.687150         NaN       NaN   77.644986\n",
      "9         NaN   0.000000        NaN         NaN       NaN    0.000000\n",
      "10   0.026140        NaN        NaN         NaN       NaN    0.026140\n",
      "11   0.826578        NaN        NaN         NaN       NaN    0.826578\n",
      "12  38.509874        NaN        NaN         NaN       NaN   38.509874\n",
      "13   0.530348   0.140177   2.089627         NaN       NaN    2.760152\n",
      "14   1.224187   5.351364   3.331637         NaN       NaN    9.907188\n",
      "15   0.428115  16.926300  56.782391         NaN       NaN   74.136806\n",
      "16   0.049467  31.435162  80.682234         NaN       NaN  112.166863\n",
      "17        NaN  15.319469  86.839367         NaN       NaN  102.158836\n",
      "18        NaN  10.768647  73.298818         NaN       NaN   84.067465\n",
      "19        NaN  16.840073  86.570489         NaN       NaN  103.410562\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Nca.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"ivao\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"IVAO_oi + IVAO_iac + IVAO_ltr + IVAO_isdfl + IVAO_uol\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"IVAO_oi\", \"IVAO_iac\", \"IVAO_ltr\", \"IVAO_isdfl\", \"IVAO_uol\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) COPY DF AND APPLY FORMULA USING 0 FOR NaN VALUES\n",
    "df_out = df.copy()\n",
    "\n",
    "# Fill only the necessary columns with 0 for the formula evaluation\n",
    "df_filled = df_out[FORMULA_COLUMNS].fillna(0)\n",
    "\n",
    "# Evaluate formula on df_filled\n",
    "df_out[NEW_COLUMN] = df_filled.eval(FORMULA_EXPRESSION)\n",
    "\n",
    "# 4) SAVE RESULT AS \"<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "output_path = OUTPUT_DIR / INPUT_FILE\n",
    "df_out.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula (NaN treated as 0):\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows processed:\", len(df_out))\n",
    "print(\"Result was saved to:\", output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX2o4ZVhGOe0"
   },
   "source": [
    "#### Oa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1765651731304,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "TCfyuqtRGR7m",
    "outputId": "ca7857da-32e8-4dcf-91ce-e08af5f7e268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'oa' created based on formula:\n",
      "    at - ca - ivao - lt + std + ltd\n",
      "Rows used (all required columns non-null): 2093162\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Nca.txt\n",
      "             at          ca        ivao           lt         std         ltd  \\\n",
      "0   1084.355949  748.140365   54.672729   705.944938   54.137880   10.096319   \n",
      "1   1267.313578  864.906741   22.806345   832.438913   31.500444  106.116741   \n",
      "2   1460.696865  975.879591   34.712392   942.485069  104.964605  130.770231   \n",
      "3   1304.537890  762.389782   48.167215   918.389404  364.770602   32.376442   \n",
      "4   1265.820386  665.184633  124.487641   878.706712  293.363860    0.124015   \n",
      "5   1225.163797  615.713580   93.446446   932.585018  354.393108   27.130200   \n",
      "6   1102.461138  486.157681  113.389841  1029.779408  427.774122   18.295417   \n",
      "7    945.993798  433.210510    0.000000   833.640821  196.429136   15.555081   \n",
      "8    945.993798  433.210510   77.644986   833.640821  196.429136   15.555081   \n",
      "9           NaN    0.031517    0.000000          NaN         NaN    0.042286   \n",
      "10          NaN    0.148791    0.026140          NaN         NaN    0.167289   \n",
      "11    23.439270    4.175760    0.826578     6.796352         NaN    3.415455   \n",
      "12   577.207234   75.003871   38.509874   139.273759         NaN   24.998296   \n",
      "13  1132.139668  162.276657    2.760152   320.623511  102.396391   23.286474   \n",
      "14  1242.711417  209.654221    9.907188   341.285700  151.992380   56.854378   \n",
      "15  1373.608080  239.874932   74.136806   448.859548  240.549808   65.839488   \n",
      "16  1263.968574  226.951206  112.166863   519.914576  153.283517  201.028979   \n",
      "17  1301.804038  222.785496  102.158836   503.563744  110.829143  192.923545   \n",
      "18   820.980063  183.657569   84.067465   500.183635   65.353735  202.169905   \n",
      "19   903.394131  259.163378  103.410562   552.839185   33.125284  306.999810   \n",
      "\n",
      "            oa  \n",
      "0  -360.167884  \n",
      "1  -315.221236  \n",
      "2  -256.645351  \n",
      "3   -27.261467  \n",
      "4  -109.070725  \n",
      "5   -35.057939  \n",
      "6   -80.796253  \n",
      "7  -108.873316  \n",
      "8  -186.518302  \n",
      "9          NaN  \n",
      "10         NaN  \n",
      "11         NaN  \n",
      "12         NaN  \n",
      "13  772.162213  \n",
      "14  890.711066  \n",
      "15  917.126090  \n",
      "16  759.248425  \n",
      "17  777.048650  \n",
      "18  320.595034  \n",
      "19  328.106100  \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Nca.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"oa\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"at - ca - ivao - lt + std + ltd\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"at\", \"ca\", \"ivao\", \"lt\", \"std\", \"ltd\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI6pdBLLMA3R"
   },
   "source": [
    "### Noa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0wBZt9PMIM1"
   },
   "source": [
    "#### Oa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1765651731624,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "He91v6qDMJ24",
    "outputId": "6e34c80d-f784-4c39-c352-a97c3965ef1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'oa' created based on formula:\n",
      "    at - cce\n",
      "Rows used (all required columns non-null): 2474750\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Noa.txt\n",
      "             at         cce           oa\n",
      "0   1084.355949  167.849993   916.505956\n",
      "1   1267.313578  320.816779   946.496799\n",
      "2   1460.696865  167.728265  1292.968600\n",
      "3   1304.537890  110.705844  1193.832046\n",
      "4   1265.820386   70.212302  1195.608084\n",
      "5   1225.163797   40.206606  1184.957191\n",
      "6   1102.461138   45.182074  1057.279064\n",
      "7    945.993798   18.238665   927.755133\n",
      "8           NaN         NaN          NaN\n",
      "9           NaN         NaN          NaN\n",
      "10    23.439270    0.346747    23.092523\n",
      "11   577.207234    6.375740   570.831494\n",
      "12  1132.139668   14.410069  1117.729599\n",
      "13  1242.711417   24.837275  1217.874142\n",
      "14  1373.608080   36.502948  1337.105132\n",
      "15  1263.968574   30.218984  1233.749590\n",
      "16  1301.804038   26.510843  1275.293195\n",
      "17   820.980063   11.388751   809.591312\n",
      "18   903.394131   29.985163   873.408968\n",
      "19   996.702454   45.541110   951.161344\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Noa.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"oa\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"at - cce\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"at\", \"cce\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyBv0xKBMX6k"
   },
   "source": [
    "#### Ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1765651731821,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "mfQtypARMbEW",
    "outputId": "b3e26624-756e-4484-da49-b4e11d7845d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'ol' created based on formula:\n",
      "    at - ltd - mi - ps - ce\n",
      "Rows used (all required columns non-null): 1811546\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Noa.txt\n",
      "             at         ltd        mi        ps          ce          ol\n",
      "0   1084.355949   10.096319  0.000000  0.000000  378.411011  695.848619\n",
      "1   1267.313578  106.116741  0.117886  0.000000  434.756779  726.322172\n",
      "2   1460.696865  130.770231       NaN  0.000000  518.204722         NaN\n",
      "3   1304.537890   32.376442       NaN  0.000000  386.142780         NaN\n",
      "4   1265.820386    0.124015       NaN  0.000000  387.109014         NaN\n",
      "5   1225.163797   27.130200       NaN  0.000000  292.572607         NaN\n",
      "6   1102.461138   18.295417       NaN  0.000000   72.674506         NaN\n",
      "7    945.993798   15.555081  0.000000  0.000000  112.352977  818.085740\n",
      "8           NaN    0.042286  0.000000       NaN    0.129111         NaN\n",
      "9           NaN    0.167289  0.000000       NaN    0.613125         NaN\n",
      "10    23.439270    3.415455  0.000000       NaN   16.638718         NaN\n",
      "11   577.207234   24.998296  0.000000  0.087964  437.845511  114.275464\n",
      "12  1132.139668   23.286474  5.817507  0.293958  805.404692  297.337037\n",
      "13  1242.711417   56.854378  2.122657  0.000000  899.303060  284.431322\n",
      "14  1373.608080   65.839488  4.194940  0.000000  920.553592  383.020060\n",
      "15  1263.968574  201.028979  1.672413  0.000000  742.381585  318.885597\n",
      "16  1301.804038  192.923545  1.653697  0.000000  796.586597  310.640199\n",
      "17   820.980063  202.169905  0.227350  0.000000  320.569078  298.013730\n",
      "18   903.394131  306.999810  0.335268  0.000000  350.219678  245.839375\n",
      "19   996.702454  229.782423  0.366367  0.000000  419.494337  347.059327\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Noa.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"ol\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"at - ltd - mi - ps - ce\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"at\", \"ltd\", \"mi\", \"ps\", \"ce\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRh0r7RtVsBu"
   },
   "source": [
    "#### Noa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1765651732114,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Ug4m3ux7Vufw",
    "outputId": "287527a0-15cd-4202-ceb7-dfdabee669d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'noa' created based on formula:\n",
      "    oa - ol\n",
      "Rows used (all required columns non-null): 1676871\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Noa.txt\n",
      "             oa          ol         noa\n",
      "0    916.505956  695.848619  220.657337\n",
      "1    946.496799  726.322172  220.174627\n",
      "2   1292.968600         NaN         NaN\n",
      "3   1193.832046         NaN         NaN\n",
      "4   1195.608084         NaN         NaN\n",
      "5   1184.957191         NaN         NaN\n",
      "6   1057.279064         NaN         NaN\n",
      "7    927.755133  818.085740  109.669393\n",
      "8           NaN         NaN         NaN\n",
      "9           NaN         NaN         NaN\n",
      "10    23.092523         NaN         NaN\n",
      "11   570.831494  114.275464  456.556030\n",
      "12  1117.729599  297.337037  820.392562\n",
      "13  1217.874142  284.431322  933.442820\n",
      "14  1337.105132  383.020060  954.085072\n",
      "15  1233.749590  318.885597  914.863993\n",
      "16  1275.293195  310.640199  964.652996\n",
      "17   809.591312  298.013730  511.577582\n",
      "18   873.408968  245.839375  627.569593\n",
      "19   951.161344  347.059327  604.102017\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Noa.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"noa\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"oa - ol\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"oa\", \"ol\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaGLNDHINGeX"
   },
   "source": [
    "### Nwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1765651732370,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "t1w_aEbGNIxF",
    "outputId": "392597c2-34dc-45d3-d71a-331cb23cae12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'nwc' created based on formula:\n",
      "    ca - cce - cl + std\n",
      "Rows used (all required columns non-null): 1923094\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Nwc.txt\n",
      "            ca         cce          cl         std         nwc\n",
      "0   748.140365  167.849993  663.515421   54.137880  -29.087169\n",
      "1   864.906741  320.816779  693.189519   31.500444 -117.599113\n",
      "2   975.879591  167.728265  773.728435  104.964605  139.387496\n",
      "3   762.389782  110.705844  852.251409  364.770602  164.203131\n",
      "4   665.184633   70.212302  841.844876  293.363860   46.491315\n",
      "5   615.713580   40.206606  876.621681  354.393108   53.278401\n",
      "6   486.157681   45.182074  978.028694  427.774122 -109.278965\n",
      "7   433.210510   18.238665  777.957861  196.429136 -166.556880\n",
      "8     0.031517         NaN    0.019879         NaN         NaN\n",
      "9     0.148791         NaN    0.136090         NaN         NaN\n",
      "10    4.175760    0.346747    3.380897         NaN         NaN\n",
      "11   75.003871    6.375740  114.275464         NaN         NaN\n",
      "12  162.276657   14.410069  265.161797  102.396391  -14.898818\n",
      "13  209.654221   24.837275  262.015290  151.992380   74.794036\n",
      "14  239.874932   36.502948  346.248448  240.549808   97.673344\n",
      "15  226.951206   30.218984  290.910245  153.283517   59.105494\n",
      "16  222.785496   26.510843  284.860052  110.829143   22.243744\n",
      "17  183.657569   11.388751  249.667381   65.353735  -12.044828\n",
      "18  259.163378   29.985163  216.087419   33.125284   46.216080\n",
      "19  325.819881   45.541110  306.730251  125.413219   98.961739\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Nwc.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"nwc\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"ca - cce - cl + std\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"ca\", \"cce\", \"cl\", \"std\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxk_1LFLPSDn"
   },
   "source": [
    "### Osc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Itq-w3b3PYBM"
   },
   "source": [
    "#### Ifo (Special Case for Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1765651732783,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Uavv5M8PPZz7",
    "outputId": "ced004c3-d841-49da-d0a5-495fca8dddda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'ifo' created based on formula (NaN treated as 0):\n",
      "    IFO_ffo + IFO_ei + IFO_dofa + IFO_ffooa\n",
      "Rows processed: 4406065\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Osc.txt\n",
      "       IFO_ffo     IFO_ei  IFO_dofa   IFO_ffooa         ifo\n",
      "0    70.253522   0.000000       0.0  -87.320244  -17.066722\n",
      "1    70.253522   0.000000       0.0  -87.320244  -17.066722\n",
      "2   240.925386  -8.619558       0.0  200.312706  432.618534\n",
      "3   240.925386  -8.619558       0.0  200.312706  432.618534\n",
      "4   317.659199 -25.858138       0.0   47.756416  339.557477\n",
      "5   317.659199 -25.858138       0.0   47.756416  339.557477\n",
      "6   112.426119 -83.558991       0.0  361.917728  390.784856\n",
      "7   112.426119 -83.558991       0.0  361.917728  390.784856\n",
      "8   129.039000 -13.904000       0.0  246.214426  361.349426\n",
      "9   129.039000 -13.904000       0.0  246.214426  361.349426\n",
      "10  132.910697 -17.775388       0.0  246.214426  361.349735\n",
      "11         NaN -16.955000       0.0  168.673517  151.718517\n",
      "12         NaN -16.955000       0.0  168.673517  151.718517\n",
      "13  -18.973881 -49.788980       0.0  168.673517   99.910656\n",
      "14   64.900968 -56.708727       0.0  192.008256  200.200497\n",
      "15   64.900968 -56.708727       0.0  192.008256  200.200497\n",
      "16         NaN        NaN       NaN         NaN    0.000000\n",
      "17         NaN        NaN       NaN         NaN    0.000000\n",
      "18   94.104647   0.000000       0.0  183.218429  277.323076\n",
      "19         NaN   0.000000       0.0         NaN    0.000000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Osc.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"ifo\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"IFO_ffo + IFO_ei + IFO_dofa + IFO_ffooa\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"IFO_ffo\", \"IFO_ei\", \"IFO_dofa\", \"IFO_ffooa\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) COPY DF AND APPLY FORMULA USING 0 FOR NaN VALUES\n",
    "df_out = df.copy()\n",
    "\n",
    "# Fill only the necessary columns with 0 for the formula evaluation\n",
    "df_filled = df_out[FORMULA_COLUMNS].fillna(0)\n",
    "\n",
    "# Evaluate formula on df_filled\n",
    "df_out[NEW_COLUMN] = df_filled.eval(FORMULA_EXPRESSION)\n",
    "\n",
    "# 4) SAVE RESULT AS \"<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "output_path = OUTPUT_DIR / INPUT_FILE\n",
    "df_out.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula (NaN treated as 0):\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows processed:\", len(df_out))\n",
    "print(\"Result was saved to:\", output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQVQUYMZOqnY"
   },
   "source": [
    "### Roe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1765651732996,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "x0h0iNMiOxnT",
    "outputId": "ce12c49a-338b-4eda-c10b-f8b6f70c0188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'be' created based on formula:\n",
      "    ce + dt\n",
      "Rows used (all required columns non-null): 1629708\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Roe.txt\n",
      "            ce        dt          be\n",
      "0   378.411011       NaN         NaN\n",
      "1          NaN       NaN         NaN\n",
      "2   434.756779       NaN         NaN\n",
      "3          NaN       NaN         NaN\n",
      "4   518.204722       NaN         NaN\n",
      "5          NaN       NaN         NaN\n",
      "6   386.142780       NaN         NaN\n",
      "7          NaN       NaN         NaN\n",
      "8   387.109014       NaN         NaN\n",
      "9          NaN       NaN         NaN\n",
      "10  292.572607  0.000000  292.572607\n",
      "11         NaN       NaN         NaN\n",
      "12   72.674506  1.902010   74.576516\n",
      "13         NaN       NaN         NaN\n",
      "14  112.352977  1.252426  113.605403\n",
      "15         NaN       NaN         NaN\n",
      "16         NaN       NaN         NaN\n",
      "17    0.129111       NaN         NaN\n",
      "18         NaN       NaN         NaN\n",
      "19    0.613125       NaN         NaN\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Roe.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"be\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"ce + dt\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"ce\", \"dt\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcPJ4gLXPWQg"
   },
   "source": [
    "### Sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1765651733142,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "3J5T87soQAR2",
    "outputId": "7dc27abb-1c53-4ce4-cd9f-b112535d1da3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'be' created based on formula:\n",
      "    ce + dt\n",
      "Rows used (all required columns non-null): 1629708\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Sg.txt\n",
      "            ce        dt          be\n",
      "0   378.411011       NaN         NaN\n",
      "1   434.756779       NaN         NaN\n",
      "2   518.204722       NaN         NaN\n",
      "3   386.142780       NaN         NaN\n",
      "4   387.109014       NaN         NaN\n",
      "5   292.572607  0.000000  292.572607\n",
      "6    72.674506  1.902010   74.576516\n",
      "7   112.352977  1.252426  113.605403\n",
      "8     0.129111       NaN         NaN\n",
      "9     0.613125       NaN         NaN\n",
      "10   16.638718       NaN         NaN\n",
      "11  437.845511       NaN         NaN\n",
      "12  805.404692  0.000000  805.404692\n",
      "13  899.303060       NaN         NaN\n",
      "14  920.553592       NaN         NaN\n",
      "15  742.381585       NaN         NaN\n",
      "16  796.586597       NaN         NaN\n",
      "17  320.569078  0.000000  320.569078\n",
      "18  350.219678  0.000000  350.219678\n",
      "19  419.494337  8.414000  427.908337\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out` and written to disk as\n",
    "#    \"formula_<INPUT_FILE>\" in the same folder.\n",
    "#\n",
    "# Examples for FORMULA_EXPRESSION:\n",
    "# - \"(at - xt) / ft\"\n",
    "# - \"col1 + col2 + col3\"\n",
    "# - \"(colA - colB) * colC\"\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Sg.txt\"               # <-- Set your input file name here\n",
    "NEW_COLUMN = \"be\"             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"ce + dt\"  # <-- Formula using column names\n",
    "FORMULA_COLUMNS = [\"ce\", \"dt\"]   # <-- All columns required to be non-null\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "# We use DataFrame.eval so column names can directly be used in FORMULA_EXPRESSION.\n",
    "# To avoid evaluating on rows that are not allowed, we apply eval on the subset\n",
    "# and then assign back.\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT AS \"formula_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out[[*FORMULA_COLUMNS, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOj3twhL-2-4"
   },
   "source": [
    "## Add Required Lagged Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zjJ6KWB6Gfr"
   },
   "source": [
    "### Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3410,
     "status": "ok",
     "timestamp": 1765651736554,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "LSXRdM5MRLBq",
    "outputId": "00c06700-446f-4b21-d716-2de3d7a5227f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'at', 'ca', 'cce', 'cl', 'std', 'itp', 'da', 'wc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/2054673312.py:161: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/2054673312.py:161: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['at', 'wc', 'at_lag1', 'wc_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1993                  A   \n",
      "3   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "4   C02500770          Ars 1995-12-29          1994                  A   \n",
      "5   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "6   C02500770          Ars 1996-05-03          1995                  A   \n",
      "7   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "8   C02500770          Ars 1998-07-03          1996                  A   \n",
      "9   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "10  C02500770          Ars 1998-07-03          1997                  A   \n",
      "11  C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "13  C02500770          Ars 1999-10-01          1998                  A   \n",
      "14  C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "12  C02500770          Ars 1999-10-08          1997               <NA>   \n",
      "15  C02500770          Ars 2000-05-19          1999                  A   \n",
      "17  C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "16  C02500770          Ars 2000-05-26          1999                  A   \n",
      "18  C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "19  C02520200          Ars 1996-05-03          1987                  A   \n",
      "20  C02520200          Ars 1996-05-03          1988                  A   \n",
      "21  C02520200          Ars 1996-05-03          1988               <NA>   \n",
      "22  C02520200          Ars 1996-05-03          1989                  A   \n",
      "23  C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "24  C02520200          Ars 1996-05-03          1990                  A   \n",
      "25  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "26  C02520200          Ars 1996-05-03          1991                  A   \n",
      "27  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "28  C02520200          Ars 1996-05-03          1992                  A   \n",
      "29  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "30  C02520200          Ars 1996-05-03          1993                  A   \n",
      "31  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "32  C02520200          Ars 1996-05-03          1994                  A   \n",
      "33  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "34  C02520200          Ars 1996-05-03          1995                  A   \n",
      "35  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "36  C02520200          Ars 1996-11-01          1996                  A   \n",
      "37  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "38  C02520200          Ars 1997-10-31          1997                  A   \n",
      "39  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "\n",
      "             at          wc      at_lag1     wc_lag1  \n",
      "0   1084.355949  -56.599177          NaN         NaN  \n",
      "1           NaN         NaN          NaN         NaN  \n",
      "2   1267.313578 -173.336614  1084.355949  -56.599177  \n",
      "3           NaN         NaN          NaN         NaN  \n",
      "4   1460.696865   58.591818  1267.313578 -173.336614  \n",
      "5           NaN         NaN          NaN         NaN  \n",
      "6   1304.537890   74.740059  1460.696865   58.591818  \n",
      "7           NaN         NaN          NaN         NaN  \n",
      "8   1265.820386  -28.061438  1304.537890   74.740059  \n",
      "9           NaN         NaN          NaN         NaN  \n",
      "10  1225.163797  -16.449604  1265.820386  -28.061438  \n",
      "11          NaN         NaN          NaN         NaN  \n",
      "13  1102.461138 -163.500400  1225.163797  -16.449604  \n",
      "14          NaN         NaN          NaN         NaN  \n",
      "12          NaN         NaN          NaN         NaN  \n",
      "15   945.993798         NaN  1102.461138 -163.500400  \n",
      "17          NaN         NaN          NaN         NaN  \n",
      "16   945.993798 -219.195942  1102.461138 -163.500400  \n",
      "18          NaN         NaN          NaN         NaN  \n",
      "19          NaN         NaN          NaN         NaN  \n",
      "20          NaN         NaN          NaN         NaN  \n",
      "21          NaN         NaN          NaN         NaN  \n",
      "22    23.439270         NaN          NaN         NaN  \n",
      "23          NaN         NaN          NaN         NaN  \n",
      "24   577.207234         NaN    23.439270         NaN  \n",
      "25          NaN         NaN          NaN         NaN  \n",
      "26  1132.139668  -55.268077   577.207234         NaN  \n",
      "27          NaN         NaN          NaN         NaN  \n",
      "28  1242.711417   39.471955  1132.139668  -55.268077  \n",
      "29          NaN         NaN          NaN         NaN  \n",
      "30  1373.608080   59.738639  1242.711417   39.471955  \n",
      "31          NaN         NaN          NaN         NaN  \n",
      "32  1263.968574   18.382917  1373.608080   59.738639  \n",
      "33          NaN         NaN          NaN         NaN  \n",
      "34  1301.804038  -27.373330  1263.968574   18.382917  \n",
      "35          NaN         NaN          NaN         NaN  \n",
      "36   820.980063  -61.499087  1301.804038  -27.373330  \n",
      "37          NaN         NaN          NaN         NaN  \n",
      "38   903.394131    3.481932   820.980063  -61.499087  \n",
      "39          NaN         NaN          NaN         NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Acc.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, a lagged column \"<col>_lag1\".\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lag1\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - 1 *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: 2020 Q1 will lag to 2019 Q1 (same period label), not to any\n",
    "#      value from 2020.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Acc.txt\"          # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"at\", \"wc\"]         # Columns for which to compute the lag \"<col>_lag1\"\n",
    "\n",
    "PERIOD_COL = \"AnnPITValue_Period\"    # NEW: period label used for lag (Q1, Q2, A, etc.)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "# Make sure period labels are strings (or NaN)\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lag1\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and latest known values for VALUE_COLUMNS.\n",
    "# - For a row with (FP = t, period = P), the lag \"<col>_lag1\" is the last known\n",
    "#   value of <col> for (FP = t-1, period = P) based on all PIT updates observed\n",
    "#   up to (and including) the current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lag1\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, using:\n",
    "      - FiscalPeriod (year), and\n",
    "      - AnnPITValue_Period (period label) for matching the lag.\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values for VALUE_COLUMNS.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column, we collect the lag values row by row\n",
    "    lag_values = {col: [] for col in VALUE_COLUMNS}\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_label_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        # Use None as key for missing labels to keep keys hashable and consistent\n",
    "        period_label = None if pd.isna(period_label_raw) else str(period_label_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS\n",
    "        for col in VALUE_COLUMNS:\n",
    "            lag_val = None\n",
    "            if pd.notna(fp):\n",
    "                target_fp = fp - 1\n",
    "                target_key = (target_fp, period_label)\n",
    "                info = last_for_fp.get(target_key)\n",
    "                if info is not None:\n",
    "                    # info[1] is the dict of last known values for that target (FP, period_label)\n",
    "                    lag_val = info[1].get(col)\n",
    "            lag_values[col].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lag1\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        group[f\"{col}_lag1\"] = lag_values[col]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "print([c for c in result.columns if any(c == v or c == f\"{v}_lag1\" for v in VALUE_COLUMNS)])\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = [\n",
    "    \"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL\n",
    "] + VALUE_COLUMNS + [f\"{c}_lag1\" for c in VALUE_COLUMNS]\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNAl51r_RLRA"
   },
   "source": [
    "### Ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1924,
     "status": "ok",
     "timestamp": 1765651738495,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "msytwyg06Fy-",
    "outputId": "6c42a89a-ff6c-4872-aeba-86ac22d506fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'at']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/4202822760.py:161: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/4202822760.py:161: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['at', 'at_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1993                  A   \n",
      "2   C02500770          Ars 1995-12-29          1994                  A   \n",
      "3   C02500770          Ars 1996-05-03          1995                  A   \n",
      "4   C02500770          Ars 1998-07-03          1996                  A   \n",
      "5   C02500770          Ars 1998-07-03          1997                  A   \n",
      "6   C02500770          Ars 1999-10-01          1998                  A   \n",
      "7   C02500770          Ars 2000-05-19          1999                  A   \n",
      "8   C02520200          Ars 1996-05-03          1989                  A   \n",
      "9   C02520200          Ars 1996-05-03          1990                  A   \n",
      "10  C02520200          Ars 1996-05-03          1991                  A   \n",
      "11  C02520200          Ars 1996-05-03          1992                  A   \n",
      "12  C02520200          Ars 1996-05-03          1993                  A   \n",
      "13  C02520200          Ars 1996-05-03          1994                  A   \n",
      "14  C02520200          Ars 1996-05-03          1995                  A   \n",
      "15  C02520200          Ars 1996-11-01          1996                  A   \n",
      "16  C02520200          Ars 1997-10-31          1997                  A   \n",
      "17  C02520200          Ars 1998-12-04          1998                  A   \n",
      "19  C02520200          Ars 1999-10-01          1999                 Q1   \n",
      "18  C02520200          Ars 1999-12-10          1999                  A   \n",
      "22  C02520200          Ars 2000-02-04          2000                 Q1   \n",
      "20  C02520200          Ars 2000-09-15          2000                  A   \n",
      "21  C02520200          Ars 2000-11-24          2000                  A   \n",
      "23  C02520200          Ars 2001-09-28          2001                  A   \n",
      "26  C02520200          Ars 2001-12-07          2002                 Q1   \n",
      "27  C02520200          Ars 2002-08-30          2002                 Q1   \n",
      "24  C02520200          Ars 2002-11-15          2001                  A   \n",
      "25  C02520200          Ars 2003-05-30          2002                  A   \n",
      "29  C02520200          Ars 2003-05-30          2003                 Q1   \n",
      "30  C02520200          Ars 2004-01-23          2003                 Q3   \n",
      "28  C02520200          Ars 2004-05-28          2003                  A   \n",
      "32  C02520200          Ars 2004-05-28          2004                 Q1   \n",
      "33  C02520200          Ars 2004-08-20          2004                 Q2   \n",
      "34  C02520200          Ars 2004-11-12          2004                 Q3   \n",
      "31  C02520200          Ars 2005-04-08          2004                  A   \n",
      "36  C02520200          Ars 2005-05-13          2005                 Q1   \n",
      "37  C02520200          Ars 2005-08-05          2005                 Q2   \n",
      "38  C02520200          Ars 2005-11-18          2005                 Q3   \n",
      "35  C02520200          Ars 2006-03-10          2005                  A   \n",
      "40  C02520200          Ars 2006-05-19          2006                 Q1   \n",
      "\n",
      "             at      at_lag1  \n",
      "0   1084.355949          NaN  \n",
      "1   1267.313578  1084.355949  \n",
      "2   1460.696865  1267.313578  \n",
      "3   1304.537890  1460.696865  \n",
      "4   1265.820386  1304.537890  \n",
      "5   1225.163797  1265.820386  \n",
      "6   1102.461138  1225.163797  \n",
      "7    945.993798  1102.461138  \n",
      "8     23.439270          NaN  \n",
      "9    577.207234    23.439270  \n",
      "10  1132.139668   577.207234  \n",
      "11  1242.711417  1132.139668  \n",
      "12  1373.608080  1242.711417  \n",
      "13  1263.968574  1373.608080  \n",
      "14  1301.804038  1263.968574  \n",
      "15   820.980063  1301.804038  \n",
      "16   903.394131   820.980063  \n",
      "17   996.702454   903.394131  \n",
      "19   993.088238          NaN  \n",
      "18   939.325665   996.702454  \n",
      "22   943.038193   993.088238  \n",
      "20   883.100000   939.325665  \n",
      "21   883.141176   939.325665  \n",
      "23   807.179058   883.141176  \n",
      "26   801.600000          NaN  \n",
      "27  1178.450000          NaN  \n",
      "24   744.540000   883.141176  \n",
      "25  1574.584000   744.540000  \n",
      "29  1652.395000  1178.450000  \n",
      "30  1598.486000          NaN  \n",
      "28  1589.720000  1574.584000  \n",
      "32  1864.086000  1652.395000  \n",
      "33  1973.796000          NaN  \n",
      "34  2185.037000  1598.486000  \n",
      "31  2430.585000  1589.720000  \n",
      "36  2324.215219  1864.086000  \n",
      "37  2155.546776  1973.796000  \n",
      "38  2352.910543  2185.037000  \n",
      "35  2554.006763  2430.585000  \n",
      "40  2901.475797  2324.215219  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Ag.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, a lagged column \"<col>_lag1\".\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lag1\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - 1 *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: 2020 Q1 will lag to 2019 Q1 (same period label), not to any\n",
    "#      value from 2020.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Ag.txt\"                 # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"at\"]         # Columns for which to compute the lag \"<col>_lag1\"\n",
    "\n",
    "PERIOD_COL = \"AnnPITValue_Period\"    # NEW: period label used for lag (Q1, Q2, A, etc.)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "# Make sure period labels are strings (or NaN)\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lag1\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and latest known values for VALUE_COLUMNS.\n",
    "# - For a row with (FP = t, period = P), the lag \"<col>_lag1\" is the last known\n",
    "#   value of <col> for (FP = t-1, period = P) based on all PIT updates observed\n",
    "#   up to (and including) the current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lag1\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, using:\n",
    "      - FiscalPeriod (year), and\n",
    "      - AnnPITValue_Period (period label) for matching the lag.\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values for VALUE_COLUMNS.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column, we collect the lag values row by row\n",
    "    lag_values = {col: [] for col in VALUE_COLUMNS}\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_label_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        # Use None as key for missing labels to keep keys hashable and consistent\n",
    "        period_label = None if pd.isna(period_label_raw) else str(period_label_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS\n",
    "        for col in VALUE_COLUMNS:\n",
    "            lag_val = None\n",
    "            if pd.notna(fp):\n",
    "                target_fp = fp - 1\n",
    "                target_key = (target_fp, period_label)\n",
    "                info = last_for_fp.get(target_key)\n",
    "                if info is not None:\n",
    "                    # info[1] is the dict of last known values for that target (FP, period_label)\n",
    "                    lag_val = info[1].get(col)\n",
    "            lag_values[col].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lag1\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        group[f\"{col}_lag1\"] = lag_values[col]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "print([c for c in result.columns if any(c == v or c == f\"{v}_lag1\" for v in VALUE_COLUMNS)])\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = [\n",
    "    \"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL\n",
    "] + VALUE_COLUMNS + [f\"{c}_lag1\" for c in VALUE_COLUMNS]\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90ijzZcYZX2r"
   },
   "source": [
    "### At"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3230,
     "status": "ok",
     "timestamp": 1765651741750,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "wQw4pGUqZb9r",
    "outputId": "8cc69bc3-ac25-4b5d-d805-628feb50a5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'rev', 'at', 'cce', 'ltd', 'mi', 'ps', 'ce', 'oa', 'ol', 'noa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/1822182343.py:161: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/1822182343.py:161: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['noa', 'noa_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1993                  A   \n",
      "3   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "4   C02500770          Ars 1995-12-29          1994                  A   \n",
      "5   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "6   C02500770          Ars 1996-05-03          1995                  A   \n",
      "7   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "8   C02500770          Ars 1998-07-03          1996                  A   \n",
      "9   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "10  C02500770          Ars 1998-07-03          1997                  A   \n",
      "11  C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "12  C02500770          Ars 1999-10-01          1998                  A   \n",
      "13  C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "14  C02500770          Ars 2000-05-19          1999                  A   \n",
      "15  C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "16  C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "17  C02520200          Ars 1996-05-03          1987                  A   \n",
      "18  C02520200          Ars 1996-05-03          1988                  A   \n",
      "19  C02520200          Ars 1996-05-03          1989                  A   \n",
      "20  C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "21  C02520200          Ars 1996-05-03          1990                  A   \n",
      "22  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "23  C02520200          Ars 1996-05-03          1991                  A   \n",
      "24  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "25  C02520200          Ars 1996-05-03          1992                  A   \n",
      "26  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "27  C02520200          Ars 1996-05-03          1993                  A   \n",
      "28  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "29  C02520200          Ars 1996-05-03          1994                  A   \n",
      "30  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "31  C02520200          Ars 1996-05-03          1995                  A   \n",
      "32  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "33  C02520200          Ars 1996-11-01          1996                  A   \n",
      "34  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "35  C02520200          Ars 1997-10-31          1997                  A   \n",
      "36  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "37  C02520200          Ars 1998-12-04          1998                  A   \n",
      "38  C02520200          Ars 1998-12-04          1998               <NA>   \n",
      "39  C02520200          Ars 1999-10-01          1998               <NA>   \n",
      "\n",
      "           noa    noa_lag1  \n",
      "0   220.657337         NaN  \n",
      "1          NaN         NaN  \n",
      "2   220.174627  220.657337  \n",
      "3          NaN         NaN  \n",
      "4          NaN  220.174627  \n",
      "5          NaN         NaN  \n",
      "6          NaN         NaN  \n",
      "7          NaN         NaN  \n",
      "8          NaN         NaN  \n",
      "9          NaN         NaN  \n",
      "10         NaN         NaN  \n",
      "11         NaN         NaN  \n",
      "12         NaN         NaN  \n",
      "13         NaN         NaN  \n",
      "14  109.669393         NaN  \n",
      "15         NaN         NaN  \n",
      "16         NaN         NaN  \n",
      "17         NaN         NaN  \n",
      "18         NaN         NaN  \n",
      "19         NaN         NaN  \n",
      "20         NaN         NaN  \n",
      "21  456.556030         NaN  \n",
      "22         NaN         NaN  \n",
      "23  820.392562  456.556030  \n",
      "24         NaN         NaN  \n",
      "25  933.442820  820.392562  \n",
      "26         NaN         NaN  \n",
      "27  954.085072  933.442820  \n",
      "28         NaN         NaN  \n",
      "29  914.863993  954.085072  \n",
      "30         NaN         NaN  \n",
      "31  964.652996  914.863993  \n",
      "32         NaN         NaN  \n",
      "33  511.577582  964.652996  \n",
      "34         NaN         NaN  \n",
      "35  627.569593  511.577582  \n",
      "36         NaN         NaN  \n",
      "37  604.102017  627.569593  \n",
      "38         NaN         NaN  \n",
      "39         NaN         NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_At.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, a lagged column \"<col>_lag1\".\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lag1\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - 1 *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: 2020 Q1 will lag to 2019 Q1 (same period label), not to any\n",
    "#      value from 2020.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_At.txt\"                 # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"noa\"]         # Columns for which to compute the lag \"<col>_lag1\"\n",
    "\n",
    "PERIOD_COL = \"AnnPITValue_Period\"    # NEW: period label used for lag (Q1, Q2, A, etc.)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "# Make sure period labels are strings (or NaN)\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lag1\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and latest known values for VALUE_COLUMNS.\n",
    "# - For a row with (FP = t, period = P), the lag \"<col>_lag1\" is the last known\n",
    "#   value of <col> for (FP = t-1, period = P) based on all PIT updates observed\n",
    "#   up to (and including) the current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lag1\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, using:\n",
    "      - FiscalPeriod (year), and\n",
    "      - AnnPITValue_Period (period label) for matching the lag.\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values for VALUE_COLUMNS.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column, we collect the lag values row by row\n",
    "    lag_values = {col: [] for col in VALUE_COLUMNS}\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_label_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        # Use None as key for missing labels to keep keys hashable and consistent\n",
    "        period_label = None if pd.isna(period_label_raw) else str(period_label_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS\n",
    "        for col in VALUE_COLUMNS:\n",
    "            lag_val = None\n",
    "            if pd.notna(fp):\n",
    "                target_fp = fp - 1\n",
    "                target_key = (target_fp, period_label)\n",
    "                info = last_for_fp.get(target_key)\n",
    "                if info is not None:\n",
    "                    # info[1] is the dict of last known values for that target (FP, period_label)\n",
    "                    lag_val = info[1].get(col)\n",
    "            lag_values[col].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lag1\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        group[f\"{col}_lag1\"] = lag_values[col]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "print([c for c in result.columns if any(c == v or c == f\"{v}_lag1\" for v in VALUE_COLUMNS)])\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = [\n",
    "    \"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL\n",
    "] + VALUE_COLUMNS + [f\"{c}_lag1\" for c in VALUE_COLUMNS]\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jD8lrPdReWP6"
   },
   "source": [
    "### Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3185,
     "status": "ok",
     "timestamp": 1765651745003,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ALzgJ5OOhQTx",
    "outputId": "6e2ac65b-6e8a-49c9-f483-be76003cbf67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'rev', 'at', 'cce', 'ltd', 'mi', 'ps', 'ce', 'oa', 'ol', 'noa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/4111774751.py:161: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/4111774751.py:161: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['noa', 'noa_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1993                  A   \n",
      "3   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "4   C02500770          Ars 1995-12-29          1994                  A   \n",
      "5   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "6   C02500770          Ars 1996-05-03          1995                  A   \n",
      "7   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "8   C02500770          Ars 1998-07-03          1996                  A   \n",
      "9   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "10  C02500770          Ars 1998-07-03          1997                  A   \n",
      "11  C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "12  C02500770          Ars 1999-10-01          1998                  A   \n",
      "13  C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "14  C02500770          Ars 2000-05-19          1999                  A   \n",
      "15  C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "16  C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "17  C02520200          Ars 1996-05-03          1987                  A   \n",
      "18  C02520200          Ars 1996-05-03          1988                  A   \n",
      "19  C02520200          Ars 1996-05-03          1989                  A   \n",
      "20  C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "21  C02520200          Ars 1996-05-03          1990                  A   \n",
      "22  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "23  C02520200          Ars 1996-05-03          1991                  A   \n",
      "24  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "25  C02520200          Ars 1996-05-03          1992                  A   \n",
      "26  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "27  C02520200          Ars 1996-05-03          1993                  A   \n",
      "28  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "29  C02520200          Ars 1996-05-03          1994                  A   \n",
      "30  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "31  C02520200          Ars 1996-05-03          1995                  A   \n",
      "32  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "33  C02520200          Ars 1996-11-01          1996                  A   \n",
      "34  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "35  C02520200          Ars 1997-10-31          1997                  A   \n",
      "36  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "37  C02520200          Ars 1998-12-04          1998                  A   \n",
      "38  C02520200          Ars 1998-12-04          1998               <NA>   \n",
      "39  C02520200          Ars 1999-10-01          1998               <NA>   \n",
      "\n",
      "           noa    noa_lag1  \n",
      "0   220.657337         NaN  \n",
      "1          NaN         NaN  \n",
      "2   220.174627  220.657337  \n",
      "3          NaN         NaN  \n",
      "4          NaN  220.174627  \n",
      "5          NaN         NaN  \n",
      "6          NaN         NaN  \n",
      "7          NaN         NaN  \n",
      "8          NaN         NaN  \n",
      "9          NaN         NaN  \n",
      "10         NaN         NaN  \n",
      "11         NaN         NaN  \n",
      "12         NaN         NaN  \n",
      "13         NaN         NaN  \n",
      "14  109.669393         NaN  \n",
      "15         NaN         NaN  \n",
      "16         NaN         NaN  \n",
      "17         NaN         NaN  \n",
      "18         NaN         NaN  \n",
      "19         NaN         NaN  \n",
      "20         NaN         NaN  \n",
      "21  456.556030         NaN  \n",
      "22         NaN         NaN  \n",
      "23  820.392562  456.556030  \n",
      "24         NaN         NaN  \n",
      "25  933.442820  820.392562  \n",
      "26         NaN         NaN  \n",
      "27  954.085072  933.442820  \n",
      "28         NaN         NaN  \n",
      "29  914.863993  954.085072  \n",
      "30         NaN         NaN  \n",
      "31  964.652996  914.863993  \n",
      "32         NaN         NaN  \n",
      "33  511.577582  964.652996  \n",
      "34         NaN         NaN  \n",
      "35  627.569593  511.577582  \n",
      "36         NaN         NaN  \n",
      "37  604.102017  627.569593  \n",
      "38         NaN         NaN  \n",
      "39         NaN         NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Cat.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, a lagged column \"<col>_lag1\".\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lag1\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - 1 *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: 2020 Q1 will lag to 2019 Q1 (same period label), not to any\n",
    "#      value from 2020.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Cat.txt\"                 # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"noa\"]         # Columns for which to compute the lag \"<col>_lag1\"\n",
    "\n",
    "PERIOD_COL = \"AnnPITValue_Period\"    # NEW: period label used for lag (Q1, Q2, A, etc.)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "# Make sure period labels are strings (or NaN)\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lag1\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and latest known values for VALUE_COLUMNS.\n",
    "# - For a row with (FP = t, period = P), the lag \"<col>_lag1\" is the last known\n",
    "#   value of <col> for (FP = t-1, period = P) based on all PIT updates observed\n",
    "#   up to (and including) the current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lag1\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, using:\n",
    "      - FiscalPeriod (year), and\n",
    "      - AnnPITValue_Period (period label) for matching the lag.\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values for VALUE_COLUMNS.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column, we collect the lag values row by row\n",
    "    lag_values = {col: [] for col in VALUE_COLUMNS}\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_label_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        # Use None as key for missing labels to keep keys hashable and consistent\n",
    "        period_label = None if pd.isna(period_label_raw) else str(period_label_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS\n",
    "        for col in VALUE_COLUMNS:\n",
    "            lag_val = None\n",
    "            if pd.notna(fp):\n",
    "                target_fp = fp - 1\n",
    "                target_key = (target_fp, period_label)\n",
    "                info = last_for_fp.get(target_key)\n",
    "                if info is not None:\n",
    "                    # info[1] is the dict of last known values for that target (FP, period_label)\n",
    "                    lag_val = info[1].get(col)\n",
    "            lag_values[col].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lag1\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        group[f\"{col}_lag1\"] = lag_values[col]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "print([c for c in result.columns if any(c == v or c == f\"{v}_lag1\" for v in VALUE_COLUMNS)])\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = [\n",
    "    \"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL\n",
    "] + VALUE_COLUMNS + [f\"{c}_lag1\" for c in VALUE_COLUMNS]\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBonCCy_hfUP"
   },
   "source": [
    "### Cpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1788,
     "status": "ok",
     "timestamp": 1765651746795,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "KxDeiP-Bhg28",
    "outputId": "1ce6bb68-3357-4862-d57f-67b0781b4da5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'rev', 'cogs', 'pm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/727171292.py:161: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/727171292.py:161: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['pm', 'pm_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "1   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "3   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "4   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "5   C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "7   C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "6   C02500770          Ars 1999-10-08          1997               <NA>   \n",
      "8   C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "9   C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "10  C02520200          Ars 1996-05-03          1987               <NA>   \n",
      "11  C02520200          Ars 1996-05-03          1988               <NA>   \n",
      "12  C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "13  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "14  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "15  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "16  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "17  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "18  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "19  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "20  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "21  C02520200          Ars 1998-12-04          1998               <NA>   \n",
      "22  C02520200          Ars 1999-10-01          1998               <NA>   \n",
      "23  C02520200          Ars 1999-12-10          1999               <NA>   \n",
      "24  C02520200          Ars 1999-12-24          1999               <NA>   \n",
      "25  C02520200          Ars 2000-09-15          2000               <NA>   \n",
      "26  C02520200          Ars 2000-11-24          2000               <NA>   \n",
      "28  C02520200          Ars 2001-09-28          2001               <NA>   \n",
      "29  C02520200          Ars 2001-10-05          2001               <NA>   \n",
      "27  C02520200          Ars 2001-11-30          2000               <NA>   \n",
      "30  C02520200          Ars 2001-11-30          2001               <NA>   \n",
      "31  C02520200          Ars 2002-11-15          2001               <NA>   \n",
      "33  C02520200          Ars 2003-05-30          2002               <NA>   \n",
      "32  C02520200          Ars 2003-09-12          2001               <NA>   \n",
      "34  C02520200          Ars 2003-10-03          2002               <NA>   \n",
      "35  C02520200          Ars 2004-05-28          2003               <NA>   \n",
      "36  C02520200          Ars 2005-04-08          2004               <NA>   \n",
      "37  C02520200          Ars 2006-03-10          2005               <NA>   \n",
      "38  C02520200          Ars 2006-03-17          2005               <NA>   \n",
      "39  C02520200          Ars 2007-03-15          2006               <NA>   \n",
      "\n",
      "          pm   pm_lag1  \n",
      "0   0.170827       NaN  \n",
      "1   0.156230  0.170827  \n",
      "2   0.130227  0.156230  \n",
      "3   0.121844  0.130227  \n",
      "4   0.172871  0.121844  \n",
      "5   0.166367  0.172871  \n",
      "7   0.255589  0.166367  \n",
      "6   0.262077  0.172871  \n",
      "8        NaN  0.255589  \n",
      "9   0.412727  0.255589  \n",
      "10       NaN       NaN  \n",
      "11       NaN       NaN  \n",
      "12  0.195754       NaN  \n",
      "13  0.071658  0.195754  \n",
      "14  0.033125  0.071658  \n",
      "15  0.263450  0.033125  \n",
      "16  0.252640  0.263450  \n",
      "17  0.256297  0.252640  \n",
      "18  0.262635  0.256297  \n",
      "19  0.243680  0.262635  \n",
      "20  0.276209  0.243680  \n",
      "21  0.274094  0.276209  \n",
      "22  0.274094  0.276209  \n",
      "23       NaN  0.274094  \n",
      "24  0.216301  0.274094  \n",
      "25       NaN  0.216301  \n",
      "26       NaN  0.216301  \n",
      "28       NaN       NaN  \n",
      "29  0.232203       NaN  \n",
      "27  0.235433  0.216301  \n",
      "30  0.232203  0.235433  \n",
      "31  0.186445  0.235433  \n",
      "33  0.417339  0.186445  \n",
      "32  0.186445  0.235433  \n",
      "34  0.417339  0.186445  \n",
      "35  0.439526  0.417339  \n",
      "36  0.489347  0.439526  \n",
      "37  0.414612  0.489347  \n",
      "38  0.414612  0.489347  \n",
      "39  0.348094  0.414612  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Cpm.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, a lagged column \"<col>_lag1\".\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lag1\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - 1 *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: 2020 Q1 will lag to 2019 Q1 (same period label), not to any\n",
    "#      value from 2020.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Cpm.txt\"                 # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"pm\"]         # Columns for which to compute the lag \"<col>_lag1\"\n",
    "\n",
    "PERIOD_COL = \"AnnPITValue_Period\"    # NEW: period label used for lag (Q1, Q2, A, etc.)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "# Make sure period labels are strings (or NaN)\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lag1\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and latest known values for VALUE_COLUMNS.\n",
    "# - For a row with (FP = t, period = P), the lag \"<col>_lag1\" is the last known\n",
    "#   value of <col> for (FP = t-1, period = P) based on all PIT updates observed\n",
    "#   up to (and including) the current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lag1\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, using:\n",
    "      - FiscalPeriod (year), and\n",
    "      - AnnPITValue_Period (period label) for matching the lag.\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values for VALUE_COLUMNS.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column, we collect the lag values row by row\n",
    "    lag_values = {col: [] for col in VALUE_COLUMNS}\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_label_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        # Use None as key for missing labels to keep keys hashable and consistent\n",
    "        period_label = None if pd.isna(period_label_raw) else str(period_label_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS\n",
    "        for col in VALUE_COLUMNS:\n",
    "            lag_val = None\n",
    "            if pd.notna(fp):\n",
    "                target_fp = fp - 1\n",
    "                target_key = (target_fp, period_label)\n",
    "                info = last_for_fp.get(target_key)\n",
    "                if info is not None:\n",
    "                    # info[1] is the dict of last known values for that target (FP, period_label)\n",
    "                    lag_val = info[1].get(col)\n",
    "            lag_values[col].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lag1\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        group[f\"{col}_lag1\"] = lag_values[col]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "print([c for c in result.columns if any(c == v or c == f\"{v}_lag1\" for v in VALUE_COLUMNS)])\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = [\n",
    "    \"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL\n",
    "] + VALUE_COLUMNS + [f\"{c}_lag1\" for c in VALUE_COLUMNS]\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ey20IFLChmnL"
   },
   "source": [
    "### Ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1900,
     "status": "ok",
     "timestamp": 1765651748697,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "rbrdLXecibV-",
    "outputId": "f91d345d-02de-4255-808a-da5efcdb2d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'eps']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/3785253462.py:164: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/3785253462.py:164: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['eps', 'eps_lag1', 'eps_lag2']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod      eps  eps_lag1  \\\n",
      "0   C02500770          Ars 1998-10-02          1992  0.87000       NaN   \n",
      "3   C02500770          Ars 1998-10-02          1993  0.82000   0.87000   \n",
      "6   C02500770          Ars 1998-10-02          1994  0.99024   0.82000   \n",
      "8   C02500770          Ars 1998-10-02          1995 -0.39000   0.99024   \n",
      "11  C02500770          Ars 1998-10-02          1996  0.35000  -0.39000   \n",
      "13  C02500770          Ars 1998-10-02          1997 -0.57000   0.35000   \n",
      "1   C02500770          Ars 1999-07-30          1992  0.88000       NaN   \n",
      "4   C02500770          Ars 1999-07-30          1993  0.83000   0.88000   \n",
      "14  C02500770          Ars 1999-07-30          1997 -0.58000   0.35000   \n",
      "2   C02500770          Ars 1999-10-01          1992  0.87691       NaN   \n",
      "5   C02500770          Ars 1999-10-01          1993  0.82611   0.87691   \n",
      "7   C02500770          Ars 1999-10-01          1994  0.99024   0.82611   \n",
      "9   C02500770          Ars 1999-10-01          1995 -0.38913   0.99024   \n",
      "12  C02500770          Ars 1999-10-01          1996  0.35101  -0.38913   \n",
      "15  C02500770          Ars 1999-10-01          1997 -0.57532   0.35101   \n",
      "17  C02500770          Ars 1999-10-01          1998 -1.12370  -0.57532   \n",
      "19  C02500770          Ars 2000-05-19          1999 -1.04847  -1.12370   \n",
      "20  C02500770          Ars 2000-05-26          1999 -1.04838  -1.12370   \n",
      "10  C02500770          Ars 2003-07-11          1995 -0.38914   0.99024   \n",
      "16  C02500770          Ars 2003-07-11          1997 -0.57533   0.35101   \n",
      "18  C02500770          Ars 2003-07-11          1998 -1.12371  -0.57533   \n",
      "22  C02520200          Ars 1996-05-03          1988  0.00003       NaN   \n",
      "24  C02520200          Ars 1996-05-03          1990  0.08000       NaN   \n",
      "26  C02520200          Ars 1996-05-03          1991 -0.45000   0.08000   \n",
      "28  C02520200          Ars 1996-05-03          1992  0.08000  -0.45000   \n",
      "30  C02520200          Ars 1996-05-03          1993 -0.11000   0.08000   \n",
      "32  C02520200          Ars 1996-05-03          1994 -0.08000  -0.11000   \n",
      "34  C02520200          Ars 1996-05-03          1995 -0.02000  -0.08000   \n",
      "37  C02520200          Ars 1996-11-01          1996 -0.35000  -0.02000   \n",
      "39  C02520200          Ars 1997-10-31          1997  0.14000  -0.35000   \n",
      "35  C02520200          Ars 1998-07-03          1995 -0.03000  -0.08000   \n",
      "41  C02520200          Ars 1998-12-04          1998  0.16000   0.14000   \n",
      "21  C02520200          Ars 1999-10-01          1987 -0.00024       NaN   \n",
      "23  C02520200          Ars 1999-10-01          1989 -0.00102   0.00003   \n",
      "25  C02520200          Ars 1999-10-01          1990  0.07808  -0.00102   \n",
      "27  C02520200          Ars 1999-10-01          1991 -0.44800   0.07808   \n",
      "29  C02520200          Ars 1999-10-01          1992  0.07839  -0.44800   \n",
      "31  C02520200          Ars 1999-10-01          1993 -0.11229   0.07839   \n",
      "33  C02520200          Ars 1999-10-01          1994 -0.07810  -0.11229   \n",
      "36  C02520200          Ars 1999-10-01          1995 -0.02517  -0.07810   \n",
      "\n",
      "    eps_lag2  \n",
      "0        NaN  \n",
      "3        NaN  \n",
      "6    0.87000  \n",
      "8    0.82000  \n",
      "11   0.99024  \n",
      "13  -0.39000  \n",
      "1        NaN  \n",
      "4        NaN  \n",
      "14  -0.39000  \n",
      "2        NaN  \n",
      "5        NaN  \n",
      "7    0.87691  \n",
      "9    0.82611  \n",
      "12   0.99024  \n",
      "15  -0.38913  \n",
      "17   0.35101  \n",
      "19  -0.57532  \n",
      "20  -0.57532  \n",
      "10   0.82611  \n",
      "16  -0.38914  \n",
      "18   0.35101  \n",
      "22       NaN  \n",
      "24   0.00003  \n",
      "26       NaN  \n",
      "28   0.08000  \n",
      "30  -0.45000  \n",
      "32   0.08000  \n",
      "34  -0.11000  \n",
      "37  -0.08000  \n",
      "39  -0.02000  \n",
      "35  -0.11000  \n",
      "41  -0.35000  \n",
      "21       NaN  \n",
      "23  -0.00024  \n",
      "25   0.00003  \n",
      "27  -0.00102  \n",
      "29   0.07808  \n",
      "31  -0.44800  \n",
      "33   0.07839  \n",
      "36  -0.11229  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Ec.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, a lagged column \"<col>_lag1\".\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lag1\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - 1 *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: 2020 Q1 will lag to 2019 Q1 (same period label), not to any\n",
    "#      value from 2020.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Ec.txt\"           # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"eps\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 2\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\"]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# Logic:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each FiscalPeriod, the latest known PIT and the latest\n",
    "#   known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with FiscalPeriod = t, the lag column \"<col>_lagk\" is defined as\n",
    "#   the last known value of <col> for FiscalPeriod = t-k based on all PIT\n",
    "#   updates observed up to (and including) the current PIT Date.\n",
    "#\n",
    "# Important:\n",
    "# - Multiple PIT updates for the same FiscalPeriod are handled.\n",
    "# - Corrections to earlier FiscalPeriods update the historical state only\n",
    "#   for subsequent PIT dates, not retroactively.\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS.\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each FiscalPeriod, the latest PIT and the known values\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       fp: (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect the lag values row by row\n",
    "    # Structure:\n",
    "    #   lag_values[col][k] = list of lagged values for that col and that k\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    info = last_for_fp.get(target_fp)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that target FiscalPeriod\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current FiscalPeriod fp\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            prev = last_for_fp.get(fp)\n",
    "\n",
    "            # Previously stored values for this FiscalPeriod (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[fp] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\"]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5PYjLZWANfm"
   },
   "source": [
    "### Es (Yearly Adaption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1950,
     "status": "ok",
     "timestamp": 1765651750650,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Zw_hMsQmANfo",
    "outputId": "32cb4be3-b72e-44ff-8e74-62cffa8f7159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'eps']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/2035131051.py:165: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/2035131051.py:165: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['eps', 'eps_lag1', 'eps_lag2', 'eps_lag3', 'eps_lag4']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1998-10-02          1992               <NA>   \n",
      "3   C02500770          Ars 1998-10-02          1993               <NA>   \n",
      "6   C02500770          Ars 1998-10-02          1994               <NA>   \n",
      "8   C02500770          Ars 1998-10-02          1995               <NA>   \n",
      "11  C02500770          Ars 1998-10-02          1996               <NA>   \n",
      "13  C02500770          Ars 1998-10-02          1997               <NA>   \n",
      "1   C02500770          Ars 1999-07-30          1992               <NA>   \n",
      "4   C02500770          Ars 1999-07-30          1993               <NA>   \n",
      "14  C02500770          Ars 1999-07-30          1997               <NA>   \n",
      "2   C02500770          Ars 1999-10-01          1992               <NA>   \n",
      "5   C02500770          Ars 1999-10-01          1993               <NA>   \n",
      "7   C02500770          Ars 1999-10-01          1994               <NA>   \n",
      "9   C02500770          Ars 1999-10-01          1995               <NA>   \n",
      "12  C02500770          Ars 1999-10-01          1996               <NA>   \n",
      "15  C02500770          Ars 1999-10-01          1997               <NA>   \n",
      "17  C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "19  C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "20  C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "10  C02500770          Ars 2003-07-11          1995               <NA>   \n",
      "16  C02500770          Ars 2003-07-11          1997               <NA>   \n",
      "18  C02500770          Ars 2003-07-11          1998               <NA>   \n",
      "22  C02520200          Ars 1996-05-03          1988               <NA>   \n",
      "24  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "26  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "28  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "30  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "32  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "34  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "37  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "39  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "35  C02520200          Ars 1998-07-03          1995               <NA>   \n",
      "41  C02520200          Ars 1998-12-04          1998               <NA>   \n",
      "21  C02520200          Ars 1999-10-01          1987               <NA>   \n",
      "23  C02520200          Ars 1999-10-01          1989               <NA>   \n",
      "25  C02520200          Ars 1999-10-01          1990               <NA>   \n",
      "27  C02520200          Ars 1999-10-01          1991               <NA>   \n",
      "29  C02520200          Ars 1999-10-01          1992               <NA>   \n",
      "31  C02520200          Ars 1999-10-01          1993               <NA>   \n",
      "33  C02520200          Ars 1999-10-01          1994               <NA>   \n",
      "36  C02520200          Ars 1999-10-01          1995               <NA>   \n",
      "\n",
      "        eps  eps_lag1  eps_lag2  eps_lag3  eps_lag4  \n",
      "0   0.87000       NaN       NaN       NaN       NaN  \n",
      "3   0.82000   0.87000       NaN       NaN       NaN  \n",
      "6   0.99024   0.82000   0.87000       NaN       NaN  \n",
      "8  -0.39000   0.99024   0.82000   0.87000       NaN  \n",
      "11  0.35000  -0.39000   0.99024   0.82000   0.87000  \n",
      "13 -0.57000   0.35000  -0.39000   0.99024   0.82000  \n",
      "1   0.88000       NaN       NaN       NaN       NaN  \n",
      "4   0.83000   0.88000       NaN       NaN       NaN  \n",
      "14 -0.58000   0.35000  -0.39000   0.99024   0.83000  \n",
      "2   0.87691       NaN       NaN       NaN       NaN  \n",
      "5   0.82611   0.87691       NaN       NaN       NaN  \n",
      "7   0.99024   0.82611   0.87691       NaN       NaN  \n",
      "9  -0.38913   0.99024   0.82611   0.87691       NaN  \n",
      "12  0.35101  -0.38913   0.99024   0.82611   0.87691  \n",
      "15 -0.57532   0.35101  -0.38913   0.99024   0.82611  \n",
      "17 -1.12370  -0.57532   0.35101  -0.38913   0.99024  \n",
      "19 -1.04847  -1.12370  -0.57532   0.35101  -0.38913  \n",
      "20 -1.04838  -1.12370  -0.57532   0.35101  -0.38913  \n",
      "10 -0.38914   0.99024   0.82611   0.87691       NaN  \n",
      "16 -0.57533   0.35101  -0.38914   0.99024   0.82611  \n",
      "18 -1.12371  -0.57533   0.35101  -0.38914   0.99024  \n",
      "22  0.00003       NaN       NaN       NaN       NaN  \n",
      "24  0.08000       NaN   0.00003       NaN       NaN  \n",
      "26 -0.45000   0.08000       NaN   0.00003       NaN  \n",
      "28  0.08000  -0.45000   0.08000       NaN   0.00003  \n",
      "30 -0.11000   0.08000  -0.45000   0.08000       NaN  \n",
      "32 -0.08000  -0.11000   0.08000  -0.45000   0.08000  \n",
      "34 -0.02000  -0.08000  -0.11000   0.08000  -0.45000  \n",
      "37 -0.35000  -0.02000  -0.08000  -0.11000   0.08000  \n",
      "39  0.14000  -0.35000  -0.02000  -0.08000  -0.11000  \n",
      "35 -0.03000  -0.08000  -0.11000   0.08000  -0.45000  \n",
      "41  0.16000   0.14000  -0.35000  -0.03000  -0.08000  \n",
      "21 -0.00024       NaN       NaN       NaN       NaN  \n",
      "23 -0.00102   0.00003  -0.00024       NaN       NaN  \n",
      "25  0.07808  -0.00102   0.00003  -0.00024       NaN  \n",
      "27 -0.44800   0.07808  -0.00102   0.00003  -0.00024  \n",
      "29  0.07839  -0.44800   0.07808  -0.00102   0.00003  \n",
      "31 -0.11229   0.07839  -0.44800   0.07808  -0.00102  \n",
      "33 -0.07810  -0.11229   0.07839  -0.44800   0.07808  \n",
      "36 -0.02517  -0.07810  -0.11229   0.07839  -0.44800  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Es.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"rev_lag1\" comes from 2019 Q1, not 2019 A.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Es.txt\"            # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"eps\"]               # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 4\n",
    "\n",
    "# Period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# New logic:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FP = t, period = P), \"<col>_lagk\" comes from (FP = t-k, period = P)\n",
    "#   based on all PIT updates up to that row.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we treat the period label as None and thus\n",
    "# fall back to the old \"by FiscalPeriod only\" behaviour for that row.\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Copy so we don't mutate previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dJywftRrUBr"
   },
   "source": [
    "### Ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2342,
     "status": "ok",
     "timestamp": 1765651752994,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "-axvSt5krdLv",
    "outputId": "5b024c26-8af0-4656-cb60-b57074342fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'at', 'inv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/2084783854.py:172: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/2084783854.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['inv', 'inv_lag1', 'at', 'at_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1993                  A   \n",
      "2   C02500770          Ars 1995-12-29          1994                  A   \n",
      "3   C02500770          Ars 1996-05-03          1995                  A   \n",
      "4   C02500770          Ars 1998-07-03          1996                  A   \n",
      "5   C02500770          Ars 1998-07-03          1997                  A   \n",
      "6   C02500770          Ars 1999-10-01          1998                  A   \n",
      "7   C02500770          Ars 2000-05-19          1999                  A   \n",
      "8   C02500770          Ars 2000-05-26          1999                  A   \n",
      "9   C02520200          Ars 1996-05-03          1987                  A   \n",
      "10  C02520200          Ars 1996-05-03          1988                  A   \n",
      "11  C02520200          Ars 1996-05-03          1989                  A   \n",
      "12  C02520200          Ars 1996-05-03          1990                  A   \n",
      "13  C02520200          Ars 1996-05-03          1991                  A   \n",
      "14  C02520200          Ars 1996-05-03          1992                  A   \n",
      "15  C02520200          Ars 1996-05-03          1993                  A   \n",
      "16  C02520200          Ars 1996-05-03          1994                  A   \n",
      "17  C02520200          Ars 1996-05-03          1995                  A   \n",
      "18  C02520200          Ars 1996-11-01          1996                  A   \n",
      "19  C02520200          Ars 1997-10-31          1997                  A   \n",
      "20  C02520200          Ars 1998-12-04          1998                  A   \n",
      "23  C02520200          Ars 1999-10-01          1999                 Q1   \n",
      "21  C02520200          Ars 1999-12-10          1999                  A   \n",
      "22  C02520200          Ars 1999-12-24          1999                  A   \n",
      "27  C02520200          Ars 2000-02-04          2000                 Q1   \n",
      "24  C02520200          Ars 2000-09-15          2000                  A   \n",
      "25  C02520200          Ars 2000-11-24          2000                  A   \n",
      "28  C02520200          Ars 2001-09-28          2001                  A   \n",
      "26  C02520200          Ars 2001-11-30          2000                  A   \n",
      "29  C02520200          Ars 2001-11-30          2001                  A   \n",
      "30  C02520200          Ars 2001-12-07          2001                  A   \n",
      "34  C02520200          Ars 2001-12-07          2002                 Q1   \n",
      "35  C02520200          Ars 2002-08-30          2002                 Q1   \n",
      "31  C02520200          Ars 2002-11-15          2001                  A   \n",
      "32  C02520200          Ars 2003-05-30          2002                  A   \n",
      "37  C02520200          Ars 2003-05-30          2003                 Q1   \n",
      "33  C02520200          Ars 2003-09-26          2002                  A   \n",
      "38  C02520200          Ars 2004-01-23          2003                 Q3   \n",
      "36  C02520200          Ars 2004-05-28          2003                  A   \n",
      "40  C02520200          Ars 2004-05-28          2004                 Q1   \n",
      "\n",
      "           inv           at    inv_lag1      at_lag1  \n",
      "0   237.409600  1084.355949         NaN          NaN  \n",
      "1   201.182443  1267.313578  237.409600  1084.355949  \n",
      "2   289.925405  1460.696865  201.182443  1267.313578  \n",
      "3   223.628192  1304.537890  289.925405  1460.696865  \n",
      "4   177.096926  1265.820386  223.628192  1304.537890  \n",
      "5   173.896930  1225.163797  177.096926  1265.820386  \n",
      "6   204.671283  1102.461138  173.896930  1225.163797  \n",
      "7          NaN   945.993798  204.671283  1102.461138  \n",
      "8   158.117346   945.993798  204.671283  1102.461138  \n",
      "9     0.020612          NaN         NaN          NaN  \n",
      "10    0.072913          NaN    0.020612          NaN  \n",
      "11    1.839860    23.439270    0.072913          NaN  \n",
      "12   40.650548   577.207234    1.839860    23.439270  \n",
      "13   76.440596  1132.139668   40.650548   577.207234  \n",
      "14   99.373808  1242.711417   76.440596  1132.139668  \n",
      "15  100.494805  1373.608080   99.373808  1242.711417  \n",
      "16  100.692784  1263.968574  100.494805  1373.608080  \n",
      "17   98.115757  1301.804038  100.692784  1263.968574  \n",
      "18   63.467400   820.980063   98.115757  1301.804038  \n",
      "19  104.144450   903.394131   63.467400   820.980063  \n",
      "20  145.107780   996.702454  104.144450   903.394131  \n",
      "23  114.794826   993.088238         NaN          NaN  \n",
      "21         NaN   939.325665  145.107780   996.702454  \n",
      "22   89.472205   939.325665  145.107780   996.702454  \n",
      "27   86.992779   943.038193  114.794826   993.088238  \n",
      "24         NaN   883.100000   89.472205   939.325665  \n",
      "25         NaN   883.141176   89.472205   939.325665  \n",
      "28         NaN   807.179058         NaN   883.141176  \n",
      "26   77.409000   883.141176   89.472205   939.325665  \n",
      "29   84.454000   807.179058   77.409000   883.141176  \n",
      "30   84.926000   807.179058   77.409000   883.141176  \n",
      "34         NaN   801.600000         NaN          NaN  \n",
      "35  100.297000  1178.450000         NaN          NaN  \n",
      "31   57.070000   744.540000   77.409000   883.141176  \n",
      "32         NaN  1574.584000   57.070000   744.540000  \n",
      "37  168.013000  1652.395000  100.297000  1178.450000  \n",
      "33  158.271000  1574.584000   57.070000   744.540000  \n",
      "38  187.991000  1598.486000         NaN          NaN  \n",
      "36  197.120000  1589.720000  158.271000  1574.584000  \n",
      "40  240.446000  1864.086000  168.013000  1652.395000  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Ig.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Ig.txt\"            # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"inv\", \"at\"]         # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 1\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mHXx1RSr4xI"
   },
   "source": [
    "### Inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1859,
     "status": "ok",
     "timestamp": 1765651754856,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "sx3N-kXar6Lr",
    "outputId": "e07b938c-2256-47ce-95b4-2146665b2f65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'rev', 'capex', 'ce']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/4244821335.py:172: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/4244821335.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['ce', 'ce_lag1', 'ce_lag2', 'ce_lag3']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "1   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "3   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "4   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "5   C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "6   C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "7   C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "8   C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "9   C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "10  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "11  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "12  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "13  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "14  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "15  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "16  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "17  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "18  C02520200          Ars 1998-12-04          1998               <NA>   \n",
      "19  C02520200          Ars 1999-10-01          1998               <NA>   \n",
      "20  C02520200          Ars 1999-12-10          1999               <NA>   \n",
      "21  C02520200          Ars 1999-12-24          1999               <NA>   \n",
      "22  C02520200          Ars 2000-09-15          2000               <NA>   \n",
      "23  C02520200          Ars 2000-11-24          2000               <NA>   \n",
      "25  C02520200          Ars 2001-09-28          2001               <NA>   \n",
      "24  C02520200          Ars 2001-11-30          2000               <NA>   \n",
      "26  C02520200          Ars 2001-11-30          2001               <NA>   \n",
      "27  C02520200          Ars 2002-11-15          2001               <NA>   \n",
      "29  C02520200          Ars 2003-05-30          2002               <NA>   \n",
      "28  C02520200          Ars 2003-09-12          2001               <NA>   \n",
      "30  C02520200          Ars 2003-09-26          2002               <NA>   \n",
      "31  C02520200          Ars 2003-10-03          2002               <NA>   \n",
      "32  C02520200          Ars 2004-05-28          2003               <NA>   \n",
      "33  C02520200          Ars 2005-04-08          2004               <NA>   \n",
      "34  C02520200          Ars 2006-03-10          2005               <NA>   \n",
      "35  C02520200          Ars 2006-03-17          2005               <NA>   \n",
      "36  C02520200          Ars 2007-03-15          2006               <NA>   \n",
      "37  C02520200          Ars 2008-02-22          2007               <NA>   \n",
      "38  C02520220          Ars 1995-12-29          1991               <NA>   \n",
      "39  C02520220          Ars 1995-12-29          1992               <NA>   \n",
      "\n",
      "          ce   ce_lag1   ce_lag2   ce_lag3  \n",
      "0   0.014476       NaN       NaN       NaN  \n",
      "1   0.032963  0.014476       NaN       NaN  \n",
      "2   0.037644  0.032963  0.014476       NaN  \n",
      "3   0.073836  0.037644  0.032963  0.014476  \n",
      "4   0.044182  0.073836  0.037644  0.032963  \n",
      "5   0.065060  0.044182  0.073836  0.037644  \n",
      "6   0.076715  0.065060  0.044182  0.073836  \n",
      "7        NaN  0.076715  0.065060  0.044182  \n",
      "8   0.047060  0.076715  0.065060  0.044182  \n",
      "9        NaN       NaN       NaN       NaN  \n",
      "10       NaN       NaN       NaN       NaN  \n",
      "11  0.008736       NaN       NaN       NaN  \n",
      "12  0.025994  0.008736       NaN       NaN  \n",
      "13  0.044773  0.025994  0.008736       NaN  \n",
      "14  0.034026  0.044773  0.025994  0.008736  \n",
      "15  0.064071  0.034026  0.044773  0.025994  \n",
      "16  0.042870  0.064071  0.034026  0.044773  \n",
      "17  0.055578  0.042870  0.064071  0.034026  \n",
      "18  0.062423  0.055578  0.042870  0.064071  \n",
      "19  0.062423  0.055578  0.042870  0.064071  \n",
      "20       NaN  0.062423  0.055578  0.042870  \n",
      "21  0.119052  0.062423  0.055578  0.042870  \n",
      "22       NaN  0.119052  0.062423  0.055578  \n",
      "23       NaN  0.119052  0.062423  0.055578  \n",
      "25       NaN       NaN  0.119052  0.062423  \n",
      "24  0.046435  0.119052  0.062423  0.055578  \n",
      "26  0.010497  0.046435  0.119052  0.062423  \n",
      "27  0.007248  0.046435  0.119052  0.062423  \n",
      "29       NaN  0.007248  0.046435  0.119052  \n",
      "28  0.007248  0.046435  0.119052  0.062423  \n",
      "30  0.000000  0.007248  0.046435  0.119052  \n",
      "31  0.000000  0.007248  0.046435  0.119052  \n",
      "32  0.000000  0.000000  0.007248  0.046435  \n",
      "33  0.023661  0.000000  0.000000  0.007248  \n",
      "34  0.029710  0.023661  0.000000  0.000000  \n",
      "35  0.029710  0.023661  0.000000  0.000000  \n",
      "36  0.078105  0.029710  0.023661  0.000000  \n",
      "37  0.125728  0.078105  0.029710  0.023661  \n",
      "38  0.064611       NaN       NaN       NaN  \n",
      "39  0.102752  0.064611       NaN       NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Inv.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Inv.txt\"           # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"ce\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 3\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfdzly-lsC0j"
   },
   "source": [
    "### Ltg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4485,
     "status": "ok",
     "timestamp": 1765651759344,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "C5z_km6psEhG",
    "outputId": "2cf904fd-3f37-4c21-e639-2a7bb4c7a3b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'at', 'ca', 'cce', 'cl', 'std', 'itp', 'da', 'ar', 'inv', 'oca', 'ppe', 'oa', 'ap', 'ocl', 'ol', 'noa', 'wc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/3727035765.py:172: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/3727035765.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['noa', 'noa_lag1', 'at', 'at_lag1', 'wc', 'wc_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1993                  A   \n",
      "3   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "4   C02500770          Ars 1995-12-29          1994                  A   \n",
      "5   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "6   C02500770          Ars 1996-05-03          1995                  A   \n",
      "7   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "8   C02500770          Ars 1998-07-03          1996                  A   \n",
      "9   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "10  C02500770          Ars 1998-07-03          1997                  A   \n",
      "11  C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "13  C02500770          Ars 1999-10-01          1998                  A   \n",
      "14  C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "12  C02500770          Ars 1999-10-08          1997               <NA>   \n",
      "15  C02500770          Ars 2000-05-19          1999                  A   \n",
      "17  C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "16  C02500770          Ars 2000-05-26          1999                  A   \n",
      "18  C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "19  C02520200          Ars 1996-05-03          1987                  A   \n",
      "20  C02520200          Ars 1996-05-03          1988                  A   \n",
      "21  C02520200          Ars 1996-05-03          1988               <NA>   \n",
      "22  C02520200          Ars 1996-05-03          1989                  A   \n",
      "23  C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "24  C02520200          Ars 1996-05-03          1990                  A   \n",
      "25  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "26  C02520200          Ars 1996-05-03          1991                  A   \n",
      "27  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "28  C02520200          Ars 1996-05-03          1992                  A   \n",
      "29  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "30  C02520200          Ars 1996-05-03          1993                  A   \n",
      "31  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "32  C02520200          Ars 1996-05-03          1994                  A   \n",
      "33  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "34  C02520200          Ars 1996-05-03          1995                  A   \n",
      "35  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "36  C02520200          Ars 1996-11-01          1996                  A   \n",
      "37  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "38  C02520200          Ars 1997-10-31          1997                  A   \n",
      "39  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "\n",
      "         noa           at          wc  noa_lag1      at_lag1     wc_lag1  \n",
      "0   0.266486  1084.355949  -56.599177       NaN          NaN         NaN  \n",
      "1        NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "2   0.240349  1267.313578 -173.336614  0.266486  1084.355949  -56.599177  \n",
      "3        NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "4   0.421504  1460.696865   58.591818  0.240349  1267.313578 -173.336614  \n",
      "5        NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "6   0.520508  1304.537890   74.740059  0.421504  1460.696865   58.591818  \n",
      "7        NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "8   0.427116  1265.820386  -28.061438  0.520508  1304.537890   74.740059  \n",
      "9        NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "10  0.478199  1225.163797  -16.449604  0.427116  1265.820386  -28.061438  \n",
      "11       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "13  0.364814  1102.461138 -163.500400  0.478199  1225.163797  -16.449604  \n",
      "14       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "12       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "15       NaN   945.993798         NaN  0.364814  1102.461138 -163.500400  \n",
      "17       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "16  0.300988   945.993798 -219.195942  0.364814  1102.461138 -163.500400  \n",
      "18       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "19       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "20       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "21       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "22       NaN    23.439270         NaN       NaN          NaN         NaN  \n",
      "23       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "24       NaN   577.207234         NaN       NaN    23.439270         NaN  \n",
      "25       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "26  0.838692  1132.139668  -55.268077       NaN   577.207234         NaN  \n",
      "27       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "28  0.881217  1242.711417   39.471955  0.838692  1132.139668  -55.268077  \n",
      "29       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "30  0.831108  1373.608080   59.738639  0.881217  1242.711417   39.471955  \n",
      "31       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "32  0.771012  1263.968574   18.382917  0.831108  1373.608080   59.738639  \n",
      "33       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "34  0.765135  1301.804038  -27.373330  0.771012  1263.968574   18.382917  \n",
      "35       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "36  0.642205   820.980063  -61.499087  0.765135  1301.804038  -27.373330  \n",
      "37       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "38  0.648958   903.394131    3.481932  0.642205   820.980063  -61.499087  \n",
      "39       NaN          NaN         NaN       NaN          NaN         NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Ltg.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Ltg.txt\"           # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"noa\", \"at\", \"wc\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 1\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_ekkq7isOnj"
   },
   "source": [
    "### Nca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3253,
     "status": "ok",
     "timestamp": 1765651762599,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "JXcyEjhMsXD4",
    "outputId": "8ac61740-7e97-4f3b-e972-95dd564dc5e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'IVAO_oi', 'IVAO_iac', 'IVAO_ltr', 'IVAO_isdfl', 'IVAO_uol', 'at', 'ca', 'lt', 'std', 'ltd', 'ivao', 'oa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/2715927853.py:172: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/2715927853.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['oa', 'oa_lag1', 'at', 'at_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1993                  A   \n",
      "2   C02500770          Ars 1995-12-29          1994                  A   \n",
      "3   C02500770          Ars 1996-05-03          1995                  A   \n",
      "4   C02500770          Ars 1998-07-03          1996                  A   \n",
      "5   C02500770          Ars 1998-07-03          1997                  A   \n",
      "6   C02500770          Ars 1999-10-01          1998                  A   \n",
      "7   C02500770          Ars 2000-05-19          1999                  A   \n",
      "8   C02500770          Ars 2000-05-26          1999                  A   \n",
      "9   C02520200          Ars 1996-05-03          1987                  A   \n",
      "10  C02520200          Ars 1996-05-03          1988                  A   \n",
      "11  C02520200          Ars 1996-05-03          1989                  A   \n",
      "12  C02520200          Ars 1996-05-03          1990                  A   \n",
      "13  C02520200          Ars 1996-05-03          1991                  A   \n",
      "14  C02520200          Ars 1996-05-03          1992                  A   \n",
      "15  C02520200          Ars 1996-05-03          1993                  A   \n",
      "16  C02520200          Ars 1996-05-03          1994                  A   \n",
      "17  C02520200          Ars 1996-05-03          1995                  A   \n",
      "18  C02520200          Ars 1996-11-01          1996                  A   \n",
      "19  C02520200          Ars 1997-10-31          1997                  A   \n",
      "20  C02520200          Ars 1998-12-04          1998                  A   \n",
      "23  C02520200          Ars 1999-10-01          1999                 Q1   \n",
      "21  C02520200          Ars 1999-12-10          1999                  A   \n",
      "22  C02520200          Ars 1999-12-24          1999                  A   \n",
      "27  C02520200          Ars 2000-02-04          2000                 Q1   \n",
      "24  C02520200          Ars 2000-09-15          2000                  A   \n",
      "25  C02520200          Ars 2000-11-24          2000                  A   \n",
      "28  C02520200          Ars 2001-09-28          2001                  A   \n",
      "26  C02520200          Ars 2001-11-30          2000                  A   \n",
      "29  C02520200          Ars 2001-11-30          2001                  A   \n",
      "33  C02520200          Ars 2001-12-07          2002                 Q1   \n",
      "34  C02520200          Ars 2002-08-30          2002                 Q1   \n",
      "30  C02520200          Ars 2002-11-15          2001                  A   \n",
      "31  C02520200          Ars 2003-05-30          2002                  A   \n",
      "36  C02520200          Ars 2003-05-30          2003                 Q1   \n",
      "32  C02520200          Ars 2003-09-26          2002                  A   \n",
      "37  C02520200          Ars 2004-01-23          2003                 Q3   \n",
      "35  C02520200          Ars 2004-05-28          2003                  A   \n",
      "39  C02520200          Ars 2004-05-28          2004                 Q1   \n",
      "40  C02520200          Ars 2004-08-20          2004                 Q2   \n",
      "\n",
      "            oa           at     oa_lag1      at_lag1  \n",
      "0  -360.167884  1084.355949         NaN          NaN  \n",
      "1  -315.221236  1267.313578 -360.167884  1084.355949  \n",
      "2  -256.645351  1460.696865 -315.221236  1267.313578  \n",
      "3   -27.261467  1304.537890 -256.645351  1460.696865  \n",
      "4  -109.070725  1265.820386  -27.261467  1304.537890  \n",
      "5   -35.057939  1225.163797 -109.070725  1265.820386  \n",
      "6   -80.796253  1102.461138  -35.057939  1225.163797  \n",
      "7  -108.873316   945.993798  -80.796253  1102.461138  \n",
      "8  -186.518302   945.993798  -80.796253  1102.461138  \n",
      "9          NaN          NaN         NaN          NaN  \n",
      "10         NaN          NaN         NaN          NaN  \n",
      "11         NaN    23.439270         NaN          NaN  \n",
      "12         NaN   577.207234         NaN    23.439270  \n",
      "13  772.162213  1132.139668         NaN   577.207234  \n",
      "14  890.711066  1242.711417  772.162213  1132.139668  \n",
      "15  917.126090  1373.608080  890.711066  1242.711417  \n",
      "16  759.248425  1263.968574  917.126090  1373.608080  \n",
      "17  777.048650  1301.804038  759.248425  1263.968574  \n",
      "18  320.595034   820.980063  777.048650  1301.804038  \n",
      "19  328.106100   903.394131  320.595034   820.980063  \n",
      "20  332.722314   996.702454  328.106100   903.394131  \n",
      "23         NaN   993.088238         NaN          NaN  \n",
      "21  492.692422   939.325665  332.722314   996.702454  \n",
      "22  365.085906   939.325665  332.722314   996.702454  \n",
      "27         NaN   943.038193         NaN   993.088238  \n",
      "24         NaN   883.100000  365.085906   939.325665  \n",
      "25  441.863652   883.141176  365.085906   939.325665  \n",
      "28  329.582936   807.179058  441.863652   883.141176  \n",
      "26  453.221688   883.141176  365.085906   939.325665  \n",
      "29  399.145104   807.179058  453.221688   883.141176  \n",
      "33         NaN   801.600000         NaN          NaN  \n",
      "34  555.911000  1178.450000         NaN          NaN  \n",
      "30  256.727000   744.540000  453.221688   883.141176  \n",
      "31  690.331000  1574.584000  256.727000   744.540000  \n",
      "36         NaN  1652.395000  555.911000  1178.450000  \n",
      "32  650.449000  1574.584000  256.727000   744.540000  \n",
      "37         NaN  1598.486000         NaN          NaN  \n",
      "35  600.315000  1589.720000  650.449000  1574.584000  \n",
      "39         NaN  1864.086000         NaN  1652.395000  \n",
      "40  618.745000  1973.796000         NaN          NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Nca.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Nca.txt\"           # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"oa\", \"at\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 1\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asxpQXwHsftR"
   },
   "source": [
    "### Noa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2314,
     "status": "ok",
     "timestamp": 1765651764916,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "YyexdBoeshUI",
    "outputId": "8a93433e-6d5b-4183-9937-45b9e23d9e06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'at', 'cce', 'ltd', 'mi', 'ps', 'ce', 'oa', 'ol', 'noa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/2213704475.py:171: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/2213704475.py:171: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['at', 'at_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1993                  A   \n",
      "2   C02500770          Ars 1995-12-29          1994                  A   \n",
      "3   C02500770          Ars 1996-05-03          1995                  A   \n",
      "4   C02500770          Ars 1998-07-03          1996                  A   \n",
      "5   C02500770          Ars 1998-07-03          1997                  A   \n",
      "6   C02500770          Ars 1999-10-01          1998                  A   \n",
      "7   C02500770          Ars 2000-05-19          1999                  A   \n",
      "8   C02520200          Ars 1996-05-03          1987                  A   \n",
      "9   C02520200          Ars 1996-05-03          1988                  A   \n",
      "10  C02520200          Ars 1996-05-03          1989                  A   \n",
      "11  C02520200          Ars 1996-05-03          1990                  A   \n",
      "12  C02520200          Ars 1996-05-03          1991                  A   \n",
      "13  C02520200          Ars 1996-05-03          1992                  A   \n",
      "14  C02520200          Ars 1996-05-03          1993                  A   \n",
      "15  C02520200          Ars 1996-05-03          1994                  A   \n",
      "16  C02520200          Ars 1996-05-03          1995                  A   \n",
      "17  C02520200          Ars 1996-11-01          1996                  A   \n",
      "18  C02520200          Ars 1997-10-31          1997                  A   \n",
      "19  C02520200          Ars 1998-12-04          1998                  A   \n",
      "21  C02520200          Ars 1999-10-01          1999                 Q1   \n",
      "20  C02520200          Ars 1999-12-10          1999                  A   \n",
      "25  C02520200          Ars 2000-02-04          2000                 Q1   \n",
      "22  C02520200          Ars 2000-09-15          2000                  A   \n",
      "23  C02520200          Ars 2000-11-24          2000                  A   \n",
      "26  C02520200          Ars 2001-09-28          2001                  A   \n",
      "24  C02520200          Ars 2001-11-30          2000                  A   \n",
      "27  C02520200          Ars 2001-11-30          2001                  A   \n",
      "31  C02520200          Ars 2001-12-07          2002                 Q1   \n",
      "32  C02520200          Ars 2002-08-30          2002                 Q1   \n",
      "28  C02520200          Ars 2002-11-15          2001                  A   \n",
      "29  C02520200          Ars 2003-05-30          2002                  A   \n",
      "34  C02520200          Ars 2003-05-30          2003                 Q1   \n",
      "30  C02520200          Ars 2003-09-26          2002                  A   \n",
      "35  C02520200          Ars 2004-01-23          2003                 Q3   \n",
      "33  C02520200          Ars 2004-05-28          2003                  A   \n",
      "37  C02520200          Ars 2004-05-28          2004                 Q1   \n",
      "38  C02520200          Ars 2004-08-20          2004                 Q2   \n",
      "39  C02520200          Ars 2004-09-03          2004                 Q2   \n",
      "40  C02520200          Ars 2004-11-12          2004                 Q3   \n",
      "\n",
      "             at      at_lag1  \n",
      "0   1084.355949          NaN  \n",
      "1   1267.313578  1084.355949  \n",
      "2   1460.696865  1267.313578  \n",
      "3   1304.537890  1460.696865  \n",
      "4   1265.820386  1304.537890  \n",
      "5   1225.163797  1265.820386  \n",
      "6   1102.461138  1225.163797  \n",
      "7    945.993798  1102.461138  \n",
      "8           NaN          NaN  \n",
      "9           NaN          NaN  \n",
      "10    23.439270          NaN  \n",
      "11   577.207234    23.439270  \n",
      "12  1132.139668   577.207234  \n",
      "13  1242.711417  1132.139668  \n",
      "14  1373.608080  1242.711417  \n",
      "15  1263.968574  1373.608080  \n",
      "16  1301.804038  1263.968574  \n",
      "17   820.980063  1301.804038  \n",
      "18   903.394131   820.980063  \n",
      "19   996.702454   903.394131  \n",
      "21   993.088238          NaN  \n",
      "20   939.325665   996.702454  \n",
      "25   943.038193   993.088238  \n",
      "22   883.100000   939.325665  \n",
      "23   883.141176   939.325665  \n",
      "26   807.179058   883.141176  \n",
      "24   883.141176   939.325665  \n",
      "27   807.179058   883.141176  \n",
      "31   801.600000          NaN  \n",
      "32  1178.450000          NaN  \n",
      "28   744.540000   883.141176  \n",
      "29  1574.584000   744.540000  \n",
      "34  1652.395000  1178.450000  \n",
      "30  1574.584000   744.540000  \n",
      "35  1598.486000          NaN  \n",
      "33  1589.720000  1574.584000  \n",
      "37  1864.086000  1652.395000  \n",
      "38  1973.796000          NaN  \n",
      "39  1973.796000          NaN  \n",
      "40  2185.037000  1598.486000  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Noa.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Noa.txt\"           # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"at\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "## Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 1\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FS-9QiQrUc6"
   },
   "source": [
    "### Nwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2427,
     "status": "ok",
     "timestamp": 1765651767346,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "1VumTYburXYP",
    "outputId": "db60b30d-bc9f-43b1-cc4c-136d5151c8f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'at', 'ca', 'cce', 'cl', 'std', 'nwc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/279907816.py:172: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/279907816.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['nwc', 'nwc_lag1', 'at', 'at_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1993                  A   \n",
      "2   C02500770          Ars 1995-12-29          1994                  A   \n",
      "3   C02500770          Ars 1996-05-03          1995                  A   \n",
      "4   C02500770          Ars 1998-07-03          1996                  A   \n",
      "5   C02500770          Ars 1998-07-03          1997                  A   \n",
      "6   C02500770          Ars 1999-10-01          1998                  A   \n",
      "7   C02500770          Ars 2000-05-19          1999                  A   \n",
      "8   C02520200          Ars 1996-05-03          1987                  A   \n",
      "9   C02520200          Ars 1996-05-03          1988                  A   \n",
      "10  C02520200          Ars 1996-05-03          1989                  A   \n",
      "11  C02520200          Ars 1996-05-03          1990                  A   \n",
      "12  C02520200          Ars 1996-05-03          1991                  A   \n",
      "13  C02520200          Ars 1996-05-03          1992                  A   \n",
      "14  C02520200          Ars 1996-05-03          1993                  A   \n",
      "15  C02520200          Ars 1996-05-03          1994                  A   \n",
      "16  C02520200          Ars 1996-05-03          1995                  A   \n",
      "17  C02520200          Ars 1996-11-01          1996                  A   \n",
      "18  C02520200          Ars 1997-10-31          1997                  A   \n",
      "19  C02520200          Ars 1998-12-04          1998                  A   \n",
      "22  C02520200          Ars 1999-10-01          1999                 Q1   \n",
      "20  C02520200          Ars 1999-12-10          1999                  A   \n",
      "21  C02520200          Ars 1999-12-24          1999                  A   \n",
      "26  C02520200          Ars 2000-02-04          2000                 Q1   \n",
      "23  C02520200          Ars 2000-09-15          2000                  A   \n",
      "24  C02520200          Ars 2000-11-24          2000                  A   \n",
      "27  C02520200          Ars 2001-09-28          2001                  A   \n",
      "25  C02520200          Ars 2001-11-30          2000                  A   \n",
      "28  C02520200          Ars 2001-11-30          2001                  A   \n",
      "32  C02520200          Ars 2001-12-07          2002                 Q1   \n",
      "33  C02520200          Ars 2002-08-30          2002                 Q1   \n",
      "29  C02520200          Ars 2002-11-15          2001                  A   \n",
      "30  C02520200          Ars 2003-05-30          2002                  A   \n",
      "35  C02520200          Ars 2003-05-30          2003                 Q1   \n",
      "31  C02520200          Ars 2003-09-26          2002                  A   \n",
      "36  C02520200          Ars 2004-01-23          2003                 Q3   \n",
      "34  C02520200          Ars 2004-05-28          2003                  A   \n",
      "38  C02520200          Ars 2004-05-28          2004                 Q1   \n",
      "39  C02520200          Ars 2004-08-20          2004                 Q2   \n",
      "40  C02520200          Ars 2004-09-03          2004                 Q2   \n",
      "\n",
      "           nwc           at    nwc_lag1      at_lag1  \n",
      "0   -29.087169  1084.355949         NaN          NaN  \n",
      "1  -117.599113  1267.313578  -29.087169  1084.355949  \n",
      "2   139.387496  1460.696865 -117.599113  1267.313578  \n",
      "3   164.203131  1304.537890  139.387496  1460.696865  \n",
      "4    46.491315  1265.820386  164.203131  1304.537890  \n",
      "5    53.278401  1225.163797   46.491315  1265.820386  \n",
      "6  -109.278965  1102.461138   53.278401  1225.163797  \n",
      "7  -166.556880   945.993798 -109.278965  1102.461138  \n",
      "8          NaN          NaN         NaN          NaN  \n",
      "9          NaN          NaN         NaN          NaN  \n",
      "10         NaN    23.439270         NaN          NaN  \n",
      "11         NaN   577.207234         NaN    23.439270  \n",
      "12  -14.898818  1132.139668         NaN   577.207234  \n",
      "13   74.794036  1242.711417  -14.898818  1132.139668  \n",
      "14   97.673344  1373.608080   74.794036  1242.711417  \n",
      "15   59.105494  1263.968574   97.673344  1373.608080  \n",
      "16   22.243744  1301.804038   59.105494  1263.968574  \n",
      "17  -12.044828   820.980063   22.243744  1301.804038  \n",
      "18   46.216080   903.394131  -12.044828   820.980063  \n",
      "19   98.961739   996.702454   46.216080   903.394131  \n",
      "22   51.301192   993.088238         NaN          NaN  \n",
      "20   32.722165   939.325665   98.961739   996.702454  \n",
      "21   24.391690   939.325665   98.961739   996.702454  \n",
      "26   58.817436   943.038193   51.301192   993.088238  \n",
      "23   12.800000   883.100000   24.391690   939.325665  \n",
      "24   12.748264   883.141176   24.391690   939.325665  \n",
      "27   52.961666   807.179058   12.748264   883.141176  \n",
      "25  115.119958   883.141176   24.391690   939.325665  \n",
      "28  158.797904   807.179058  115.119958   883.141176  \n",
      "32         NaN   801.600000         NaN          NaN  \n",
      "33         NaN  1178.450000         NaN          NaN  \n",
      "29   28.514000   744.540000  115.119958   883.141176  \n",
      "30  177.466000  1574.584000   28.514000   744.540000  \n",
      "35         NaN  1652.395000         NaN  1178.450000  \n",
      "31  170.538000  1574.584000   28.514000   744.540000  \n",
      "36         NaN  1598.486000         NaN          NaN  \n",
      "34  174.165000  1589.720000  170.538000  1574.584000  \n",
      "38         NaN  1864.086000         NaN  1652.395000  \n",
      "39  264.964000  1973.796000         NaN          NaN  \n",
      "40  248.169000  1973.796000         NaN          NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Nwc.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Nwc.txt\"           # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"nwc\", \"at\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 1\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPp60Qu_rlMi"
   },
   "source": [
    "### Osc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3321,
     "status": "ok",
     "timestamp": 1765651770670,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "RGNWNZp-rn0F",
    "outputId": "eca5f7ca-efba-4096-a278-8eda583ba42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'at', 'tl', 'ca', 'cl', 'ni_extra', 'IFO_ffo', 'IFO_ei', 'IFO_dofa', 'IFO_ffooa', 'ifo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/2332176187.py:172: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/2332176187.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['ni_extra', 'ni_extra_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1993                  A   \n",
      "3   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "4   C02500770          Ars 1995-12-29          1994                  A   \n",
      "5   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "6   C02500770          Ars 1996-05-03          1995                  A   \n",
      "7   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "8   C02500770          Ars 1998-07-03          1996                  A   \n",
      "9   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "11  C02500770          Ars 1998-07-03          1997                  A   \n",
      "12  C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "10  C02500770          Ars 1998-07-31          1996               <NA>   \n",
      "13  C02500770          Ars 1998-07-31          1997               <NA>   \n",
      "14  C02500770          Ars 1999-10-01          1998                  A   \n",
      "15  C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "16  C02500770          Ars 2000-05-19          1999                  A   \n",
      "17  C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "18  C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "19  C02520200          Ars 1996-05-03          1987                  A   \n",
      "20  C02520200          Ars 1996-05-03          1987               <NA>   \n",
      "21  C02520200          Ars 1996-05-03          1988                  A   \n",
      "22  C02520200          Ars 1996-05-03          1988               <NA>   \n",
      "23  C02520200          Ars 1996-05-03          1989                  A   \n",
      "24  C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "25  C02520200          Ars 1996-05-03          1990                  A   \n",
      "26  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "27  C02520200          Ars 1996-05-03          1991                  A   \n",
      "28  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "29  C02520200          Ars 1996-05-03          1992                  A   \n",
      "30  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "31  C02520200          Ars 1996-05-03          1993                  A   \n",
      "32  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "33  C02520200          Ars 1996-05-03          1994                  A   \n",
      "34  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "35  C02520200          Ars 1996-05-03          1995                  A   \n",
      "36  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "37  C02520200          Ars 1996-11-01          1996                  A   \n",
      "38  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "39  C02520200          Ars 1997-10-31          1997                  A   \n",
      "\n",
      "      ni_extra  ni_extra_lag1  \n",
      "0    97.902980            NaN  \n",
      "1    97.902980            NaN  \n",
      "2    92.230294      97.902980  \n",
      "3    92.230294      97.902980  \n",
      "4   110.555887      92.230294  \n",
      "5   110.555887      92.230294  \n",
      "6   -43.448501     110.555887  \n",
      "7   -43.448501     110.555887  \n",
      "8    39.188143     -43.448501  \n",
      "9    39.188143     -43.448501  \n",
      "11  -76.849781      39.188143  \n",
      "12  -76.849781      39.188143  \n",
      "10   39.188143     -43.448501  \n",
      "13  -76.849781      39.188143  \n",
      "14 -183.703294     -76.849781  \n",
      "15 -183.703294     -76.849781  \n",
      "16 -233.466939    -183.703294  \n",
      "17 -233.466939    -183.703294  \n",
      "18 -233.446939    -183.703294  \n",
      "19    0.000000            NaN  \n",
      "20    0.000000            NaN  \n",
      "21    0.000000       0.000000  \n",
      "22    0.000000       0.000000  \n",
      "23   -0.097085       0.000000  \n",
      "24   -0.097085       0.000000  \n",
      "25   10.346499      -0.097085  \n",
      "26   10.346499      -0.097085  \n",
      "27  -89.747974      10.346499  \n",
      "28  -89.747974      10.346499  \n",
      "29   15.771662     -89.747974  \n",
      "30   15.771662     -89.747974  \n",
      "31  -22.573776      15.771662  \n",
      "32  -22.573776      15.771662  \n",
      "33  -15.711121     -22.573776  \n",
      "34  -15.711121     -22.573776  \n",
      "35   -5.063625     -15.711121  \n",
      "36   -5.063625     -15.711121  \n",
      "37  -69.781976      -5.063625  \n",
      "38  -69.781976      -5.063625  \n",
      "39   28.173400     -69.781976  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Osc.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Osc.txt\"           # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"ni_extra\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 1\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRIDrpHqsP-3"
   },
   "source": [
    "### Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2760,
     "status": "ok",
     "timestamp": 1765651773450,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "BzU5WNYqsRke",
    "outputId": "246a588d-7abc-436c-e438-66f0473998a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'ni_eps', 'at']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/3953908033.py:172: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/3953908033.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['at', 'at_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1993                  A   \n",
      "3   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "4   C02500770          Ars 1995-12-29          1994                  A   \n",
      "5   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "6   C02500770          Ars 1996-05-03          1995                  A   \n",
      "7   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "8   C02500770          Ars 1998-07-03          1996                  A   \n",
      "9   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "10  C02500770          Ars 1998-07-03          1997                  A   \n",
      "11  C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "12  C02500770          Ars 1999-10-01          1998                  A   \n",
      "13  C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "14  C02500770          Ars 2000-05-19          1999                  A   \n",
      "15  C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "16  C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "17  C02520200          Ars 1996-05-03          1987               <NA>   \n",
      "18  C02520200          Ars 1996-05-03          1988               <NA>   \n",
      "19  C02520200          Ars 1996-05-03          1989                  A   \n",
      "20  C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "21  C02520200          Ars 1996-05-03          1990                  A   \n",
      "22  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "23  C02520200          Ars 1996-05-03          1991                  A   \n",
      "24  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "25  C02520200          Ars 1996-05-03          1992                  A   \n",
      "26  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "27  C02520200          Ars 1996-05-03          1993                  A   \n",
      "28  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "29  C02520200          Ars 1996-05-03          1994                  A   \n",
      "30  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "31  C02520200          Ars 1996-05-03          1995                  A   \n",
      "32  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "33  C02520200          Ars 1996-11-01          1996                  A   \n",
      "34  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "35  C02520200          Ars 1997-10-31          1997                  A   \n",
      "36  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "37  C02520200          Ars 1998-12-04          1998                  A   \n",
      "38  C02520200          Ars 1998-12-04          1998               <NA>   \n",
      "40  C02520200          Ars 1999-10-01          1999                 Q1   \n",
      "\n",
      "             at      at_lag1  \n",
      "0   1084.355949          NaN  \n",
      "1           NaN          NaN  \n",
      "2   1267.313578  1084.355949  \n",
      "3           NaN          NaN  \n",
      "4   1460.696865  1267.313578  \n",
      "5           NaN          NaN  \n",
      "6   1304.537890  1460.696865  \n",
      "7           NaN          NaN  \n",
      "8   1265.820386  1304.537890  \n",
      "9           NaN          NaN  \n",
      "10  1225.163797  1265.820386  \n",
      "11          NaN          NaN  \n",
      "12  1102.461138  1225.163797  \n",
      "13          NaN          NaN  \n",
      "14   945.993798  1102.461138  \n",
      "15          NaN          NaN  \n",
      "16          NaN          NaN  \n",
      "17          NaN          NaN  \n",
      "18          NaN          NaN  \n",
      "19    23.439270          NaN  \n",
      "20          NaN          NaN  \n",
      "21   577.207234    23.439270  \n",
      "22          NaN          NaN  \n",
      "23  1132.139668   577.207234  \n",
      "24          NaN          NaN  \n",
      "25  1242.711417  1132.139668  \n",
      "26          NaN          NaN  \n",
      "27  1373.608080  1242.711417  \n",
      "28          NaN          NaN  \n",
      "29  1263.968574  1373.608080  \n",
      "30          NaN          NaN  \n",
      "31  1301.804038  1263.968574  \n",
      "32          NaN          NaN  \n",
      "33   820.980063  1301.804038  \n",
      "34          NaN          NaN  \n",
      "35   903.394131   820.980063  \n",
      "36          NaN          NaN  \n",
      "37   996.702454   903.394131  \n",
      "38          NaN          NaN  \n",
      "40   993.088238          NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Pro.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Pro.txt\"           # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"at\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 1\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYvpA1c8sYnf"
   },
   "source": [
    "### Rs (Yearly Adaption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2247,
     "status": "ok",
     "timestamp": 1765651775700,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "F5d9NveiKcEl",
    "outputId": "1c7e6145-56b1-40e9-a5fe-363f0b4750be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'rev']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/3712779696.py:165: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/3712779696.py:165: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['rev', 'rev_lag1', 'rev_lag2', 'rev_lag3', 'rev_lag4']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "1   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "3   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "4   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "5   C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "6   C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "7   C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "8   C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "9   C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "10  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "11  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "12  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "13  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "14  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "15  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "16  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "17  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "18  C02520200          Ars 1998-12-04          1998               <NA>   \n",
      "19  C02520200          Ars 1999-10-01          1998               <NA>   \n",
      "20  C02520200          Ars 1999-12-10          1999               <NA>   \n",
      "21  C02520200          Ars 2000-09-15          2000               <NA>   \n",
      "22  C02520200          Ars 2000-11-24          2000               <NA>   \n",
      "24  C02520200          Ars 2001-09-28          2001               <NA>   \n",
      "23  C02520200          Ars 2001-11-30          2000               <NA>   \n",
      "25  C02520200          Ars 2001-11-30          2001               <NA>   \n",
      "26  C02520200          Ars 2002-11-15          2001               <NA>   \n",
      "28  C02520200          Ars 2003-05-30          2002               <NA>   \n",
      "27  C02520200          Ars 2003-09-12          2001               <NA>   \n",
      "29  C02520200          Ars 2003-10-03          2002               <NA>   \n",
      "30  C02520200          Ars 2004-05-28          2003               <NA>   \n",
      "31  C02520200          Ars 2005-04-08          2004               <NA>   \n",
      "32  C02520200          Ars 2006-03-10          2005               <NA>   \n",
      "33  C02520200          Ars 2006-03-17          2005               <NA>   \n",
      "34  C02520200          Ars 2007-03-15          2006               <NA>   \n",
      "35  C02520200          Ars 2008-02-22          2007               <NA>   \n",
      "36  C02520220          Ars 1995-12-29          1991               <NA>   \n",
      "37  C02520220          Ars 1995-12-29          1992               <NA>   \n",
      "38  C02520220          Ars 1995-12-29          1993               <NA>   \n",
      "39  C02520220          Ars 1995-12-29          1994               <NA>   \n",
      "\n",
      "            rev     rev_lag1     rev_lag2     rev_lag3     rev_lag4  \n",
      "0   1707.334371          NaN          NaN          NaN          NaN  \n",
      "1   1921.517802  1707.334371          NaN          NaN          NaN  \n",
      "2   2263.705199  1921.517802  1707.334371          NaN          NaN  \n",
      "3   1447.331160  2263.705199  1921.517802  1707.334371          NaN  \n",
      "4   1217.952697  1447.331160  2263.705199  1921.517802  1707.334371  \n",
      "5    693.936079  1217.952697  1447.331160  2263.705199  1921.517802  \n",
      "6    609.279252   693.936079  1217.952697  1447.331160  2263.705199  \n",
      "7    559.817319   609.279252   693.936079  1217.952697  1447.331160  \n",
      "8    501.495166   609.279252   693.936079  1217.952697  1447.331160  \n",
      "9      8.063120          NaN          NaN          NaN          NaN  \n",
      "10   212.478635     8.063120          NaN          NaN          NaN  \n",
      "11   341.614996   212.478635     8.063120          NaN          NaN  \n",
      "12   466.963321   341.614996   212.478635     8.063120          NaN  \n",
      "13   473.429402   466.963321   341.614996   212.478635     8.063120  \n",
      "14   528.266494   473.429402   466.963321   341.614996   212.478635  \n",
      "15   590.330351   528.266494   473.429402   466.963321   341.614996  \n",
      "16   567.494357   590.330351   528.266494   473.429402   466.963321  \n",
      "17   600.805773   567.494357   590.330351   528.266494   473.429402  \n",
      "18   672.248976   600.805773   567.494357   590.330351   528.266494  \n",
      "19   672.248976   600.805773   567.494357   590.330351   528.266494  \n",
      "20   559.036090   672.248976   600.805773   567.494357   590.330351  \n",
      "21   493.200000   559.036090   672.248976   600.805773   567.494357  \n",
      "22   493.248201   559.036090   672.248976   600.805773   567.494357  \n",
      "24   489.951220   493.248201   559.036090   672.248976   600.805773  \n",
      "23   493.248000   559.036090   672.248976   600.805773   567.494357  \n",
      "25   489.951000   493.248000   559.036090   672.248976   600.805773  \n",
      "26   384.092000   493.248000   559.036090   672.248976   600.805773  \n",
      "28  1062.211000   384.092000   493.248000   559.036090   672.248976  \n",
      "27   384.092000   493.248000   559.036090   672.248976   600.805773  \n",
      "29  1062.211000   384.092000   493.248000   559.036090   672.248976  \n",
      "30  1347.114000  1062.211000   384.092000   493.248000   559.036090  \n",
      "31  2119.374000  1347.114000  1062.211000   384.092000   493.248000  \n",
      "32  2543.229357  2119.374000  1347.114000  1062.211000   384.092000  \n",
      "33  2543.229357  2119.374000  1347.114000  1062.211000   384.092000  \n",
      "34  2779.139622  2543.229357  2119.374000  1347.114000  1062.211000  \n",
      "35  3107.929339  2779.139622  2543.229357  2119.374000  1347.114000  \n",
      "36   350.433117          NaN          NaN          NaN          NaN  \n",
      "37   371.958889   350.433117          NaN          NaN          NaN  \n",
      "38   330.198427   371.958889   350.433117          NaN          NaN  \n",
      "39   379.520044   330.198427   371.958889   350.433117          NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Rs.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"rev_lag1\" comes from 2019 Q1, not 2019 A.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Rs.txt\"            # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"rev\"]               # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 4\n",
    "\n",
    "# Period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# New logic:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FP = t, period = P), \"<col>_lagk\" comes from (FP = t-k, period = P)\n",
    "#   based on all PIT updates up to that row.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we treat the period label as None and thus\n",
    "# fall back to the old \"by FiscalPeriod only\" behaviour for that row.\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Copy so we don't mutate previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_U_zo4cstxB"
   },
   "source": [
    "### Sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2060,
     "status": "ok",
     "timestamp": 1765651777764,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "D_uvb2VEs2Hs",
    "outputId": "68c2dde2-0449-4ccf-94f4-c7fe2984b944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'ce', 'dt', 'be']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/571143861.py:172: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/571143861.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['be', 'be_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1993                  A   \n",
      "2   C02500770          Ars 1995-12-29          1994                  A   \n",
      "3   C02500770          Ars 1996-05-03          1995                  A   \n",
      "4   C02500770          Ars 1998-07-03          1996                  A   \n",
      "5   C02500770          Ars 1998-07-03          1997                  A   \n",
      "6   C02500770          Ars 1999-10-01          1998                  A   \n",
      "7   C02500770          Ars 2000-05-19          1999                  A   \n",
      "8   C02520200          Ars 1996-05-03          1987                  A   \n",
      "9   C02520200          Ars 1996-05-03          1988                  A   \n",
      "10  C02520200          Ars 1996-05-03          1989                  A   \n",
      "11  C02520200          Ars 1996-05-03          1990                  A   \n",
      "12  C02520200          Ars 1996-05-03          1991                  A   \n",
      "13  C02520200          Ars 1996-05-03          1992                  A   \n",
      "14  C02520200          Ars 1996-05-03          1993                  A   \n",
      "15  C02520200          Ars 1996-05-03          1994                  A   \n",
      "16  C02520200          Ars 1996-05-03          1995                  A   \n",
      "17  C02520200          Ars 1996-11-01          1996                  A   \n",
      "18  C02520200          Ars 1997-10-31          1997                  A   \n",
      "19  C02520200          Ars 1998-12-04          1998                  A   \n",
      "20  C02520200          Ars 1999-04-02          1998                  A   \n",
      "22  C02520200          Ars 1999-10-01          1999                 Q1   \n",
      "21  C02520200          Ars 1999-12-10          1999                  A   \n",
      "25  C02520200          Ars 2000-02-04          2000                 Q1   \n",
      "23  C02520200          Ars 2000-09-15          2000                  A   \n",
      "24  C02520200          Ars 2000-11-24          2000                  A   \n",
      "26  C02520200          Ars 2001-09-28          2001                  A   \n",
      "27  C02520200          Ars 2002-11-15          2001                  A   \n",
      "28  C02520200          Ars 2003-05-30          2002                  A   \n",
      "30  C02520200          Ars 2003-05-30          2003                 Q1   \n",
      "31  C02520200          Ars 2004-01-23          2003                 Q3   \n",
      "29  C02520200          Ars 2004-05-28          2003                  A   \n",
      "33  C02520200          Ars 2004-08-20          2004                 Q2   \n",
      "34  C02520200          Ars 2004-11-12          2004                 Q3   \n",
      "32  C02520200          Ars 2005-04-08          2004                  A   \n",
      "37  C02520200          Ars 2005-05-13          2005                 Q1   \n",
      "38  C02520200          Ars 2005-08-05          2005                 Q2   \n",
      "39  C02520200          Ars 2005-11-18          2005                 Q3   \n",
      "35  C02520200          Ars 2006-03-10          2005                  A   \n",
      "36  C02520200          Ars 2006-03-17          2005                  A   \n",
      "\n",
      "             be      be_lag1  \n",
      "0           NaN          NaN  \n",
      "1           NaN          NaN  \n",
      "2           NaN          NaN  \n",
      "3           NaN          NaN  \n",
      "4           NaN          NaN  \n",
      "5    292.572607          NaN  \n",
      "6     74.576516   292.572607  \n",
      "7    113.605403    74.576516  \n",
      "8           NaN          NaN  \n",
      "9           NaN          NaN  \n",
      "10          NaN          NaN  \n",
      "11          NaN          NaN  \n",
      "12   805.404692          NaN  \n",
      "13          NaN   805.404692  \n",
      "14          NaN          NaN  \n",
      "15          NaN          NaN  \n",
      "16          NaN          NaN  \n",
      "17   320.569078          NaN  \n",
      "18   350.219678   320.569078  \n",
      "19   427.908337   350.219678  \n",
      "20   419.494337   350.219678  \n",
      "22   436.098868          NaN  \n",
      "21          NaN   419.494337  \n",
      "25   376.314678   436.098868  \n",
      "23          NaN          NaN  \n",
      "24          NaN          NaN  \n",
      "26          NaN          NaN  \n",
      "27   131.412000          NaN  \n",
      "28   -29.012000   131.412000  \n",
      "30          NaN          NaN  \n",
      "31   380.912000          NaN  \n",
      "29          NaN   -29.012000  \n",
      "33          NaN          NaN  \n",
      "34          NaN   380.912000  \n",
      "32  1483.436000          NaN  \n",
      "37          NaN          NaN  \n",
      "38  1451.330881          NaN  \n",
      "39  1586.148895          NaN  \n",
      "35          NaN  1483.436000  \n",
      "36  1712.637432  1483.436000  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Sg.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Sg.txt\"           # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"be\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 1\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnClUmvQs9tS"
   },
   "source": [
    "### Sli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2862,
     "status": "ok",
     "timestamp": 1765651780656,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "SnPOPuartDwc",
    "outputId": "e3d9d736-1f6f-4d27-8e0f-dda30f5682a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'rev', 'inv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/4160811033.py:172: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/4160811033.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['rev', 'rev_lag1', 'rev_lag2', 'inv', 'inv_lag1', 'inv_lag2']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1993                  A   \n",
      "3   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "4   C02500770          Ars 1995-12-29          1994                  A   \n",
      "5   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "6   C02500770          Ars 1996-05-03          1995                  A   \n",
      "7   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "8   C02500770          Ars 1998-07-03          1996                  A   \n",
      "9   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "10  C02500770          Ars 1998-07-03          1997                  A   \n",
      "11  C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "12  C02500770          Ars 1999-10-01          1998                  A   \n",
      "13  C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "15  C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "14  C02500770          Ars 2000-05-26          1999                  A   \n",
      "16  C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "17  C02520200          Ars 1996-05-03          1987                  A   \n",
      "18  C02520200          Ars 1996-05-03          1988                  A   \n",
      "19  C02520200          Ars 1996-05-03          1989                  A   \n",
      "20  C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "21  C02520200          Ars 1996-05-03          1990                  A   \n",
      "22  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "23  C02520200          Ars 1996-05-03          1991                  A   \n",
      "24  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "25  C02520200          Ars 1996-05-03          1992                  A   \n",
      "26  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "27  C02520200          Ars 1996-05-03          1993                  A   \n",
      "28  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "29  C02520200          Ars 1996-05-03          1994                  A   \n",
      "30  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "31  C02520200          Ars 1996-05-03          1995                  A   \n",
      "32  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "33  C02520200          Ars 1996-11-01          1996                  A   \n",
      "34  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "35  C02520200          Ars 1997-10-31          1997                  A   \n",
      "36  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "37  C02520200          Ars 1998-12-04          1998                  A   \n",
      "38  C02520200          Ars 1998-12-04          1998               <NA>   \n",
      "39  C02520200          Ars 1999-10-01          1998               <NA>   \n",
      "\n",
      "            rev         inv     rev_lag1     rev_lag2    inv_lag1    inv_lag2  \n",
      "0   1707.334371  237.409600          NaN          NaN         NaN         NaN  \n",
      "1   1707.334371         NaN          NaN          NaN         NaN         NaN  \n",
      "2   1921.517802  201.182443  1707.334371          NaN  237.409600         NaN  \n",
      "3   1921.517802         NaN  1707.334371          NaN         NaN         NaN  \n",
      "4   2263.705199  289.925405  1921.517802  1707.334371  201.182443  237.409600  \n",
      "5   2263.705199         NaN  1921.517802  1707.334371         NaN         NaN  \n",
      "6   1447.331160  223.628192  2263.705199  1921.517802  289.925405  201.182443  \n",
      "7   1447.331160         NaN  2263.705199  1921.517802         NaN         NaN  \n",
      "8   1217.952697  177.096926  1447.331160  2263.705199  223.628192  289.925405  \n",
      "9   1217.952697         NaN  1447.331160  2263.705199         NaN         NaN  \n",
      "10   693.936079  173.896930  1217.952697  1447.331160  177.096926  223.628192  \n",
      "11   693.936079         NaN  1217.952697  1447.331160         NaN         NaN  \n",
      "12   609.279252  204.671283   693.936079  1217.952697  173.896930  177.096926  \n",
      "13   609.279252         NaN   693.936079  1217.952697         NaN         NaN  \n",
      "15   559.817319         NaN   609.279252   693.936079         NaN         NaN  \n",
      "14   501.495166  158.117346   609.279252   693.936079  204.671283  173.896930  \n",
      "16   501.495166         NaN   609.279252   693.936079         NaN         NaN  \n",
      "17          NaN    0.020612          NaN          NaN         NaN         NaN  \n",
      "18          NaN    0.072913          NaN          NaN    0.020612         NaN  \n",
      "19     8.063120    1.839860          NaN          NaN    0.072913    0.020612  \n",
      "20     8.063120         NaN          NaN          NaN         NaN         NaN  \n",
      "21   212.478635   40.650548     8.063120          NaN    1.839860    0.072913  \n",
      "22   212.478635         NaN     8.063120          NaN         NaN         NaN  \n",
      "23   341.614996   76.440596   212.478635     8.063120   40.650548    1.839860  \n",
      "24   341.614996         NaN   212.478635     8.063120         NaN         NaN  \n",
      "25   466.963321   99.373808   341.614996   212.478635   76.440596   40.650548  \n",
      "26   466.963321         NaN   341.614996   212.478635         NaN         NaN  \n",
      "27   473.429402  100.494805   466.963321   341.614996   99.373808   76.440596  \n",
      "28   473.429402         NaN   466.963321   341.614996         NaN         NaN  \n",
      "29   528.266494  100.692784   473.429402   466.963321  100.494805   99.373808  \n",
      "30   528.266494         NaN   473.429402   466.963321         NaN         NaN  \n",
      "31   590.330351   98.115757   528.266494   473.429402  100.692784  100.494805  \n",
      "32   590.330351         NaN   528.266494   473.429402         NaN         NaN  \n",
      "33   567.494357   63.467400   590.330351   528.266494   98.115757  100.692784  \n",
      "34   567.494357         NaN   590.330351   528.266494         NaN         NaN  \n",
      "35   600.805773  104.144450   567.494357   590.330351   63.467400   98.115757  \n",
      "36   600.805773         NaN   567.494357   590.330351         NaN         NaN  \n",
      "37   672.248976  145.107780   600.805773   567.494357  104.144450   63.467400  \n",
      "38   672.248976         NaN   600.805773   567.494357         NaN         NaN  \n",
      "39   672.248976         NaN   600.805773   567.494357         NaN         NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Sli.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Sli.txt\"           # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"rev\", \"inv\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 2\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlLntIw1tJ_7"
   },
   "source": [
    "### Slx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2105,
     "status": "ok",
     "timestamp": 1765651782764,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "DwI_oblZtL49",
    "outputId": "72ad9886-18d2-47f2-83e3-96d99f1c7853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'rev', 'sga']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/2288214793.py:172: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/2288214793.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['rev', 'rev_lag1', 'rev_lag2', 'sga', 'sga_lag1', 'sga_lag2']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "1   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "3   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "4   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "5   C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "7   C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "6   C02500770          Ars 1999-10-08          1997               <NA>   \n",
      "8   C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "9   C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "10  C02520200          Ars 1996-05-03          1988               <NA>   \n",
      "11  C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "12  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "13  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "14  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "15  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "16  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "17  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "18  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "19  C02520200          Ars 1997-02-28          1996               <NA>   \n",
      "20  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "21  C02520200          Ars 1998-12-04          1998               <NA>   \n",
      "22  C02520200          Ars 1999-10-01          1998               <NA>   \n",
      "23  C02520200          Ars 1999-12-10          1999               <NA>   \n",
      "24  C02520200          Ars 1999-12-24          1999               <NA>   \n",
      "25  C02520200          Ars 2000-09-15          2000               <NA>   \n",
      "26  C02520200          Ars 2000-11-24          2000               <NA>   \n",
      "28  C02520200          Ars 2001-09-28          2001               <NA>   \n",
      "27  C02520200          Ars 2001-11-30          2000               <NA>   \n",
      "29  C02520200          Ars 2001-11-30          2001               <NA>   \n",
      "30  C02520200          Ars 2002-11-15          2001               <NA>   \n",
      "32  C02520200          Ars 2003-05-30          2002               <NA>   \n",
      "31  C02520200          Ars 2003-09-12          2001               <NA>   \n",
      "33  C02520200          Ars 2003-09-26          2002               <NA>   \n",
      "34  C02520200          Ars 2003-10-03          2002               <NA>   \n",
      "35  C02520200          Ars 2004-05-28          2003               <NA>   \n",
      "36  C02520200          Ars 2005-04-08          2004               <NA>   \n",
      "37  C02520200          Ars 2006-03-10          2005               <NA>   \n",
      "38  C02520200          Ars 2006-03-17          2005               <NA>   \n",
      "39  C02520200          Ars 2007-03-15          2006               <NA>   \n",
      "\n",
      "            rev         sga     rev_lag1     rev_lag2    sga_lag1    sga_lag2  \n",
      "0   1707.334371  171.617279          NaN          NaN         NaN         NaN  \n",
      "1   1921.517802  189.549299  1707.334371          NaN  171.617279         NaN  \n",
      "2   2263.705199  129.874835  1921.517802  1707.334371  189.549299  171.617279  \n",
      "3   1447.331160  137.582823  2263.705199  1921.517802  129.874835  189.549299  \n",
      "4   1217.952697   98.561032  1447.331160  2263.705199  137.582823  129.874835  \n",
      "5    693.936079   87.766458  1217.952697  1447.331160   98.561032  137.582823  \n",
      "7    609.279252   59.705642   693.936079  1217.952697   87.766458   98.561032  \n",
      "6    693.936079   77.840413  1217.952697  1447.331160   98.561032  137.582823  \n",
      "8    559.817319         NaN   609.279252   693.936079   59.705642   77.840413  \n",
      "9    501.495166   55.599353   609.279252   693.936079   59.705642   77.840413  \n",
      "10          NaN    0.030673          NaN          NaN         NaN         NaN  \n",
      "11     8.063120    0.538848          NaN          NaN    0.030673         NaN  \n",
      "12   212.478635   13.761389     8.063120          NaN    0.538848    0.030673  \n",
      "13   341.614996   41.612142   212.478635     8.063120   13.761389    0.538848  \n",
      "14   466.963321   37.745526   341.614996   212.478635   41.612142   13.761389  \n",
      "15   473.429402   43.272056   466.963321   341.614996   37.745526   41.612142  \n",
      "16   528.266494   41.026957   473.429402   466.963321   43.272056   37.745526  \n",
      "17   590.330351   47.615547   528.266494   473.429402   41.026957   43.272056  \n",
      "18   567.494357   43.145000   590.330351   528.266494   47.615547   41.026957  \n",
      "19   567.494357   50.583162   590.330351   528.266494   47.615547   41.026957  \n",
      "20   600.805773   51.624044   567.494357   590.330351   50.583162   47.615547  \n",
      "21   672.248976   50.386344   600.805773   567.494357   51.624044   50.583162  \n",
      "22   672.248976   50.386344   600.805773   567.494357   51.624044   50.583162  \n",
      "23   559.036090         NaN   672.248976   600.805773   50.386344   51.624044  \n",
      "24   559.036090   50.112686   672.248976   600.805773   50.386344   51.624044  \n",
      "25   493.200000         NaN   559.036090   672.248976   50.112686   50.386344  \n",
      "26   493.248201         NaN   559.036090   672.248976   50.112686   50.386344  \n",
      "28   489.951220         NaN   493.248201   559.036090         NaN   50.112686  \n",
      "27   493.248000   60.251000   559.036090   672.248976   50.112686   50.386344  \n",
      "29   489.951000   61.433000   493.248000   559.036090   60.251000   50.112686  \n",
      "30   384.092000   38.380000   493.248000   559.036090   60.251000   50.112686  \n",
      "32  1062.211000         NaN   384.092000   493.248000   38.380000   60.251000  \n",
      "31   384.092000   38.380000   493.248000   559.036090   60.251000   50.112686  \n",
      "33  1062.211000   99.372000   384.092000   493.248000   38.380000   60.251000  \n",
      "34  1062.211000   99.372000   384.092000   493.248000   38.380000   60.251000  \n",
      "35  1347.114000   46.236000  1062.211000   384.092000   99.372000   38.380000  \n",
      "36  2119.374000   64.087000  1347.114000  1062.211000   46.236000   99.372000  \n",
      "37  2543.229357   87.396763  2119.374000  1347.114000   64.087000   46.236000  \n",
      "38  2543.229357   85.478517  2119.374000  1347.114000   64.087000   46.236000  \n",
      "39  2779.139622  105.631523  2543.229357  2119.374000   85.478517   64.087000  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Slx.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Slx.txt\"           # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"rev\", \"sga\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 2\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9FctGGaKNNQ"
   },
   "source": [
    "### Txf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2899,
     "status": "ok",
     "timestamp": 1765651785747,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "cLmlrLEeKT21",
    "outputId": "7fb7515f-5ee7-4d9e-9659-42e8d221ba93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'socaps', 'pocaps', 'div', 'diss', 'dred', 'at']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/3082064015.py:172: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/3082064015.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['at', 'at_lag1', 'at_lag2']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1993                  A   \n",
      "3   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "4   C02500770          Ars 1995-12-29          1994                  A   \n",
      "5   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "6   C02500770          Ars 1996-05-03          1995                  A   \n",
      "7   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "8   C02500770          Ars 1996-05-31          1995               <NA>   \n",
      "9   C02500770          Ars 1998-07-03          1996                  A   \n",
      "10  C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "11  C02500770          Ars 1998-07-03          1997                  A   \n",
      "12  C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "13  C02500770          Ars 1999-10-01          1998                  A   \n",
      "14  C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "15  C02500770          Ars 2000-05-19          1999                  A   \n",
      "16  C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "17  C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "18  C02520200          Ars 1996-05-03          1987               <NA>   \n",
      "19  C02520200          Ars 1996-05-03          1989                  A   \n",
      "20  C02520200          Ars 1996-05-03          1990                  A   \n",
      "21  C02520200          Ars 1996-05-03          1991                  A   \n",
      "22  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "23  C02520200          Ars 1996-05-03          1992                  A   \n",
      "24  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "25  C02520200          Ars 1996-05-03          1993                  A   \n",
      "26  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "27  C02520200          Ars 1996-05-03          1994                  A   \n",
      "28  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "29  C02520200          Ars 1996-05-03          1995                  A   \n",
      "30  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "31  C02520200          Ars 1996-11-01          1996                  A   \n",
      "32  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "33  C02520200          Ars 1997-10-31          1997                  A   \n",
      "34  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "35  C02520200          Ars 1998-12-04          1998                  A   \n",
      "36  C02520200          Ars 1998-12-04          1998               <NA>   \n",
      "38  C02520200          Ars 1999-10-01          1999                 Q1   \n",
      "37  C02520200          Ars 1999-12-10          1999                  A   \n",
      "39  C02520200          Ars 1999-12-10          1999               <NA>   \n",
      "\n",
      "             at      at_lag1      at_lag2  \n",
      "0   1084.355949          NaN          NaN  \n",
      "1           NaN          NaN          NaN  \n",
      "2   1267.313578  1084.355949          NaN  \n",
      "3           NaN          NaN          NaN  \n",
      "4   1460.696865  1267.313578  1084.355949  \n",
      "5           NaN          NaN          NaN  \n",
      "6   1304.537890  1460.696865  1267.313578  \n",
      "7           NaN          NaN          NaN  \n",
      "8           NaN          NaN          NaN  \n",
      "9   1265.820386  1304.537890  1460.696865  \n",
      "10          NaN          NaN          NaN  \n",
      "11  1225.163797  1265.820386  1304.537890  \n",
      "12          NaN          NaN          NaN  \n",
      "13  1102.461138  1225.163797  1265.820386  \n",
      "14          NaN          NaN          NaN  \n",
      "15   945.993798  1102.461138  1225.163797  \n",
      "16          NaN          NaN          NaN  \n",
      "17          NaN          NaN          NaN  \n",
      "18          NaN          NaN          NaN  \n",
      "19    23.439270          NaN          NaN  \n",
      "20   577.207234    23.439270          NaN  \n",
      "21  1132.139668   577.207234    23.439270  \n",
      "22          NaN          NaN          NaN  \n",
      "23  1242.711417  1132.139668   577.207234  \n",
      "24          NaN          NaN          NaN  \n",
      "25  1373.608080  1242.711417  1132.139668  \n",
      "26          NaN          NaN          NaN  \n",
      "27  1263.968574  1373.608080  1242.711417  \n",
      "28          NaN          NaN          NaN  \n",
      "29  1301.804038  1263.968574  1373.608080  \n",
      "30          NaN          NaN          NaN  \n",
      "31   820.980063  1301.804038  1263.968574  \n",
      "32          NaN          NaN          NaN  \n",
      "33   903.394131   820.980063  1301.804038  \n",
      "34          NaN          NaN          NaN  \n",
      "35   996.702454   903.394131   820.980063  \n",
      "36          NaN          NaN          NaN  \n",
      "38   993.088238          NaN          NaN  \n",
      "37   939.325665   996.702454   903.394131  \n",
      "39          NaN          NaN          NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Txf.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Txf.txt\"           # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"at\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 2\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixdERgFueQeI"
   },
   "source": [
    "## Calculate Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SelT2RuFmB-V"
   },
   "source": [
    "### Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1305,
     "status": "ok",
     "timestamp": 1765720511690,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "DTKtkZREeYGg",
    "outputId": "c802f7d2-9aa1-46c3-f680-41b0f2dfaf6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'acc' created based on formula:\n",
      "    (wc - wc_lag1) / (0.5 * (at + at_lag1))\n",
      "Rows used (all required columns non-null): 527155\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/ACC.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod       acc\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1992       NaN\n",
      "2   C02500770  1995-12-29          Ars          1993 -0.099280\n",
      "3   C02500770  1995-12-29          Ars          1993       NaN\n",
      "4   C02500770  1995-12-29          Ars          1994  0.170035\n",
      "5   C02500770  1995-12-29          Ars          1994       NaN\n",
      "6   C02500770  1996-05-03          Ars          1995  0.011679\n",
      "7   C02500770  1996-05-03          Ars          1995       NaN\n",
      "8   C02500770  1998-07-03          Ars          1996 -0.079990\n",
      "9   C02500770  1998-07-03          Ars          1996       NaN\n",
      "10  C02500770  1998-07-03          Ars          1997  0.009323\n",
      "11  C02500770  1998-07-03          Ars          1997       NaN\n",
      "12  C02500770  1999-10-01          Ars          1998 -0.126353\n",
      "13  C02500770  1999-10-01          Ars          1998       NaN\n",
      "14  C02500770  1999-10-08          Ars          1997       NaN\n",
      "15  C02500770  2000-05-19          Ars          1999       NaN\n",
      "16  C02500770  2000-05-19          Ars          1999       NaN\n",
      "17  C02500770  2000-05-26          Ars          1999 -0.054378\n",
      "18  C02500770  2000-05-26          Ars          1999       NaN\n",
      "19  C02520200  1996-05-03          Ars          1987       NaN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Acc.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"ACC.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"acc\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(wc - wc_lag1) / (0.5 * (at + at_lag1))\"\n",
    "FORMULA_COLUMNS = [\"wc\", \"wc_lag1\", \"at\", \"at_lag1\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmYxcS66m6ae"
   },
   "source": [
    "### Ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1765720512309,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "SAxcFn0am95p",
    "outputId": "911eb8d8-4cb4-4eba-8084-487d943a4e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'ag' created based on formula:\n",
      "    (at - at_lag1) / at_lag1\n",
      "Rows used (all required columns non-null): 2074894\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/AG.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod         ag\n",
      "0   C02500770  1995-12-29          Ars          1992        NaN\n",
      "1   C02500770  1995-12-29          Ars          1993   0.168725\n",
      "2   C02500770  1995-12-29          Ars          1994   0.152593\n",
      "3   C02500770  1996-05-03          Ars          1995  -0.106907\n",
      "4   C02500770  1998-07-03          Ars          1996  -0.029679\n",
      "5   C02500770  1998-07-03          Ars          1997  -0.032119\n",
      "6   C02500770  1999-10-01          Ars          1998  -0.100152\n",
      "7   C02500770  2000-05-19          Ars          1999  -0.141925\n",
      "8   C02520200  1996-05-03          Ars          1989        NaN\n",
      "9   C02520200  1996-05-03          Ars          1990  23.625648\n",
      "10  C02520200  1996-05-03          Ars          1991   0.961409\n",
      "11  C02520200  1996-05-03          Ars          1992   0.097666\n",
      "12  C02520200  1996-05-03          Ars          1993   0.105332\n",
      "13  C02520200  1996-05-03          Ars          1994  -0.079819\n",
      "14  C02520200  1996-05-03          Ars          1995   0.029934\n",
      "15  C02520200  1996-11-01          Ars          1996  -0.369352\n",
      "16  C02520200  1997-10-31          Ars          1997   0.100385\n",
      "17  C02520200  1998-12-04          Ars          1998   0.103286\n",
      "18  C02520200  1999-10-01          Ars          1999        NaN\n",
      "19  C02520200  1999-12-10          Ars          1999  -0.057567\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Ag.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"AG.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"ag\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(at - at_lag1) / at_lag1\"\n",
    "FORMULA_COLUMNS = [\"at\", \"at_lag1\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APemjsQ1nSdP"
   },
   "source": [
    "### At"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1765720513236,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "0HU2Hnd6nUoD",
    "outputId": "72a43290-5145-4cb6-b2aa-150d87b09f64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'at' created based on formula:\n",
      "    rev / (0.5 * (noa + noa_lag1))\n",
      "Rows used (all required columns non-null): 816843\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/AT.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod        at\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1992       NaN\n",
      "2   C02500770  1995-12-29          Ars          1993  8.717688\n",
      "3   C02500770  1995-12-29          Ars          1993       NaN\n",
      "4   C02500770  1995-12-29          Ars          1994       NaN\n",
      "5   C02500770  1995-12-29          Ars          1994       NaN\n",
      "6   C02500770  1996-05-03          Ars          1995       NaN\n",
      "7   C02500770  1996-05-03          Ars          1995       NaN\n",
      "8   C02500770  1998-07-03          Ars          1996       NaN\n",
      "9   C02500770  1998-07-03          Ars          1996       NaN\n",
      "10  C02500770  1998-07-03          Ars          1997       NaN\n",
      "11  C02500770  1998-07-03          Ars          1997       NaN\n",
      "12  C02500770  1999-10-01          Ars          1998       NaN\n",
      "13  C02500770  1999-10-01          Ars          1998       NaN\n",
      "14  C02500770  2000-05-19          Ars          1999       NaN\n",
      "15  C02500770  2000-05-19          Ars          1999       NaN\n",
      "16  C02500770  2000-05-26          Ars          1999       NaN\n",
      "17  C02520200  1996-05-03          Ars          1987       NaN\n",
      "18  C02520200  1996-05-03          Ars          1988       NaN\n",
      "19  C02520200  1996-05-03          Ars          1989       NaN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_At.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"AT.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"at\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"rev / (0.5 * (noa + noa_lag1))\"\n",
    "FORMULA_COLUMNS = [\"rev\", \"noa_lag1\", \"noa\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv4yt7aspLpl"
   },
   "source": [
    "### Cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrnwfeeSpyYa"
   },
   "source": [
    "#### 1) Calculate AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 918,
     "status": "ok",
     "timestamp": 1765720514155,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "DKWNrQpMpaCJ",
    "outputId": "d372803a-5a28-4d1e-c5e7-899766899eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'at' created based on formula:\n",
      "    rev / (0.5 * (noa + noa_lag1))\n",
      "Rows used (all required columns non-null): 816843\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Cat.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770  1995-12-29          Ars          1992                  A   \n",
      "1   C02500770  1995-12-29          Ars          1992                NaN   \n",
      "2   C02500770  1995-12-29          Ars          1993                  A   \n",
      "3   C02500770  1995-12-29          Ars          1993                NaN   \n",
      "4   C02500770  1995-12-29          Ars          1994                  A   \n",
      "5   C02500770  1995-12-29          Ars          1994                NaN   \n",
      "6   C02500770  1996-05-03          Ars          1995                  A   \n",
      "7   C02500770  1996-05-03          Ars          1995                NaN   \n",
      "8   C02500770  1998-07-03          Ars          1996                  A   \n",
      "9   C02500770  1998-07-03          Ars          1996                NaN   \n",
      "10  C02500770  1998-07-03          Ars          1997                  A   \n",
      "11  C02500770  1998-07-03          Ars          1997                NaN   \n",
      "12  C02500770  1999-10-01          Ars          1998                  A   \n",
      "13  C02500770  1999-10-01          Ars          1998                NaN   \n",
      "14  C02500770  2000-05-19          Ars          1999                  A   \n",
      "15  C02500770  2000-05-19          Ars          1999                NaN   \n",
      "16  C02500770  2000-05-26          Ars          1999                NaN   \n",
      "17  C02520200  1996-05-03          Ars          1987                  A   \n",
      "18  C02520200  1996-05-03          Ars          1988                  A   \n",
      "19  C02520200  1996-05-03          Ars          1989                  A   \n",
      "\n",
      "            rev        at         cce         ltd        mi   ps          ce  \\\n",
      "0   1707.334371       NaN  167.849993   10.096319  0.000000  0.0  378.411011   \n",
      "1   1707.334371       NaN         NaN         NaN       NaN  NaN         NaN   \n",
      "2   1921.517802  8.717688  320.816779  106.116741  0.117886  0.0  434.756779   \n",
      "3   1921.517802       NaN         NaN         NaN       NaN  NaN         NaN   \n",
      "4   2263.705199       NaN  167.728265  130.770231       NaN  0.0  518.204722   \n",
      "5   2263.705199       NaN         NaN         NaN       NaN  NaN         NaN   \n",
      "6   1447.331160       NaN  110.705844   32.376442       NaN  0.0  386.142780   \n",
      "7   1447.331160       NaN         NaN         NaN       NaN  NaN         NaN   \n",
      "8   1217.952697       NaN   70.212302    0.124015       NaN  0.0  387.109014   \n",
      "9   1217.952697       NaN         NaN         NaN       NaN  NaN         NaN   \n",
      "10   693.936079       NaN   40.206606   27.130200       NaN  0.0  292.572607   \n",
      "11   693.936079       NaN         NaN         NaN       NaN  NaN         NaN   \n",
      "12   609.279252       NaN   45.182074   18.295417       NaN  0.0   72.674506   \n",
      "13   609.279252       NaN         NaN         NaN       NaN  NaN         NaN   \n",
      "14   559.817319       NaN   18.238665   15.555081  0.000000  0.0  112.352977   \n",
      "15   559.817319       NaN         NaN         NaN       NaN  NaN         NaN   \n",
      "16   501.495166       NaN         NaN         NaN       NaN  NaN         NaN   \n",
      "17          NaN       NaN         NaN    0.042286  0.000000  NaN    0.129111   \n",
      "18          NaN       NaN         NaN    0.167289  0.000000  NaN    0.613125   \n",
      "19     8.063120       NaN    0.346747    3.415455  0.000000  NaN   16.638718   \n",
      "\n",
      "             oa          ol         noa    noa_lag1  \n",
      "0    916.505956  695.848619  220.657337         NaN  \n",
      "1           NaN         NaN         NaN         NaN  \n",
      "2    946.496799  726.322172  220.174627  220.657337  \n",
      "3           NaN         NaN         NaN         NaN  \n",
      "4   1292.968600         NaN         NaN  220.174627  \n",
      "5           NaN         NaN         NaN         NaN  \n",
      "6   1193.832046         NaN         NaN         NaN  \n",
      "7           NaN         NaN         NaN         NaN  \n",
      "8   1195.608084         NaN         NaN         NaN  \n",
      "9           NaN         NaN         NaN         NaN  \n",
      "10  1184.957191         NaN         NaN         NaN  \n",
      "11          NaN         NaN         NaN         NaN  \n",
      "12  1057.279064         NaN         NaN         NaN  \n",
      "13          NaN         NaN         NaN         NaN  \n",
      "14   927.755133  818.085740  109.669393         NaN  \n",
      "15          NaN         NaN         NaN         NaN  \n",
      "16          NaN         NaN         NaN         NaN  \n",
      "17          NaN         NaN         NaN         NaN  \n",
      "18          NaN         NaN         NaN         NaN  \n",
      "19    23.092523         NaN         NaN         NaN  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Cat.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"processed_data_Cat.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"at\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"rev / (0.5 * (noa + noa_lag1))\"\n",
    "FORMULA_COLUMNS = [\"rev\", \"noa_lag1\", \"noa\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-rKHHxFp3LM"
   },
   "source": [
    "#### 2) Get Lagged AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2946,
     "status": "ok",
     "timestamp": 1765720517102,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "W131m5dCp50t",
    "outputId": "11c06bf2-72d1-490d-d296-66bfe35c2662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'rev', 'at', 'cce', 'ltd', 'mi', 'ps', 'ce', 'oa', 'ol', 'noa', 'noa_lag1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/1495520497.py:174: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/1495520497.py:174: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['at', 'at_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1993                  A   \n",
      "3   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "4   C02500770          Ars 1995-12-29          1994                  A   \n",
      "5   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "6   C02500770          Ars 1996-05-03          1995                  A   \n",
      "7   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "8   C02500770          Ars 1998-07-03          1996                  A   \n",
      "9   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "10  C02500770          Ars 1998-07-03          1997                  A   \n",
      "11  C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "12  C02500770          Ars 1999-10-01          1998                  A   \n",
      "13  C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "14  C02500770          Ars 2000-05-19          1999                  A   \n",
      "15  C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "16  C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "17  C02520200          Ars 1996-05-03          1987                  A   \n",
      "18  C02520200          Ars 1996-05-03          1988                  A   \n",
      "19  C02520200          Ars 1996-05-03          1989                  A   \n",
      "20  C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "21  C02520200          Ars 1996-05-03          1990                  A   \n",
      "22  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "23  C02520200          Ars 1996-05-03          1991                  A   \n",
      "24  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "25  C02520200          Ars 1996-05-03          1992                  A   \n",
      "26  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "27  C02520200          Ars 1996-05-03          1993                  A   \n",
      "28  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "29  C02520200          Ars 1996-05-03          1994                  A   \n",
      "30  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "31  C02520200          Ars 1996-05-03          1995                  A   \n",
      "32  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "33  C02520200          Ars 1996-11-01          1996                  A   \n",
      "34  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "35  C02520200          Ars 1997-10-31          1997                  A   \n",
      "36  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "37  C02520200          Ars 1998-12-04          1998                  A   \n",
      "38  C02520200          Ars 1998-12-04          1998               <NA>   \n",
      "39  C02520200          Ars 1999-10-01          1998               <NA>   \n",
      "\n",
      "          at   at_lag1  \n",
      "0        NaN       NaN  \n",
      "1        NaN       NaN  \n",
      "2   8.717688       NaN  \n",
      "3        NaN       NaN  \n",
      "4        NaN  8.717688  \n",
      "5        NaN       NaN  \n",
      "6        NaN       NaN  \n",
      "7        NaN       NaN  \n",
      "8        NaN       NaN  \n",
      "9        NaN       NaN  \n",
      "10       NaN       NaN  \n",
      "11       NaN       NaN  \n",
      "12       NaN       NaN  \n",
      "13       NaN       NaN  \n",
      "14       NaN       NaN  \n",
      "15       NaN       NaN  \n",
      "16       NaN       NaN  \n",
      "17       NaN       NaN  \n",
      "18       NaN       NaN  \n",
      "19       NaN       NaN  \n",
      "20       NaN       NaN  \n",
      "21       NaN       NaN  \n",
      "22       NaN       NaN  \n",
      "23  0.535049       NaN  \n",
      "24       NaN       NaN  \n",
      "25  0.532505  0.535049  \n",
      "26       NaN       NaN  \n",
      "27  0.501640  0.532505  \n",
      "28       NaN       NaN  \n",
      "29  0.565309  0.501640  \n",
      "30       NaN       NaN  \n",
      "31  0.628172  0.565309  \n",
      "32       NaN       NaN  \n",
      "33  0.768842  0.628172  \n",
      "34       NaN       NaN  \n",
      "35  1.054834  0.768842  \n",
      "36       NaN       NaN  \n",
      "37  1.091604  1.054834  \n",
      "38       NaN       NaN  \n",
      "39       NaN       NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Cat.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"processed_data_Cat.txt\"                 # Input file\n",
    "OUTPUT_FILE_NAME = \"processed_data_Cat.txt\" # <-- CONFIGURABLE OUTPUT FILE\n",
    "\n",
    "VALUE_COLUMNS = [\"at\"]                # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 1\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfMvs3qXtJoT"
   },
   "source": [
    "#### 3) Calculate CAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 966,
     "status": "ok",
     "timestamp": 1765720518070,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "5BqJV6QXtLgJ",
    "outputId": "9e9c7aef-89fd-4ed6-d3c4-32ac0febba16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'cat' created based on formula:\n",
      "    at - at_lag1\n",
      "Rows used (all required columns non-null): 712657\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/CAT.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod  cat\n",
      "0   C02500770  1995-12-29          Ars          1992  NaN\n",
      "1   C02500770  1995-12-29          Ars          1992  NaN\n",
      "2   C02500770  1995-12-29          Ars          1993  NaN\n",
      "3   C02500770  1995-12-29          Ars          1993  NaN\n",
      "4   C02500770  1995-12-29          Ars          1994  NaN\n",
      "5   C02500770  1995-12-29          Ars          1994  NaN\n",
      "6   C02500770  1996-05-03          Ars          1995  NaN\n",
      "7   C02500770  1996-05-03          Ars          1995  NaN\n",
      "8   C02500770  1998-07-03          Ars          1996  NaN\n",
      "9   C02500770  1998-07-03          Ars          1996  NaN\n",
      "10  C02500770  1998-07-03          Ars          1997  NaN\n",
      "11  C02500770  1998-07-03          Ars          1997  NaN\n",
      "12  C02500770  1999-10-01          Ars          1998  NaN\n",
      "13  C02500770  1999-10-01          Ars          1998  NaN\n",
      "14  C02500770  2000-05-19          Ars          1999  NaN\n",
      "15  C02500770  2000-05-19          Ars          1999  NaN\n",
      "16  C02500770  2000-05-26          Ars          1999  NaN\n",
      "17  C02520200  1996-05-03          Ars          1987  NaN\n",
      "18  C02520200  1996-05-03          Ars          1988  NaN\n",
      "19  C02520200  1996-05-03          Ars          1989  NaN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Cat.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"CAT.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"cat\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"at - at_lag1\"\n",
    "FORMULA_COLUMNS = [\"at\", \"at_lag1\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kk1fhEwtcnk"
   },
   "source": [
    "### Cpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1765720518468,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "rTv7N9gNtfiu",
    "outputId": "1bdf08cd-31c7-4d4a-c448-7ce9a44f1e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'cpm' created based on formula:\n",
      "    pm - pm_lag1\n",
      "Rows used (all required columns non-null): 1495014\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/CPM.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod       cpm\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1993 -0.014597\n",
      "2   C02500770  1995-12-29          Ars          1994 -0.026003\n",
      "3   C02500770  1996-05-03          Ars          1995 -0.008383\n",
      "4   C02500770  1998-07-03          Ars          1996  0.051027\n",
      "5   C02500770  1998-07-03          Ars          1997 -0.006505\n",
      "6   C02500770  1999-10-01          Ars          1998  0.089222\n",
      "7   C02500770  1999-10-08          Ars          1997  0.089205\n",
      "8   C02500770  2000-05-19          Ars          1999       NaN\n",
      "9   C02500770  2000-05-26          Ars          1999  0.157138\n",
      "10  C02520200  1996-05-03          Ars          1987       NaN\n",
      "11  C02520200  1996-05-03          Ars          1988       NaN\n",
      "12  C02520200  1996-05-03          Ars          1989       NaN\n",
      "13  C02520200  1996-05-03          Ars          1990 -0.124096\n",
      "14  C02520200  1996-05-03          Ars          1991 -0.038533\n",
      "15  C02520200  1996-05-03          Ars          1992  0.230324\n",
      "16  C02520200  1996-05-03          Ars          1993 -0.010809\n",
      "17  C02520200  1996-05-03          Ars          1994  0.003656\n",
      "18  C02520200  1996-05-03          Ars          1995  0.006338\n",
      "19  C02520200  1996-11-01          Ars          1996 -0.018955\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Cpm.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"CPM.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"cpm\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"pm - pm_lag1\"\n",
    "FORMULA_COLUMNS = [\"pm\", \"pm_lag1\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLXQuKSkvA0I"
   },
   "source": [
    "### Ec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v50n6z-Lvv9g"
   },
   "source": [
    "#### 1) Calculate EG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1765720519248,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "AlP7IMzPvFC8",
    "outputId": "6813d84a-e3d0-4cf8-8d4c-c076d106d142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'eg' created based on formula:\n",
      "    (eps - eps_lag1) / (0.5 * (eps_lag1 + eps_lag2))\n",
      "Rows used (all required columns non-null & same sign): 1376979\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Ec.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod  AnnPITValue_Period  \\\n",
      "0   C02500770  1998-10-02          Ars          1992                 NaN   \n",
      "1   C02500770  1998-10-02          Ars          1993                 NaN   \n",
      "2   C02500770  1998-10-02          Ars          1994                 NaN   \n",
      "3   C02500770  1998-10-02          Ars          1995                 NaN   \n",
      "4   C02500770  1998-10-02          Ars          1996                 NaN   \n",
      "5   C02500770  1998-10-02          Ars          1997                 NaN   \n",
      "6   C02500770  1999-07-30          Ars          1992                 NaN   \n",
      "7   C02500770  1999-07-30          Ars          1993                 NaN   \n",
      "8   C02500770  1999-07-30          Ars          1997                 NaN   \n",
      "9   C02500770  1999-10-01          Ars          1992                 NaN   \n",
      "10  C02500770  1999-10-01          Ars          1993                 NaN   \n",
      "11  C02500770  1999-10-01          Ars          1994                 NaN   \n",
      "12  C02500770  1999-10-01          Ars          1995                 NaN   \n",
      "13  C02500770  1999-10-01          Ars          1996                 NaN   \n",
      "14  C02500770  1999-10-01          Ars          1997                 NaN   \n",
      "15  C02500770  1999-10-01          Ars          1998                 NaN   \n",
      "16  C02500770  2000-05-19          Ars          1999                 NaN   \n",
      "17  C02500770  2000-05-26          Ars          1999                 NaN   \n",
      "18  C02500770  2003-07-11          Ars          1995                 NaN   \n",
      "19  C02500770  2003-07-11          Ars          1997                 NaN   \n",
      "\n",
      "        eps  eps_lag1  eps_lag2        eg  \n",
      "0   0.87000       NaN       NaN       NaN  \n",
      "1   0.82000   0.87000       NaN       NaN  \n",
      "2   0.99024   0.82000   0.87000  0.201467  \n",
      "3  -0.39000   0.99024   0.82000       NaN  \n",
      "4   0.35000  -0.39000   0.99024       NaN  \n",
      "5  -0.57000   0.35000  -0.39000       NaN  \n",
      "6   0.88000       NaN       NaN       NaN  \n",
      "7   0.83000   0.88000       NaN       NaN  \n",
      "8  -0.58000   0.35000  -0.39000       NaN  \n",
      "9   0.87691       NaN       NaN       NaN  \n",
      "10  0.82611   0.87691       NaN       NaN  \n",
      "11  0.99024   0.82611   0.87691  0.192752  \n",
      "12 -0.38913   0.99024   0.82611       NaN  \n",
      "13  0.35101  -0.38913   0.99024       NaN  \n",
      "14 -0.57532   0.35101  -0.38913       NaN  \n",
      "15 -1.12370  -0.57532   0.35101  4.889483  \n",
      "16 -1.04847  -1.12370  -0.57532 -0.088557  \n",
      "17 -1.04838  -1.12370  -0.57532 -0.088663  \n",
      "18 -0.38914   0.99024   0.82611       NaN  \n",
      "19 -0.57533   0.35101  -0.38914       NaN  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where:\n",
    "#      - ALL columns listed in FORMULA_COLUMNS are non-null, AND\n",
    "#      - eps_y and eps_y_lag1 are either both negative or both positive.\n",
    "#    Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)  # assumes Temp_file_path_A is defined elsewhere\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Ec.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"processed_data_Ec.txt\"             # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"eg\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(eps - eps_lag1) / (0.5 * (eps_lag1 + eps_lag2))\"\n",
    "FORMULA_COLUMNS = [\"eps\", \"eps_lag1\", \"eps_lag2\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 4a) BUILD MASK WHERE eps_y AND eps_y_lag1 HAVE THE SAME SIGN (both + or both -)\n",
    "mask_same_sign = (df_out[\"eps\"] * df_out[\"eps_lag1\"]) > 0\n",
    "\n",
    "# 4b) FINAL MASK: NON-NULL AND SAME SIGN\n",
    "final_mask = mask_all_present & mask_same_sign\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE final_mask IS TRUE\n",
    "df_out.loc[final_mask, NEW_COLUMN] = df_out.loc[final_mask].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null & same sign):\", final_mask.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VO7U975lv0Ru"
   },
   "source": [
    "#### 2) Get Lagged EG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1702,
     "status": "ok",
     "timestamp": 1765720520952,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "rrgJFgYZv2-s",
    "outputId": "f35185f0-588c-4cf5-d3ae-c20ec444b153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'eps', 'eps_lag1', 'eps_lag2', 'eg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/2051357781.py:174: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/2051357781.py:174: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['eg', 'eg_lag1', 'eg_lag2', 'eg_lag3', 'eg_lag4']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1998-10-02          1992               <NA>   \n",
      "1   C02500770          Ars 1998-10-02          1993               <NA>   \n",
      "2   C02500770          Ars 1998-10-02          1994               <NA>   \n",
      "3   C02500770          Ars 1998-10-02          1995               <NA>   \n",
      "4   C02500770          Ars 1998-10-02          1996               <NA>   \n",
      "5   C02500770          Ars 1998-10-02          1997               <NA>   \n",
      "6   C02500770          Ars 1999-07-30          1992               <NA>   \n",
      "7   C02500770          Ars 1999-07-30          1993               <NA>   \n",
      "8   C02500770          Ars 1999-07-30          1997               <NA>   \n",
      "9   C02500770          Ars 1999-10-01          1992               <NA>   \n",
      "10  C02500770          Ars 1999-10-01          1993               <NA>   \n",
      "11  C02500770          Ars 1999-10-01          1994               <NA>   \n",
      "12  C02500770          Ars 1999-10-01          1995               <NA>   \n",
      "13  C02500770          Ars 1999-10-01          1996               <NA>   \n",
      "14  C02500770          Ars 1999-10-01          1997               <NA>   \n",
      "15  C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "16  C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "17  C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "18  C02500770          Ars 2003-07-11          1995               <NA>   \n",
      "19  C02500770          Ars 2003-07-11          1997               <NA>   \n",
      "20  C02500770          Ars 2003-07-11          1998               <NA>   \n",
      "21  C02520200          Ars 1996-05-03          1988               <NA>   \n",
      "22  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "23  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "24  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "25  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "26  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "27  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "28  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "29  C02520200          Ars 1997-10-31          1997               <NA>   \n",
      "30  C02520200          Ars 1998-07-03          1995               <NA>   \n",
      "31  C02520200          Ars 1998-12-04          1998               <NA>   \n",
      "32  C02520200          Ars 1999-10-01          1987               <NA>   \n",
      "33  C02520200          Ars 1999-10-01          1989               <NA>   \n",
      "34  C02520200          Ars 1999-10-01          1990               <NA>   \n",
      "35  C02520200          Ars 1999-10-01          1991               <NA>   \n",
      "36  C02520200          Ars 1999-10-01          1992               <NA>   \n",
      "37  C02520200          Ars 1999-10-01          1993               <NA>   \n",
      "38  C02520200          Ars 1999-10-01          1994               <NA>   \n",
      "39  C02520200          Ars 1999-10-01          1995               <NA>   \n",
      "\n",
      "          eg   eg_lag1   eg_lag2   eg_lag3   eg_lag4  \n",
      "0        NaN       NaN       NaN       NaN       NaN  \n",
      "1        NaN       NaN       NaN       NaN       NaN  \n",
      "2   0.201467       NaN       NaN       NaN       NaN  \n",
      "3        NaN  0.201467       NaN       NaN       NaN  \n",
      "4        NaN       NaN  0.201467       NaN       NaN  \n",
      "5        NaN       NaN       NaN  0.201467       NaN  \n",
      "6        NaN       NaN       NaN       NaN       NaN  \n",
      "7        NaN       NaN       NaN       NaN       NaN  \n",
      "8        NaN       NaN       NaN  0.201467       NaN  \n",
      "9        NaN       NaN       NaN       NaN       NaN  \n",
      "10       NaN       NaN       NaN       NaN       NaN  \n",
      "11  0.192752       NaN       NaN       NaN       NaN  \n",
      "12       NaN  0.192752       NaN       NaN       NaN  \n",
      "13       NaN       NaN  0.192752       NaN       NaN  \n",
      "14       NaN       NaN       NaN  0.192752       NaN  \n",
      "15  4.889483       NaN       NaN       NaN  0.192752  \n",
      "16 -0.088557  4.889483       NaN       NaN       NaN  \n",
      "17 -0.088663  4.889483       NaN       NaN       NaN  \n",
      "18       NaN  0.192752       NaN       NaN       NaN  \n",
      "19       NaN       NaN       NaN  0.192752       NaN  \n",
      "20  4.889265       NaN       NaN       NaN  0.192752  \n",
      "21       NaN       NaN       NaN       NaN       NaN  \n",
      "22       NaN       NaN       NaN       NaN       NaN  \n",
      "23       NaN       NaN       NaN       NaN       NaN  \n",
      "24       NaN       NaN       NaN       NaN       NaN  \n",
      "25       NaN       NaN       NaN       NaN       NaN  \n",
      "26 -2.000000       NaN       NaN       NaN       NaN  \n",
      "27 -0.631579 -2.000000       NaN       NaN       NaN  \n",
      "28  6.600000 -0.631579 -2.000000       NaN       NaN  \n",
      "29       NaN  6.600000 -0.631579 -2.000000       NaN  \n",
      "30 -0.526316 -2.000000       NaN       NaN       NaN  \n",
      "31 -0.190476       NaN  6.600000 -0.526316 -2.000000  \n",
      "32       NaN       NaN       NaN       NaN       NaN  \n",
      "33       NaN       NaN       NaN       NaN       NaN  \n",
      "34       NaN       NaN       NaN       NaN       NaN  \n",
      "35       NaN       NaN       NaN       NaN       NaN  \n",
      "36       NaN       NaN       NaN       NaN       NaN  \n",
      "37       NaN       NaN       NaN       NaN       NaN  \n",
      "38 -2.017109       NaN       NaN       NaN       NaN  \n",
      "39 -0.556017 -2.017109       NaN       NaN       NaN  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Ec.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, lagged columns \"<col>_lagk\"\n",
    "#    for k = 1..MAX_LAG_PERIODS.\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lagk\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - k *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: for a row 2020 Q1, \"<col>_lag1\" is the value from 2019 Q1,\n",
    "#      not from 2019 A or 2020 anything.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"processed_data_Ec.txt\"           # Name of the input file to read\n",
    "OUTPUT_FILE_NAME = \"processed_data_Ec.txt\" # <-- CONFIGURABLE OUTPUT FILE\n",
    "\n",
    "VALUE_COLUMNS = [\"eg\"]            # Columns for which to compute the lags \"<col>_lagk\"\n",
    "\n",
    "# Number of prior FiscalPeriods to compute lags for:\n",
    "#   1 -> only t-1\n",
    "#   2 -> t-1 and t-2\n",
    "#   ...\n",
    "MAX_LAG_PERIODS = 4\n",
    "\n",
    "# NEW: period label column (Q1, Q2, A, etc.)\n",
    "PERIOD_COL = \"AnnPITValue_Period\"\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lagk\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and the latest known values for all columns in VALUE_COLUMNS.\n",
    "# - For a row with (FiscalPeriod = t, period = P), the lag column \"<col>_lagk\"\n",
    "#   is defined as the last known value of <col> for (FiscalPeriod = t-k,\n",
    "#   period = P) based on all PIT updates observed up to (and including) the\n",
    "#   current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lagk\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, for k=1..MAX_LAG_PERIODS,\n",
    "    keyed by (FiscalPeriod, AnnPITValue_Period).\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column and lag k, we collect lag values row by row\n",
    "    lag_values = {\n",
    "        col: {k: [] for k in range(1, MAX_LAG_PERIODS + 1)}\n",
    "        for col in VALUE_COLUMNS\n",
    "    }\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        period_label = None if pd.isna(period_raw) else str(period_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS and each k\n",
    "        for col in VALUE_COLUMNS:\n",
    "            for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "                lag_val = None\n",
    "                if pd.notna(fp):\n",
    "                    target_fp = fp - k\n",
    "                    target_key = (target_fp, period_label)\n",
    "                    info = last_for_fp.get(target_key)\n",
    "                    if info is not None:\n",
    "                        # info[1] is the dict of last known values for that (target_fp, period_label)\n",
    "                        lag_val = info[1].get(col)\n",
    "                lag_values[col][k].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lagk\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "            group[f\"{col}_lag{k}\"] = lag_values[col][k]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "created_cols = []\n",
    "for v in VALUE_COLUMNS:\n",
    "    created_cols.append(v)\n",
    "    for k in range(1, MAX_LAG_PERIODS + 1):\n",
    "        created_cols.append(f\"{v}_lag{k}\")\n",
    "print(created_cols)\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = (\n",
    "    [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL]\n",
    "    + VALUE_COLUMNS\n",
    "    + [f\"{c}_lag{k}\" for c in VALUE_COLUMNS for k in range(1, MAX_LAG_PERIODS + 1)]\n",
    ")\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4Kn6kKWwFUp"
   },
   "source": [
    "#### 3) Calculate EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1765720521411,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "b1ubL5k-wLq4",
    "outputId": "12bf3564-6d93-4669-887a-a8040881f143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'ec' created based on formula:\n",
      "    0.2 * (eg + eg_lag1 + eg_lag2 + eg_lag3 + eg_lag4)\n",
      "Rows used (all required columns non-null): 640521\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/EC.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod  ec\n",
      "0   C02500770  1998-10-02          Ars          1992 NaN\n",
      "1   C02500770  1998-10-02          Ars          1993 NaN\n",
      "2   C02500770  1998-10-02          Ars          1994 NaN\n",
      "3   C02500770  1998-10-02          Ars          1995 NaN\n",
      "4   C02500770  1998-10-02          Ars          1996 NaN\n",
      "5   C02500770  1998-10-02          Ars          1997 NaN\n",
      "6   C02500770  1999-07-30          Ars          1992 NaN\n",
      "7   C02500770  1999-07-30          Ars          1993 NaN\n",
      "8   C02500770  1999-07-30          Ars          1997 NaN\n",
      "9   C02500770  1999-10-01          Ars          1992 NaN\n",
      "10  C02500770  1999-10-01          Ars          1993 NaN\n",
      "11  C02500770  1999-10-01          Ars          1994 NaN\n",
      "12  C02500770  1999-10-01          Ars          1995 NaN\n",
      "13  C02500770  1999-10-01          Ars          1996 NaN\n",
      "14  C02500770  1999-10-01          Ars          1997 NaN\n",
      "15  C02500770  1999-10-01          Ars          1998 NaN\n",
      "16  C02500770  2000-05-19          Ars          1999 NaN\n",
      "17  C02500770  2000-05-26          Ars          1999 NaN\n",
      "18  C02500770  2003-07-11          Ars          1995 NaN\n",
      "19  C02500770  2003-07-11          Ars          1997 NaN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Ec.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"EC.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"ec\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"0.2 * (eg + eg_lag1 + eg_lag2 + eg_lag3 + eg_lag4)\"\n",
    "FORMULA_COLUMNS = [\"eg\", \"eg_lag1\", \"eg_lag2\", \"eg_lag3\", \"eg_lag4\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNsVYlH1AWGR"
   },
   "source": [
    "### Es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J391VaJk8o-E"
   },
   "source": [
    "#### Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 661,
     "status": "ok",
     "timestamp": 1765720522073,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "5C6OUeEQ8o-I",
    "outputId": "068f07ec-d9b3-4591-836d-ff4d7a1cae9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'Drift' created as mean of prior three EPS changes.\n",
      "Rows with all required non-null lags used: 1381544\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Es.txt\n",
      "    eps_lag1  eps_lag2  eps_lag3  eps_lag4     Drift\n",
      "0        NaN       NaN       NaN       NaN       NaN\n",
      "1    0.87000       NaN       NaN       NaN       NaN\n",
      "2    0.82000   0.87000       NaN       NaN       NaN\n",
      "3    0.99024   0.82000   0.87000       NaN       NaN\n",
      "4   -0.39000   0.99024   0.82000   0.87000 -0.420000\n",
      "5    0.35000  -0.39000   0.99024   0.82000 -0.156667\n",
      "6        NaN       NaN       NaN       NaN       NaN\n",
      "7    0.88000       NaN       NaN       NaN       NaN\n",
      "8    0.35000  -0.39000   0.99024   0.83000 -0.160000\n",
      "9        NaN       NaN       NaN       NaN       NaN\n",
      "10   0.87691       NaN       NaN       NaN       NaN\n",
      "11   0.82611   0.87691       NaN       NaN       NaN\n",
      "12   0.99024   0.82611   0.87691       NaN       NaN\n",
      "13  -0.38913   0.99024   0.82611   0.87691 -0.422013\n",
      "14   0.35101  -0.38913   0.99024   0.82611 -0.158367\n",
      "15  -0.57532   0.35101  -0.38913   0.99024 -0.521853\n",
      "16  -1.12370  -0.57532   0.35101  -0.38913 -0.244857\n",
      "17  -1.12370  -0.57532   0.35101  -0.38913 -0.244857\n",
      "18   0.99024   0.82611   0.87691       NaN       NaN\n",
      "19   0.35101  -0.38914   0.99024   0.82611 -0.158367\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN = \"Drift\") representing the expected\n",
    "#    annual EPS change (\"drift\") at time t.\n",
    "#\n",
    "# Definition of Drift:\n",
    "# --------------------\n",
    "# Drift_t is defined as the mean of the prior three year-to-year EPS changes,\n",
    "# using only information available strictly before time t:\n",
    "#\n",
    "#   Drift_t = [ (eps_{t-1} - eps_{t-2})\n",
    "#             + (eps_{t-2} - eps_{t-3})\n",
    "#             + (eps_{t-3} - eps_{t-4}) ] / 3\n",
    "#\n",
    "# Column conventions:\n",
    "#   eps        = EPS at time t\n",
    "#   eps_lag1   = EPS at time t-1\n",
    "#   eps_lag2   = EPS at time t-2\n",
    "#   eps_lag3   = EPS at time t-3\n",
    "#   eps_lag4   = EPS at time t-4\n",
    "#\n",
    "# Implementation details:\n",
    "# -----------------------\n",
    "# - Drift is computed ONLY for rows where eps_lag1 .. eps_lag4 are all non-null.\n",
    "# - If any required lag is missing, Drift is set to NaN.\n",
    "# - No partial averages, fallbacks, or tolerance logic are applied.\n",
    "# - The updated DataFrame overwrites the original file on disk.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Es.txt\"   # Input file name\n",
    "NEW_COLUMN = \"Drift\"                   # Name of the computed column\n",
    "\n",
    "# Required EPS level columns (see summary for definitions)\n",
    "REQUIRED_COLUMNS = [\"eps_lag1\", \"eps_lag2\", \"eps_lag3\", \"eps_lag4\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_cols = [c for c in REQUIRED_COLUMNS if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for drift calculation: {missing_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE OUTPUT DATAFRAME AND NEW COLUMN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK: ALL REQUIRED LAGS MUST BE NON-NULL\n",
    "mask_all_present = df_out[REQUIRED_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) COMPUTE PRIOR EPS CHANGES\n",
    "d1 = df_out[\"eps_lag1\"] - df_out[\"eps_lag2\"]  # eps_{t-1} - eps_{t-2}\n",
    "d2 = df_out[\"eps_lag2\"] - df_out[\"eps_lag3\"]  # eps_{t-2} - eps_{t-3}\n",
    "d3 = df_out[\"eps_lag3\"] - df_out[\"eps_lag4\"]  # eps_{t-3} - eps_{t-4}\n",
    "\n",
    "# 6) COMPUTE DRIFT (ONLY WHERE ALL INPUTS ARE PRESENT)\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = (\n",
    "    d1 + d2 + d3\n",
    ").loc[mask_all_present] / 3.0\n",
    "\n",
    "# 7) SAVE RESULT (OVERWRITE INPUT FILE)\n",
    "output_path = OUTPUT_DIR / INPUT_FILE\n",
    "df_out.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "# 8) LOG SUMMARY\n",
    "print(f\"New column '{NEW_COLUMN}' created as mean of prior three EPS changes.\")\n",
    "print(\"Rows with all required non-null lags used:\", int(mask_all_present.sum()))\n",
    "print(\"Result was saved to:\", output_path)\n",
    "print(df_out[REQUIRED_COLUMNS + [NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MCpu1xg8o-K"
   },
   "source": [
    "#### sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1765720522260,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "zUPfgu9B8o-L",
    "outputId": "749ec765-063f-4b30-84e7-ac6d7d506fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'sd' created as sample std (ddof=1) of [(d1-Drift),(d2-Drift),(d3-Drift)], where d1=eps_lag1-eps_lag2, d2=eps_lag2-eps_lag3, d3=eps_lag3-eps_lag4.\n",
      "Rows with all required non-null lags and non-null Drift used: 1381544\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Es.txt\n",
      "    eps_lag1  eps_lag2  eps_lag3  eps_lag4     Drift        sd\n",
      "0        NaN       NaN       NaN       NaN       NaN       NaN\n",
      "1    0.87000       NaN       NaN       NaN       NaN       NaN\n",
      "2    0.82000   0.87000       NaN       NaN       NaN       NaN\n",
      "3    0.99024   0.82000   0.87000       NaN       NaN       NaN\n",
      "4   -0.39000   0.99024   0.82000   0.87000 -0.420000  0.838852\n",
      "5    0.35000  -0.39000   0.99024   0.82000 -0.156667  1.097272\n",
      "6        NaN       NaN       NaN       NaN       NaN       NaN\n",
      "7    0.88000       NaN       NaN       NaN       NaN       NaN\n",
      "8    0.35000  -0.39000   0.99024   0.83000 -0.160000  1.095796\n",
      "9        NaN       NaN       NaN       NaN       NaN       NaN\n",
      "10   0.87691       NaN       NaN       NaN       NaN       NaN\n",
      "11   0.82611   0.87691       NaN       NaN       NaN       NaN\n",
      "12   0.99024   0.82611   0.87691       NaN       NaN       NaN\n",
      "13  -0.38913   0.99024   0.82611   0.87691 -0.422013  0.836031\n",
      "14   0.35101  -0.38913   0.99024   0.82611 -0.158367  1.095940\n",
      "15  -0.57532   0.35101  -0.38913   0.99024 -0.521853  1.116146\n",
      "16  -1.12370  -0.57532   0.35101  -0.38913 -0.244857  0.873714\n",
      "17  -1.12370  -0.57532   0.35101  -0.38913 -0.244857  0.873714\n",
      "18   0.99024   0.82611   0.87691       NaN       NaN       NaN\n",
      "19   0.35101  -0.38914   0.99024   0.82611 -0.158367  1.095949\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN = \"sd\") representing the SAMPLE standard\n",
    "#    deviation (ddof=1) of the prior-three EPS CHANGE deviations from Drift.\n",
    "#\n",
    "# Context / consistency with Drift:\n",
    "# ---------------------------------\n",
    "# You defined:\n",
    "#   eps        = EPS at time t\n",
    "#   eps_lag1   = EPS at time t-1\n",
    "#   eps_lag2   = EPS at time t-2\n",
    "#   eps_lag3   = EPS at time t-3\n",
    "#   eps_lag4   = EPS at time t-4\n",
    "#\n",
    "# Drift_t is assumed to already exist in the file and is defined as:\n",
    "#   Drift_t = mean( (eps_{t-1}-eps_{t-2}), (eps_{t-2}-eps_{t-3}), (eps_{t-3}-eps_{t-4}) )\n",
    "#\n",
    "# SD definition:\n",
    "# --------------\n",
    "# Let the three prior changes be:\n",
    "#   d1 = eps_lag1 - eps_lag2   # (t-1) - (t-2)\n",
    "#   d2 = eps_lag2 - eps_lag3   # (t-2) - (t-3)\n",
    "#   d3 = eps_lag3 - eps_lag4   # (t-3) - (t-4)\n",
    "#\n",
    "# Then:\n",
    "#   sd_t = sample_std( [d1 - Drift_t, d2 - Drift_t, d3 - Drift_t] )  with ddof=1\n",
    "#\n",
    "# Implementation details:\n",
    "# -----------------------\n",
    "# - sd is computed ONLY for rows where eps_lag1..eps_lag4 are all non-null AND\n",
    "#   Drift is non-null.\n",
    "# - If any required input is missing, sd is set to NaN.\n",
    "# - No partial computations, fallbacks, or tolerance logic are applied.\n",
    "# - The updated DataFrame overwrites the original file on disk.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Es.txt\"  # File that already contains 'Drift'\n",
    "NEW_COLUMN = \"sd\"                     # Name of the new computed column\n",
    "\n",
    "# Required EPS level columns to build the three prior changes\n",
    "LEVEL_COLUMNS = [\"eps_lag1\", \"eps_lag2\", \"eps_lag3\", \"eps_lag4\"]\n",
    "DRIFT_COL = \"Drift\"\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing = [c for c in (LEVEL_COLUMNS + [DRIFT_COL]) if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for sd calculation: {missing}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE OUTPUT DATAFRAME AND NEW COLUMN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) ENSURE NUMERIC TYPES (COERCE NON-NUMERIC TO NaN)\n",
    "df_out[DRIFT_COL] = pd.to_numeric(df_out[DRIFT_COL], errors=\"coerce\")\n",
    "for c in LEVEL_COLUMNS:\n",
    "    df_out[c] = pd.to_numeric(df_out[c], errors=\"coerce\")\n",
    "\n",
    "# 5) BUILD MASK: ALL REQUIRED LAGS MUST BE NON-NULL AND Drift MUST BE NON-NULL\n",
    "mask_all_present = df_out[LEVEL_COLUMNS].notna().all(axis=1) & df_out[DRIFT_COL].notna()\n",
    "\n",
    "# 6) COMPUTE THE THREE PRIOR EPS CHANGES (d1, d2, d3)\n",
    "d1 = df_out[\"eps_lag1\"] - df_out[\"eps_lag2\"]  # eps_{t-1} - eps_{t-2}\n",
    "d2 = df_out[\"eps_lag2\"] - df_out[\"eps_lag3\"]  # eps_{t-2} - eps_{t-3}\n",
    "d3 = df_out[\"eps_lag3\"] - df_out[\"eps_lag4\"]  # eps_{t-3} - eps_{t-4}\n",
    "\n",
    "# 7) COMPUTE SAMPLE STANDARD DEVIATION OF (di - Drift), i in {1,2,3}\n",
    "#    We form a 3-column DataFrame of deviations and then take row-wise std(ddof=1).\n",
    "dev = pd.DataFrame(\n",
    "    {\n",
    "        \"dev_d1\": d1 - df_out[DRIFT_COL],\n",
    "        \"dev_d2\": d2 - df_out[DRIFT_COL],\n",
    "        \"dev_d3\": d3 - df_out[DRIFT_COL],\n",
    "    },\n",
    "    index=df_out.index,\n",
    ")\n",
    "\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = dev.loc[mask_all_present].std(axis=1, ddof=1)\n",
    "\n",
    "# 8) SAVE RESULT (OVERWRITE INPUT FILE)\n",
    "output_path = OUTPUT_DIR / INPUT_FILE\n",
    "df_out.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "# 9) LOG SUMMARY\n",
    "print(\n",
    "    f\"New column '{NEW_COLUMN}' created as sample std (ddof=1) of [(d1-Drift),(d2-Drift),(d3-Drift)], \"\n",
    "    f\"where d1=eps_lag1-eps_lag2, d2=eps_lag2-eps_lag3, d3=eps_lag3-eps_lag4.\"\n",
    ")\n",
    "print(\"Rows with all required non-null lags and non-null Drift used:\", int(mask_all_present.sum()))\n",
    "print(\"Result was saved to:\", output_path)\n",
    "print(df_out[LEVEL_COLUMNS + [DRIFT_COL, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lldVuPr5_D-I"
   },
   "source": [
    "#### Es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1765720522613,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "QssqU6tWAWGT",
    "outputId": "15f4ad80-452b-487f-ec55-aee773bf32fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'es' created based on formula:\n",
      "    ((eps - eps_lag1) - Drift) / sd\n",
      "Rows used (inputs present and sd != 0 at 5 decimals): 1380848\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/ES.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod        es\n",
      "0   C02500770  1998-10-02          Ars          1992       NaN\n",
      "1   C02500770  1998-10-02          Ars          1993       NaN\n",
      "2   C02500770  1998-10-02          Ars          1994       NaN\n",
      "3   C02500770  1998-10-02          Ars          1995       NaN\n",
      "4   C02500770  1998-10-02          Ars          1996  1.382843\n",
      "5   C02500770  1998-10-02          Ars          1997 -0.695665\n",
      "6   C02500770  1999-07-30          Ars          1992       NaN\n",
      "7   C02500770  1999-07-30          Ars          1993       NaN\n",
      "8   C02500770  1999-07-30          Ars          1997 -0.702685\n",
      "9   C02500770  1999-10-01          Ars          1992       NaN\n",
      "10  C02500770  1999-10-01          Ars          1993       NaN\n",
      "11  C02500770  1999-10-01          Ars          1994       NaN\n",
      "12  C02500770  1999-10-01          Ars          1995       NaN\n",
      "13  C02500770  1999-10-01          Ars          1996  1.390084\n",
      "14  C02500770  1999-10-01          Ars          1997 -0.700735\n",
      "15  C02500770  1999-10-01          Ars          1998 -0.023766\n",
      "16  C02500770  2000-05-19          Ars          1999  0.366352\n",
      "17  C02500770  2000-05-26          Ars          1999  0.366455\n",
      "18  C02500770  2003-07-11          Ars          1995       NaN\n",
      "19  C02500770  2003-07-11          Ars          1997 -0.700738\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN = \"es\") as a standardized EPS surprise:\n",
    "#\n",
    "#       es_t = ( (eps_t - eps_{t-1}) - Drift_t ) / sd_t\n",
    "#\n",
    "#    where:\n",
    "#      - eps        = eps_t\n",
    "#      - eps_lag1   = eps_{t-1}\n",
    "#      - Drift      = mean of prior EPS changes (already computed in the file)\n",
    "#      - sd         = sample std (ddof=1) of prior-change deviations from Drift\n",
    "#                    (already computed in the file)\n",
    "#\n",
    "# 3) The formula is applied ONLY when ALL required inputs are non-null AND\n",
    "#    sd is non-zero (checking 5 decimal places). Otherwise, es is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) The formula is computed using DataFrame.eval (engine=\"python\").\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The result is written to disk under OUTPUT_FILE_NAME.\n",
    "#\n",
    "# Notes:\n",
    "# - No partial computations are performed: if any input is missing, output is NaN.\n",
    "# - An explicit guard is applied to avoid division by zero (sd rounded to 5 decimals == 0).\n",
    "# - If the expression evaluates to non-finite values (inf/-inf), they are set to NaN.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Es.txt\"      # Input file (must already contain Drift and sd)\n",
    "OUTPUT_FILE_NAME = \"ES.txt\"               # Output file\n",
    "\n",
    "NEW_COLUMN = \"es\"                         # Name of the computed column\n",
    "FORMULA_EXPRESSION = \"((eps - eps_lag1) - Drift) / sd\"\n",
    "FORMULA_COLUMNS = [\"eps\", \"eps_lag1\", \"Drift\", \"sd\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) ENSURE NUMERIC TYPES (COERCE NON-NUMERIC TO NaN)\n",
    "for c in FORMULA_COLUMNS:\n",
    "    df_out[c] = pd.to_numeric(df_out[c], errors=\"coerce\")\n",
    "\n",
    "# 5) BUILD MASK WHERE:\n",
    "#    - all required inputs are non-null, AND\n",
    "#    - sd is non-zero (TREATING ANYTHING < 0.00001 AS ZERO)\n",
    "#    We round 'sd' to 5 decimals; if that is 0, we treat it as 0.\n",
    "mask_all_present = (\n",
    "    df_out[FORMULA_COLUMNS].notna().all(axis=1) \n",
    "    & (df_out[\"sd\"].round(5) != 0)\n",
    ")\n",
    "\n",
    "# 6) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 7) SANITIZE NON-FINITE RESULTS (inf/-inf) -> NaN\n",
    "#    This can happen if sd is extremely close to zero or due to numeric issues.\n",
    "df_out.loc[~np.isfinite(df_out[NEW_COLUMN]), NEW_COLUMN] = np.nan\n",
    "\n",
    "# 8) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 9) DEFINE OUTPUT FILE PATH (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 10) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "# 11) PRINT SUMMARY\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (inputs present and sd != 0 at 5 decimals):\", int(mask_all_present.sum()))\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfJTpeNYw2CV"
   },
   "source": [
    "### Gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1045,
     "status": "ok",
     "timestamp": 1765720523662,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "2j57npbHw5d8",
    "outputId": "5fe59002-fd6d-4885-afbb-530d0377324d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'gp' created based on formula:\n",
      "    (rev - cogs) / (at)\n",
      "Rows used (all required columns non-null): 904364\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/GP.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod        gp\n",
      "0   C02500770  1995-12-29          Ars          1992  0.268970\n",
      "1   C02500770  1995-12-29          Ars          1992       NaN\n",
      "2   C02500770  1995-12-29          Ars          1993  0.236878\n",
      "3   C02500770  1995-12-29          Ars          1993       NaN\n",
      "4   C02500770  1995-12-29          Ars          1994  0.201818\n",
      "5   C02500770  1995-12-29          Ars          1994       NaN\n",
      "6   C02500770  1996-05-03          Ars          1995  0.135181\n",
      "7   C02500770  1996-05-03          Ars          1995       NaN\n",
      "8   C02500770  1998-07-03          Ars          1996  0.166334\n",
      "9   C02500770  1998-07-03          Ars          1996       NaN\n",
      "10  C02500770  1998-07-03          Ars          1997  0.094231\n",
      "11  C02500770  1998-07-03          Ars          1997       NaN\n",
      "12  C02500770  1999-10-08          Ars          1997       NaN\n",
      "13  C02500770  1999-10-01          Ars          1998  0.141252\n",
      "14  C02500770  1999-10-01          Ars          1998       NaN\n",
      "15  C02500770  2000-05-19          Ars          1999       NaN\n",
      "16  C02500770  2000-05-19          Ars          1999       NaN\n",
      "17  C02500770  2000-05-26          Ars          1999       NaN\n",
      "18  C02520200  1996-05-03          Ars          1987       NaN\n",
      "19  C02520200  1996-05-03          Ars          1988       NaN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Gp.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"GP.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"gp\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(rev - cogs) / (at)\"\n",
    "FORMULA_COLUMNS = [\"rev\", \"cogs\", \"at\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62PDf8WZyI8k"
   },
   "source": [
    "### Ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 448,
     "status": "ok",
     "timestamp": 1765720524112,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "CUpC_5R9yM-s",
    "outputId": "df67bbc5-31c1-4c42-da6a-797e82cf91ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'ig' created based on formula:\n",
      "    (inv - inv_lag1) / (0.5 * (at + at_lag1))\n",
      "Rows used (all required columns non-null): 1884025\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/IG.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod        ig\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1993 -0.030810\n",
      "2   C02500770  1995-12-29          Ars          1994  0.065061\n",
      "3   C02500770  1996-05-03          Ars          1995 -0.047951\n",
      "4   C02500770  1998-07-03          Ars          1996 -0.036206\n",
      "5   C02500770  1998-07-03          Ars          1997 -0.002569\n",
      "6   C02500770  1999-10-01          Ars          1998  0.026443\n",
      "7   C02500770  2000-05-19          Ars          1999       NaN\n",
      "8   C02500770  2000-05-26          Ars          1999 -0.045453\n",
      "9   C02520200  1996-05-03          Ars          1987       NaN\n",
      "10  C02520200  1996-05-03          Ars          1988       NaN\n",
      "11  C02520200  1996-05-03          Ars          1989       NaN\n",
      "12  C02520200  1996-05-03          Ars          1990  0.129230\n",
      "13  C02520200  1996-05-03          Ars          1991  0.041876\n",
      "14  C02520200  1996-05-03          Ars          1992  0.019313\n",
      "15  C02520200  1996-05-03          Ars          1993  0.000857\n",
      "16  C02520200  1996-05-03          Ars          1994  0.000150\n",
      "17  C02520200  1996-05-03          Ars          1995 -0.002009\n",
      "18  C02520200  1996-11-01          Ars          1996 -0.032644\n",
      "19  C02520200  1997-10-31          Ars          1997  0.047179\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Ig.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"IG.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"ig\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(inv - inv_lag1) / (0.5 * (at + at_lag1))\"\n",
    "FORMULA_COLUMNS = [\"inv\", \"inv_lag1\", \"at\", \"at_lag1\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1am1BoJygqf"
   },
   "source": [
    "### Inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 897,
     "status": "ok",
     "timestamp": 1765720525012,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "45w7_JJ6yjcY",
    "outputId": "00d09096-d3ea-4153-b498-f57a6ea26a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'inv' created based on formula:\n",
      "    ce / ((1/3) * (ce_lag1 + ce_lag2 + ce_lag3))\n",
      "Rows used (all required columns non-null): 988891\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/INV.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod       inv\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1993       NaN\n",
      "2   C02500770  1995-12-29          Ars          1994       NaN\n",
      "3   C02500770  1996-05-03          Ars          1995  2.603457\n",
      "4   C02500770  1998-07-03          Ars          1996  0.917631\n",
      "5   C02500770  1998-07-03          Ars          1997  1.253880\n",
      "6   C02500770  1999-10-01          Ars          1998  1.257093\n",
      "7   C02500770  2000-05-19          Ars          1999       NaN\n",
      "8   C02500770  2000-05-26          Ars          1999  0.759205\n",
      "9   C02520200  1996-05-03          Ars          1989       NaN\n",
      "10  C02520200  1996-05-03          Ars          1990       NaN\n",
      "11  C02520200  1996-05-03          Ars          1991       NaN\n",
      "12  C02520200  1996-05-03          Ars          1992       NaN\n",
      "13  C02520200  1996-05-03          Ars          1993       NaN\n",
      "14  C02520200  1996-05-03          Ars          1994  1.283974\n",
      "15  C02520200  1996-05-03          Ars          1995  1.834208\n",
      "16  C02520200  1996-11-01          Ars          1996  0.900180\n",
      "17  C02520200  1997-10-31          Ars          1997  1.182784\n",
      "18  C02520200  1998-12-04          Ars          1998  1.152296\n",
      "19  C02520200  1999-10-01          Ars          1998  1.152296\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Inv.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"INV.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"inv\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"ce / ((1/3) * (ce_lag1 + ce_lag2 + ce_lag3))\"\n",
    "FORMULA_COLUMNS = [\"ce\", \"ce_lag1\", \"ce_lag2\", \"ce_lag3\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e63md-nzTWN"
   },
   "source": [
    "### Ltg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgzlO8WbzWnX"
   },
   "source": [
    "#### 1) Calculate ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1336,
     "status": "ok",
     "timestamp": 1765720526351,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "rrSDyhKc4hCd",
    "outputId": "04ce933f-ce38-49b9-b3e7-e25c9522e5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'acc' created based on formula:\n",
      "    (wc - wc_lag1) / (0.5 * (at + at_lag1))\n",
      "Rows used (all required columns non-null): 676068\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Ltg.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770  1995-12-29          Ars          1992                  A   \n",
      "1   C02500770  1995-12-29          Ars          1992                NaN   \n",
      "2   C02500770  1995-12-29          Ars          1993                  A   \n",
      "3   C02500770  1995-12-29          Ars          1993                NaN   \n",
      "4   C02500770  1995-12-29          Ars          1994                  A   \n",
      "5   C02500770  1995-12-29          Ars          1994                NaN   \n",
      "6   C02500770  1996-05-03          Ars          1995                  A   \n",
      "7   C02500770  1996-05-03          Ars          1995                NaN   \n",
      "8   C02500770  1998-07-03          Ars          1996                  A   \n",
      "9   C02500770  1998-07-03          Ars          1996                NaN   \n",
      "10  C02500770  1998-07-03          Ars          1997                  A   \n",
      "11  C02500770  1998-07-03          Ars          1997                NaN   \n",
      "12  C02500770  1999-10-01          Ars          1998                  A   \n",
      "13  C02500770  1999-10-01          Ars          1998                NaN   \n",
      "14  C02500770  1999-10-08          Ars          1997                NaN   \n",
      "15  C02500770  2000-05-19          Ars          1999                  A   \n",
      "16  C02500770  2000-05-19          Ars          1999                NaN   \n",
      "17  C02500770  2000-05-26          Ars          1999                  A   \n",
      "18  C02500770  2000-05-26          Ars          1999                NaN   \n",
      "19  C02520200  1996-05-03          Ars          1987                  A   \n",
      "\n",
      "             at          ca         cce          cl         std  ...  \\\n",
      "0   1084.355949  748.140365  167.849993  663.515421   54.137880  ...   \n",
      "1           NaN         NaN         NaN         NaN         NaN  ...   \n",
      "2   1267.313578  864.906741  320.816779  693.189519   31.500444  ...   \n",
      "3           NaN         NaN         NaN         NaN         NaN  ...   \n",
      "4   1460.696865  975.879591  167.728265  773.728435  104.964605  ...   \n",
      "5           NaN         NaN         NaN         NaN         NaN  ...   \n",
      "6   1304.537890  762.389782  110.705844  852.251409  364.770602  ...   \n",
      "7           NaN         NaN         NaN         NaN         NaN  ...   \n",
      "8   1265.820386  665.184633   70.212302  841.844876  293.363860  ...   \n",
      "9           NaN         NaN         NaN         NaN         NaN  ...   \n",
      "10  1225.163797  615.713580   40.206606  876.621681  354.393108  ...   \n",
      "11          NaN         NaN         NaN         NaN         NaN  ...   \n",
      "12  1102.461138  486.157681   45.182074  978.028694  427.774122  ...   \n",
      "13          NaN         NaN         NaN         NaN         NaN  ...   \n",
      "14          NaN         NaN         NaN         NaN         NaN  ...   \n",
      "15   945.993798  433.210510   18.238665  777.957861  196.429136  ...   \n",
      "16          NaN         NaN         NaN         NaN         NaN  ...   \n",
      "17   945.993798  433.210510   18.238665  777.957861  196.429136  ...   \n",
      "18          NaN         NaN         NaN         NaN         NaN  ...   \n",
      "19          NaN    0.031517         NaN    0.019879         NaN  ...   \n",
      "\n",
      "           oa          ap         ocl        ol       noa          wc  \\\n",
      "0   34.493544  380.421036  192.428709  0.018000  0.266486  -56.599177   \n",
      "1         NaN         NaN         NaN       NaN       NaN         NaN   \n",
      "2   45.444753  369.240530  249.834966  0.018000  0.240349 -173.336614   \n",
      "3         NaN         NaN         NaN       NaN       NaN         NaN   \n",
      "4   59.092809  401.581395  240.967714  0.018000  0.421504   58.591818   \n",
      "5         NaN         NaN         NaN       NaN       NaN         NaN   \n",
      "6   27.082113  228.545754  236.361064  1.735377  0.520508   74.740059   \n",
      "7         NaN         NaN         NaN       NaN       NaN         NaN   \n",
      "8   57.544877  264.087772  263.192178  3.187839  0.427116  -28.061438   \n",
      "9         NaN         NaN         NaN       NaN       NaN         NaN   \n",
      "10  53.211953  218.467531  286.822744  0.347800  0.478199  -16.449604   \n",
      "11        NaN         NaN         NaN       NaN       NaN         NaN   \n",
      "12  56.178105  334.376797  207.046239  0.273000  0.364814 -163.500400   \n",
      "13        NaN         NaN         NaN       NaN       NaN         NaN   \n",
      "14        NaN         NaN         NaN       NaN       NaN         NaN   \n",
      "15        NaN         NaN         NaN       NaN       NaN         NaN   \n",
      "16        NaN         NaN         NaN       NaN       NaN         NaN   \n",
      "17  33.483882  384.499236  180.754431  0.123672  0.300988 -219.195942   \n",
      "18        NaN         NaN         NaN       NaN       NaN         NaN   \n",
      "19        NaN         NaN         NaN       NaN       NaN         NaN   \n",
      "\n",
      "    noa_lag1      at_lag1     wc_lag1       acc  \n",
      "0        NaN          NaN         NaN       NaN  \n",
      "1        NaN          NaN         NaN       NaN  \n",
      "2   0.266486  1084.355949  -56.599177 -0.099280  \n",
      "3        NaN          NaN         NaN       NaN  \n",
      "4   0.240349  1267.313578 -173.336614  0.170035  \n",
      "5        NaN          NaN         NaN       NaN  \n",
      "6   0.421504  1460.696865   58.591818  0.011679  \n",
      "7        NaN          NaN         NaN       NaN  \n",
      "8   0.520508  1304.537890   74.740059 -0.079990  \n",
      "9        NaN          NaN         NaN       NaN  \n",
      "10  0.427116  1265.820386  -28.061438  0.009323  \n",
      "11       NaN          NaN         NaN       NaN  \n",
      "12  0.478199  1225.163797  -16.449604 -0.126353  \n",
      "13       NaN          NaN         NaN       NaN  \n",
      "14       NaN          NaN         NaN       NaN  \n",
      "15  0.364814  1102.461138 -163.500400       NaN  \n",
      "16       NaN          NaN         NaN       NaN  \n",
      "17  0.364814  1102.461138 -163.500400 -0.054378  \n",
      "18       NaN          NaN         NaN       NaN  \n",
      "19       NaN          NaN         NaN       NaN  \n",
      "\n",
      "[20 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Ltg.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"processed_data_Ltg.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"acc\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(wc - wc_lag1) / (0.5 * (at + at_lag1))\"\n",
    "FORMULA_COLUMNS = [\"wc\", \"wc_lag1\", \"at\", \"at_lag1\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNby3OAT5sok"
   },
   "source": [
    "#### 2) Calculate Ltg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1765720526755,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "SjM7q9Kf5wWl",
    "outputId": "e8e0bcbf-9476-4d04-c902-e054cc25b7c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'ltg' created based on formula:\n",
      "    noa - noa_lag1 - acc\n",
      "Rows used (all required columns non-null): 421940\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/LTG.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod       ltg\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1992       NaN\n",
      "2   C02500770  1995-12-29          Ars          1993  0.073143\n",
      "3   C02500770  1995-12-29          Ars          1993       NaN\n",
      "4   C02500770  1995-12-29          Ars          1994  0.011120\n",
      "5   C02500770  1995-12-29          Ars          1994       NaN\n",
      "6   C02500770  1996-05-03          Ars          1995  0.087325\n",
      "7   C02500770  1996-05-03          Ars          1995       NaN\n",
      "8   C02500770  1998-07-03          Ars          1996 -0.013402\n",
      "9   C02500770  1998-07-03          Ars          1996       NaN\n",
      "10  C02500770  1998-07-03          Ars          1997  0.041760\n",
      "11  C02500770  1998-07-03          Ars          1997       NaN\n",
      "12  C02500770  1999-10-01          Ars          1998  0.012967\n",
      "13  C02500770  1999-10-01          Ars          1998       NaN\n",
      "14  C02500770  1999-10-08          Ars          1997       NaN\n",
      "15  C02500770  2000-05-19          Ars          1999       NaN\n",
      "16  C02500770  2000-05-19          Ars          1999       NaN\n",
      "17  C02500770  2000-05-26          Ars          1999 -0.009448\n",
      "18  C02500770  2000-05-26          Ars          1999       NaN\n",
      "19  C02520200  1996-05-03          Ars          1987       NaN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Ltg.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"LTG.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"ltg\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"noa - noa_lag1 - acc\"\n",
    "FORMULA_COLUMNS = [\"noa\", \"noa_lag1\", \"acc\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coI9PVy06Uvm"
   },
   "source": [
    "### Nca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 761,
     "status": "ok",
     "timestamp": 1765720527519,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "T1Kfv9b_6Wog",
    "outputId": "5d736378-dc7b-4a33-8661-3d889adf6786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'nca' created based on formula:\n",
      "    (oa - oa_lag1) / (0.5 * (at + at_lag1))\n",
      "Rows used (all required columns non-null): 1638112\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/NCA.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod       nca\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1993  0.038225\n",
      "2   C02500770  1995-12-29          Ars          1994  0.042944\n",
      "3   C02500770  1996-05-03          Ars          1995  0.165906\n",
      "4   C02500770  1998-07-03          Ars          1996 -0.063656\n",
      "5   C02500770  1998-07-03          Ars          1997  0.059425\n",
      "6   C02500770  1999-10-01          Ars          1998 -0.039300\n",
      "7   C02500770  2000-05-19          Ars          1999 -0.027413\n",
      "8   C02500770  2000-05-26          Ars          1999 -0.103221\n",
      "9   C02520200  1996-05-03          Ars          1987       NaN\n",
      "10  C02520200  1996-05-03          Ars          1988       NaN\n",
      "11  C02520200  1996-05-03          Ars          1989       NaN\n",
      "12  C02520200  1996-05-03          Ars          1990       NaN\n",
      "13  C02520200  1996-05-03          Ars          1991       NaN\n",
      "14  C02520200  1996-05-03          Ars          1992  0.099837\n",
      "15  C02520200  1996-05-03          Ars          1993  0.020193\n",
      "16  C02520200  1996-05-03          Ars          1994 -0.119714\n",
      "17  C02520200  1996-05-03          Ars          1995  0.013875\n",
      "18  C02520200  1996-11-01          Ars          1996 -0.430052\n",
      "19  C02520200  1997-10-31          Ars          1997  0.008712\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Nca.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"NCA.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"nca\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(oa - oa_lag1) / (0.5 * (at + at_lag1))\"\n",
    "FORMULA_COLUMNS = [\"oa\", \"oa_lag1\", \"at\", \"at_lag1\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fVkSOIy6wWv"
   },
   "source": [
    "### Noa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 641,
     "status": "ok",
     "timestamp": 1765720528162,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "49fwJlOT6yEW",
    "outputId": "1745bf7b-d503-4c22-9429-5f5ea8d11758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'noa_anomaly' created based on formula:\n",
      "    noa / at_lag1\n",
      "Rows used (all required columns non-null): 1531325\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/NOA.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod  noa_anomaly\n",
      "0   C02500770  1995-12-29          Ars          1992          NaN\n",
      "1   C02500770  1995-12-29          Ars          1993     0.203046\n",
      "2   C02500770  1995-12-29          Ars          1994          NaN\n",
      "3   C02500770  1996-05-03          Ars          1995          NaN\n",
      "4   C02500770  1998-07-03          Ars          1996          NaN\n",
      "5   C02500770  1998-07-03          Ars          1997          NaN\n",
      "6   C02500770  1999-10-01          Ars          1998          NaN\n",
      "7   C02500770  2000-05-19          Ars          1999     0.099477\n",
      "8   C02520200  1996-05-03          Ars          1987          NaN\n",
      "9   C02520200  1996-05-03          Ars          1988          NaN\n",
      "10  C02520200  1996-05-03          Ars          1989          NaN\n",
      "11  C02520200  1996-05-03          Ars          1990    19.478253\n",
      "12  C02520200  1996-05-03          Ars          1991     1.421314\n",
      "13  C02520200  1996-05-03          Ars          1992     0.824494\n",
      "14  C02520200  1996-05-03          Ars          1993     0.767745\n",
      "15  C02520200  1996-05-03          Ars          1994     0.666030\n",
      "16  C02520200  1996-05-03          Ars          1995     0.763194\n",
      "17  C02520200  1996-11-01          Ars          1996     0.392976\n",
      "18  C02520200  1997-10-31          Ars          1997     0.764415\n",
      "19  C02520200  1998-12-04          Ars          1998     0.668703\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Noa.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"NOA.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"noa_anomaly\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"noa / at_lag1\"\n",
    "FORMULA_COLUMNS = [\"noa\", \"at_lag1\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BITXbd_9F_f"
   },
   "source": [
    "### Nwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1765720529047,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Myhk6W8v9H7i",
    "outputId": "51db87c2-f376-4286-d0d0-4f2ba6064b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'nwc_anomaly' created based on formula:\n",
      "    (nwc - nwc_lag1) / (0.5 * (at + at_lag1))\n",
      "Rows used (all required columns non-null): 1486513\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/NWC.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod  nwc_anomaly\n",
      "0   C02500770  1995-12-29          Ars          1992          NaN\n",
      "1   C02500770  1995-12-29          Ars          1993    -0.075276\n",
      "2   C02500770  1995-12-29          Ars          1994     0.188406\n",
      "3   C02500770  1996-05-03          Ars          1995     0.017948\n",
      "4   C02500770  1998-07-03          Ars          1996    -0.091592\n",
      "5   C02500770  1998-07-03          Ars          1997     0.005449\n",
      "6   C02500770  1999-10-01          Ars          1998    -0.139677\n",
      "7   C02500770  2000-05-19          Ars          1999    -0.055923\n",
      "8   C02520200  1996-05-03          Ars          1987          NaN\n",
      "9   C02520200  1996-05-03          Ars          1988          NaN\n",
      "10  C02520200  1996-05-03          Ars          1989          NaN\n",
      "11  C02520200  1996-05-03          Ars          1990          NaN\n",
      "12  C02520200  1996-05-03          Ars          1991          NaN\n",
      "13  C02520200  1996-05-03          Ars          1992     0.075536\n",
      "14  C02520200  1996-05-03          Ars          1993     0.017490\n",
      "15  C02520200  1996-05-03          Ars          1994    -0.029245\n",
      "16  C02520200  1996-05-03          Ars          1995    -0.028733\n",
      "17  C02520200  1996-11-01          Ars          1996    -0.032305\n",
      "18  C02520200  1997-10-31          Ars          1997     0.067573\n",
      "19  C02520200  1998-12-04          Ars          1998     0.055519\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Nwc.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"NWC.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"nwc_anomaly\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(nwc - nwc_lag1) / (0.5 * (at + at_lag1))\"\n",
    "FORMULA_COLUMNS = [\"nwc\", \"nwc_lag1\", \"at\", \"at_lag1\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p7KOFkD9gqG"
   },
   "source": [
    "### Ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1765720529646,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "bREPu-UF9iS0",
    "outputId": "864377a7-3387-4f7a-a6fd-9779e85f8eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'ol' created based on formula:\n",
      "    (cogs + sga) / at\n",
      "Rows used (all required columns non-null): 616148\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/OL.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod        ol\n",
      "0   C02500770  1995-12-29          Ars          1992  0.181059\n",
      "1   C02500770  1995-12-29          Ars          1992       NaN\n",
      "2   C02500770  1995-12-29          Ars          1993  0.199546\n",
      "3   C02500770  1995-12-29          Ars          1993       NaN\n",
      "4   C02500770  1995-12-29          Ars          1994  0.147251\n",
      "5   C02500770  1995-12-29          Ars          1994       NaN\n",
      "6   C02500770  1996-05-03          Ars          1995  0.187383\n",
      "7   C02500770  1996-05-03          Ars          1995       NaN\n",
      "8   C02500770  1998-07-03          Ars          1996  0.120374\n",
      "9   C02500770  1998-07-03          Ars          1996       NaN\n",
      "10  C02500770  1998-07-03          Ars          1997  0.108487\n",
      "11  C02500770  1998-07-03          Ars          1997       NaN\n",
      "12  C02500770  1999-10-08          Ars          1997       NaN\n",
      "13  C02500770  1999-10-01          Ars          1998  0.096554\n",
      "14  C02500770  1999-10-01          Ars          1998       NaN\n",
      "15  C02500770  2000-05-19          Ars          1999       NaN\n",
      "16  C02500770  2000-05-26          Ars          1999       NaN\n",
      "17  C02520200  1996-05-03          Ars          1988       NaN\n",
      "18  C02520200  1996-05-03          Ars          1989       NaN\n",
      "19  C02520200  1996-05-03          Ars          1989       NaN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Ol.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"OL.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"ol\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(cogs + sga) / at\"\n",
    "FORMULA_COLUMNS = [\"cogs\", \"sga\", \"at\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6niDGbEE91XQ"
   },
   "source": [
    "### Osc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1765720530457,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "WV2ye5Cv94nm",
    "outputId": "7a1d6d04-9a3a-41ef-b178-e4e1b16173c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'osc' created based on formula:\n",
      "    - 1.32 - 0.407 * log(at) + 6.03 * (tl / at) - 1.43 * ((ca - cl) / at) + 0.076 * (cl / ca) - 1.72 * I - 2.37 * (ni_extra / at) - 1.83 * (ifo / tl) + 0.285 * J - 0.521 * ((ni_extra - ni_extra_lag1) / (abs(ni_extra) + abs(ni_extra_lag1)))\n",
      "Rows used (all required columns non-null): 987762\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/OSC.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod       osc\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1992       NaN\n",
      "2   C02500770  1995-12-29          Ars          1993 -1.507886\n",
      "3   C02500770  1995-12-29          Ars          1993       NaN\n",
      "4   C02500770  1995-12-29          Ars          1994 -1.418356\n",
      "5   C02500770  1995-12-29          Ars          1994       NaN\n",
      "6   C02500770  1996-05-03          Ars          1995  0.010150\n",
      "7   C02500770  1996-05-03          Ars          1995       NaN\n",
      "8   C02500770  1998-07-03          Ars          1996 -0.807655\n",
      "9   C02500770  1998-07-03          Ars          1996       NaN\n",
      "10  C02500770  1998-07-03          Ars          1997  1.445561\n",
      "11  C02500770  1998-07-03          Ars          1997       NaN\n",
      "12  C02500770  1998-07-31          Ars          1996       NaN\n",
      "13  C02500770  1998-07-31          Ars          1997       NaN\n",
      "14  C02500770  1999-10-01          Ars          1998  2.790007\n",
      "15  C02500770  1999-10-01          Ars          1998       NaN\n",
      "16  C02500770  2000-05-19          Ars          1999  2.794642\n",
      "17  C02500770  2000-05-19          Ars          1999       NaN\n",
      "18  C02500770  2000-05-26          Ars          1999       NaN\n",
      "19  C02520200  1996-05-03          Ars          1987       NaN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Computes indicator variables I and J:\n",
    "#       I = 1 if tl > at, else 0\n",
    "#       J = 1 if ni_eps + ni_eps_lag1 < 0, else 0\n",
    "# 3) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns (including I, J).\n",
    "# 4) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 5) The updated DataFrame is stored in `df_out`.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)  # e.g. Temp_file_path_A = \"/path/to/dir\"\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Osc.txt\"   # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"OSC.txt\"            # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"osc\"                      # <-- Name of the new computed column\n",
    "\n",
    "# NOTE:\n",
    "# - use log(at) instead of ln(at); pandas.eval uses log() for natural log\n",
    "# - abs() works as absolute value\n",
    "FORMULA_EXPRESSION = (\n",
    "    \"- 1.32\"\n",
    "    \" - 0.407 * log(at)\"\n",
    "    \" + 6.03 * (tl / at)\"\n",
    "    \" - 1.43 * ((ca - cl) / at)\"\n",
    "    \" + 0.076 * (cl / ca)\"\n",
    "    \" - 1.72 * I\"\n",
    "    \" - 2.37 * (ni_extra / at)\"\n",
    "    \" - 1.83 * (ifo / tl)\"\n",
    "    \" + 0.285 * J\"\n",
    "    \" - 0.521 * ((ni_extra - ni_extra_lag1) / (abs(ni_extra) + abs(ni_extra_lag1)))\"\n",
    ")\n",
    "\n",
    "# Columns that must be non-null for the formula to be applied\n",
    "FORMULA_COLUMNS = [\"at\", \"tl\", \"ca\", \"cl\", \"ifo\", \"ni_extra\", \"ni_extra_lag1\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) PREPARE OUTPUT DATAFRAME AND INDICATORS\n",
    "df_out = df.copy()\n",
    "\n",
    "# Indicator variables:\n",
    "# I = 1 if tl > at, else 0\n",
    "df_out[\"I\"] = (df_out[\"tl\"] > df_out[\"at\"]).astype(int)\n",
    "\n",
    "# J = 1 if ni_extra + ni_extra_lag1 < 0, else 0\n",
    "df_out[\"J\"] = ((df_out[\"ni_extra\"] + df_out[\"ni_extra_lag1\"]) < 0).astype(int)\n",
    "\n",
    "# Initialize new column with NaN\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaPzAippM-If"
   },
   "source": [
    "### Pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 864,
     "status": "ok",
     "timestamp": 1765720531323,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "YldVqalbM_kA",
    "outputId": "4ca8621a-ecdf-44a1-e909-b89a4526a009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'pm' created based on formula:\n",
      "    (rev - cogs) / rev\n",
      "Rows used (all required columns non-null): 1613228\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/PM.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod        pm\n",
      "0   C02500770  1995-12-29          Ars          1992  0.170827\n",
      "1   C02500770  1995-12-29          Ars          1993  0.156230\n",
      "2   C02500770  1995-12-29          Ars          1994  0.130227\n",
      "3   C02500770  1996-05-03          Ars          1995  0.121844\n",
      "4   C02500770  1998-07-03          Ars          1996  0.172871\n",
      "5   C02500770  1998-07-03          Ars          1997  0.166367\n",
      "6   C02500770  1999-10-08          Ars          1997  0.262077\n",
      "7   C02500770  1999-10-01          Ars          1998  0.255589\n",
      "8   C02500770  2000-05-19          Ars          1999       NaN\n",
      "9   C02500770  2000-05-26          Ars          1999  0.412727\n",
      "10  C02520200  1996-05-03          Ars          1987       NaN\n",
      "11  C02520200  1996-05-03          Ars          1988       NaN\n",
      "12  C02520200  1996-05-03          Ars          1989  0.195754\n",
      "13  C02520200  1996-05-03          Ars          1990  0.071658\n",
      "14  C02520200  1996-05-03          Ars          1991  0.033125\n",
      "15  C02520200  1996-05-03          Ars          1992  0.263450\n",
      "16  C02520200  1996-05-03          Ars          1993  0.252640\n",
      "17  C02520200  1996-05-03          Ars          1994  0.256297\n",
      "18  C02520200  1996-05-03          Ars          1995  0.262635\n",
      "19  C02520200  1996-11-01          Ars          1996  0.243680\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Pm.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"PM.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"pm\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(rev - cogs) / rev\"\n",
    "FORMULA_COLUMNS = [\"rev\", \"cogs\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrQTywxkNNCs"
   },
   "source": [
    "### Poa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1765720531812,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "gBwEcRp8NPO6",
    "outputId": "7cd24333-e645-4846-99e7-7775bf569fd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'poa' created based on formula:\n",
      "    (ni_extra - opcf) / abs(ni_extra)\n",
      "Rows used (all required columns non-null): 1277110\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/POA.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod        poa\n",
      "0   C02500770  1995-12-29          Ars          1992        NaN\n",
      "1   C02500770  1995-12-29          Ars          1993        NaN\n",
      "2   C02500770  1995-12-29          Ars          1994        NaN\n",
      "3   C02500770  1996-05-03          Ars          1995        NaN\n",
      "4   C02500770  1998-07-03          Ars          1996        NaN\n",
      "5   C02500770  1998-07-03          Ars          1997        NaN\n",
      "6   C02500770  1999-10-01          Ars          1998        NaN\n",
      "7   C02500770  2000-05-19          Ars          1999        NaN\n",
      "8   C02500770  2000-05-26          Ars          1999        NaN\n",
      "9   C02520200  1996-05-03          Ars          1987        NaN\n",
      "10  C02520200  1996-05-03          Ars          1988        NaN\n",
      "11  C02520200  1996-05-03          Ars          1989        NaN\n",
      "12  C02520200  1996-05-03          Ars          1990        NaN\n",
      "13  C02520200  1996-05-03          Ars          1991        NaN\n",
      "14  C02520200  1996-05-03          Ars          1992        NaN\n",
      "15  C02520200  1996-05-03          Ars          1993        NaN\n",
      "16  C02520200  1996-05-03          Ars          1994  -1.227491\n",
      "17  C02520200  1996-05-03          Ars          1995 -14.113936\n",
      "18  C02520200  1996-11-01          Ars          1996  -1.589398\n",
      "19  C02520200  1997-10-31          Ars          1997   1.563491\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Poa.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"POA.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"poa\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(ni_extra - opcf) / abs(ni_extra)\"\n",
    "FORMULA_COLUMNS = [\"ni_extra\", \"opcf\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNzFRJnpN35N"
   },
   "source": [
    "### Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1765720532548,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "0xJetUGON5ue",
    "outputId": "1098ddfc-0710-4ac9-92f8-903c03c58859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'pro' created based on formula:\n",
      "    ni_eps / at_lag1\n",
      "Rows used (all required columns non-null): 897896\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/PRO.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod       pro\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1992       NaN\n",
      "2   C02500770  1995-12-29          Ars          1993  0.085055\n",
      "3   C02500770  1995-12-29          Ars          1993       NaN\n",
      "4   C02500770  1995-12-29          Ars          1994  0.087236\n",
      "5   C02500770  1995-12-29          Ars          1994       NaN\n",
      "6   C02500770  1996-05-03          Ars          1995 -0.029745\n",
      "7   C02500770  1996-05-03          Ars          1995       NaN\n",
      "8   C02500770  1998-07-03          Ars          1996  0.030040\n",
      "9   C02500770  1998-07-03          Ars          1996       NaN\n",
      "10  C02500770  1998-07-03          Ars          1997 -0.060711\n",
      "11  C02500770  1998-07-03          Ars          1997       NaN\n",
      "12  C02500770  1999-10-01          Ars          1998 -0.149942\n",
      "13  C02500770  1999-10-01          Ars          1998       NaN\n",
      "14  C02500770  2000-05-19          Ars          1999 -0.211769\n",
      "15  C02500770  2000-05-19          Ars          1999       NaN\n",
      "16  C02500770  2000-05-26          Ars          1999       NaN\n",
      "17  C02520200  1996-05-03          Ars          1987       NaN\n",
      "18  C02520200  1996-05-03          Ars          1988       NaN\n",
      "19  C02520200  1996-05-03          Ars          1989       NaN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Pro.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"PRO.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"pro\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"ni_eps / at_lag1\"\n",
    "FORMULA_COLUMNS = [\"ni_eps\", \"at_lag1\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-aaPKFEAzOC"
   },
   "source": [
    "### Pta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1765727844815,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "WJYRps2kA2a1",
    "outputId": "9e7bba14-d017-4b67-d888-9b4938da2868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'pta' created based on formula:\n",
      "    (ni_extra + socaps - pocaps - div - opcf - fincf + invcf) / abs(ni_extra)\n",
      "Rows with valid inputs (ni_extra & opcf non-null and ni_extra != 0): 998787\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/PTA.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod       pta\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1993       NaN\n",
      "2   C02500770  1995-12-29          Ars          1994       NaN\n",
      "3   C02500770  1996-05-03          Ars          1995       NaN\n",
      "4   C02500770  1996-05-31          Ars          1995       NaN\n",
      "5   C02500770  1998-07-03          Ars          1996       NaN\n",
      "6   C02500770  1998-07-03          Ars          1997       NaN\n",
      "7   C02500770  1999-10-01          Ars          1998       NaN\n",
      "8   C02500770  2000-05-19          Ars          1999       NaN\n",
      "9   C02500770  2000-05-26          Ars          1999       NaN\n",
      "10  C02520200  1996-05-03          Ars          1987       NaN\n",
      "11  C02520200  1996-05-03          Ars          1988       NaN\n",
      "12  C02520200  1996-05-03          Ars          1989       NaN\n",
      "13  C02520200  1996-05-03          Ars          1990       NaN\n",
      "14  C02520200  1996-05-03          Ars          1991       NaN\n",
      "15  C02520200  1996-05-03          Ars          1992       NaN\n",
      "16  C02520200  1996-05-03          Ars          1993       NaN\n",
      "17  C02520200  1996-05-03          Ars          1994 -0.993094\n",
      "18  C02520200  1996-05-03          Ars          1995  0.627866\n",
      "19  C02520200  1996-11-01          Ars          1996 -0.929217\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is applied only to rows where the REQUIRED columns are non-null\n",
    "#    (ni_extra, opcf). Optional columns (socaps, pocaps, div, fincf, invcf)\n",
    "#    may be NaN and are treated as 0 for the calculation.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Pta.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"PTA.txt\"              # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"pta\"                        # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = (\n",
    "    \"(ni_extra + socaps - pocaps - div - opcf - fincf + invcf) / abs(ni_extra)\"\n",
    ")\n",
    "\n",
    "# Columns that participate in the formula (must exist as columns)\n",
    "FORMULA_COLUMNS = [\"ni_extra\", \"socaps\", \"pocaps\", \"div\", \"opcf\", \"fincf\", \"invcf\"]\n",
    "\n",
    "# REQUIRED: must be non-null for PTA to be computed\n",
    "REQUIRED_COLUMNS = [\"ni_extra\", \"opcf\", \"fincf\", \"invcf\", \"socaps\", \"pocaps\", \"div\"]\n",
    "\n",
    "# OPTIONAL: may be NaN, will be treated as 0 in the formula\n",
    "OPTIONAL_COLUMNS = []\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS FOR THE FORMULA EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK:\n",
    "#    - REQUIRED columns must be non-null\n",
    "#    - ni_extra must be non-zero (avoid division by zero)\n",
    "mask_required_present = df_out[REQUIRED_COLUMNS].notna().all(axis=1)\n",
    "mask_valid = mask_required_present & (df_out[\"ni_extra\"] != 0)\n",
    "\n",
    "# 5) TEMPORARILY FILL OPTIONAL COLUMNS WITH 0 FOR CALCULATION\n",
    "df_tmp = df_out.copy()\n",
    "df_tmp[OPTIONAL_COLUMNS] = df_tmp[OPTIONAL_COLUMNS].fillna(0)\n",
    "\n",
    "#    APPLY FORMULA ONLY WHERE mask_valid IS TRUE\n",
    "df_out.loc[mask_valid, NEW_COLUMN] = df_tmp.loc[mask_valid].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows with valid inputs (ni_extra & opcf non-null and ni_extra != 0):\", mask_valid.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7y7F9u8mB7Ur"
   },
   "source": [
    "### Roe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 664,
     "status": "ok",
     "timestamp": 1765720534359,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "-KzwvcnkB9c-",
    "outputId": "7c244030-f692-42bb-894e-009c0f397acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'roe' created based on formula:\n",
      "    ni_eps / be\n",
      "Rows used (all required columns non-null): 808418\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/ROE.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod       roe\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1992       NaN\n",
      "2   C02500770  1995-12-29          Ars          1993       NaN\n",
      "3   C02500770  1995-12-29          Ars          1993       NaN\n",
      "4   C02500770  1995-12-29          Ars          1994       NaN\n",
      "5   C02500770  1995-12-29          Ars          1994       NaN\n",
      "6   C02500770  1996-05-03          Ars          1995       NaN\n",
      "7   C02500770  1996-05-03          Ars          1995       NaN\n",
      "8   C02500770  1998-07-03          Ars          1996       NaN\n",
      "9   C02500770  1998-07-03          Ars          1996       NaN\n",
      "10  C02500770  1998-07-03          Ars          1997 -0.262669\n",
      "11  C02500770  1998-07-03          Ars          1997       NaN\n",
      "12  C02500770  1999-10-01          Ars          1998 -2.463286\n",
      "13  C02500770  1999-10-01          Ars          1998       NaN\n",
      "14  C02500770  2000-05-19          Ars          1999 -2.055069\n",
      "15  C02500770  2000-05-19          Ars          1999       NaN\n",
      "16  C02500770  2000-05-26          Ars          1999       NaN\n",
      "17  C02520200  1996-05-03          Ars          1987       NaN\n",
      "18  C02520200  1996-05-03          Ars          1987       NaN\n",
      "19  C02520200  1996-05-03          Ars          1988       NaN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Roe.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"ROE.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"roe\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"ni_eps / be\"\n",
    "FORMULA_COLUMNS = [\"ni_eps\", \"be\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsQt0BezK93J"
   },
   "source": [
    "### Rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fy1IrXy2_Lc7"
   },
   "source": [
    "#### Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1765720534599,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "YIyNz6pH_Lc8",
    "outputId": "732d16a1-b0d2-4de6-e1c6-dc39a6a14abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'Drift' created as mean of prior three Revenue changes.\n",
      "Rows with all required non-null lags used: 1026549\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Rs.txt\n",
      "       rev_lag1     rev_lag2     rev_lag3     rev_lag4       Drift\n",
      "0           NaN          NaN          NaN          NaN         NaN\n",
      "1   1707.334371          NaN          NaN          NaN         NaN\n",
      "2   1921.517802  1707.334371          NaN          NaN         NaN\n",
      "3   2263.705199  1921.517802  1707.334371          NaN         NaN\n",
      "4   1447.331160  2263.705199  1921.517802  1707.334371  -86.667737\n",
      "5   1217.952697  1447.331160  2263.705199  1921.517802 -234.521702\n",
      "6    693.936079  1217.952697  1447.331160  2263.705199 -523.256373\n",
      "7    609.279252   693.936079  1217.952697  1447.331160 -279.350636\n",
      "8    609.279252   693.936079  1217.952697  1447.331160 -279.350636\n",
      "9           NaN          NaN          NaN          NaN         NaN\n",
      "10     8.063120          NaN          NaN          NaN         NaN\n",
      "11   212.478635     8.063120          NaN          NaN         NaN\n",
      "12   341.614996   212.478635     8.063120          NaN         NaN\n",
      "13   466.963321   341.614996   212.478635     8.063120  152.966734\n",
      "14   473.429402   466.963321   341.614996   212.478635   86.983589\n",
      "15   528.266494   473.429402   466.963321   341.614996   62.217166\n",
      "16   590.330351   528.266494   473.429402   466.963321   41.122343\n",
      "17   567.494357   590.330351   528.266494   473.429402   31.354985\n",
      "18   600.805773   567.494357   590.330351   528.266494   24.179760\n",
      "19   600.805773   567.494357   590.330351   528.266494   24.179760\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN = \"Drift\") representing the expected\n",
    "#    annual Revenue change (\"drift\") at time t.\n",
    "#\n",
    "# Definition of Drift:\n",
    "# --------------------\n",
    "# Drift_t is defined as the mean of the prior three year-to-year Revenue changes,\n",
    "# using only information available strictly before time t:\n",
    "#\n",
    "#   Drift_t = [ (rev_{t-1} - rev_{t-2})\n",
    "#             + (rev_{t-2} - rev_{t-3})\n",
    "#             + (rev_{t-3} - rev_{t-4}) ] / 3\n",
    "#\n",
    "# Column conventions:\n",
    "#   rev        = Revenue at time t\n",
    "#   rev_lag1   = Revenue at time t-1\n",
    "#   rev_lag2   = Revenue at time t-2\n",
    "#   rev_lag3   = Revenue at time t-3\n",
    "#   rev_lag4   = Revenue at time t-4\n",
    "#\n",
    "# Implementation details:\n",
    "# -----------------------\n",
    "# - Drift is computed ONLY for rows where rev_lag1 .. rev_lag4 are all non-null.\n",
    "# - If any required lag is missing, Drift is set to NaN.\n",
    "# - No partial averages, fallbacks, or tolerance logic are applied.\n",
    "# - The updated DataFrame overwrites the original file on disk.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Rs.txt\"   # Input file name\n",
    "NEW_COLUMN = \"Drift\"                   # Name of the computed column\n",
    "\n",
    "# Required Revenue level columns (see summary for definitions)\n",
    "REQUIRED_COLUMNS = [\"rev_lag1\", \"rev_lag2\", \"rev_lag3\", \"rev_lag4\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_cols = [c for c in REQUIRED_COLUMNS if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for drift calculation: {missing_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE OUTPUT DATAFRAME AND NEW COLUMN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK: ALL REQUIRED LAGS MUST BE NON-NULL\n",
    "mask_all_present = df_out[REQUIRED_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) COMPUTE PRIOR Revenue CHANGES\n",
    "d1 = df_out[\"rev_lag1\"] - df_out[\"rev_lag2\"]  # rev_{t-1} - rev_{t-2}\n",
    "d2 = df_out[\"rev_lag2\"] - df_out[\"rev_lag3\"]  # rev_{t-2} - rev_{t-3}\n",
    "d3 = df_out[\"rev_lag3\"] - df_out[\"rev_lag4\"]  # rev_{t-3} - rev_{t-4}\n",
    "\n",
    "# 6) COMPUTE DRIFT (ONLY WHERE ALL INPUTS ARE PRESENT)\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = (\n",
    "    d1 + d2 + d3\n",
    ").loc[mask_all_present] / 3.0\n",
    "\n",
    "# 7) SAVE RESULT (OVERWRITE INPUT FILE)\n",
    "output_path = OUTPUT_DIR / INPUT_FILE\n",
    "df_out.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "# 8) LOG SUMMARY\n",
    "print(f\"New column '{NEW_COLUMN}' created as mean of prior three Revenue changes.\")\n",
    "print(\"Rows with all required non-null lags used:\", int(mask_all_present.sum()))\n",
    "print(\"Result was saved to:\", output_path)\n",
    "print(df_out[REQUIRED_COLUMNS + [NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJP3mjuM_Lc-"
   },
   "source": [
    "#### sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1765720534803,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "IaYl1ZUs_Lc-",
    "outputId": "9d943da6-bb70-45cb-f1e1-fb9ed40af2e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'sd' created as sample std (ddof=1) of [(d1-Drift),(d2-Drift),(d3-Drift)], where d1=rev_lag1-rev_lag2, d2=rev_lag2-rev_lag3, d3=rev_lag3-rev_lag4.\n",
      "Rows with all required non-null lags and non-null Drift used: 1026549\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Rs.txt\n",
      "       rev_lag1     rev_lag2     rev_lag3     rev_lag4       Drift          sd\n",
      "0           NaN          NaN          NaN          NaN         NaN         NaN\n",
      "1   1707.334371          NaN          NaN          NaN         NaN         NaN\n",
      "2   1921.517802  1707.334371          NaN          NaN         NaN         NaN\n",
      "3   2263.705199  1921.517802  1707.334371          NaN         NaN         NaN\n",
      "4   1447.331160  2263.705199  1921.517802  1707.334371  -86.667737  635.176920\n",
      "5   1217.952697  1447.331160  2263.705199  1921.517802 -234.521702  579.297842\n",
      "6    693.936079  1217.952697  1447.331160  2263.705199 -523.256373  293.498526\n",
      "7    609.279252   693.936079  1217.952697  1447.331160 -279.350636  223.902144\n",
      "8    609.279252   693.936079  1217.952697  1447.331160 -279.350636  223.902144\n",
      "9           NaN          NaN          NaN          NaN         NaN         NaN\n",
      "10     8.063120          NaN          NaN          NaN         NaN         NaN\n",
      "11   212.478635     8.063120          NaN          NaN         NaN         NaN\n",
      "12   341.614996   212.478635     8.063120          NaN         NaN         NaN\n",
      "13   466.963321   341.614996   212.478635     8.063120  152.966734   44.596190\n",
      "14   473.429402   466.963321   341.614996   212.478635   86.983589   69.755925\n",
      "15   528.266494   473.429402   466.963321   341.614996   62.217166   59.783744\n",
      "16   590.330351   528.266494   473.429402   466.963321   41.122343   30.229934\n",
      "17   567.494357   590.330351   528.266494   473.429402   31.354985   47.069663\n",
      "18   600.805773   567.494357   590.330351   528.266494   24.179760   43.180279\n",
      "19   600.805773   567.494357   590.330351   528.266494   24.179760   43.180279\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN = \"sd\") representing the SAMPLE standard\n",
    "#    deviation (ddof=1) of the prior-three Revenue CHANGE deviations from Drift.\n",
    "#\n",
    "# Context / consistency with Drift:\n",
    "# ---------------------------------\n",
    "# You defined:\n",
    "#   rev        = Revenue at time t\n",
    "#   rev_lag1   = Revenue at time t-1\n",
    "#   rev_lag2   = Revenue at time t-2\n",
    "#   rev_lag3   = Revenue at time t-3\n",
    "#   rev_lag4   = Revenue at time t-4\n",
    "#\n",
    "# Drift_t is assumed to already exist in the file and is defined as:\n",
    "#   Drift_t = mean( (rev_{t-1}-rev_{t-2}), (rev_{t-2}-rev_{t-3}), (rev_{t-3}-rev_{t-4}) )\n",
    "#\n",
    "# SD definition:\n",
    "# --------------\n",
    "# Let the three prior changes be:\n",
    "#   d1 = rev_lag1 - rev_lag2   # (t-1) - (t-2)\n",
    "#   d2 = rev_lag2 - rev_lag3   # (t-2) - (t-3)\n",
    "#   d3 = rev_lag3 - rev_lag4   # (t-3) - (t-4)\n",
    "#\n",
    "# Then:\n",
    "#   sd_t = sample_std( [d1 - Drift_t, d2 - Drift_t, d3 - Drift_t] )  with ddof=1\n",
    "#\n",
    "# Implementation details:\n",
    "# -----------------------\n",
    "# - sd is computed ONLY for rows where erev_lag1..rev_lag4 are all non-null AND\n",
    "#   Drift is non-null.\n",
    "# - If any required input is missing, sd is set to NaN.\n",
    "# - No partial computations, fallbacks, or tolerance logic are applied.\n",
    "# - The updated DataFrame overwrites the original file on disk.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Rs.txt\"  # File that already contains 'Drift'\n",
    "NEW_COLUMN = \"sd\"                     # Name of the new computed column\n",
    "\n",
    "# Required Revenue level columns to build the three prior changes\n",
    "LEVEL_COLUMNS = [\"rev_lag1\", \"rev_lag2\", \"rev_lag3\", \"rev_lag4\"]\n",
    "DRIFT_COL = \"Drift\"\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing = [c for c in (LEVEL_COLUMNS + [DRIFT_COL]) if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for sd calculation: {missing}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE OUTPUT DATAFRAME AND NEW COLUMN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) ENSURE NUMERIC TYPES (COERCE NON-NUMERIC TO NaN)\n",
    "df_out[DRIFT_COL] = pd.to_numeric(df_out[DRIFT_COL], errors=\"coerce\")\n",
    "for c in LEVEL_COLUMNS:\n",
    "    df_out[c] = pd.to_numeric(df_out[c], errors=\"coerce\")\n",
    "\n",
    "# 5) BUILD MASK: ALL REQUIRED LAGS MUST BE NON-NULL AND Drift MUST BE NON-NULL\n",
    "mask_all_present = df_out[LEVEL_COLUMNS].notna().all(axis=1) & df_out[DRIFT_COL].notna()\n",
    "\n",
    "# 6) COMPUTE THE THREE PRIOR Revenue CHANGES (d1, d2, d3)\n",
    "d1 = df_out[\"rev_lag1\"] - df_out[\"rev_lag2\"]  # rev_{t-1} - rev_{t-2}\n",
    "d2 = df_out[\"rev_lag2\"] - df_out[\"rev_lag3\"]  # rev_{t-2} - rev_{t-3}\n",
    "d3 = df_out[\"rev_lag3\"] - df_out[\"rev_lag4\"]  # rev_{t-3} - rev_{t-4}\n",
    "\n",
    "# 7) COMPUTE SAMPLE STANDARD DEVIATION OF (di - Drift), i in {1,2,3}\n",
    "#    We form a 3-column DataFrame of deviations and then take row-wise std(ddof=1).\n",
    "dev = pd.DataFrame(\n",
    "    {\n",
    "        \"dev_d1\": d1 - df_out[DRIFT_COL],\n",
    "        \"dev_d2\": d2 - df_out[DRIFT_COL],\n",
    "        \"dev_d3\": d3 - df_out[DRIFT_COL],\n",
    "    },\n",
    "    index=df_out.index,\n",
    ")\n",
    "\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = dev.loc[mask_all_present].std(axis=1, ddof=1)\n",
    "\n",
    "# 8) SAVE RESULT (OVERWRITE INPUT FILE)\n",
    "output_path = OUTPUT_DIR / INPUT_FILE\n",
    "df_out.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "# 9) LOG SUMMARY\n",
    "print(\n",
    "    f\"New column '{NEW_COLUMN}' created as sample std (ddof=1) of [(d1-Drift),(d2-Drift),(d3-Drift)], \"\n",
    "    f\"where d1=rev_lag1-rev_lag2, d2=rev_lag2-rev_lag3, d3=rev_lag3-rev_lag4.\"\n",
    ")\n",
    "print(\"Rows with all required non-null lags and non-null Drift used:\", int(mask_all_present.sum()))\n",
    "print(\"Result was saved to:\", output_path)\n",
    "print(df_out[LEVEL_COLUMNS + [DRIFT_COL, NEW_COLUMN]].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Suolm-aZ_Mmi"
   },
   "source": [
    "#### Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1765720535016,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "J6kT4hZLK93R",
    "outputId": "32a665dc-e867-477a-915a-b3e384e1a8bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'rs' created based on formula:\n",
      "    ((rev - rev_lag1) - Drift) / sd\n",
      "Rows used (all required inputs non-null and sd != 0): 1007091\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/RS.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod        rs\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1993       NaN\n",
      "2   C02500770  1995-12-29          Ars          1994       NaN\n",
      "3   C02500770  1996-05-03          Ars          1995       NaN\n",
      "4   C02500770  1998-07-03          Ars          1996 -0.224679\n",
      "5   C02500770  1998-07-03          Ars          1997 -0.499734\n",
      "6   C02500770  1999-10-01          Ars          1998  1.494384\n",
      "7   C02500770  2000-05-19          Ars          1999  1.026737\n",
      "8   C02500770  2000-05-26          Ars          1999  0.766257\n",
      "9   C02520200  1996-05-03          Ars          1989       NaN\n",
      "10  C02520200  1996-05-03          Ars          1990       NaN\n",
      "11  C02520200  1996-05-03          Ars          1991       NaN\n",
      "12  C02520200  1996-05-03          Ars          1992       NaN\n",
      "13  C02520200  1996-05-03          Ars          1993 -3.285049\n",
      "14  C02520200  1996-05-03          Ars          1994 -0.460843\n",
      "15  C02520200  1996-05-03          Ars          1995 -0.002564\n",
      "16  C02520200  1996-11-01          Ars          1996 -2.115729\n",
      "17  C02520200  1997-10-31          Ars          1997  0.041565\n",
      "18  C02520200  1998-12-04          Ars          1998  1.094561\n",
      "19  C02520200  1999-10-01          Ars          1998  1.094561\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN = \"rs\") as a standardized Revenue surprise:\n",
    "#\n",
    "#       rs_t = ( (rev_t - rev_{t-1}) - Drift_t ) / sd_t\n",
    "#\n",
    "#    where:\n",
    "#      - rev        = rev_t\n",
    "#      - rev_lag1   = rev_{t-1}\n",
    "#      - Drift      = mean of prior Revenue changes (already computed in the file)\n",
    "#      - sd         = sample std (ddof=1) of prior-change deviations from Drift\n",
    "#                    (already computed in the file)\n",
    "#\n",
    "# 3) The formula is applied ONLY when ALL required inputs are non-null AND\n",
    "#    sd is non-zero. Otherwise, es is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) The formula is computed using DataFrame.eval (engine=\"python\").\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The result is written to disk under OUTPUT_FILE_NAME.\n",
    "#\n",
    "# Notes:\n",
    "# - No partial computations are performed: if any input is missing, output is NaN.\n",
    "# - An explicit guard is applied to avoid division by zero (sd == 0).\n",
    "# - If the expression evaluates to non-finite values (inf/-inf), they are set to NaN.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Rs.txt\"      # Input file (must already contain Drift and sd)\n",
    "OUTPUT_FILE_NAME = \"RS.txt\"               # Output file\n",
    "\n",
    "NEW_COLUMN = \"rs\"                         # Name of the computed column\n",
    "FORMULA_EXPRESSION = \"((rev - rev_lag1) - Drift) / sd\"\n",
    "FORMULA_COLUMNS = [\"rev\", \"rev_lag1\", \"Drift\", \"sd\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) ENSURE NUMERIC TYPES (COERCE NON-NUMERIC TO NaN)\n",
    "for c in FORMULA_COLUMNS:\n",
    "    df_out[c] = pd.to_numeric(df_out[c], errors=\"coerce\")\n",
    "\n",
    "# 5) BUILD MASK WHERE:\n",
    "#    - all required inputs are non-null, AND\n",
    "#    - sd is non-zero (avoid division by zero)\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1) & (df_out[\"sd\"] != 0)\n",
    "\n",
    "# 6) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 7) SANITIZE NON-FINITE RESULTS (inf/-inf) -> NaN\n",
    "#    This can happen if sd is extremely close to zero or due to numeric issues.\n",
    "df_out.loc[~np.isfinite(df_out[NEW_COLUMN]), NEW_COLUMN] = np.nan\n",
    "\n",
    "# 8) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 9) DEFINE OUTPUT FILE PATH (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 10) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "# 11) PRINT SUMMARY\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required inputs non-null and sd != 0):\", int(mask_all_present.sum()))\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnC2RuXoHjC4"
   },
   "source": [
    "### Sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 635,
     "status": "ok",
     "timestamp": 1765720535656,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "LCnlEfS-Hm0s",
    "outputId": "e2f386e6-a5a7-456a-ae0a-eb25bce5a230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'sg' created based on formula:\n",
      "    (be / be_lag1) - 1\n",
      "Rows used (all required columns non-null): 1286154\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/SG.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod        sg\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1993       NaN\n",
      "2   C02500770  1995-12-29          Ars          1994       NaN\n",
      "3   C02500770  1996-05-03          Ars          1995       NaN\n",
      "4   C02500770  1998-07-03          Ars          1996       NaN\n",
      "5   C02500770  1998-07-03          Ars          1997       NaN\n",
      "6   C02500770  1999-10-01          Ars          1998 -0.745101\n",
      "7   C02500770  2000-05-19          Ars          1999  0.523340\n",
      "8   C02520200  1996-05-03          Ars          1987       NaN\n",
      "9   C02520200  1996-05-03          Ars          1988       NaN\n",
      "10  C02520200  1996-05-03          Ars          1989       NaN\n",
      "11  C02520200  1996-05-03          Ars          1990       NaN\n",
      "12  C02520200  1996-05-03          Ars          1991       NaN\n",
      "13  C02520200  1996-05-03          Ars          1992       NaN\n",
      "14  C02520200  1996-05-03          Ars          1993       NaN\n",
      "15  C02520200  1996-05-03          Ars          1994       NaN\n",
      "16  C02520200  1996-05-03          Ars          1995       NaN\n",
      "17  C02520200  1996-11-01          Ars          1996       NaN\n",
      "18  C02520200  1997-10-31          Ars          1997  0.092494\n",
      "19  C02520200  1998-12-04          Ars          1998  0.221828\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Sg.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"SG.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"sg\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(be / be_lag1) - 1\"\n",
    "FORMULA_COLUMNS = [\"be_lag1\", \"be\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rESFxSfaIXWz"
   },
   "source": [
    "### Sli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMSMkZBuIaQQ"
   },
   "source": [
    "#### 1) Calculate Sag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 603,
     "status": "ok",
     "timestamp": 1765720536262,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "r-uywCelIkT0",
    "outputId": "b92a65be-6193-476f-8aea-75e36a620f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'sag' created based on formula:\n",
      "    (rev - 0.5 * (rev_lag1 + rev_lag2)) / (0.5 * (rev_lag1 + rev_lag2))\n",
      "Rows used (all required columns non-null): 1933038\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Sli.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770  1995-12-29          Ars          1992                  A   \n",
      "1   C02500770  1995-12-29          Ars          1992                NaN   \n",
      "2   C02500770  1995-12-29          Ars          1993                  A   \n",
      "3   C02500770  1995-12-29          Ars          1993                NaN   \n",
      "4   C02500770  1995-12-29          Ars          1994                  A   \n",
      "5   C02500770  1995-12-29          Ars          1994                NaN   \n",
      "6   C02500770  1996-05-03          Ars          1995                  A   \n",
      "7   C02500770  1996-05-03          Ars          1995                NaN   \n",
      "8   C02500770  1998-07-03          Ars          1996                  A   \n",
      "9   C02500770  1998-07-03          Ars          1996                NaN   \n",
      "10  C02500770  1998-07-03          Ars          1997                  A   \n",
      "11  C02500770  1998-07-03          Ars          1997                NaN   \n",
      "12  C02500770  1999-10-01          Ars          1998                  A   \n",
      "13  C02500770  1999-10-01          Ars          1998                NaN   \n",
      "14  C02500770  2000-05-19          Ars          1999                NaN   \n",
      "15  C02500770  2000-05-26          Ars          1999                  A   \n",
      "16  C02500770  2000-05-26          Ars          1999                NaN   \n",
      "17  C02520200  1996-05-03          Ars          1987                  A   \n",
      "18  C02520200  1996-05-03          Ars          1988                  A   \n",
      "19  C02520200  1996-05-03          Ars          1989                  A   \n",
      "\n",
      "            rev         inv     rev_lag1     rev_lag2    inv_lag1    inv_lag2  \\\n",
      "0   1707.334371  237.409600          NaN          NaN         NaN         NaN   \n",
      "1   1707.334371         NaN          NaN          NaN         NaN         NaN   \n",
      "2   1921.517802  201.182443  1707.334371          NaN  237.409600         NaN   \n",
      "3   1921.517802         NaN  1707.334371          NaN         NaN         NaN   \n",
      "4   2263.705199  289.925405  1921.517802  1707.334371  201.182443  237.409600   \n",
      "5   2263.705199         NaN  1921.517802  1707.334371         NaN         NaN   \n",
      "6   1447.331160  223.628192  2263.705199  1921.517802  289.925405  201.182443   \n",
      "7   1447.331160         NaN  2263.705199  1921.517802         NaN         NaN   \n",
      "8   1217.952697  177.096926  1447.331160  2263.705199  223.628192  289.925405   \n",
      "9   1217.952697         NaN  1447.331160  2263.705199         NaN         NaN   \n",
      "10   693.936079  173.896930  1217.952697  1447.331160  177.096926  223.628192   \n",
      "11   693.936079         NaN  1217.952697  1447.331160         NaN         NaN   \n",
      "12   609.279252  204.671283   693.936079  1217.952697  173.896930  177.096926   \n",
      "13   609.279252         NaN   693.936079  1217.952697         NaN         NaN   \n",
      "14   559.817319         NaN   609.279252   693.936079         NaN         NaN   \n",
      "15   501.495166  158.117346   609.279252   693.936079  204.671283  173.896930   \n",
      "16   501.495166         NaN   609.279252   693.936079         NaN         NaN   \n",
      "17          NaN    0.020612          NaN          NaN         NaN         NaN   \n",
      "18          NaN    0.072913          NaN          NaN    0.020612         NaN   \n",
      "19     8.063120    1.839860          NaN          NaN    0.072913    0.020612   \n",
      "\n",
      "         sag  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4   0.247615  \n",
      "5   0.247615  \n",
      "6  -0.308361  \n",
      "7  -0.308361  \n",
      "8  -0.343605  \n",
      "9  -0.343605  \n",
      "10 -0.479278  \n",
      "11 -0.479278  \n",
      "12 -0.362642  \n",
      "13 -0.362642  \n",
      "14 -0.140868  \n",
      "15 -0.230373  \n",
      "16 -0.230373  \n",
      "17       NaN  \n",
      "18       NaN  \n",
      "19       NaN  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Sli.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"processed_data_Sli.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"sag\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(rev - 0.5 * (rev_lag1 + rev_lag2)) / (0.5 * (rev_lag1 + rev_lag2))\"\n",
    "FORMULA_COLUMNS = [\"rev\", \"rev_lag1\", \"rev_lag2\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnAh06oTJJsN"
   },
   "source": [
    "#### 2) Calculate Ivg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1765720536714,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "z-ZnnYDSJQ7u",
    "outputId": "04fa9271-1b76-4b24-9841-2ccc15d4c577"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'ivg' created based on formula:\n",
      "    (inv - 0.5 * (inv_lag1 + inv_lag2)) / (0.5 * (inv_lag1 + inv_lag2))\n",
      "Rows used (all required columns non-null): 1636079\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Sli.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770  1995-12-29          Ars          1992                  A   \n",
      "1   C02500770  1995-12-29          Ars          1992                NaN   \n",
      "2   C02500770  1995-12-29          Ars          1993                  A   \n",
      "3   C02500770  1995-12-29          Ars          1993                NaN   \n",
      "4   C02500770  1995-12-29          Ars          1994                  A   \n",
      "5   C02500770  1995-12-29          Ars          1994                NaN   \n",
      "6   C02500770  1996-05-03          Ars          1995                  A   \n",
      "7   C02500770  1996-05-03          Ars          1995                NaN   \n",
      "8   C02500770  1998-07-03          Ars          1996                  A   \n",
      "9   C02500770  1998-07-03          Ars          1996                NaN   \n",
      "10  C02500770  1998-07-03          Ars          1997                  A   \n",
      "11  C02500770  1998-07-03          Ars          1997                NaN   \n",
      "12  C02500770  1999-10-01          Ars          1998                  A   \n",
      "13  C02500770  1999-10-01          Ars          1998                NaN   \n",
      "14  C02500770  2000-05-19          Ars          1999                NaN   \n",
      "15  C02500770  2000-05-26          Ars          1999                  A   \n",
      "16  C02500770  2000-05-26          Ars          1999                NaN   \n",
      "17  C02520200  1996-05-03          Ars          1987                  A   \n",
      "18  C02520200  1996-05-03          Ars          1988                  A   \n",
      "19  C02520200  1996-05-03          Ars          1989                  A   \n",
      "\n",
      "            rev         inv     rev_lag1     rev_lag2    inv_lag1    inv_lag2  \\\n",
      "0   1707.334371  237.409600          NaN          NaN         NaN         NaN   \n",
      "1   1707.334371         NaN          NaN          NaN         NaN         NaN   \n",
      "2   1921.517802  201.182443  1707.334371          NaN  237.409600         NaN   \n",
      "3   1921.517802         NaN  1707.334371          NaN         NaN         NaN   \n",
      "4   2263.705199  289.925405  1921.517802  1707.334371  201.182443  237.409600   \n",
      "5   2263.705199         NaN  1921.517802  1707.334371         NaN         NaN   \n",
      "6   1447.331160  223.628192  2263.705199  1921.517802  289.925405  201.182443   \n",
      "7   1447.331160         NaN  2263.705199  1921.517802         NaN         NaN   \n",
      "8   1217.952697  177.096926  1447.331160  2263.705199  223.628192  289.925405   \n",
      "9   1217.952697         NaN  1447.331160  2263.705199         NaN         NaN   \n",
      "10   693.936079  173.896930  1217.952697  1447.331160  177.096926  223.628192   \n",
      "11   693.936079         NaN  1217.952697  1447.331160         NaN         NaN   \n",
      "12   609.279252  204.671283   693.936079  1217.952697  173.896930  177.096926   \n",
      "13   609.279252         NaN   693.936079  1217.952697         NaN         NaN   \n",
      "14   559.817319         NaN   609.279252   693.936079         NaN         NaN   \n",
      "15   501.495166  158.117346   609.279252   693.936079  204.671283  173.896930   \n",
      "16   501.495166         NaN   609.279252   693.936079         NaN         NaN   \n",
      "17          NaN    0.020612          NaN          NaN         NaN         NaN   \n",
      "18          NaN    0.072913          NaN          NaN    0.020612         NaN   \n",
      "19     8.063120    1.839860          NaN          NaN    0.072913    0.020612   \n",
      "\n",
      "         sag        ivg  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4   0.247615   0.322073  \n",
      "5   0.247615        NaN  \n",
      "6  -0.308361  -0.089291  \n",
      "7  -0.308361        NaN  \n",
      "8  -0.343605  -0.310308  \n",
      "9  -0.343605        NaN  \n",
      "10 -0.479278  -0.132089  \n",
      "11 -0.479278        NaN  \n",
      "12 -0.362642   0.166239  \n",
      "13 -0.362642        NaN  \n",
      "14 -0.140868        NaN  \n",
      "15 -0.230373  -0.164656  \n",
      "16 -0.230373        NaN  \n",
      "17       NaN        NaN  \n",
      "18       NaN        NaN  \n",
      "19       NaN  38.344657  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Sli.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"processed_data_Sli.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"ivg\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(inv - 0.5 * (inv_lag1 + inv_lag2)) / (0.5 * (inv_lag1 + inv_lag2))\"\n",
    "FORMULA_COLUMNS = [\"inv\", \"inv_lag1\", \"inv_lag2\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxoCn0ONJmaA"
   },
   "source": [
    "#### 3) Calculate Sli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1765720537004,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "7p-IfV-bJk19",
    "outputId": "815de192-bcd5-45d5-e283-d6cf967a91c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'sli' created based on formula:\n",
      "    sag - ivg\n",
      "Rows used (all required columns non-null): 680711\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/SLI.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod       sli\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1992       NaN\n",
      "2   C02500770  1995-12-29          Ars          1993       NaN\n",
      "3   C02500770  1995-12-29          Ars          1993       NaN\n",
      "4   C02500770  1995-12-29          Ars          1994 -0.074458\n",
      "5   C02500770  1995-12-29          Ars          1994       NaN\n",
      "6   C02500770  1996-05-03          Ars          1995 -0.219070\n",
      "7   C02500770  1996-05-03          Ars          1995       NaN\n",
      "8   C02500770  1998-07-03          Ars          1996 -0.033297\n",
      "9   C02500770  1998-07-03          Ars          1996       NaN\n",
      "10  C02500770  1998-07-03          Ars          1997 -0.347189\n",
      "11  C02500770  1998-07-03          Ars          1997       NaN\n",
      "12  C02500770  1999-10-01          Ars          1998 -0.528880\n",
      "13  C02500770  1999-10-01          Ars          1998       NaN\n",
      "14  C02500770  2000-05-19          Ars          1999       NaN\n",
      "15  C02500770  2000-05-26          Ars          1999 -0.065717\n",
      "16  C02500770  2000-05-26          Ars          1999       NaN\n",
      "17  C02520200  1996-05-03          Ars          1987       NaN\n",
      "18  C02520200  1996-05-03          Ars          1988       NaN\n",
      "19  C02520200  1996-05-03          Ars          1989       NaN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Sli.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"SLI.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"sli\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"sag - ivg\"\n",
    "FORMULA_COLUMNS = [\"sag\", \"ivg\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkRyP3irJ-CM"
   },
   "source": [
    "### Slx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfGPN92zJ-CV"
   },
   "source": [
    "#### 1) Calculate Sag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1765720537620,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "oa0hw7JgJ-CW",
    "outputId": "f5f25974-7908-4f04-d950-f8ce8d5fbf57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'sag' created based on formula:\n",
      "    (rev - 0.5 * (rev_lag1 + rev_lag2)) / (0.5 * (rev_lag1 + rev_lag2))\n",
      "Rows used (all required columns non-null): 1481572\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Slx.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod  AnnPITValue_Period  \\\n",
      "0   C02500770  1995-12-29          Ars          1992                 NaN   \n",
      "1   C02500770  1995-12-29          Ars          1993                 NaN   \n",
      "2   C02500770  1995-12-29          Ars          1994                 NaN   \n",
      "3   C02500770  1996-05-03          Ars          1995                 NaN   \n",
      "4   C02500770  1998-07-03          Ars          1996                 NaN   \n",
      "5   C02500770  1998-07-03          Ars          1997                 NaN   \n",
      "6   C02500770  1999-10-01          Ars          1998                 NaN   \n",
      "7   C02500770  1999-10-08          Ars          1997                 NaN   \n",
      "8   C02500770  2000-05-19          Ars          1999                 NaN   \n",
      "9   C02500770  2000-05-26          Ars          1999                 NaN   \n",
      "10  C02520200  1996-05-03          Ars          1988                 NaN   \n",
      "11  C02520200  1996-05-03          Ars          1989                 NaN   \n",
      "12  C02520200  1996-05-03          Ars          1990                 NaN   \n",
      "13  C02520200  1996-05-03          Ars          1991                 NaN   \n",
      "14  C02520200  1996-05-03          Ars          1992                 NaN   \n",
      "15  C02520200  1996-05-03          Ars          1993                 NaN   \n",
      "16  C02520200  1996-05-03          Ars          1994                 NaN   \n",
      "17  C02520200  1996-05-03          Ars          1995                 NaN   \n",
      "18  C02520200  1996-11-01          Ars          1996                 NaN   \n",
      "19  C02520200  1997-02-28          Ars          1996                 NaN   \n",
      "\n",
      "            rev         sga     rev_lag1     rev_lag2    sga_lag1    sga_lag2  \\\n",
      "0   1707.334371  171.617279          NaN          NaN         NaN         NaN   \n",
      "1   1921.517802  189.549299  1707.334371          NaN  171.617279         NaN   \n",
      "2   2263.705199  129.874835  1921.517802  1707.334371  189.549299  171.617279   \n",
      "3   1447.331160  137.582823  2263.705199  1921.517802  129.874835  189.549299   \n",
      "4   1217.952697   98.561032  1447.331160  2263.705199  137.582823  129.874835   \n",
      "5    693.936079   87.766458  1217.952697  1447.331160   98.561032  137.582823   \n",
      "6    609.279252   59.705642   693.936079  1217.952697   87.766458   98.561032   \n",
      "7    693.936079   77.840413  1217.952697  1447.331160   98.561032  137.582823   \n",
      "8    559.817319         NaN   609.279252   693.936079   59.705642   77.840413   \n",
      "9    501.495166   55.599353   609.279252   693.936079   59.705642   77.840413   \n",
      "10          NaN    0.030673          NaN          NaN         NaN         NaN   \n",
      "11     8.063120    0.538848          NaN          NaN    0.030673         NaN   \n",
      "12   212.478635   13.761389     8.063120          NaN    0.538848    0.030673   \n",
      "13   341.614996   41.612142   212.478635     8.063120   13.761389    0.538848   \n",
      "14   466.963321   37.745526   341.614996   212.478635   41.612142   13.761389   \n",
      "15   473.429402   43.272056   466.963321   341.614996   37.745526   41.612142   \n",
      "16   528.266494   41.026957   473.429402   466.963321   43.272056   37.745526   \n",
      "17   590.330351   47.615547   528.266494   473.429402   41.026957   43.272056   \n",
      "18   567.494357   43.145000   590.330351   528.266494   47.615547   41.026957   \n",
      "19   567.494357   50.583162   590.330351   528.266494   47.615547   41.026957   \n",
      "\n",
      "         sag  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2   0.247615  \n",
      "3  -0.308361  \n",
      "4  -0.343605  \n",
      "5  -0.479278  \n",
      "6  -0.362642  \n",
      "7  -0.479278  \n",
      "8  -0.140868  \n",
      "9  -0.230373  \n",
      "10       NaN  \n",
      "11       NaN  \n",
      "12       NaN  \n",
      "13  2.097962  \n",
      "14  0.685503  \n",
      "15  0.171017  \n",
      "16  0.123502  \n",
      "17  0.178662  \n",
      "18  0.014654  \n",
      "19  0.014654  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Slx.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"processed_data_Slx.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"sag\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(rev - 0.5 * (rev_lag1 + rev_lag2)) / (0.5 * (rev_lag1 + rev_lag2))\"\n",
    "FORMULA_COLUMNS = [\"rev\", \"rev_lag1\", \"rev_lag2\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2kN05RdJ-CY"
   },
   "source": [
    "#### 2) Calculate Xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1765720537872,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "8TXoYX6qJ-CY",
    "outputId": "2f51787d-e9f3-480d-a520-970181e498c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'xg' created based on formula:\n",
      "    (sga - 0.5 * (sga_lag1 + sga_lag2)) / (0.5 * (sga_lag1 + sga_lag2))\n",
      "Rows used (all required columns non-null): 1128389\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Slx.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod  AnnPITValue_Period  \\\n",
      "0   C02500770  1995-12-29          Ars          1992                 NaN   \n",
      "1   C02500770  1995-12-29          Ars          1993                 NaN   \n",
      "2   C02500770  1995-12-29          Ars          1994                 NaN   \n",
      "3   C02500770  1996-05-03          Ars          1995                 NaN   \n",
      "4   C02500770  1998-07-03          Ars          1996                 NaN   \n",
      "5   C02500770  1998-07-03          Ars          1997                 NaN   \n",
      "6   C02500770  1999-10-01          Ars          1998                 NaN   \n",
      "7   C02500770  1999-10-08          Ars          1997                 NaN   \n",
      "8   C02500770  2000-05-19          Ars          1999                 NaN   \n",
      "9   C02500770  2000-05-26          Ars          1999                 NaN   \n",
      "10  C02520200  1996-05-03          Ars          1988                 NaN   \n",
      "11  C02520200  1996-05-03          Ars          1989                 NaN   \n",
      "12  C02520200  1996-05-03          Ars          1990                 NaN   \n",
      "13  C02520200  1996-05-03          Ars          1991                 NaN   \n",
      "14  C02520200  1996-05-03          Ars          1992                 NaN   \n",
      "15  C02520200  1996-05-03          Ars          1993                 NaN   \n",
      "16  C02520200  1996-05-03          Ars          1994                 NaN   \n",
      "17  C02520200  1996-05-03          Ars          1995                 NaN   \n",
      "18  C02520200  1996-11-01          Ars          1996                 NaN   \n",
      "19  C02520200  1997-02-28          Ars          1996                 NaN   \n",
      "\n",
      "            rev         sga     rev_lag1     rev_lag2    sga_lag1    sga_lag2  \\\n",
      "0   1707.334371  171.617279          NaN          NaN         NaN         NaN   \n",
      "1   1921.517802  189.549299  1707.334371          NaN  171.617279         NaN   \n",
      "2   2263.705199  129.874835  1921.517802  1707.334371  189.549299  171.617279   \n",
      "3   1447.331160  137.582823  2263.705199  1921.517802  129.874835  189.549299   \n",
      "4   1217.952697   98.561032  1447.331160  2263.705199  137.582823  129.874835   \n",
      "5    693.936079   87.766458  1217.952697  1447.331160   98.561032  137.582823   \n",
      "6    609.279252   59.705642   693.936079  1217.952697   87.766458   98.561032   \n",
      "7    693.936079   77.840413  1217.952697  1447.331160   98.561032  137.582823   \n",
      "8    559.817319         NaN   609.279252   693.936079   59.705642   77.840413   \n",
      "9    501.495166   55.599353   609.279252   693.936079   59.705642   77.840413   \n",
      "10          NaN    0.030673          NaN          NaN         NaN         NaN   \n",
      "11     8.063120    0.538848          NaN          NaN    0.030673         NaN   \n",
      "12   212.478635   13.761389     8.063120          NaN    0.538848    0.030673   \n",
      "13   341.614996   41.612142   212.478635     8.063120   13.761389    0.538848   \n",
      "14   466.963321   37.745526   341.614996   212.478635   41.612142   13.761389   \n",
      "15   473.429402   43.272056   466.963321   341.614996   37.745526   41.612142   \n",
      "16   528.266494   41.026957   473.429402   466.963321   43.272056   37.745526   \n",
      "17   590.330351   47.615547   528.266494   473.429402   41.026957   43.272056   \n",
      "18   567.494357   43.145000   590.330351   528.266494   47.615547   41.026957   \n",
      "19   567.494357   50.583162   590.330351   528.266494   47.615547   41.026957   \n",
      "\n",
      "         sag         xg  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2   0.247615  -0.280804  \n",
      "3  -0.308361  -0.138557  \n",
      "4  -0.343605  -0.262978  \n",
      "5  -0.479278  -0.256670  \n",
      "6  -0.362642  -0.359132  \n",
      "7  -0.479278  -0.340737  \n",
      "8  -0.140868        NaN  \n",
      "9  -0.230373  -0.191553  \n",
      "10       NaN        NaN  \n",
      "11       NaN        NaN  \n",
      "12       NaN  47.326127  \n",
      "13  2.097962   4.819783  \n",
      "14  0.685503   0.363306  \n",
      "15  0.171017   0.090558  \n",
      "16  0.123502   0.012791  \n",
      "17  0.178662   0.129682  \n",
      "18  0.014654  -0.026539  \n",
      "19  0.014654   0.141285  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Slx.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"processed_data_Slx.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"xg\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(sga - 0.5 * (sga_lag1 + sga_lag2)) / (0.5 * (sga_lag1 + sga_lag2))\"\n",
    "FORMULA_COLUMNS = [\"sga\", \"sga_lag1\", \"sga_lag2\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKfqt10DJ-CZ"
   },
   "source": [
    "#### 3) Calculate Slx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1765720538158,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Jl9Sl-qTJ-Ca",
    "outputId": "6a151038-9295-486b-9e47-90de422d933b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'slx' created based on formula:\n",
      "    sag - xg\n",
      "Rows used (all required columns non-null): 1053800\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/SLX.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod       slx\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1993       NaN\n",
      "2   C02500770  1995-12-29          Ars          1994  0.528419\n",
      "3   C02500770  1996-05-03          Ars          1995 -0.169804\n",
      "4   C02500770  1998-07-03          Ars          1996 -0.080627\n",
      "5   C02500770  1998-07-03          Ars          1997 -0.222608\n",
      "6   C02500770  1999-10-01          Ars          1998 -0.003509\n",
      "7   C02500770  1999-10-08          Ars          1997 -0.138541\n",
      "8   C02500770  2000-05-19          Ars          1999       NaN\n",
      "9   C02500770  2000-05-26          Ars          1999 -0.038820\n",
      "10  C02520200  1996-05-03          Ars          1988       NaN\n",
      "11  C02520200  1996-05-03          Ars          1989       NaN\n",
      "12  C02520200  1996-05-03          Ars          1990       NaN\n",
      "13  C02520200  1996-05-03          Ars          1991 -2.721821\n",
      "14  C02520200  1996-05-03          Ars          1992  0.322198\n",
      "15  C02520200  1996-05-03          Ars          1993  0.080459\n",
      "16  C02520200  1996-05-03          Ars          1994  0.110710\n",
      "17  C02520200  1996-05-03          Ars          1995  0.048980\n",
      "18  C02520200  1996-11-01          Ars          1996  0.041193\n",
      "19  C02520200  1997-02-28          Ars          1996 -0.126631\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Slx.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"SLX.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"slx\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"sag - xg\"\n",
    "FORMULA_COLUMNS = [\"sag\", \"xg\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I1BdtJQKeJ_"
   },
   "source": [
    "### Tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1765720538717,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "sx1Hy1YrKhA9",
    "outputId": "7de5f696-b29e-4bfe-c56a-f5a7ae12375b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'tx' created based on formula:\n",
      "    itx / (0.35 * ni_extra)\n",
      "Rows used (all required columns non-null): 1341007\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/TX.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod        tx\n",
      "0   C02500770  1995-12-29          Ars          1992  0.718369\n",
      "1   C02500770  1995-12-29          Ars          1993  0.619786\n",
      "2   C02500770  1995-12-29          Ars          1994  0.694892\n",
      "3   C02500770  1996-05-03          Ars          1995 -0.311765\n",
      "4   C02500770  1998-07-03          Ars          1996  0.184203\n",
      "5   C02500770  1998-07-03          Ars          1997       NaN\n",
      "6   C02500770  1999-10-01          Ars          1998 -0.000245\n",
      "7   C02500770  2000-05-19          Ars          1999       NaN\n",
      "8   C02500770  2000-05-26          Ars          1999 -0.086758\n",
      "9   C02520200  1996-05-03          Ars          1987       NaN\n",
      "10  C02520200  1996-05-03          Ars          1988       NaN\n",
      "11  C02520200  1996-05-03          Ars          1989 -0.000000\n",
      "12  C02520200  1996-05-03          Ars          1990  0.429310\n",
      "13  C02520200  1996-05-03          Ars          1991       NaN\n",
      "14  C02520200  1996-05-03          Ars          1992  1.119980\n",
      "15  C02520200  1996-05-03          Ars          1993 -1.474975\n",
      "16  C02520200  1996-05-03          Ars          1994 -1.047230\n",
      "17  C02520200  1996-05-03          Ars          1995 -2.645435\n",
      "18  C02520200  1996-11-01          Ars          1996 -0.092353\n",
      "19  C02520200  1997-10-31          Ars          1997  0.390496\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Tx.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"TX.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"tx\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"itx / (0.35 * ni_extra)\"\n",
    "FORMULA_COLUMNS = [\"itx\", \"ni_extra\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTtZneigLhwC"
   },
   "source": [
    "### Txf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 694,
     "status": "ok",
     "timestamp": 1765720539413,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "9zlGcFR3Lk3l",
    "outputId": "83117899-180b-4b60-b5d1-36fedfc6e6af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'txf' created based on formula:\n",
      "    (socaps - pocaps - div + diss - dred) / (0.5 * (at_lag1 + at_lag2))\n",
      "Rows used (all required columns non-null): 339351\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/TXF.txt\n",
      "           ID    PIT Date HistCurrency  FiscalPeriod       txf\n",
      "0   C02500770  1995-12-29          Ars          1992       NaN\n",
      "1   C02500770  1995-12-29          Ars          1992       NaN\n",
      "2   C02500770  1995-12-29          Ars          1993       NaN\n",
      "3   C02500770  1995-12-29          Ars          1993       NaN\n",
      "4   C02500770  1995-12-29          Ars          1994 -0.006296\n",
      "5   C02500770  1995-12-29          Ars          1994       NaN\n",
      "6   C02500770  1996-05-03          Ars          1995       NaN\n",
      "7   C02500770  1996-05-03          Ars          1995       NaN\n",
      "8   C02500770  1996-05-31          Ars          1995       NaN\n",
      "9   C02500770  1998-07-03          Ars          1996  0.000000\n",
      "10  C02500770  1998-07-03          Ars          1996       NaN\n",
      "11  C02500770  1998-07-03          Ars          1997  0.030085\n",
      "12  C02500770  1998-07-03          Ars          1997       NaN\n",
      "13  C02500770  1999-10-01          Ars          1998       NaN\n",
      "14  C02500770  1999-10-01          Ars          1998       NaN\n",
      "15  C02500770  2000-05-19          Ars          1999       NaN\n",
      "16  C02500770  2000-05-19          Ars          1999       NaN\n",
      "17  C02500770  2000-05-26          Ars          1999       NaN\n",
      "18  C02520200  1996-05-03          Ars          1987       NaN\n",
      "19  C02520200  1996-05-03          Ars          1989       NaN\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column (NEW_COLUMN) based on a flexible arithmetic formula\n",
    "#    defined in FORMULA_EXPRESSION using existing columns.\n",
    "# 3) The formula is only applied to rows where ALL columns listed in\n",
    "#    FORMULA_COLUMNS are non-null. Otherwise, the result is set to NaN.\n",
    "# 4) The updated DataFrame is stored in `df_out`.\n",
    "# 5) A new column (NEW_COLUMN) is computed using DataFrame.eval.\n",
    "# 6) All columns except \"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\",\n",
    "#    and NEW_COLUMN are dropped.\n",
    "# 7) The output filename is defined via config.\n",
    "# 8) The result is written to disk using the configured output name.\n",
    "# =============================================================================\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"processed_data_Txf.txt\"               # <-- Input file\n",
    "OUTPUT_FILE_NAME = \"TXF.txt\"  # <-- Output file\n",
    "\n",
    "NEW_COLUMN = \"txf\"                             # <-- Name of the new computed column\n",
    "FORMULA_EXPRESSION = \"(socaps - pocaps - div + diss - dred) / (0.5 * (at_lag1 + at_lag2))\"\n",
    "FORMULA_COLUMNS = [\"socaps\", \"pocaps\", \"div\", \"diss\", \"dred\", \"at_lag1\", \"at_lag2\"]\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT ALL REQUIRED COLUMNS EXIST\n",
    "missing_formula_cols = [c for c in FORMULA_COLUMNS if c not in df.columns]\n",
    "if missing_formula_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing required columns for formula: {missing_formula_cols}\"\n",
    "    )\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) BUILD MASK WHERE ALL REQUIRED COLUMNS ARE NON-NULL\n",
    "mask_all_present = df_out[FORMULA_COLUMNS].notna().all(axis=1)\n",
    "\n",
    "# 5) APPLY FORMULA ONLY WHERE mask_all_present IS TRUE\n",
    "df_out.loc[mask_all_present, NEW_COLUMN] = df_out.loc[mask_all_present].eval(\n",
    "    FORMULA_EXPRESSION,\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# 6) DROP ALL COLUMNS EXCEPT THE REQUIRED ONES\n",
    "KEEP_COLUMNS = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", NEW_COLUMN]\n",
    "\n",
    "missing_keep_cols = [c for c in KEEP_COLUMNS if c not in df_out.columns]\n",
    "if missing_keep_cols:\n",
    "    raise ValueError(f\"Columns expected to be kept but not found: {missing_keep_cols}\")\n",
    "\n",
    "df_out = df_out[KEEP_COLUMNS]\n",
    "\n",
    "# 7) DEFINE OUTPUT FILE NAME (FROM CONFIG)\n",
    "formula_output_path = OUTPUT_DIR / OUTPUT_FILE_NAME\n",
    "\n",
    "# 8) SAVE RESULT\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created based on formula:\")\n",
    "print(\"   \", FORMULA_EXPRESSION)\n",
    "print(\"Rows used (all required columns non-null):\", mask_all_present.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(df_out.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjb4NzSZPzsj"
   },
   "source": [
    "## Clean Anomaly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2208,
     "status": "ok",
     "timestamp": 1765727912031,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "jHlO0vCwP4MB",
    "outputId": "570d9ae3-a78b-4015-94ec-2ec0466b5128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing file: ACC.txt\n",
      "  Rows BEFORE: 4218013\n",
      "  Firm-years BEFORE: 886391\n",
      "  Target column found: 'acc'\n",
      "  Using target column: 'acc'\n",
      "  Dropping 5 rows with ±infinity in 'acc'.\n",
      "  Dropped 3666407 empty rows in 'acc'.\n",
      "  Dropped 5265 stale rows (>3 years old).\n",
      "  Removed 1601 duplicates.\n",
      "  Trimmed 4456 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 4218013          -0.0     886391        -0.0\n",
      "        2. Country Filter 4190972       27041.0     880071      6320.0\n",
      "   Clean: Remove Infinity 4190967           5.0     880071        -0.0\n",
      " Clean: Remove NaNs/Empty  524560     3666407.0     398911    481160.0\n",
      "Clean: Valid FiscalPeriod  524560          -0.0     398911        -0.0\n",
      " Filter: Best FP per Date  446396       78164.0     364241     34670.0\n",
      "   Filter: Stale (>3 Yrs)  441131        5265.0     362173      2068.0\n",
      " 3. Anomaly Filter [None]  441131          -0.0     362173        -0.0\n",
      "  4. Dedup (Earliest PIT)  439530        1601.0     362173        -0.0\n",
      "   5. Outlier Trim (0.5%)  435074        4456.0     358448      3725.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/ACC_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       acc\n",
      "0  C02500770          Ars 1995-12-29          1994  0.170035\n",
      "1  C02500770          Ars 1996-05-03          1995  0.011679\n",
      "2  C02500770          Ars 1998-07-03          1997  0.009323\n",
      "3  C02500770          Ars 1999-10-01          1998 -0.126353\n",
      "4  C02500770          Ars 2000-05-26          1999 -0.054378\n",
      "\n",
      "================================================================================\n",
      "Processing file: AG.txt\n",
      "  Rows BEFORE: 2354025\n",
      "  Firm-years BEFORE: 871391\n",
      "  Target column found: 'ag'\n",
      "  Using target column: 'ag'\n",
      "  Dropping 76 rows with ±infinity in 'ag'.\n",
      "  Dropped 277025 empty rows in 'ag'.\n",
      "  Dropped 7147 stale rows (>3 years old).\n",
      "  Removed 4924 duplicates.\n",
      "  Trimmed 18824 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 2354025          -0.0     871391        -0.0\n",
      "        2. Country Filter 2339016       15009.0     865108      6283.0\n",
      "   Clean: Remove Infinity 2338940          76.0     865059        49.0\n",
      " Clean: Remove NaNs/Empty 2061915      277025.0     795378     69681.0\n",
      "Clean: Valid FiscalPeriod 2061915          -0.0     795378        -0.0\n",
      " Filter: Best FP per Date 1887031      174884.0     712449     82929.0\n",
      "   Filter: Stale (>3 Yrs) 1879884        7147.0     709750      2699.0\n",
      " 3. Anomaly Filter [None] 1879884          -0.0     709750        -0.0\n",
      "  4. Dedup (Earliest PIT) 1874960        4924.0     709750        -0.0\n",
      "   5. Outlier Trim (0.5%) 1856136       18824.0     699954      9796.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/AG_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod        ag\n",
      "0  C02500770          Ars 1995-12-29          1994  0.152593\n",
      "1  C02500770          Ars 1996-05-03          1995 -0.106907\n",
      "2  C02500770          Ars 1998-07-03          1997 -0.032119\n",
      "3  C02500770          Ars 1999-10-01          1998 -0.100152\n",
      "4  C02500770          Ars 2000-05-19          1999 -0.141925\n",
      "\n",
      "================================================================================\n",
      "Processing file: AT.txt\n",
      "  Rows BEFORE: 4251336\n",
      "  Firm-years BEFORE: 887991\n",
      "  Target column found: 'at'\n",
      "  Using target column: 'at'\n",
      "  Dropped 3412465 empty rows in 'at'.\n",
      "  Dropped 8238 stale rows (>3 years old).\n",
      "  Removed 14243 duplicates.\n",
      "  Trimmed 6705 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 4251336          -0.0     887991        -0.0\n",
      "        2. Country Filter 4223935       27401.0     881653      6338.0\n",
      "   Clean: Remove Infinity 4223935          -0.0     881653        -0.0\n",
      " Clean: Remove NaNs/Empty  811470     3412465.0     632334    249319.0\n",
      "Clean: Valid FiscalPeriod  811470          -0.0     632334        -0.0\n",
      " Filter: Best FP per Date  686985      124485.0     558064     74270.0\n",
      "   Filter: Stale (>3 Yrs)  678747        8238.0     554040      4024.0\n",
      " 3. Anomaly Filter [None]  678747          -0.0     554040        -0.0\n",
      "  4. Dedup (Earliest PIT)  664504       14243.0     554040        -0.0\n",
      "   5. Outlier Trim (0.5%)  657799        6705.0     548343      5697.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/AT_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod        at\n",
      "0  C02500770          Ars 1995-12-29          1993  8.717688\n",
      "1  C02520200          Ars 1996-05-03          1995  0.628172\n",
      "2  C02520200          Ars 1996-11-01          1996  0.768842\n",
      "3  C02520200          Ars 1997-10-31          1997  1.054834\n",
      "4  C02520200          Ars 1998-12-04          1998  1.091604\n",
      "\n",
      "================================================================================\n",
      "Processing file: CAT.txt\n",
      "  Rows BEFORE: 4251336\n",
      "  Firm-years BEFORE: 887991\n",
      "  Target column found: 'cat'\n",
      "  Using target column: 'cat'\n",
      "  Dropped 3515864 empty rows in 'cat'.\n",
      "  Dropped 4058 stale rows (>3 years old).\n",
      "  Removed 12047 duplicates.\n",
      "  Trimmed 6080 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 4251336          -0.0     887991        -0.0\n",
      "        2. Country Filter 4223935       27401.0     881653      6338.0\n",
      "   Clean: Remove Infinity 4223935          -0.0     881653        -0.0\n",
      " Clean: Remove NaNs/Empty  708071     3515864.0     547519    334134.0\n",
      "Clean: Valid FiscalPeriod  708071          -0.0     547519        -0.0\n",
      " Filter: Best FP per Date  617524       90547.0     501840     45679.0\n",
      "   Filter: Stale (>3 Yrs)  613466        4058.0     499721      2119.0\n",
      " 3. Anomaly Filter [None]  613466          -0.0     499721        -0.0\n",
      "  4. Dedup (Earliest PIT)  601419       12047.0     499721        -0.0\n",
      "   5. Outlier Trim (0.5%)  595339        6080.0     494563      5158.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/CAT_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       cat\n",
      "0  C02520200          Ars 1996-05-03          1995  0.062864\n",
      "1  C02520200          Ars 1996-11-01          1996  0.140670\n",
      "2  C02520200          Ars 1997-10-31          1997  0.285992\n",
      "3  C02520200          Ars 1998-12-04          1998  0.036770\n",
      "4  C02520200          Ars 1999-12-10          1999 -0.143405\n",
      "\n",
      "================================================================================\n",
      "Processing file: CPM.txt\n",
      "  Rows BEFORE: 1785864\n",
      "  Firm-years BEFORE: 840910\n",
      "  Target column found: 'cpm'\n",
      "  Using target column: 'cpm'\n",
      "  Dropping 2706 rows with ±infinity in 'cpm'.\n",
      "  Dropped 301207 empty rows in 'cpm'.\n",
      "  Dropped 15209 stale rows (>3 years old).\n",
      "  Removed 259041 duplicates.\n",
      "  Trimmed 8966 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 1785864          -0.0     840910        -0.0\n",
      "        2. Country Filter 1773840       12024.0     834941      5969.0\n",
      "   Clean: Remove Infinity 1771134        2706.0     834012       929.0\n",
      " Clean: Remove NaNs/Empty 1469927      301207.0     698348    135664.0\n",
      "Clean: Valid FiscalPeriod 1469927          -0.0     698348        -0.0\n",
      " Filter: Best FP per Date 1164123      305804.0     618040     80308.0\n",
      "   Filter: Stale (>3 Yrs) 1148914       15209.0     614848      3192.0\n",
      " 3. Anomaly Filter [None] 1148914          -0.0     614848        -0.0\n",
      "  4. Dedup (Earliest PIT)  889873      259041.0     614848        -0.0\n",
      "   5. Outlier Trim (0.5%)  880907        8966.0     608915      5933.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/CPM_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       cpm\n",
      "0  C02500770          Ars 1995-12-29          1994 -0.026003\n",
      "1  C02500770          Ars 1996-05-03          1995 -0.008383\n",
      "2  C02500770          Ars 1998-07-03          1997 -0.006505\n",
      "3  C02500770          Ars 1999-10-08          1997  0.089205\n",
      "4  C02500770          Ars 1999-10-01          1998  0.089222\n",
      "\n",
      "================================================================================\n",
      "Processing file: EC.txt\n",
      "  Rows BEFORE: 1888303\n",
      "  Firm-years BEFORE: 840371\n",
      "  Target column found: 'ec'\n",
      "  Using target column: 'ec'\n",
      "  Dropping 114 rows with ±infinity in 'ec'.\n",
      "  Dropped 1238993 empty rows in 'ec'.\n",
      "  Dropped 15621 stale rows (>3 years old).\n",
      "  Removed 115265 duplicates.\n",
      "  Trimmed 3068 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 1888303          -0.0     840371        -0.0\n",
      "        2. Country Filter 1875061       13242.0     834403      5968.0\n",
      "   Clean: Remove Infinity 1874947         114.0     834358        45.0\n",
      " Clean: Remove NaNs/Empty  635954     1238993.0     276767    557591.0\n",
      "Clean: Valid FiscalPeriod  635954          -0.0     276767        -0.0\n",
      " Filter: Best FP per Date  431063      204891.0     227290     49477.0\n",
      "   Filter: Stale (>3 Yrs)  415442       15621.0     222674      4616.0\n",
      " 3. Anomaly Filter [None]  415442          -0.0     222674        -0.0\n",
      "  4. Dedup (Earliest PIT)  300177      115265.0     222674        -0.0\n",
      "   5. Outlier Trim (0.5%)  297109        3068.0     220366      2308.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/EC_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod        ec\n",
      "0  C02520220          Ars 2004-04-09          2003 -0.049009\n",
      "1  C02520220          Ars 2005-04-01          2004 -0.235158\n",
      "2  C02520230          Ars 2015-09-14          2015  0.768876\n",
      "3  C02520230          Ars 2016-09-08          2016  0.403489\n",
      "4  C02520230          Ars 2017-09-08          2017  0.665104\n",
      "\n",
      "================================================================================\n",
      "Processing file: ES.txt\n",
      "  Rows BEFORE: 1888303\n",
      "  Firm-years BEFORE: 840371\n",
      "  Target column found: 'es'\n",
      "  Using target column: 'es'\n",
      "  Dropped 503338 empty rows in 'es'.\n",
      "  Dropped 21116 stale rows (>3 years old).\n",
      "  ES filter applied: 36 rows dropped.\n",
      "  Removed 236462 duplicates.\n",
      "  Trimmed 6348 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                           Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "                    1. Raw Load 1888303          -0.0     840371        -0.0\n",
      "              2. Country Filter 1875061       13242.0     834403      5968.0\n",
      "         Clean: Remove Infinity 1875061          -0.0     834403        -0.0\n",
      "       Clean: Remove NaNs/Empty 1371723      503338.0     607446    226957.0\n",
      "      Clean: Valid FiscalPeriod 1371723          -0.0     607446        -0.0\n",
      "       Filter: Best FP per Date  885733      485990.0     486367    121079.0\n",
      "         Filter: Stale (>3 Yrs)  864617       21116.0     479726      6641.0\n",
      "3. Anomaly Filter [ES (<= 600)]  864581          36.0     479701        25.0\n",
      "        4. Dedup (Earliest PIT)  628119      236462.0     479701        -0.0\n",
      "         5. Outlier Trim (0.5%)  621771        6348.0     474992      4709.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/ES_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod        es\n",
      "0  C02500770          Ars 1999-07-30          1997 -0.702685\n",
      "1  C02500770          Ars 1998-10-02          1997 -0.695665\n",
      "2  C02500770          Ars 1999-10-01          1998 -0.023766\n",
      "3  C02500770          Ars 2000-05-19          1999  0.366352\n",
      "4  C02500770          Ars 2000-05-26          1999  0.366455\n",
      "\n",
      "================================================================================\n",
      "Processing file: GP.txt\n",
      "  Rows BEFORE: 4139889\n",
      "  Firm-years BEFORE: 886329\n",
      "  Target column found: 'gp'\n",
      "  Using target column: 'gp'\n",
      "  Dropping 18 rows with ±infinity in 'gp'.\n",
      "  Dropped 3215758 empty rows in 'gp'.\n",
      "  Dropped 13298 stale rows (>3 years old).\n",
      "  Removed 7572 duplicates.\n",
      "  Trimmed 7108 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 4139889          -0.0     886329        -0.0\n",
      "        2. Country Filter 4112856       27033.0     880005      6324.0\n",
      "   Clean: Remove Infinity 4112838          18.0     880005        -0.0\n",
      " Clean: Remove NaNs/Empty  897080     3215758.0     739274    140731.0\n",
      "Clean: Valid FiscalPeriod  897080          -0.0     739274        -0.0\n",
      " Filter: Best FP per Date  725274      171806.0     624488    114786.0\n",
      "   Filter: Stale (>3 Yrs)  711976       13298.0     618041      6447.0\n",
      " 3. Anomaly Filter [None]  711976          -0.0     618041        -0.0\n",
      "  4. Dedup (Earliest PIT)  704404        7572.0     618041        -0.0\n",
      "   5. Outlier Trim (0.5%)  697296        7108.0     611795      6246.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/GP_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod        gp\n",
      "0  C02500770          Ars 1995-12-29          1994  0.201818\n",
      "1  C02500770          Ars 1996-05-03          1995  0.135181\n",
      "2  C02500770          Ars 1998-07-03          1997  0.094231\n",
      "3  C02500770          Ars 1999-10-01          1998  0.141252\n",
      "4  C02520200          Ars 1996-05-03          1995  0.119097\n",
      "\n",
      "================================================================================\n",
      "Processing file: IG.txt\n",
      "  Rows BEFORE: 2549446\n",
      "  Firm-years BEFORE: 883525\n",
      "  Target column found: 'ig'\n",
      "  Using target column: 'ig'\n",
      "  Dropped 661478 empty rows in 'ig'.\n",
      "  Dropped 7744 stale rows (>3 years old).\n",
      "  Removed 105237 duplicates.\n",
      "  Trimmed 15914 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 2549446          -0.0     883525        -0.0\n",
      "        2. Country Filter 2533624       15822.0     877217      6308.0\n",
      "   Clean: Remove Infinity 2533624          -0.0     877217        -0.0\n",
      " Clean: Remove NaNs/Empty 1872146      661478.0     778734     98483.0\n",
      "Clean: Valid FiscalPeriod 1872146          -0.0     778734        -0.0\n",
      " Filter: Best FP per Date 1697787      174359.0     696681     82053.0\n",
      "   Filter: Stale (>3 Yrs) 1690043        7744.0     693363      3318.0\n",
      " 3. Anomaly Filter [None] 1690043          -0.0     693363        -0.0\n",
      "  4. Dedup (Earliest PIT) 1584806      105237.0     693363        -0.0\n",
      "   5. Outlier Trim (0.5%) 1568892       15914.0     686871      6492.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/IG_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod        ig\n",
      "0  C02500770          Ars 1995-12-29          1994  0.065061\n",
      "1  C02500770          Ars 1996-05-03          1995 -0.047951\n",
      "2  C02500770          Ars 1998-07-03          1997 -0.002569\n",
      "3  C02500770          Ars 1999-10-01          1998  0.026443\n",
      "4  C02500770          Ars 2000-05-26          1999 -0.045453\n",
      "\n",
      "================================================================================\n",
      "Processing file: INV.txt\n",
      "  Rows BEFORE: 1550264\n",
      "  Firm-years BEFORE: 838148\n",
      "  Target column found: 'inv'\n",
      "  Using target column: 'inv'\n",
      "  Dropping 1319 rows with ±infinity in 'inv'.\n",
      "  Dropped 569847 empty rows in 'inv'.\n",
      "  Dropped 8832 stale rows (>3 years old).\n",
      "  Removed 190554 duplicates.\n",
      "  Trimmed 4323 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 1550264          -0.0     838148        -0.0\n",
      "        2. Country Filter 1539100       11164.0     832194      5954.0\n",
      "   Clean: Remove Infinity 1537781        1319.0     831814       380.0\n",
      " Clean: Remove NaNs/Empty  967934      569847.0     563413    268401.0\n",
      "Clean: Valid FiscalPeriod  967934          -0.0     563413        -0.0\n",
      " Filter: Best FP per Date  820188      147746.0     534939     28474.0\n",
      "   Filter: Stale (>3 Yrs)  811356        8832.0     533930      1009.0\n",
      " 3. Anomaly Filter [None]  811356          -0.0     533930        -0.0\n",
      "  4. Dedup (Earliest PIT)  620802      190554.0     533930        -0.0\n",
      "   5. Outlier Trim (0.5%)  616479        4323.0     530140      3790.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/INV_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       inv\n",
      "0  C02500770          Ars 1996-05-03          1995  2.603457\n",
      "1  C02500770          Ars 1998-07-03          1997  1.253880\n",
      "2  C02500770          Ars 1999-10-01          1998  1.257093\n",
      "3  C02500770          Ars 2000-05-26          1999  0.759205\n",
      "4  C02520200          Ars 1996-05-03          1995  1.834208\n",
      "\n",
      "================================================================================\n",
      "Processing file: LTG.txt\n",
      "  Rows BEFORE: 4891955\n",
      "  Firm-years BEFORE: 886585\n",
      "  Target column found: 'ltg'\n",
      "  Using target column: 'ltg'\n",
      "  Dropping 3 rows with ±infinity in 'ltg'.\n",
      "  Dropped 4438634 empty rows in 'ltg'.\n",
      "  Dropped 8868 stale rows (>3 years old).\n",
      "  Removed 3744 duplicates.\n",
      "  Trimmed 3250 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 4891955          -0.0     886585        -0.0\n",
      "        2. Country Filter 4859065       32890.0     880259      6326.0\n",
      "   Clean: Remove Infinity 4859062           3.0     880259        -0.0\n",
      " Clean: Remove NaNs/Empty  420428     4438634.0     302644    577615.0\n",
      "Clean: Valid FiscalPeriod  420428          -0.0     302644        -0.0\n",
      " Filter: Best FP per Date  331036       89392.0     261357     41287.0\n",
      "   Filter: Stale (>3 Yrs)  322168        8868.0     257425      3932.0\n",
      " 3. Anomaly Filter [None]  322168          -0.0     257425        -0.0\n",
      "  4. Dedup (Earliest PIT)  318424        3744.0     257425        -0.0\n",
      "   5. Outlier Trim (0.5%)  315174        3250.0     254895      2530.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/LTG_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       ltg\n",
      "0  C02500770          Ars 1995-12-29          1994  0.011120\n",
      "1  C02500770          Ars 1996-05-03          1995  0.087325\n",
      "2  C02500770          Ars 1998-07-03          1997  0.041760\n",
      "3  C02500770          Ars 1999-10-01          1998  0.012967\n",
      "4  C02500770          Ars 2000-05-26          1999 -0.009448\n",
      "\n",
      "================================================================================\n",
      "Processing file: NCA.txt\n",
      "  Rows BEFORE: 3108973\n",
      "  Firm-years BEFORE: 884534\n",
      "  Target column found: 'nca'\n",
      "  Using target column: 'nca'\n",
      "  Dropping 15 rows with ±infinity in 'nca'.\n",
      "  Dropped 1462198 empty rows in 'nca'.\n",
      "  Dropped 10221 stale rows (>3 years old).\n",
      "  Removed 101366 duplicates.\n",
      "  Trimmed 12988 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 3108973          -0.0     884534        -0.0\n",
      "        2. Country Filter 3090833       18140.0     878214      6320.0\n",
      "   Clean: Remove Infinity 3090818          15.0     878206         8.0\n",
      " Clean: Remove NaNs/Empty 1628620     1462198.0     687396    190810.0\n",
      "Clean: Valid FiscalPeriod 1628620          -0.0     687396        -0.0\n",
      " Filter: Best FP per Date 1403646      224974.0     617876     69520.0\n",
      "   Filter: Stale (>3 Yrs) 1393425       10221.0     614358      3518.0\n",
      " 3. Anomaly Filter [None] 1393425          -0.0     614358        -0.0\n",
      "  4. Dedup (Earliest PIT) 1292059      101366.0     614358        -0.0\n",
      "   5. Outlier Trim (0.5%) 1279071       12988.0     607210      7148.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/NCA_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       nca\n",
      "0  C02500770          Ars 1995-12-29          1994  0.042944\n",
      "1  C02500770          Ars 1996-05-03          1995  0.165906\n",
      "2  C02500770          Ars 1998-07-03          1997  0.059425\n",
      "3  C02500770          Ars 1999-10-01          1998 -0.039300\n",
      "4  C02500770          Ars 2000-05-26          1999 -0.103221\n",
      "\n",
      "================================================================================\n",
      "Processing file: NOA.txt\n",
      "  Rows BEFORE: 2899491\n",
      "  Firm-years BEFORE: 884811\n",
      "  Base target 'noa' not found. Using 'noa_anomaly'.\n",
      "  Using target column: 'noa_anomaly'\n",
      "  Dropping 129 rows with ±infinity in 'noa_anomaly'.\n",
      "  Dropped 1360163 empty rows in 'noa_anomaly'.\n",
      "  Dropped 8953 stale rows (>3 years old).\n",
      "  Renamed column 'noa_anomaly' to 'noa'.\n",
      "  Removed 17586 duplicates.\n",
      "  Trimmed 13409 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 2899491          -0.0     884811        -0.0\n",
      "        2. Country Filter 2882258       17233.0     878491      6320.0\n",
      "   Clean: Remove Infinity 2882129         129.0     878429        62.0\n",
      " Clean: Remove NaNs/Empty 1521966     1360163.0     736760    141669.0\n",
      "Clean: Valid FiscalPeriod 1521966          -0.0     736760        -0.0\n",
      " Filter: Best FP per Date 1360925      161041.0     652812     83948.0\n",
      "   Filter: Stale (>3 Yrs) 1351972        8953.0     648906      3906.0\n",
      " 3. Anomaly Filter [None] 1351972          -0.0     648906        -0.0\n",
      "  4. Dedup (Earliest PIT) 1334386       17586.0     648906        -0.0\n",
      "   5. Outlier Trim (0.5%) 1320977       13409.0     640423      8483.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/NOA_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       noa\n",
      "0  C02500770          Ars 1995-12-29          1993  0.203046\n",
      "1  C02500770          Ars 2000-05-19          1999  0.099477\n",
      "2  C02520200          Ars 1996-05-03          1995  0.763194\n",
      "3  C02520200          Ars 1996-11-01          1996  0.392976\n",
      "4  C02520200          Ars 1997-10-31          1997  0.764415\n",
      "\n",
      "================================================================================\n",
      "Processing file: NWC.txt\n",
      "  Rows BEFORE: 2721575\n",
      "  Firm-years BEFORE: 884296\n",
      "  Base target 'nwc' not found. Using 'nwc_anomaly'.\n",
      "  Using target column: 'nwc_anomaly'\n",
      "  Dropping 72 rows with ±infinity in 'nwc_anomaly'.\n",
      "  Dropped 1227040 empty rows in 'nwc_anomaly'.\n",
      "  Dropped 8841 stale rows (>3 years old).\n",
      "  Renamed column 'nwc_anomaly' to 'nwc'.\n",
      "  Removed 5097 duplicates.\n",
      "  Trimmed 12923 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 2721575          -0.0     884296        -0.0\n",
      "        2. Country Filter 2704758       16817.0     877982      6314.0\n",
      "   Clean: Remove Infinity 2704686          72.0     877944        38.0\n",
      " Clean: Remove NaNs/Empty 1477646     1227040.0     721851    156093.0\n",
      "Clean: Valid FiscalPeriod 1477646          -0.0     721851        -0.0\n",
      " Filter: Best FP per Date 1298994      178652.0     649651     72200.0\n",
      "   Filter: Stale (>3 Yrs) 1290153        8841.0     646310      3341.0\n",
      " 3. Anomaly Filter [None] 1290153          -0.0     646310        -0.0\n",
      "  4. Dedup (Earliest PIT) 1285056        5097.0     646310        -0.0\n",
      "   5. Outlier Trim (0.5%) 1272133       12923.0     638090      8220.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/NWC_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       nwc\n",
      "0  C02500770          Ars 1995-12-29          1994  0.188406\n",
      "1  C02500770          Ars 1996-05-03          1995  0.017948\n",
      "2  C02500770          Ars 1998-07-03          1997  0.005449\n",
      "3  C02500770          Ars 1999-10-01          1998 -0.139677\n",
      "4  C02500770          Ars 2000-05-19          1999 -0.055923\n",
      "\n",
      "================================================================================\n",
      "Processing file: OL.txt\n",
      "  Rows BEFORE: 3890568\n",
      "  Firm-years BEFORE: 885171\n",
      "  Target column found: 'ol'\n",
      "  Using target column: 'ol'\n",
      "  Dropping 104 rows with ±infinity in 'ol'.\n",
      "  Dropped 3253419 empty rows in 'ol'.\n",
      "  Dropped 11889 stale rows (>3 years old).\n",
      "  Removed 2364 duplicates.\n",
      "  Trimmed 4882 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 3890568          -0.0     885171        -0.0\n",
      "        2. Country Filter 3864104       26464.0     878851      6320.0\n",
      "   Clean: Remove Infinity 3864000         104.0     878851        -0.0\n",
      " Clean: Remove NaNs/Empty  610581     3253419.0     535252    343599.0\n",
      "Clean: Valid FiscalPeriod  610581          -0.0     535252        -0.0\n",
      " Filter: Best FP per Date  496399      114182.0     448949     86303.0\n",
      "   Filter: Stale (>3 Yrs)  484510       11889.0     442766      6183.0\n",
      " 3. Anomaly Filter [None]  484510          -0.0     442766        -0.0\n",
      "  4. Dedup (Earliest PIT)  482146        2364.0     442766        -0.0\n",
      "   5. Outlier Trim (0.5%)  477264        4882.0     438222      4544.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/OL_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod        ol\n",
      "0  C02500770          Ars 1995-12-29          1994  0.147251\n",
      "1  C02500770          Ars 1996-05-03          1995  0.187383\n",
      "2  C02500770          Ars 1998-07-03          1997  0.108487\n",
      "3  C02500770          Ars 1999-10-01          1998  0.096554\n",
      "4  C02520200          Ars 1996-05-03          1995  0.065631\n",
      "\n",
      "================================================================================\n",
      "Processing file: OSC.txt\n",
      "  Rows BEFORE: 4406065\n",
      "  Firm-years BEFORE: 887559\n",
      "  Target column found: 'osc'\n",
      "  Using target column: 'osc'\n",
      "  Dropping 63 rows with ±infinity in 'osc'.\n",
      "  Dropped 3396648 empty rows in 'osc'.\n",
      "  Dropped 7735 stale rows (>3 years old).\n",
      "  Removed 2217 duplicates.\n",
      "  Trimmed 8142 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 4406065          -0.0     887559        -0.0\n",
      "        2. Country Filter 4377063       29002.0     881222      6337.0\n",
      "   Clean: Remove Infinity 4377000          63.0     881222        -0.0\n",
      " Clean: Remove NaNs/Empty  980352     3396648.0     729575    151647.0\n",
      "Clean: Valid FiscalPeriod  980352          -0.0     729575        -0.0\n",
      " Filter: Best FP per Date  817694      162658.0     648903     80672.0\n",
      "   Filter: Stale (>3 Yrs)  809959        7735.0     645792      3111.0\n",
      " 3. Anomaly Filter [None]  809959          -0.0     645792        -0.0\n",
      "  4. Dedup (Earliest PIT)  807742        2217.0     645792        -0.0\n",
      "   5. Outlier Trim (0.5%)  799600        8142.0     639096      6696.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/OSC_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       osc\n",
      "0  C02500770          Ars 1995-12-29          1994 -1.418356\n",
      "1  C02500770          Ars 1996-05-03          1995  0.010150\n",
      "2  C02500770          Ars 1998-07-03          1997  1.445561\n",
      "3  C02500770          Ars 1999-10-01          1998  2.790007\n",
      "4  C02500770          Ars 2000-05-19          1999  2.794642\n",
      "\n",
      "================================================================================\n",
      "Processing file: PM.txt\n",
      "  Rows BEFORE: 1785864\n",
      "  Firm-years BEFORE: 840910\n",
      "  Target column found: 'pm'\n",
      "  Using target column: 'pm'\n",
      "  Dropping 17311 rows with ±infinity in 'pm'.\n",
      "  Dropped 197474 empty rows in 'pm'.\n",
      "  Dropped 17263 stale rows (>3 years old).\n",
      "  Removed 277422 duplicates.\n",
      "  Trimmed 7132 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 1785864          -0.0     840910        -0.0\n",
      "        2. Country Filter 1773840       12024.0     834941      5969.0\n",
      "   Clean: Remove Infinity 1756529       17311.0     826296      8645.0\n",
      " Clean: Remove NaNs/Empty 1559055      197474.0     760471     65825.0\n",
      "Clean: Valid FiscalPeriod 1559055          -0.0     760471        -0.0\n",
      " Filter: Best FP per Date 1199237      359818.0     639429    121042.0\n",
      "   Filter: Stale (>3 Yrs) 1181974       17263.0     634839      4590.0\n",
      " 3. Anomaly Filter [None] 1181974          -0.0     634839        -0.0\n",
      "  4. Dedup (Earliest PIT)  904552      277422.0     634839        -0.0\n",
      "   5. Outlier Trim (0.5%)  897420        7132.0     629763      5076.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/PM_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod        pm\n",
      "0  C02500770          Ars 1995-12-29          1994  0.130227\n",
      "1  C02500770          Ars 1996-05-03          1995  0.121844\n",
      "2  C02500770          Ars 1998-07-03          1997  0.166367\n",
      "3  C02500770          Ars 1999-10-08          1997  0.262077\n",
      "4  C02500770          Ars 1999-10-01          1998  0.255589\n",
      "\n",
      "================================================================================\n",
      "Processing file: POA.txt\n",
      "  Rows BEFORE: 1527513\n",
      "  Firm-years BEFORE: 842220\n",
      "  Target column found: 'poa'\n",
      "  Using target column: 'poa'\n",
      "  Dropping 4974 rows with ±infinity in 'poa'.\n",
      "  Dropped 249657 empty rows in 'poa'.\n",
      "  Dropped 14075 stale rows (>3 years old).\n",
      "  POA filter applied: 6672 rows dropped.\n",
      "  Removed 238058 duplicates.\n",
      "  Trimmed 7804 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                                 Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "                          1. Raw Load 1527513          -0.0     842220        -0.0\n",
      "                    2. Country Filter 1515830       11683.0     836228      5992.0\n",
      "               Clean: Remove Infinity 1510856        4974.0     833045      3183.0\n",
      "             Clean: Remove NaNs/Empty 1261199      249657.0     741685     91360.0\n",
      "            Clean: Valid FiscalPeriod 1261199          -0.0     741685        -0.0\n",
      "             Filter: Best FP per Date 1032766      228433.0     654080     87605.0\n",
      "               Filter: Stale (>3 Yrs) 1018691       14075.0     650480      3600.0\n",
      "3. Anomaly Filter [POA (-50 to 6000)] 1012019        6672.0     646202      4278.0\n",
      "              4. Dedup (Earliest PIT)  773961      238058.0     646202        -0.0\n",
      "               5. Outlier Trim (0.5%)  766157        7804.0     639587      6615.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/POA_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod        poa\n",
      "0  C02520200          Ars 1996-05-03          1995 -14.113936\n",
      "1  C02520200          Ars 1996-11-01          1996  -1.589398\n",
      "2  C02520200          Ars 1997-10-31          1997   1.563491\n",
      "3  C02520200          Ars 1998-12-04          1998  -1.617731\n",
      "4  C02520200          Ars 1999-12-24          1999  -5.251340\n",
      "\n",
      "================================================================================\n",
      "Processing file: PRO.txt\n",
      "  Rows BEFORE: 3700378\n",
      "  Firm-years BEFORE: 886613\n",
      "  Target column found: 'pro'\n",
      "  Using target column: 'pro'\n",
      "  Dropping 101 rows with ±infinity in 'pro'.\n",
      "  Dropped 2783659 empty rows in 'pro'.\n",
      "  Dropped 7224 stale rows (>3 years old).\n",
      "  Removed 46674 duplicates.\n",
      "  Trimmed 7164 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 3700378          -0.0     886613        -0.0\n",
      "        2. Country Filter 3674812       25566.0     880281      6332.0\n",
      "   Clean: Remove Infinity 3674711         101.0     880281        -0.0\n",
      " Clean: Remove NaNs/Empty  891052     2783659.0     725682    154599.0\n",
      "Clean: Valid FiscalPeriod  891052          -0.0     725682        -0.0\n",
      " Filter: Best FP per Date  763592      127460.0     653749     71933.0\n",
      "   Filter: Stale (>3 Yrs)  756368        7224.0     650885      2864.0\n",
      " 3. Anomaly Filter [None]  756368          -0.0     650885        -0.0\n",
      "  4. Dedup (Earliest PIT)  709694       46674.0     650885        -0.0\n",
      "   5. Outlier Trim (0.5%)  702530        7164.0     644238      6647.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/PRO_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       pro\n",
      "0  C02500770          Ars 1995-12-29          1994  0.087236\n",
      "1  C02500770          Ars 1996-05-03          1995 -0.029745\n",
      "2  C02500770          Ars 1998-07-03          1997 -0.060711\n",
      "3  C02500770          Ars 1999-10-01          1998 -0.149942\n",
      "4  C02500770          Ars 2000-05-19          1999 -0.211769\n",
      "\n",
      "================================================================================\n",
      "Processing file: PTA.txt\n",
      "  Rows BEFORE: 1765455\n",
      "  Firm-years BEFORE: 842327\n",
      "  Target column found: 'pta'\n",
      "  Using target column: 'pta'\n",
      "  Dropped 761335 empty rows in 'pta'.\n",
      "  Dropped 16424 stale rows (>3 years old).\n",
      "  PTA filter applied: 5819 rows dropped.\n",
      "  Removed 195407 duplicates.\n",
      "  Trimmed 5964 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                                 Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "                          1. Raw Load 1765455          -0.0     842327        -0.0\n",
      "                    2. Country Filter 1753091       12364.0     836335      5992.0\n",
      "               Clean: Remove Infinity 1753091          -0.0     836335        -0.0\n",
      "             Clean: Remove NaNs/Empty  991756      761335.0     605735    230600.0\n",
      "            Clean: Valid FiscalPeriod  991756          -0.0     605735        -0.0\n",
      "             Filter: Best FP per Date  807485      184271.0     534112     71623.0\n",
      "               Filter: Stale (>3 Yrs)  791061       16424.0     529653      4459.0\n",
      "3. Anomaly Filter [PTA (-20 to 6000)]  785242        5819.0     525670      3983.0\n",
      "              4. Dedup (Earliest PIT)  589835      195407.0     525670        -0.0\n",
      "               5. Outlier Trim (0.5%)  583871        5964.0     520335      5335.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/PTA_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       pta\n",
      "0  C02520200          Ars 1996-05-03          1995  0.627866\n",
      "1  C02520200          Ars 1996-11-01          1996 -0.929217\n",
      "2  C02520200          Ars 1997-10-31          1997  0.372476\n",
      "3  C02520200          Ars 1998-12-04          1998  0.665810\n",
      "4  C02520200          Ars 1999-12-24          1999 -0.810537\n",
      "\n",
      "================================================================================\n",
      "Processing file: ROE.txt\n",
      "  Rows BEFORE: 3785314\n",
      "  Firm-years BEFORE: 887427\n",
      "  Target column found: 'roe'\n",
      "  Using target column: 'roe'\n",
      "  Dropping 22 rows with ±infinity in 'roe'.\n",
      "  Dropped 2955598 empty rows in 'roe'.\n",
      "  Dropped 11879 stale rows (>3 years old).\n",
      "  Removed 10730 duplicates.\n",
      "  Trimmed 6613 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 3785314          -0.0     887427        -0.0\n",
      "        2. Country Filter 3759582       25732.0     881090      6337.0\n",
      "   Clean: Remove Infinity 3759560          22.0     881090        -0.0\n",
      " Clean: Remove NaNs/Empty  803962     2955598.0     676628    204462.0\n",
      "Clean: Valid FiscalPeriod  803962          -0.0     676628        -0.0\n",
      " Filter: Best FP per Date  677264      126698.0     587341     89287.0\n",
      "   Filter: Stale (>3 Yrs)  665385       11879.0     582402      4939.0\n",
      " 3. Anomaly Filter [None]  665385          -0.0     582402        -0.0\n",
      "  4. Dedup (Earliest PIT)  654655       10730.0     582402        -0.0\n",
      "   5. Outlier Trim (0.5%)  648042        6613.0     576443      5959.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/ROE_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       roe\n",
      "0  C02500770          Ars 1998-07-03          1997 -0.262669\n",
      "1  C02500770          Ars 1999-10-01          1998 -2.463286\n",
      "2  C02500770          Ars 2000-05-19          1999 -2.055069\n",
      "3  C02520200          Ars 1996-11-01          1996 -0.217682\n",
      "4  C02520200          Ars 1997-10-31          1997  0.080445\n",
      "\n",
      "================================================================================\n",
      "Processing file: RS.txt\n",
      "  Rows BEFORE: 1351845\n",
      "  Firm-years BEFORE: 817863\n",
      "  Target column found: 'rs'\n",
      "  Using target column: 'rs'\n",
      "  Dropped 342087 empty rows in 'rs'.\n",
      "  Dropped 7270 stale rows (>3 years old).\n",
      "  Removed 197721 duplicates.\n",
      "  Trimmed 6666 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 1351845          -0.0     817863        -0.0\n",
      "        2. Country Filter 1341677       10168.0     811978      5885.0\n",
      "   Clean: Remove Infinity 1341677          -0.0     811978        -0.0\n",
      " Clean: Remove NaNs/Empty  999590      342087.0     567361    244617.0\n",
      "Clean: Valid FiscalPeriod  999590          -0.0     567361        -0.0\n",
      " Filter: Best FP per Date  865454      134136.0     545300     22061.0\n",
      "   Filter: Stale (>3 Yrs)  858184        7270.0     544657       643.0\n",
      " 3. Anomaly Filter [None]  858184          -0.0     544657        -0.0\n",
      "  4. Dedup (Earliest PIT)  660463      197721.0     544657        -0.0\n",
      "   5. Outlier Trim (0.5%)  653797        6666.0     539444      5213.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/RS_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod        rs\n",
      "0  C02500770          Ars 1998-07-03          1997 -0.499734\n",
      "1  C02500770          Ars 1999-10-01          1998  1.494384\n",
      "2  C02500770          Ars 2000-05-26          1999  0.766257\n",
      "3  C02500770          Ars 2000-05-19          1999  1.026737\n",
      "4  C02520200          Ars 1996-05-03          1995 -0.002564\n",
      "\n",
      "================================================================================\n",
      "Processing file: SG.txt\n",
      "  Rows BEFORE: 2438961\n",
      "  Firm-years BEFORE: 883772\n",
      "  Target column found: 'sg'\n",
      "  Using target column: 'sg'\n",
      "  Dropping 32 rows with ±infinity in 'sg'.\n",
      "  Dropped 1143743 empty rows in 'sg'.\n",
      "  Dropped 6268 stale rows (>3 years old).\n",
      "  Removed 4005 duplicates.\n",
      "  Trimmed 11734 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 2438961          -0.0     883772        -0.0\n",
      "        2. Country Filter 2423786       15175.0     877466      6306.0\n",
      "   Clean: Remove Infinity 2423754          32.0     877445        21.0\n",
      " Clean: Remove NaNs/Empty 1280011     1143743.0     656371    221074.0\n",
      "Clean: Valid FiscalPeriod 1280011          -0.0     656371        -0.0\n",
      " Filter: Best FP per Date 1177771      102240.0     600711     55660.0\n",
      "   Filter: Stale (>3 Yrs) 1171503        6268.0     597895      2816.0\n",
      " 3. Anomaly Filter [None] 1171503          -0.0     597895        -0.0\n",
      "  4. Dedup (Earliest PIT) 1167498        4005.0     597895        -0.0\n",
      "   5. Outlier Trim (0.5%) 1155764       11734.0     588632      9263.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/SG_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod        sg\n",
      "0  C02500770          Ars 1999-10-01          1998 -0.745101\n",
      "1  C02500770          Ars 2000-05-19          1999  0.523340\n",
      "2  C02520200          Ars 1997-10-31          1997  0.092494\n",
      "3  C02520200          Ars 1999-04-02          1998  0.197803\n",
      "4  C02520200          Ars 1998-12-04          1998  0.221828\n",
      "\n",
      "================================================================================\n",
      "Processing file: SLI.txt\n",
      "  Rows BEFORE: 3538149\n",
      "  Firm-years BEFORE: 885595\n",
      "  Target column found: 'sli'\n",
      "  Using target column: 'sli'\n",
      "  Dropping 3180 rows with ±infinity in 'sli'.\n",
      "  Dropped 2839210 empty rows in 'sli'.\n",
      "  Dropped 2575 stale rows (>3 years old).\n",
      "  Removed 1055 duplicates.\n",
      "  Trimmed 5934 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 3538149          -0.0     885595        -0.0\n",
      "        2. Country Filter 3514203       23946.0     879297      6298.0\n",
      "   Clean: Remove Infinity 3511023        3180.0     879297        -0.0\n",
      " Clean: Remove NaNs/Empty  671813     2839210.0     590844    288453.0\n",
      "Clean: Valid FiscalPeriod  671813          -0.0     590844        -0.0\n",
      " Filter: Best FP per Date  590132       81681.0     545081     45763.0\n",
      "   Filter: Stale (>3 Yrs)  587557        2575.0     543795      1286.0\n",
      " 3. Anomaly Filter [None]  587557          -0.0     543795        -0.0\n",
      "  4. Dedup (Earliest PIT)  586502        1055.0     543795        -0.0\n",
      "   5. Outlier Trim (0.5%)  580568        5934.0     538279      5516.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/SLI_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       sli\n",
      "0  C02500770          Ars 1995-12-29          1994 -0.074458\n",
      "1  C02500770          Ars 1996-05-03          1995 -0.219070\n",
      "2  C02500770          Ars 1998-07-03          1997 -0.347189\n",
      "3  C02500770          Ars 1999-10-01          1998 -0.528880\n",
      "4  C02500770          Ars 2000-05-26          1999 -0.065717\n",
      "\n",
      "================================================================================\n",
      "Processing file: SLX.txt\n",
      "  Rows BEFORE: 1694448\n",
      "  Firm-years BEFORE: 839740\n",
      "  Target column found: 'slx'\n",
      "  Using target column: 'slx'\n",
      "  Dropping 2298 rows with ±infinity in 'slx'.\n",
      "  Dropped 636950 empty rows in 'slx'.\n",
      "  Dropped 9960 stale rows (>3 years old).\n",
      "  Removed 181599 duplicates.\n",
      "  Trimmed 6739 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 1694448          -0.0     839740        -0.0\n",
      "        2. Country Filter 1682418       12030.0     833763      5977.0\n",
      "   Clean: Remove Infinity 1680120        2298.0     832733      1030.0\n",
      " Clean: Remove NaNs/Empty 1043170      636950.0     553562    279171.0\n",
      "Clean: Valid FiscalPeriod 1043170          -0.0     553562        -0.0\n",
      " Filter: Best FP per Date  858375      184795.0     507884     45678.0\n",
      "   Filter: Stale (>3 Yrs)  848415        9960.0     506071      1813.0\n",
      " 3. Anomaly Filter [None]  848415          -0.0     506071        -0.0\n",
      "  4. Dedup (Earliest PIT)  666816      181599.0     506071        -0.0\n",
      "   5. Outlier Trim (0.5%)  660077        6739.0     501025      5046.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/SLX_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       slx\n",
      "0  C02500770          Ars 1995-12-29          1994  0.528419\n",
      "1  C02500770          Ars 1996-05-03          1995 -0.169804\n",
      "2  C02500770          Ars 1998-07-03          1997 -0.222608\n",
      "3  C02500770          Ars 1999-10-08          1997 -0.138541\n",
      "4  C02500770          Ars 1999-10-01          1998 -0.003509\n",
      "\n",
      "================================================================================\n",
      "Processing file: TX.txt\n",
      "  Rows BEFORE: 1451441\n",
      "  Firm-years BEFORE: 842220\n",
      "  Target column found: 'tx'\n",
      "  Using target column: 'tx'\n",
      "  Dropping 2672 rows with ±infinity in 'tx'.\n",
      "  Dropped 113003 empty rows in 'tx'.\n",
      "  Dropped 14971 stale rows (>3 years old).\n",
      "  Removed 259064 duplicates.\n",
      "  Trimmed 7843 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 1451441          -0.0     842220        -0.0\n",
      "        2. Country Filter 1440496       10945.0     836230      5990.0\n",
      "   Clean: Remove Infinity 1437824        2672.0     834739      1491.0\n",
      " Clean: Remove NaNs/Empty 1324821      113003.0     808734     26005.0\n",
      "Clean: Valid FiscalPeriod 1324821          -0.0     808734        -0.0\n",
      " Filter: Best FP per Date 1051216      273605.0     675527    133207.0\n",
      "   Filter: Stale (>3 Yrs) 1036245       14971.0     670391      5136.0\n",
      " 3. Anomaly Filter [None] 1036245          -0.0     670391        -0.0\n",
      "  4. Dedup (Earliest PIT)  777181      259064.0     670391        -0.0\n",
      "   5. Outlier Trim (0.5%)  769338        7843.0     663595      6796.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/TX_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod        tx\n",
      "0  C02500770          Ars 1995-12-29          1994  0.694892\n",
      "1  C02500770          Ars 1996-05-03          1995 -0.311765\n",
      "2  C02500770          Ars 1998-07-03          1996  0.184203\n",
      "3  C02500770          Ars 1999-10-01          1998 -0.000245\n",
      "4  C02500770          Ars 2000-05-26          1999 -0.086758\n",
      "\n",
      "================================================================================\n",
      "Processing file: TXF.txt\n",
      "  Rows BEFORE: 3798877\n",
      "  Firm-years BEFORE: 884793\n",
      "  Target column found: 'txf'\n",
      "  Using target column: 'txf'\n",
      "  Dropping 7 rows with ±infinity in 'txf'.\n",
      "  Dropped 3436447 empty rows in 'txf'.\n",
      "  Dropped 2076 stale rows (>3 years old).\n",
      "  Removed 14489 duplicates.\n",
      "  Trimmed 2884 outlier rows (Top/Bottom 0.5% per country).\n",
      "\n",
      "----------------------------------------\n",
      "DATA LOSS ANALYSIS REPORT\n",
      "----------------------------------------\n",
      "                     Step    Rows  Rows Dropped  FirmYears  FY Dropped\n",
      "              1. Raw Load 3798877          -0.0     884793        -0.0\n",
      "        2. Country Filter 3773475       25402.0     878477      6316.0\n",
      "   Clean: Remove Infinity 3773468           7.0     878477        -0.0\n",
      " Clean: Remove NaNs/Empty  337021     3436447.0     303065    575412.0\n",
      "Clean: Valid FiscalPeriod  337021          -0.0     303065        -0.0\n",
      " Filter: Best FP per Date  298273       38748.0     273976     29089.0\n",
      "   Filter: Stale (>3 Yrs)  296197        2076.0     272664      1312.0\n",
      " 3. Anomaly Filter [None]  296197          -0.0     272664        -0.0\n",
      "  4. Dedup (Earliest PIT)  281708       14489.0     272664        -0.0\n",
      "   5. Outlier Trim (0.5%)  278824        2884.0     269859      2805.0\n",
      "----------------------------------------\n",
      "  Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/TXF_clean.txt\n",
      "\n",
      "Preview:\n",
      "          ID HistCurrency   PIT Date  FiscalPeriod       txf\n",
      "0  C02500770          Ars 1995-12-29          1994 -0.006296\n",
      "1  C02500770          Ars 1998-07-03          1997  0.030085\n",
      "2  C02520200          Ars 1996-05-03          1995 -0.034681\n",
      "3  C02520200          Ars 1996-11-01          1996 -0.019196\n",
      "4  C02520200          Ars 1998-12-04          1998 -0.031036\n",
      "\n",
      "SUMMARY OF REMOVED COUNTRIES (INSUFFICIENT UNIQUE FIRMS):\n",
      "      dataset country_code  unique_ids\n",
      "0     ACC.txt          044           1\n",
      "1     ACC.txt          048          18\n",
      "2     ACC.txt          052           1\n",
      "3     ACC.txt          068           5\n",
      "4     ACC.txt          070          29\n",
      "...       ...          ...         ...\n",
      "1102  TXF.txt          833           2\n",
      "1103  TXF.txt          834           9\n",
      "1104  TXF.txt          862          15\n",
      "1105  TXF.txt          894          12\n",
      "1106  TXF.txt          897          17\n",
      "\n",
      "[1107 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY OF THIS SCRIPT\n",
    "# -----------------------------------------------------------------------------\n",
    "# This script performs standardized cleaning and filtering of firm-level\n",
    "# financial indicator datasets. It includes detailed DATA LOSS TRACKING.\n",
    "#\n",
    "# Steps:\n",
    "# 1. Loads the dataset.\n",
    "# 2. Tracks Row/Firm-Year counts at every step.\n",
    "# 3. Filters out countries with fewer than MIN_UNIQUE_IDS_PER_COUNTRY firms.\n",
    "# 4. Determines target column (base stem or anomaly).\n",
    "# 5. Cleans target column (Numeric, remove NaN/Inf).\n",
    "# 6. Keeps only the newest FiscalPeriod per firm.\n",
    "# 7. Applies anomaly filters from Bowles et al. (ES, POA, PTA).\n",
    "# 8. Removes duplicates (retries earliest PIT Date).\n",
    "# 9. Trims top/bottom 0.5% of values per country.\n",
    "# 10. Saves output and prints a detailed Loss Report.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Base directory containing all raw input text files\n",
    "# Ensure 'Temp_file_path_A' is defined in your environment\n",
    "DATA_DIR = Path(Temp_file_path_A)\n",
    "\n",
    "# Explicit list of datasets to process\n",
    "input_files = [\n",
    "    DATA_DIR / \"ACC.txt\",\n",
    "    DATA_DIR / \"AG.txt\",\n",
    "    DATA_DIR / \"AT.txt\",\n",
    "    DATA_DIR / \"CAT.txt\",\n",
    "    DATA_DIR / \"CPM.txt\",\n",
    "    DATA_DIR / \"EC.txt\",\n",
    "    DATA_DIR / \"ES.txt\",\n",
    "    DATA_DIR / \"GP.txt\",\n",
    "    DATA_DIR / \"IG.txt\",\n",
    "    DATA_DIR / \"INV.txt\",\n",
    "    DATA_DIR / \"LTG.txt\",\n",
    "    DATA_DIR / \"NCA.txt\",\n",
    "    DATA_DIR / \"NOA.txt\",\n",
    "    DATA_DIR / \"NWC.txt\",\n",
    "    DATA_DIR / \"OL.txt\",\n",
    "    DATA_DIR / \"OSC.txt\",\n",
    "    DATA_DIR / \"PM.txt\",\n",
    "    DATA_DIR / \"POA.txt\",\n",
    "    DATA_DIR / \"PRO.txt\",\n",
    "    DATA_DIR / \"PTA.txt\",\n",
    "    DATA_DIR / \"ROE.txt\",\n",
    "    DATA_DIR / \"RS.txt\",\n",
    "    #DATA_DIR / \"SAG.txt\",\n",
    "    DATA_DIR / \"SG.txt\",\n",
    "    DATA_DIR / \"SLI.txt\",\n",
    "    DATA_DIR / \"SLX.txt\",\n",
    "    DATA_DIR / \"TX.txt\",\n",
    "    DATA_DIR / \"TXF.txt\",\n",
    "]\n",
    "\n",
    "# Minimum number of distinct firms required per country\n",
    "MIN_UNIQUE_IDS_PER_COUNTRY = 30\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER: TRACKING FUNCTION\n",
    "# =============================================================================\n",
    "def get_counts(df):\n",
    "    \"\"\"\n",
    "    Returns a tuple: (Total Rows, Unique Firm-Years).\n",
    "    Unique Firm-Years are defined as unique combinations of (ID, FiscalPeriod).\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return 0, 0\n",
    "    \n",
    "    rows = len(df)\n",
    "    \n",
    "    # Count unique (ID, FiscalPeriod) pairs\n",
    "    if \"ID\" in df.columns and \"FiscalPeriod\" in df.columns:\n",
    "        # Create a temporary view to ensure we are counting valid numeric periods\n",
    "        # (matches the logic used in cleaning)\n",
    "        temp_fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "        valid_mask = temp_fp.notna()\n",
    "        \n",
    "        unique_fy = (\n",
    "            df.loc[valid_mask, [\"ID\", \"FiscalPeriod\"]]\n",
    "            .drop_duplicates(subset=[\"ID\", \"FiscalPeriod\"])\n",
    "            .shape[0]\n",
    "        )\n",
    "    else:\n",
    "        unique_fy = 0\n",
    "        \n",
    "    return rows, unique_fy\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLEANING FUNCTION (Verified & Updated)\n",
    "# =============================================================================\n",
    "def clean_single_result(df: pd.DataFrame, target_col: str, tracker_list: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs column cleaning and tracks data loss inside the function.\n",
    "    \"\"\"\n",
    "    # Work on a defensive copy\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- 1. Validate Column ---\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(\n",
    "            f\"Expected target column '{target_col}' not found. \"\n",
    "            f\"Available columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    print(f\"  Using target column: '{target_col}'\")\n",
    "\n",
    "    # --- 2. Numeric Conversion & Infinity Check ---\n",
    "    df[target_col] = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "\n",
    "    mask_inf = df[target_col].isin([np.inf, -np.inf])\n",
    "    if mask_inf.any():\n",
    "        print(f\"  Dropping {mask_inf.sum()} rows with ±infinity in '{target_col}'.\")\n",
    "        df = df[~mask_inf].reset_index(drop=True)\n",
    "    \n",
    "    # LOG STEP\n",
    "    r, fy = get_counts(df)\n",
    "    tracker_list.append({\"Step\": \"Clean: Remove Infinity\", \"Rows\": r, \"FirmYears\": fy})\n",
    "\n",
    "    # --- 3. Drop Missing/Empty ---\n",
    "    before_rows = len(df)\n",
    "    df = df[df[target_col].notna() & (df[target_col] != \"\")].reset_index(drop=True)\n",
    "    print(f\"  Dropped {before_rows - len(df)} empty rows in '{target_col}'.\")\n",
    "    \n",
    "    # LOG STEP\n",
    "    r, fy = get_counts(df)\n",
    "    tracker_list.append({\"Step\": \"Clean: Remove NaNs/Empty\", \"Rows\": r, \"FirmYears\": fy})\n",
    "\n",
    "    # --- 4. FiscalPeriod Processing ---\n",
    "    if \"FiscalPeriod\" not in df.columns:\n",
    "        raise KeyError(\"Column 'FiscalPeriod' is missing.\")\n",
    "    df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "    \n",
    "    # Drop rows where FiscalPeriod became NaN\n",
    "    df = df[df[\"FiscalPeriod\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    # LOG STEP\n",
    "    r, fy = get_counts(df)\n",
    "    tracker_list.append({\"Step\": \"Clean: Valid FiscalPeriod\", \"Rows\": r, \"FirmYears\": fy})\n",
    "\n",
    "    # =========================================================================\n",
    "    # FILTER 1: DEDUPLICATE ON DATE (Handle same-day conflicts)\n",
    "    # =========================================================================\n",
    "    if \"PIT Date\" in df.columns:\n",
    "        df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"PIT Date\"])\n",
    "\n",
    "        # Sort by ID -> Date -> FiscalPeriod (Ascending)\n",
    "        # Result: If multiple rows have the SAME ID and SAME Date, \n",
    "        # the one with the highest (newest) FiscalPeriod is last.\n",
    "        df = df.sort_values(by=[\"ID\", \"PIT Date\", \"FiscalPeriod\"])\n",
    "        \n",
    "        # Keep ONLY the last row for each unique (ID, Date) pair\n",
    "        df = df.drop_duplicates(subset=[\"ID\", \"PIT Date\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "        # LOG STEP\n",
    "        r, fy = get_counts(df)\n",
    "        tracker_list.append({\"Step\": \"Filter: Best FP per Date\", \"Rows\": r, \"FirmYears\": fy})\n",
    "    else:\n",
    "        print(\"  WARNING: 'PIT Date' missing. Skipping Date-level dedup.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # FILTER 2: STALE DATA CHECK (Handle Zombie/Digitized Data)\n",
    "    # =========================================================================\n",
    "    # Logic: If (PIT Date - Fiscal Year End) > 3 Years (1095 Days), DROP IT.\n",
    "    # This specifically targets rows like: Date=2022, FiscalPeriod=1986.\n",
    "    if \"PIT Date\" in df.columns:\n",
    "        # Create a temporary FYE date (Assume Dec 31 of that Fiscal Period)\n",
    "        temp_years = df[\"FiscalPeriod\"].astype(int)\n",
    "        \n",
    "        fye_dates = pd.to_datetime(\n",
    "            temp_years.astype(str) + \"-12-31\", \n",
    "            format=\"%Y-%m-%d\", \n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "        \n",
    "        # Calculate lag in days\n",
    "        lag_days = (df[\"PIT Date\"] - fye_dates).dt.days\n",
    "        \n",
    "        # Keep if Fresh (<= 3 years). We do NOT drop negative lags here \n",
    "        # (interim data is usually fine here, or handled elsewhere).\n",
    "        mask_fresh = lag_days <= 1095\n",
    "        \n",
    "        dropped_count = (~mask_fresh).sum()\n",
    "        df = df[mask_fresh].reset_index(drop=True)\n",
    "        \n",
    "        if dropped_count > 0:\n",
    "            print(f\"  Dropped {dropped_count} stale rows (>3 years old).\")\n",
    "\n",
    "        # LOG STEP\n",
    "        r, fy = get_counts(df)\n",
    "        tracker_list.append({\"Step\": \"Filter: Stale (>3 Yrs)\", \"Rows\": r, \"FirmYears\": fy})\n",
    "\n",
    "    \n",
    "    # Cast FiscalPeriod to Int64 for clean formatting\n",
    "    df[\"FiscalPeriod\"] = df[\"FiscalPeriod\"].astype(\"Int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PROCESSING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "# Collector for all dropped countries across datasets\n",
    "country_removal_log = []\n",
    "\n",
    "for input_path in input_files:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Processing file: {input_path.name}\")\n",
    "    \n",
    "    # Initialize tracker for this file\n",
    "    file_tracker = []\n",
    "\n",
    "    try:\n",
    "        # Load dataset\n",
    "        df_in = pd.read_csv(input_path, sep=\"|\")\n",
    "        \n",
    "        # LOG: Initial Load\n",
    "        r, fy = get_counts(df_in)\n",
    "        file_tracker.append({\"Step\": \"1. Raw Load\", \"Rows\": r, \"FirmYears\": fy})\n",
    "        print(f\"  Rows BEFORE: {r}\")\n",
    "        print(f\"  Firm-years BEFORE: {fy}\")\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 1. COUNTRY FILTERING\n",
    "        # ---------------------------------------------------------------------\n",
    "        df_in[\"country_code\"] = df_in[\"ID\"].astype(str).str[1:4]\n",
    "\n",
    "        unique_ids_per_country = (\n",
    "            df_in.groupby(\"country_code\")[\"ID\"]\n",
    "            .nunique()\n",
    "            .reset_index(name=\"unique_ids\")\n",
    "        )\n",
    "\n",
    "        countries_below = unique_ids_per_country[\n",
    "            unique_ids_per_country[\"unique_ids\"] < MIN_UNIQUE_IDS_PER_COUNTRY\n",
    "        ]\n",
    "\n",
    "        for _, row in countries_below.iterrows():\n",
    "            country_removal_log.append({\n",
    "                \"dataset\": input_path.name,\n",
    "                \"country_code\": row[\"country_code\"],\n",
    "                \"unique_ids\": int(row[\"unique_ids\"]),\n",
    "            })\n",
    "\n",
    "        valid_countries = unique_ids_per_country[\n",
    "            unique_ids_per_country[\"unique_ids\"] >= MIN_UNIQUE_IDS_PER_COUNTRY\n",
    "        ][\"country_code\"]\n",
    "\n",
    "        df_in = df_in[df_in[\"country_code\"].isin(valid_countries)].copy()\n",
    "        \n",
    "        # NOTE: We drop 'country_code' here for cleanliness, \n",
    "        # but we will need to regenerate it later for the trimming step.\n",
    "        df_in.drop(columns=[\"country_code\"], inplace=True)\n",
    "\n",
    "        # LOG: Country Filter\n",
    "        r, fy = get_counts(df_in)\n",
    "        file_tracker.append({\"Step\": \"2. Country Filter\", \"Rows\": r, \"FirmYears\": fy})\n",
    "\n",
    "        if df_in.empty:\n",
    "            print(\"  WARNING: Entire dataset removed by country filter.\")\n",
    "            continue\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 2. TARGET COLUMN DETECTION AND NORMALIZATION\n",
    "        # ---------------------------------------------------------------------\n",
    "        base_target = input_path.stem.lower()\n",
    "        anomaly_target = f\"{base_target}_anomaly\"\n",
    "        normalized_target = base_target\n",
    "\n",
    "        if base_target in df_in.columns:\n",
    "            target_col = base_target\n",
    "            print(f\"  Target column found: '{target_col}'\")\n",
    "        elif anomaly_target in df_in.columns:\n",
    "            target_col = anomaly_target\n",
    "            print(f\"  Base target '{base_target}' not found. Using '{target_col}'.\")\n",
    "        else:\n",
    "            raise KeyError(f\"Neither '{base_target}' nor '{anomaly_target}' found.\")\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 3. APPLY CLEANING (With Internal Tracking)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # We pass file_tracker so the function can append steps directly\n",
    "        result_clean = clean_single_result(df_in, target_col, file_tracker)\n",
    "\n",
    "        if target_col != normalized_target and target_col in result_clean.columns:\n",
    "            result_clean = result_clean.rename(columns={target_col: normalized_target})\n",
    "            print(f\"  Renamed column '{target_col}' to '{normalized_target}'.\")\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 4. ANOMALY-SPECIFIC FILTERS\n",
    "        # ---------------------------------------------------------------------\n",
    "        value_col = normalized_target\n",
    "        filter_name = \"None\"\n",
    "\n",
    "        if value_col.lower() == \"es\":\n",
    "            before = len(result_clean)\n",
    "            result_clean = result_clean[result_clean[value_col] <= 600].reset_index(drop=True)\n",
    "            filter_name = \"ES (<= 600)\"\n",
    "            print(f\"  ES filter applied: {before - len(result_clean)} rows dropped.\")\n",
    "\n",
    "        elif value_col.lower() == \"poa\":\n",
    "            before = len(result_clean)\n",
    "            result_clean = result_clean[\n",
    "                (result_clean[value_col] <= 6000) & (result_clean[value_col] >= -50)\n",
    "            ].reset_index(drop=True)\n",
    "            filter_name = \"POA (-50 to 6000)\"\n",
    "            print(f\"  POA filter applied: {before - len(result_clean)} rows dropped.\")\n",
    "\n",
    "        elif value_col.lower() == \"pta\":\n",
    "            before = len(result_clean)\n",
    "            result_clean = result_clean[\n",
    "                (result_clean[value_col] <= 6000) & (result_clean[value_col] >= -20)\n",
    "            ].reset_index(drop=True)\n",
    "            filter_name = \"PTA (-20 to 6000)\"\n",
    "            print(f\"  PTA filter applied: {before - len(result_clean)} rows dropped.\")\n",
    "\n",
    "        # LOG: Anomaly Filter\n",
    "        r, fy = get_counts(result_clean)\n",
    "        file_tracker.append({\"Step\": f\"3. Anomaly Filter [{filter_name}]\", \"Rows\": r, \"FirmYears\": fy})\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 5. REMOVE DUPLICATES (Earliest PIT Date)\n",
    "        # ---------------------------------------------------------------------\n",
    "        if \"PIT Date\" in result_clean.columns:\n",
    "            result_clean[\"PIT Date\"] = pd.to_datetime(result_clean[\"PIT Date\"], errors=\"coerce\")\n",
    "            \n",
    "            result_clean = result_clean.sort_values(\n",
    "                by=[\"ID\", \"FiscalPeriod\", value_col, \"PIT Date\"]\n",
    "            )\n",
    "\n",
    "            before_dups = len(result_clean)\n",
    "            result_clean = result_clean.drop_duplicates(\n",
    "                subset=[\"ID\", \"FiscalPeriod\", value_col],\n",
    "                keep=\"first\"\n",
    "            ).reset_index(drop=True)\n",
    "            print(f\"  Removed {before_dups - len(result_clean)} duplicates.\")\n",
    "            \n",
    "            # LOG: Dedup\n",
    "            r, fy = get_counts(result_clean)\n",
    "            file_tracker.append({\"Step\": \"4. Dedup (Earliest PIT)\", \"Rows\": r, \"FirmYears\": fy})\n",
    "        else:\n",
    "            r, fy = get_counts(result_clean)\n",
    "            file_tracker.append({\"Step\": \"4. Dedup (Skipped - No PIT)\", \"Rows\": r, \"FirmYears\": fy})\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 6. [NEW] OUTLIER TRIMMING (Top/Bottom 0.5% per Country)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Re-derive country_code because it was dropped earlier\n",
    "        result_clean[\"country_code\"] = result_clean[\"ID\"].astype(str).str[1:4]\n",
    "        \n",
    "        # Calculate quantiles per country\n",
    "        # 0.5% = 0.005, 99.5% = 0.995\n",
    "        lower_q = result_clean.groupby(\"country_code\")[value_col].transform(lambda x: x.quantile(0.005))\n",
    "        upper_q = result_clean.groupby(\"country_code\")[value_col].transform(lambda x: x.quantile(0.995))\n",
    "        \n",
    "        before_trim = len(result_clean)\n",
    "        \n",
    "        # Filter: Keep rows that are between lower and upper bounds (inclusive)\n",
    "        # Note: Using inclusive bounds. Adjust to exclusive if strictly desired.\n",
    "        mask_keep = (result_clean[value_col] >= lower_q) & (result_clean[value_col] <= upper_q)\n",
    "        result_clean = result_clean[mask_keep].reset_index(drop=True)\n",
    "        \n",
    "        # Clean up helper column\n",
    "        result_clean.drop(columns=[\"country_code\"], inplace=True)\n",
    "        \n",
    "        print(f\"  Trimmed {before_trim - len(result_clean)} outlier rows (Top/Bottom 0.5% per country).\")\n",
    "        \n",
    "        # LOG: Trim\n",
    "        r, fy = get_counts(result_clean)\n",
    "        file_tracker.append({\"Step\": \"5. Outlier Trim (0.5%)\", \"Rows\": r, \"FirmYears\": fy})\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # PRINT LOSS REPORT\n",
    "        # ---------------------------------------------------------------------\n",
    "        df_tracker = pd.DataFrame(file_tracker)\n",
    "        \n",
    "        # Calculate drops\n",
    "        df_tracker[\"Rows Dropped\"] = df_tracker[\"Rows\"].diff().fillna(0) * -1\n",
    "        df_tracker[\"FY Dropped\"] = df_tracker[\"FirmYears\"].diff().fillna(0) * -1\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*40)\n",
    "        print(\"DATA LOSS ANALYSIS REPORT\")\n",
    "        print(\"-\" * 40)\n",
    "        print(df_tracker[[\"Step\", \"Rows\", \"Rows Dropped\", \"FirmYears\", \"FY Dropped\"]].to_string(index=False))\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # SAVE CLEANED OUTPUT\n",
    "        # ---------------------------------------------------------------------\n",
    "        output_path = input_path.with_name(f\"{input_path.stem}_clean{input_path.suffix}\")\n",
    "        result_clean.to_csv(output_path, sep=\"|\", index=False)\n",
    "        print(f\"  Saved to: {output_path}\")\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # PREVIEW\n",
    "        # ---------------------------------------------------------------------\n",
    "        preview_cols = [\n",
    "            c for c in [\"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", value_col]\n",
    "            if c in result_clean.columns\n",
    "        ]\n",
    "        print(\"\\nPreview:\")\n",
    "        print(result_clean[preview_cols].head(5))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR processing {input_path.name}: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# COUNTRY REMOVAL SUMMARY (GLOBAL)\n",
    "# =============================================================================\n",
    "if country_removal_log:\n",
    "    country_removal_log_df = (\n",
    "        pd.DataFrame(country_removal_log)\n",
    "        .sort_values(by=[\"dataset\", \"country_code\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(\"\\nSUMMARY OF REMOVED COUNTRIES (INSUFFICIENT UNIQUE FIRMS):\")\n",
    "    print(country_removal_log_df)\n",
    "else:\n",
    "    print(\"\\nNo countries were removed in any dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIzwhFQuvOPc"
   },
   "source": [
    "# 3.0. Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Base Dataset (Fundamentals, PIT Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies\n",
      "\n",
      "--- Loading input files ---\n",
      "Loading and preprocessing finished in 12.1 seconds.\n",
      "\n",
      "--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\n",
      "Total_Assets.txt: 871,391 unique (ID, FiscalPeriod) combinations\n",
      "Net_Sales_or_Revenues.txt: 817,863 unique (ID, FiscalPeriod) combinations\n",
      "Cost_of_Goods_Sold_Excl_Depreciation.txt: 818,491 unique (ID, FiscalPeriod) combinations\n",
      "Selling_General__Administrative_Expenses.txt: 721,610 unique (ID, FiscalPeriod) combinations\n",
      "Interest_Expense___Total.txt: 108,197 unique (ID, FiscalPeriod) combinations\n",
      "Common_Equity.txt: 883,328 unique (ID, FiscalPeriod) combinations\n",
      "Deferred_Taxes.txt: 750,382 unique (ID, FiscalPeriod) combinations\n",
      "\n",
      "--- Building base dataset ---\n",
      "Base dataset has 4,436,437 rows and was built in 10.6 seconds.\n",
      "\n",
      "--- Starting as-of merges ---\n",
      "[1/7] Merging value column 'at' ...\n",
      "    Done in 8.7 seconds. Result currently has 4,436,437 rows.\n",
      "[2/7] Merging value column 'rev' ...\n",
      "    Done in 9.3 seconds. Result currently has 4,436,437 rows.\n",
      "[3/7] Merging value column 'cogs' ...\n",
      "    Done in 9.1 seconds. Result currently has 4,436,437 rows.\n",
      "[4/7] Merging value column 'sga' ...\n",
      "    Done in 8.6 seconds. Result currently has 4,436,437 rows.\n",
      "[5/7] Merging value column 'int' ...\n",
      "    Done in 6.6 seconds. Result currently has 4,436,437 rows.\n",
      "[6/7] Merging value column 'ce' ...\n",
      "    Done in 9.2 seconds. Result currently has 4,436,437 rows.\n",
      "[7/7] Merging value column 'dt' ...\n",
      "    Done in 8.4 seconds. Result currently has 4,436,437 rows.\n",
      "\n",
      "--- Final dataset statistics ---\n",
      "Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: 54,827\n",
      "\n",
      "Empty (NaN) values per value column:\n",
      "  - at: 1,984,505 empty values\n",
      "  - rev: 1,478,880 empty values\n",
      "  - cogs: 1,659,959 empty values\n",
      "  - sga: 2,211,926 empty values\n",
      "  - int: 4,171,861 empty values\n",
      "  - ce: 2,016,294 empty values\n",
      "  - dt: 2,730,551 empty values\n",
      "\n",
      "Final view written to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Factors.txt\n",
      "Total runtime: 117.5 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "This cell\n",
    "1) reads multiple input files from OUTPUT_DIR,\n",
    "2) builds a base table with all unique combinations of\n",
    "   (ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period),\n",
    "3) for each input file, performs an \"as-of\" merge:\n",
    "   for every base row it takes the latest AnnPITValue from that dataset\n",
    "   with the same (ID, HistCurrency, FiscalPeriod) and PIT Date <= base PIT Date.\n",
    "   If the dataset has AnnPITValue_Period, the merge is also grouped by that\n",
    "   period label; if it does NOT have AnnPITValue_Period, it is treated as\n",
    "   period-agnostic (\"can fit all\" periods).\n",
    "4) writes the final combined view to OUTPUT_FILE in OUTPUT_DIR.\n",
    "\n",
    "Additional tracking added:\n",
    "- For each imported dataset, the code prints how many unique\n",
    "  firm-year combinations exist (unique ID × FiscalPeriod).\n",
    "- After all merging is complete, the code counts how many\n",
    "  firm-year combinations have **no missing values** across ANY\n",
    "  of the added value columns.\n",
    "- At the end, the code reports how many missing values exist for each\n",
    "  value column (ca, cce, cl, etc.).\n",
    "\n",
    "The final table has the columns:\n",
    "    ID, PIT Date, HistCurrency, FiscalPeriod, AnnPITValue_Period,\n",
    "    Value1, Value2, ..., ValueN\n",
    "where each ValueX is defined by VALUE_COLUMN_NAMES in the config.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILES = [\n",
    "    \"Total_Assets.txt\",\n",
    "    \"Net_Sales_or_Revenues.txt\",\n",
    "    \"Cost_of_Goods_Sold_Excl_Depreciation.txt\",\n",
    "    \"Selling_General__Administrative_Expenses.txt\",\n",
    "    \"Interest_Expense___Total.txt\",\n",
    "    \"Common_Equity.txt\",\n",
    "    \"Deferred_Taxes.txt\",\n",
    "\n",
    "    \n",
    "    # add more file names here if needed ...\n",
    "]\n",
    "\n",
    "VALUE_COLUMN_NAMES = [\n",
    "    \"at\",\n",
    "    \"rev\",\n",
    "    \"cogs\",\n",
    "    \"sga\",\n",
    "    \"int\",\n",
    "    \"ce\",\n",
    "    \"dt\",\n",
    "    \n",
    "    # add more names here, one for each input file ...\n",
    "]\n",
    "\n",
    "OUTPUT_FILE = \"data_Factors.txt\"\n",
    "\n",
    "ID_COL = \"ID\"\n",
    "PIT_DATE_COL = \"PIT Date\"\n",
    "HIST_CURR_COL = \"HistCurrency\"\n",
    "FISCAL_PER_COL = \"FiscalPeriod\"\n",
    "VALUE_COL = \"AnnPITValue\"\n",
    "PERIOD_COL = \"AnnPITValue_Period\"   # NEW: period label column\n",
    "\n",
    "BASE_COLS = [ID_COL, PIT_DATE_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "SAVE_INTERMEDIATE = False\n",
    "# ==============\n",
    "\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "if not OUTPUT_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"OUTPUT_DIR does not exist:\\n{OUTPUT_DIR}\\n\"\n",
    "        f\"Please make sure Temp_file_path_A is set correctly.\"\n",
    "    )\n",
    "print(\"Using data folder:\", OUTPUT_DIR.resolve())\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single dataset from path and keep only the relevant columns.\n",
    "\n",
    "    The function:\n",
    "    - checks if the file exists,\n",
    "    - reads it using the configured separator,\n",
    "    - checks that all required columns are present,\n",
    "    - keeps AnnPITValue_Period if present (and creates it as NA if not),\n",
    "    - converts PIT Date to datetime,\n",
    "    - casts ID to string,\n",
    "    - converts AnnPITValue to numeric.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "    needed_cols = BASE_COLS + [VALUE_COL]\n",
    "    missing = [c for c in needed_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Required columns {missing} are missing in file: {path}\")\n",
    "\n",
    "    # Keep base + AnnPITValue + (optional) AnnPITValue_Period\n",
    "    extra_cols = [PERIOD_COL] if PERIOD_COL in df.columns else []\n",
    "    df = df[needed_cols + extra_cols].copy()\n",
    "\n",
    "    # Ensure AnnPITValue_Period exists in all datasets\n",
    "    if PERIOD_COL not in df.columns:\n",
    "        df[PERIOD_COL] = pd.NA\n",
    "\n",
    "    # Re-order columns for consistency\n",
    "    df = df[BASE_COLS + [PERIOD_COL, VALUE_COL]]\n",
    "\n",
    "    # Ensure consistent data types\n",
    "    df[PIT_DATE_COL] = pd.to_datetime(df[PIT_DATE_COL])\n",
    "    df[ID_COL] = df[ID_COL].astype(str)\n",
    "    df[VALUE_COL] = pd.to_numeric(df[VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_frame(dfs: list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the base dataset from a list of dataframes.\n",
    "\n",
    "    Steps:\n",
    "    - stack the identifier columns (including AnnPITValue_Period) from all dataframes,\n",
    "    - drop duplicate combinations,\n",
    "    - enforce correct data types,\n",
    "    - sort the base dataset by (ID, HistCurrency, FiscalPeriod, AnnPITValue_Period, PIT Date).\n",
    "\n",
    "    The result is the \"skeleton\" upon which all value columns will be merged.\n",
    "    \"\"\"\n",
    "    # Concatenate identifier columns from all datasets (base cols + period col)\n",
    "    base = pd.concat(\n",
    "        [df[BASE_COLS + [PERIOD_COL]] for df in dfs],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Remove duplicate rows of identifiers\n",
    "    base = base.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Ensure consistent types\n",
    "    base[PIT_DATE_COL] = pd.to_datetime(base[PIT_DATE_COL])\n",
    "    base[ID_COL] = base[ID_COL].astype(str)\n",
    "\n",
    "    # Sort by identifier columns and date\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    base = base.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def asof_merge_one(base: pd.DataFrame, df: pd.DataFrame, new_col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform an \"as-of\" merge of one dataset into the base dataframe.\n",
    "\n",
    "    Concept:\n",
    "    - For each combination of (ID, HistCurrency, FiscalPeriod)\n",
    "      and for each PIT Date in the base table,\n",
    "      we want the latest AnnPITValue from df with\n",
    "          df.PIT Date <= base.PIT Date\n",
    "      for the same (ID, HistCurrency, FiscalPeriod).\n",
    "    - If df has a non-empty AnnPITValue_Period column, the as-of grouping\n",
    "      is also done by AnnPITValue_Period.\n",
    "      If AnnPITValue_Period is all missing (e.g. revenue), df is treated\n",
    "      as period-agnostic and can \"fit all\" periods.\n",
    "\n",
    "    Implementation (vectorized, without merge_asof):\n",
    "      (same as before, but grouping keys dynamically include PERIOD_COL\n",
    "       only for period-aware datasets)\n",
    "    \"\"\"\n",
    "    # Determine if this dataset is period-aware (has any non-NA period labels)\n",
    "    has_period = PERIOD_COL in df.columns and df[PERIOD_COL].notna().any()\n",
    "\n",
    "    # Work on copies to avoid modifying original base/df\n",
    "    base_tmp = base.copy()\n",
    "    base_tmp[new_col_name] = pd.NA\n",
    "    base_tmp[\"__marker\"] = \"base\"\n",
    "\n",
    "    # Keep only identifier columns and the value column from df, then rename\n",
    "    if has_period:\n",
    "        df_tmp = df[BASE_COLS + [PERIOD_COL, VALUE_COL]].copy()\n",
    "    else:\n",
    "        df_tmp = df[BASE_COLS + [VALUE_COL]].copy()\n",
    "        # Ensure PERIOD_COL exists but remains NA (period-agnostic)\n",
    "        df_tmp[PERIOD_COL] = pd.NA\n",
    "\n",
    "    df_tmp = df_tmp.rename(columns={VALUE_COL: new_col_name})\n",
    "    df_tmp[\"__marker\"] = \"df\"\n",
    "\n",
    "    # Ensure consistent types for safety\n",
    "    base_tmp[ID_COL] = base_tmp[ID_COL].astype(str)\n",
    "    df_tmp[ID_COL] = df_tmp[ID_COL].astype(str)\n",
    "    base_tmp[PIT_DATE_COL] = pd.to_datetime(base_tmp[PIT_DATE_COL])\n",
    "    df_tmp[PIT_DATE_COL] = pd.to_datetime(df_tmp[PIT_DATE_COL])\n",
    "\n",
    "    base_tmp[new_col_name] = pd.to_numeric(base_tmp[new_col_name], errors=\"coerce\")\n",
    "    df_tmp[new_col_name] = pd.to_numeric(df_tmp[new_col_name], errors=\"coerce\")\n",
    "\n",
    "    # Concatenate base rows and df rows\n",
    "    combined = pd.concat([base_tmp, df_tmp], ignore_index=True)\n",
    "\n",
    "    # Define an order so that df rows come before base rows on the same PIT Date\n",
    "    marker_order = {\"df\": 0, \"base\": 1}\n",
    "    combined[\"__order\"] = combined[\"__marker\"].map(marker_order).astype(\"int8\")\n",
    "\n",
    "    # Build sort and group keys\n",
    "    group_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "    sort_cols = [ID_COL, HIST_CURR_COL, FISCAL_PER_COL]\n",
    "\n",
    "    if has_period:\n",
    "        # Period-aware dataset: also group by AnnPITValue_Period\n",
    "        group_cols.append(PERIOD_COL)\n",
    "        sort_cols.append(PERIOD_COL)\n",
    "\n",
    "    sort_cols += [PIT_DATE_COL, \"__order\"]\n",
    "\n",
    "    # Sort accordingly\n",
    "    combined = combined.sort_values(sort_cols)\n",
    "\n",
    "    # Forward-fill the value within each group to implement \"as-of\" logic\n",
    "    combined[new_col_name] = combined.groupby(group_cols)[new_col_name].ffill()\n",
    "\n",
    "    # Keep only rows that belong to the base dataset\n",
    "    result = combined[combined[\"__marker\"] == \"base\"].copy()\n",
    "\n",
    "    # Drop helper columns and sort final result\n",
    "    result = result.drop(columns=[\"__marker\", \"__order\"])\n",
    "    result = result.sort_values(\n",
    "        [ID_COL, HIST_CURR_COL, FISCAL_PER_COL, PERIOD_COL, PIT_DATE_COL]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_and_save_variable(\n",
    "    input_files,\n",
    "    value_column_names,\n",
    "    output_file,\n",
    "    save_intermediate: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the final \"view\" for a variable based on multiple input files and save it.\n",
    "\n",
    "    Steps:\n",
    "    1) Validate arguments (non-empty, same length for files and column names).\n",
    "    2) Load and preprocess each input file.\n",
    "    3) Build the base dataset with all unique identifier combinations\n",
    "       (including AnnPITValue_Period).\n",
    "    4) For each input dataframe, perform an as-of merge of its AnnPITValue into the base,\n",
    "       using AnnPITValue_Period in the grouping if present in that dataset.\n",
    "    5) Keep only the base columns and the value columns.\n",
    "    6) Write the final result to output_file in OUTPUT_DIR.\n",
    "    \"\"\"\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(\"No INPUT_FILES were provided.\")\n",
    "    if len(input_files) != len(value_column_names):\n",
    "        raise ValueError(\"INPUT_FILES and VALUE_COLUMN_NAMES must have the same length.\")\n",
    "\n",
    "    start_total = time.time()\n",
    "\n",
    "    # Build full paths\n",
    "    paths = [OUTPUT_DIR / f for f in input_files]\n",
    "\n",
    "    print(\"\\n--- Loading input files ---\")\n",
    "    t0 = time.time()\n",
    "    dfs = [load_dataset(p) for p in paths]\n",
    "    print(f\"Loading and preprocessing finished in {time.time() - t0:.1f} seconds.\")\n",
    "\n",
    "    # Unique firm-year counts per input dataset\n",
    "    print(\"\\n--- Unique firm-year (ID, FiscalPeriod) counts per input file ---\")\n",
    "    for path, df in zip(paths, dfs):\n",
    "        n_firm_years = df[[ID_COL, FISCAL_PER_COL]].drop_duplicates().shape[0]\n",
    "        print(f\"{path.name}: {n_firm_years:,} unique (ID, FiscalPeriod) combinations\")\n",
    "\n",
    "    print(\"\\n--- Building base dataset ---\")\n",
    "    t0 = time.time()\n",
    "    base = build_base_frame(dfs)\n",
    "    print(\n",
    "        f\"Base dataset has {len(base):,} rows and was built in \"\n",
    "        f\"{time.time() - t0:.1f} seconds.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Starting as-of merges ---\")\n",
    "    result = base\n",
    "    for idx, (df, col_name) in enumerate(zip(dfs, value_column_names), start=1):\n",
    "        print(f\"[{idx}/{len(dfs)}] Merging value column '{col_name}' ...\")\n",
    "        t_merge = time.time()\n",
    "        result = asof_merge_one(result, df, col_name)\n",
    "        print(\n",
    "            f\"    Done in {time.time() - t_merge:.1f} seconds. \"\n",
    "            f\"Result currently has {len(result):,} rows.\"\n",
    "        )\n",
    "\n",
    "        if save_intermediate:\n",
    "            stem = output_file.rsplit(\".\", 1)[0]\n",
    "            temp_out = OUTPUT_DIR / f\"{stem}_partial_{idx}.txt\"\n",
    "            result.to_csv(temp_out, sep=SEP, index=False)\n",
    "            print(f\"    Intermediate file written to: {temp_out}\")\n",
    "\n",
    "    # Keep only the base identifier columns (including period) and the value columns\n",
    "    final_cols = BASE_COLS + [PERIOD_COL] + value_column_names\n",
    "    result = result[final_cols]\n",
    "\n",
    "    # Final stats on firm-years and missing values\n",
    "    print(\"\\n--- Final dataset statistics ---\")\n",
    "    mask_complete = result[value_column_names].notna().all(axis=1)\n",
    "    complete_firm_years = (\n",
    "        result.loc[mask_complete, [ID_COL, FISCAL_PER_COL]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_complete_firm_years = complete_firm_years.shape[0]\n",
    "    print(\n",
    "        f\"Unique (ID, FiscalPeriod) combinations with ALL value columns non-empty: \"\n",
    "        f\"{n_complete_firm_years:,}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmpty (NaN) values per value column:\")\n",
    "    for col in value_column_names:\n",
    "        n_missing = result[col].isna().sum()\n",
    "        print(f\"  - {col}: {n_missing:,} empty values\")\n",
    "\n",
    "    # Final output\n",
    "    out_path = OUTPUT_DIR / output_file\n",
    "    result.to_csv(out_path, sep=SEP, index=False)\n",
    "\n",
    "    print(f\"\\nFinal view written to:\\n{out_path.resolve()}\")\n",
    "    print(f\"Total runtime: {time.time() - start_total:.1f} seconds.\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "out_path = build_and_save_variable(\n",
    "    input_files=INPUT_FILES,\n",
    "    value_column_names=VALUE_COLUMN_NAMES,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    save_intermediate=SAVE_INTERMEDIATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate In Between Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column 'be' created.\n",
      "Logic applied: be = ce + dt (where dt is 0 if missing). If ce missing -> NaN.\n",
      "Rows calculated: 2420143\n",
      "Result was saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/data_Factors.txt\n",
      "------------------------------\n",
      "            ce        dt          be\n",
      "0   378.411011       NaN  378.411011\n",
      "1          NaN       NaN         NaN\n",
      "2   434.756779       NaN  434.756779\n",
      "3          NaN       NaN         NaN\n",
      "4   518.204722       NaN  518.204722\n",
      "5          NaN       NaN         NaN\n",
      "6   386.142780       NaN  386.142780\n",
      "7          NaN       NaN         NaN\n",
      "8   387.109014       NaN  387.109014\n",
      "9          NaN       NaN         NaN\n",
      "10  292.572607  0.000000  292.572607\n",
      "11         NaN       NaN         NaN\n",
      "12         NaN       NaN         NaN\n",
      "13   72.674506  1.902010   74.576516\n",
      "14         NaN       NaN         NaN\n",
      "15  112.352977  1.252426  113.605403\n",
      "16         NaN       NaN         NaN\n",
      "17         NaN       NaN         NaN\n",
      "18    0.129111       NaN    0.129111\n",
      "19         NaN       NaN         NaN\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file (INPUT_FILE) from OUTPUT_DIR.\n",
    "# 2) Creates a new column 'be'.\n",
    "# 3) LOGIC:\n",
    "#    - be = ce + dt (if both exist)\n",
    "#    - be = ce      (if ce exists, but dt is missing)\n",
    "#    - be = NaN     (if ce is missing, even if dt exists)\n",
    "# 4) The updated DataFrame is saved to disk.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========================= CONFIG ============================================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)\n",
    "SEP = \"|\"\n",
    "\n",
    "INPUT_FILE = \"data_Factors.txt\"               \n",
    "NEW_COLUMN = \"be\"\n",
    "# ============================================================================\n",
    "\n",
    "# 1) LOAD INPUT FILE\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# 2) CHECK THAT MANDATORY COLUMNS EXIST IN DATAFRAME\n",
    "# 'ce' is strictly required for the calculation logic to valid\n",
    "required_cols = [\"ce\", \"dt\"]\n",
    "missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing columns: {missing_cols}\")\n",
    "\n",
    "# 3) INITIALIZE NEW COLUMN WITH NaN\n",
    "df_out = df.copy()\n",
    "df_out[NEW_COLUMN] = np.nan\n",
    "\n",
    "# 4) DEFINE CALCULATION LOGIC\n",
    "# Rule: Calculation is valid ONLY if 'ce' is not null.\n",
    "mask_valid_rows = df_out[\"ce\"].notna()\n",
    "\n",
    "# Create a temporary 'dt' series where NaNs are treated as 0.\n",
    "# We do not overwrite the original 'dt' in df_out, we just use this for the math.\n",
    "dt_safe_for_math = df_out[\"dt\"].fillna(0)\n",
    "\n",
    "# 5) APPLY CALCULATION\n",
    "# We take 'ce' and add the safe version of 'dt'.\n",
    "# Because we filter with df_out.loc[mask_valid_rows], rows where 'ce' is NaN \n",
    "# remain NaN in the NEW_COLUMN.\n",
    "df_out.loc[mask_valid_rows, NEW_COLUMN] = (\n",
    "    df_out.loc[mask_valid_rows, \"ce\"] + dt_safe_for_math.loc[mask_valid_rows]\n",
    ")\n",
    "\n",
    "# 6) SAVE RESULT\n",
    "formula_output_filename = f\"{INPUT_FILE}\"\n",
    "formula_output_path = OUTPUT_DIR / formula_output_filename\n",
    "df_out.to_csv(formula_output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"New column '{NEW_COLUMN}' created.\")\n",
    "print(\"Logic applied: be = ce + dt (where dt is 0 if missing). If ce missing -> NaN.\")\n",
    "print(\"Rows calculated:\", mask_valid_rows.sum())\n",
    "print(\"Result was saved to:\", formula_output_path)\n",
    "print(\"-\" * 30)\n",
    "print(df_out[[\"ce\", \"dt\", NEW_COLUMN]].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Lagged Columns (PIT Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset loaded into `result` (no rows dropped).\n",
      "Columns in result:\n",
      "['ID', 'PIT Date', 'HistCurrency', 'FiscalPeriod', 'AnnPITValue_Period', 'at', 'rev', 'cogs', 'sga', 'int', 'ce', 'dt', 'be']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/596260779.py:161: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  .apply(compute_lags_for_group)\n",
      "/tmp/ipykernel_847332/596260779.py:161: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_lags_for_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag columns created and added to `result`:\n",
      "['at', 'at_lag1']\n",
      "           ID HistCurrency   PIT Date  FiscalPeriod AnnPITValue_Period  \\\n",
      "0   C02500770          Ars 1995-12-29          1992                  A   \n",
      "1   C02500770          Ars 1995-12-29          1992               <NA>   \n",
      "2   C02500770          Ars 1995-12-29          1993                  A   \n",
      "3   C02500770          Ars 1995-12-29          1993               <NA>   \n",
      "4   C02500770          Ars 1995-12-29          1994                  A   \n",
      "5   C02500770          Ars 1995-12-29          1994               <NA>   \n",
      "6   C02500770          Ars 1996-05-03          1995                  A   \n",
      "7   C02500770          Ars 1996-05-03          1995               <NA>   \n",
      "8   C02500770          Ars 1998-07-03          1996                  A   \n",
      "9   C02500770          Ars 1998-07-03          1996               <NA>   \n",
      "10  C02500770          Ars 1998-07-03          1997                  A   \n",
      "11  C02500770          Ars 1998-07-03          1997               <NA>   \n",
      "13  C02500770          Ars 1999-10-01          1998                  A   \n",
      "14  C02500770          Ars 1999-10-01          1998               <NA>   \n",
      "12  C02500770          Ars 1999-10-08          1997               <NA>   \n",
      "15  C02500770          Ars 2000-05-19          1999                  A   \n",
      "16  C02500770          Ars 2000-05-19          1999               <NA>   \n",
      "17  C02500770          Ars 2000-05-26          1999               <NA>   \n",
      "18  C02520200          Ars 1996-05-03          1987                  A   \n",
      "19  C02520200          Ars 1996-05-03          1987               <NA>   \n",
      "20  C02520200          Ars 1996-05-03          1988                  A   \n",
      "21  C02520200          Ars 1996-05-03          1988               <NA>   \n",
      "22  C02520200          Ars 1996-05-03          1989                  A   \n",
      "23  C02520200          Ars 1996-05-03          1989               <NA>   \n",
      "24  C02520200          Ars 1996-05-03          1990                  A   \n",
      "25  C02520200          Ars 1996-05-03          1990               <NA>   \n",
      "26  C02520200          Ars 1996-05-03          1991                  A   \n",
      "27  C02520200          Ars 1996-05-03          1991               <NA>   \n",
      "28  C02520200          Ars 1996-05-03          1992                  A   \n",
      "29  C02520200          Ars 1996-05-03          1992               <NA>   \n",
      "30  C02520200          Ars 1996-05-03          1993                  A   \n",
      "31  C02520200          Ars 1996-05-03          1993               <NA>   \n",
      "32  C02520200          Ars 1996-05-03          1994                  A   \n",
      "33  C02520200          Ars 1996-05-03          1994               <NA>   \n",
      "34  C02520200          Ars 1996-05-03          1995                  A   \n",
      "35  C02520200          Ars 1996-05-03          1995               <NA>   \n",
      "36  C02520200          Ars 1996-11-01          1996                  A   \n",
      "37  C02520200          Ars 1996-11-01          1996               <NA>   \n",
      "38  C02520200          Ars 1997-02-28          1996               <NA>   \n",
      "39  C02520200          Ars 1997-10-31          1997                  A   \n",
      "\n",
      "             at      at_lag1  \n",
      "0   1084.355949          NaN  \n",
      "1           NaN          NaN  \n",
      "2   1267.313578  1084.355949  \n",
      "3           NaN          NaN  \n",
      "4   1460.696865  1267.313578  \n",
      "5           NaN          NaN  \n",
      "6   1304.537890  1460.696865  \n",
      "7           NaN          NaN  \n",
      "8   1265.820386  1304.537890  \n",
      "9           NaN          NaN  \n",
      "10  1225.163797  1265.820386  \n",
      "11          NaN          NaN  \n",
      "13  1102.461138  1225.163797  \n",
      "14          NaN          NaN  \n",
      "12          NaN          NaN  \n",
      "15   945.993798  1102.461138  \n",
      "16          NaN          NaN  \n",
      "17          NaN          NaN  \n",
      "18          NaN          NaN  \n",
      "19          NaN          NaN  \n",
      "20          NaN          NaN  \n",
      "21          NaN          NaN  \n",
      "22    23.439270          NaN  \n",
      "23          NaN          NaN  \n",
      "24   577.207234    23.439270  \n",
      "25          NaN          NaN  \n",
      "26  1132.139668   577.207234  \n",
      "27          NaN          NaN  \n",
      "28  1242.711417  1132.139668  \n",
      "29          NaN          NaN  \n",
      "30  1373.608080  1242.711417  \n",
      "31          NaN          NaN  \n",
      "32  1263.968574  1373.608080  \n",
      "33          NaN          NaN  \n",
      "34  1301.804038  1263.968574  \n",
      "35          NaN          NaN  \n",
      "36   820.980063  1301.804038  \n",
      "37          NaN          NaN  \n",
      "38          NaN          NaN  \n",
      "39   903.394131   820.980063  \n",
      "\n",
      "Processed file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Factors.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# --------\n",
    "# This cell:\n",
    "# 1) Loads an input file from OUTPUT_DIR.\n",
    "# 2) Computes, for each column in VALUE_COLUMNS, a lagged column \"<col>_lag1\".\n",
    "#    - For each (ID, HistCurrency) group, sorted by PIT Date and FiscalPeriod,\n",
    "#      \"<col>_lag1\" is the most recent known value of <col> for\n",
    "#      FiscalPeriod - 1 *and the same AnnPITValue_Period* based on all PIT\n",
    "#      updates observed up to that row.\n",
    "#      Example: 2020 Q1 will lag to 2019 Q1 (same period label), not to any\n",
    "#      value from 2020.\n",
    "# 3) Keeps all rows (no deletions), only converts types and appends lag columns.\n",
    "# 4) Saves the resulting DataFrame to the same folder as:\n",
    "#       processed_<INPUT_FILE>\n",
    "#    e.g. INPUT_FILE = \"ag.txt\"  ->  \"processed_ag.txt\"\n",
    "#\n",
    "# The final result is stored in the variable `result`.\n",
    "# =============================================================================\n",
    "\n",
    "# ================= CONFIG =================\n",
    "OUTPUT_DIR = Path(Temp_file_path_A)   # Base directory for input/output files\n",
    "SEP = \"|\"                             # Delimiter used in the text files\n",
    "\n",
    "INPUT_FILE = \"data_Factors.txt\"          # Name of the input file to read\n",
    "VALUE_COLUMNS = [\"at\"]         # Columns for which to compute the lag \"<col>_lag1\"\n",
    "\n",
    "PERIOD_COL = \"AnnPITValue_Period\"    # NEW: period label used for lag (Q1, Q2, A, etc.)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) LOAD INPUT FILE (NO ROWS DROPPED)\n",
    "# =============================================================================\n",
    "path = OUTPUT_DIR / INPUT_FILE\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f\"{INPUT_FILE}: file not found in {OUTPUT_DIR}\")\n",
    "\n",
    "# Read raw data\n",
    "df = pd.read_csv(path, sep=SEP)\n",
    "\n",
    "# Required base columns for the PIT logic\n",
    "required_base_cols = [\"ID\", \"PIT Date\", \"HistCurrency\", \"FiscalPeriod\", PERIOD_COL]\n",
    "missing_base = [c for c in required_base_cols if c not in df.columns]\n",
    "if missing_base:\n",
    "    raise ValueError(f\"{INPUT_FILE}: missing required base columns {missing_base}\")\n",
    "\n",
    "# Check that all requested value columns exist\n",
    "missing_value_cols = [c for c in VALUE_COLUMNS if c not in df.columns]\n",
    "if missing_value_cols:\n",
    "    raise ValueError(\n",
    "        f\"{INPUT_FILE}: missing value columns specified in VALUE_COLUMNS: {missing_value_cols}\"\n",
    "    )\n",
    "\n",
    "# Type casting only, no row drops\n",
    "df[\"PIT Date\"] = pd.to_datetime(df[\"PIT Date\"], errors=\"coerce\")\n",
    "df[\"FiscalPeriod\"] = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "# Make sure period labels are strings (or NaN)\n",
    "df[PERIOD_COL] = df[PERIOD_COL].astype(\"string\")\n",
    "\n",
    "# Work on a copy to keep the original df untouched\n",
    "result = df.copy()\n",
    "\n",
    "print(\"Input dataset loaded into `result` (no rows dropped).\")\n",
    "print(\"Columns in result:\")\n",
    "print(list(result.columns))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) COMPUTE LAG COLUMNS \"<col>_lag1\" FOR EACH COLUMN IN VALUE_COLUMNS\n",
    "# =============================================================================\n",
    "# NEW LOGIC:\n",
    "# For each group (ID, HistCurrency) along the PIT timeline:\n",
    "# - We maintain, for each (FiscalPeriod, AnnPITValue_Period) pair, the latest PIT\n",
    "#   and latest known values for VALUE_COLUMNS.\n",
    "# - For a row with (FP = t, period = P), the lag \"<col>_lag1\" is the last known\n",
    "#   value of <col> for (FP = t-1, period = P) based on all PIT updates observed\n",
    "#   up to (and including) the current PIT Date.\n",
    "#\n",
    "# If AnnPITValue_Period is missing, we fall back to using only FiscalPeriod\n",
    "# (i.e., we treat the period label as None).\n",
    "\n",
    "df_calc = result.copy()\n",
    "\n",
    "def compute_lags_for_group(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lagged values \"<col>_lag1\" for all columns in VALUE_COLUMNS\n",
    "    within a single (ID, HistCurrency) group, using:\n",
    "      - FiscalPeriod (year), and\n",
    "      - AnnPITValue_Period (period label) for matching the lag.\n",
    "    \"\"\"\n",
    "    # Sort chronologically so updates are processed in correct temporal order\n",
    "    group = group.sort_values([\"PIT Date\", \"FiscalPeriod\"], ascending=[True, True])\n",
    "\n",
    "    # last_for_fp stores, for each (FiscalPeriod, PeriodLabel), the latest PIT\n",
    "    # and the known values for VALUE_COLUMNS.\n",
    "    # Structure:\n",
    "    #   last_for_fp = {\n",
    "    #       (fp, period_label): (last_pit, {col: last_value_for_col, ...})\n",
    "    #   }\n",
    "    last_for_fp = {}\n",
    "\n",
    "    # For each value column, we collect the lag values row by row\n",
    "    lag_values = {col: [] for col in VALUE_COLUMNS}\n",
    "\n",
    "    # Iterate through rows in PIT and FiscalPeriod order\n",
    "    for _, row in group.iterrows():\n",
    "        pit = row[\"PIT Date\"]\n",
    "        fp = row[\"FiscalPeriod\"]\n",
    "        period_label_raw = row.get(PERIOD_COL, pd.NA)\n",
    "        # Use None as key for missing labels to keep keys hashable and consistent\n",
    "        period_label = None if pd.isna(period_label_raw) else str(period_label_raw)\n",
    "\n",
    "        # 1) Determine lag values for each column in VALUE_COLUMNS\n",
    "        for col in VALUE_COLUMNS:\n",
    "            lag_val = None\n",
    "            if pd.notna(fp):\n",
    "                target_fp = fp - 1\n",
    "                target_key = (target_fp, period_label)\n",
    "                info = last_for_fp.get(target_key)\n",
    "                if info is not None:\n",
    "                    # info[1] is the dict of last known values for that target (FP, period_label)\n",
    "                    lag_val = info[1].get(col)\n",
    "            lag_values[col].append(lag_val)\n",
    "\n",
    "        # 2) Update our knowledge for the current (FiscalPeriod, period_label)\n",
    "        if pd.notna(fp) and pd.notna(pit):\n",
    "            key_curr = (fp, period_label)\n",
    "            prev = last_for_fp.get(key_curr)\n",
    "\n",
    "            # Previously stored values for this key (if any)\n",
    "            prev_values = prev[1] if prev is not None else {}\n",
    "\n",
    "            # Create a copy so we do not mutate the previous dict reference\n",
    "            updated_values = dict(prev_values)\n",
    "\n",
    "            # Update with any non-NaN values from this row\n",
    "            for col in VALUE_COLUMNS:\n",
    "                v = row[col]\n",
    "                if pd.notna(v):\n",
    "                    updated_values[col] = v\n",
    "\n",
    "            # Only overwrite if this PIT is newer or equal to the previous PIT\n",
    "            if prev is None or pit >= prev[0]:\n",
    "                last_for_fp[key_curr] = (pit, updated_values)\n",
    "\n",
    "    # 3) Attach lag columns \"<col>_lag1\" to the group DataFrame\n",
    "    for col in VALUE_COLUMNS:\n",
    "        group[f\"{col}_lag1\"] = lag_values[col]\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the lag computation per (ID, HistCurrency) group\n",
    "df_calc = (\n",
    "    df_calc\n",
    "    .groupby([\"ID\", \"HistCurrency\"], dropna=False, group_keys=False)\n",
    "    .apply(compute_lags_for_group)\n",
    ")\n",
    "\n",
    "# Optionally convert FiscalPeriod back to Int64 (nullable integer)\n",
    "df_calc[\"FiscalPeriod\"] = pd.to_numeric(df_calc[\"FiscalPeriod\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Final result\n",
    "result = df_calc\n",
    "\n",
    "print(\"\\nLag columns created and added to `result`:\")\n",
    "print([c for c in result.columns if any(c == v or c == f\"{v}_lag1\" for v in VALUE_COLUMNS)])\n",
    "\n",
    "# Optional preview\n",
    "cols_to_show = [\n",
    "    \"ID\", \"HistCurrency\", \"PIT Date\", \"FiscalPeriod\", PERIOD_COL\n",
    "] + VALUE_COLUMNS + [f\"{c}_lag1\" for c in VALUE_COLUMNS]\n",
    "print(result[cols_to_show].head(40))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) SAVE RESULTING FILE AS \"processed_<INPUT_FILE>\" IN THE SAME FOLDER\n",
    "# =============================================================================\n",
    "output_filename = f\"processed_{INPUT_FILE}\"\n",
    "output_path = OUTPUT_DIR / output_filename\n",
    "\n",
    "result.to_csv(output_path, sep=SEP, index=False)\n",
    "\n",
    "print(f\"\\nProcessed file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with Market Data (PIT Logic, Forward Filled until Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/FF_Benchmark_data_clean.txt\n",
      "Factors:   /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/processed_data_Factors.txt\n",
      "Output:    /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/FF_Benchmark_Factors_Merged_Clean.txt\n",
      "\n",
      "Loading and Deduplicating Factors Data...\n",
      "Converting Factor dates to datetime...\n",
      "Sorting factors to resolve duplicates...\n",
      "Dropping duplicates (keeping top row per ID x PIT Date)...\n",
      "Unique Factor Dates (ID/Date pairs): 2,679,366\n",
      "\n",
      "Loading Benchmark Data...\n",
      "Benchmark rows: 149,520,735\n",
      "Converting Benchmark dates to datetime...\n",
      "Sorting Benchmark by DayDate (Required for merge_asof)...\n",
      "Merging (As-Of Backward: Writing Forward)...\n",
      "\n",
      "Applying Post-Merge Filters...\n",
      "\n",
      "Splitting Clean vs Dropped data...\n",
      "Saving Clean Data (128,221,039) rows to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/FF_Benchmark_Factors_Merged_Clean.txt\n",
      "Saving Dropped Data (21,299,696) rows to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/DroppedFromFFMerge.txt\n",
      "\n",
      "=== FINAL SUMMARY ===\n",
      "Total Processed: 149,520,735\n",
      "Clean Rows:      128,221,039\n",
      "Dropped Rows:    21,299,696\n",
      "Unique IDs:      51,995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MERGE BENCHMARK WITH FACTORS (Forward Fill / As-Of Logic)\n",
    "#\n",
    "# Inputs:\n",
    "#   - FF_Benchmark_data_clean.txt (Benchmark - Daily)\n",
    "#   - processed_data_Factors.txt (Factors - Sparse)\n",
    "#   - (REMOVED) CurrencyMapping.txt\n",
    "#\n",
    "# Logic:\n",
    "#   1. Factors Deduplication (Simplified priority logic).\n",
    "#   2. MERGE ASOF (Backward): For every Benchmark day, find the most recent \n",
    "#      past Factor row for that ID. This \"writes forward\" the data.\n",
    "#   3. (REMOVED) Post-Merge Filter\n",
    "#   4. Track dropped rows.\n",
    "#\n",
    "# Output:\n",
    "#   - FF_Benchmark_Factors_Merged_Clean.txt\n",
    "#   - DroppedFromFFMerge.txt\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# =====================================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================================\n",
    "benchmark_file = f\"{Temp_file_path_DP}/FF_Benchmark_data_clean.txt\"\n",
    "factors_file   = f\"{Temp_file_path_A}/processed_data_Factors.txt\"\n",
    "# mapping_file   = f\"{Input_file_path}/CurrencyMapping.txt\" # REMOVED\n",
    "\n",
    "merged_clean_file = f\"{Temp_file_path_A}/FF_Benchmark_Factors_Merged_Clean.txt\"\n",
    "dropped_log_file  = f\"{Temp_file_path_A}/DroppedFromFFMerge.txt\"\n",
    "\n",
    "print(f\"Benchmark: {benchmark_file}\")\n",
    "print(f\"Factors:   {factors_file}\")\n",
    "# print(f\"Mapping:   {mapping_file}\")\n",
    "print(f\"Output:    {merged_clean_file}\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 1: Load Currency Mapping (SKIPPED)\n",
    "# =====================================================================\n",
    "# print(\"\\nLoading Currency Mapping...\")\n",
    "# map_df = pd.read_csv(...)\n",
    "# valid_country_hist_set = ...\n",
    "# valid_full_combo_set   = ...\n",
    "# print(f\"Loaded {len(map_df)} mapping rules.\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 2: Load & Deduplicate Factors\n",
    "# =====================================================================\n",
    "print(\"\\nLoading and Deduplicating Factors Data...\")\n",
    "df_fact = pd.read_csv(factors_file, sep=\"|\", dtype=\"string\", engine=\"c\")\n",
    "\n",
    "# 2a. Convert Date (Crucial for merge_asof)\n",
    "print(\"Converting Factor dates to datetime...\")\n",
    "df_fact[\"PIT Date\"] = pd.to_datetime(df_fact[\"PIT Date\"])\n",
    "\n",
    "# 2b. Sorting Helpers\n",
    "df_fact[\"_Country\"] = df_fact[\"ID\"].str[1:4]\n",
    "df_fact[\"_FiscalPeriod_Num\"] = pd.to_numeric(df_fact[\"FiscalPeriod\"], errors='coerce').fillna(-9999)\n",
    "# Fix for boolean ambiguity: fillna(False) before casting\n",
    "is_a_mask = (df_fact[\"AnnPITValue_Period\"] == \"A\").fillna(False)\n",
    "df_fact[\"_IsAnnPIT_A\"] = is_a_mask.astype(int)\n",
    "\n",
    "# ### REMOVED: Check valid history currency (Requires mapping file)\n",
    "# fact_tuples = zip(df_fact[\"_Country\"], df_fact[\"HistCurrency\"])\n",
    "# df_fact[\"_IsValidHistCurr\"] = [1 if t in valid_country_hist_set else 0 for t in fact_tuples]\n",
    "\n",
    "# 2c. Sort and Deduplicate\n",
    "print(\"Sorting factors to resolve duplicates...\")\n",
    "# REMOVED \"_IsValidHistCurr\" from sort keys\n",
    "df_fact.sort_values(\n",
    "    by=[\"ID\", \"PIT Date\", \"_FiscalPeriod_Num\", \"_IsAnnPIT_A\"], # removed _IsValidHistCurr\n",
    "    ascending=[True, True, False, False],                      # removed False\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "print(\"Dropping duplicates (keeping top row per ID x PIT Date)...\")\n",
    "df_fact_dedup = df_fact.drop_duplicates(subset=[\"ID\", \"PIT Date\"], keep=\"first\")\n",
    "print(f\"Unique Factor Dates (ID/Date pairs): {len(df_fact_dedup):,}\")\n",
    "\n",
    "# Clean up helpers\n",
    "# REMOVED \"_IsValidHistCurr\" from drops\n",
    "cols_to_drop = [\"_Country\", \"_FiscalPeriod_Num\", \"_IsAnnPIT_A\"]\n",
    "df_fact_dedup = df_fact_dedup.drop(columns=cols_to_drop)\n",
    "\n",
    "# Sort strictly by Date for merge_asof\n",
    "df_fact_dedup = df_fact_dedup.sort_values(\"PIT Date\")\n",
    "\n",
    "del df_fact, is_a_mask # removed fact_tuples\n",
    "gc.collect()\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 3: Load Benchmark & Merge As-Of (Forward Fill)\n",
    "# =====================================================================\n",
    "print(\"\\nLoading Benchmark Data...\")\n",
    "df_bench = pd.read_csv(benchmark_file, sep=\"|\", dtype=\"string\", engine=\"c\")\n",
    "print(f\"Benchmark rows: {len(df_bench):,}\")\n",
    "\n",
    "print(\"Converting Benchmark dates to datetime...\")\n",
    "df_bench[\"DayDate\"] = pd.to_datetime(df_bench[\"DayDate\"])\n",
    "\n",
    "print(\"Sorting Benchmark by DayDate (Required for merge_asof)...\")\n",
    "df_bench = df_bench.sort_values(\"DayDate\")\n",
    "\n",
    "print(\"Merging (As-Of Backward: Writing Forward)...\")\n",
    "# merge_asof does NOT support 'indicator=True', so we must detect drops manually later.\n",
    "merged = pd.merge_asof(\n",
    "    df_bench,\n",
    "    df_fact_dedup,\n",
    "    left_on=\"DayDate\",\n",
    "    right_on=\"PIT Date\",\n",
    "    by=\"ID\",                # Match strictly on ID\n",
    "    direction=\"backward\"    # Look for last available date <= Benchmark Date\n",
    ")\n",
    "\n",
    "# Free input memory\n",
    "del df_bench, df_fact_dedup\n",
    "gc.collect()\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 4: Post-Merge Filtering\n",
    "# =====================================================================\n",
    "print(\"\\nApplying Post-Merge Filters...\")\n",
    "\n",
    "# Initialize DropReason as OBJECT type to prevent FutureWarning\n",
    "merged[\"DropReason\"] = np.nan\n",
    "merged[\"DropReason\"] = merged[\"DropReason\"].astype(\"object\")\n",
    "\n",
    "# Condition A: No Factor Data Found\n",
    "merged.loc[merged[\"PIT Date\"].isna(), \"DropReason\"] = \"No Factor Data (Prior to first PIT Date)\"\n",
    "\n",
    "# ### REMOVED: Condition B (Invalid Final Currency Combo) entirely\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 5: Split and Save\n",
    "# =====================================================================\n",
    "print(\"\\nSplitting Clean vs Dropped data...\")\n",
    "\n",
    "# Clean rows (DropReason is NaN)\n",
    "df_clean = merged[merged[\"DropReason\"].isna()].copy()\n",
    "df_clean.drop(columns=[\"DropReason\"], inplace=True)\n",
    "\n",
    "# Dropped rows\n",
    "df_dropped = merged[merged[\"DropReason\"].notna()].copy()\n",
    "\n",
    "print(f\"Saving Clean Data ({len(df_clean):,}) rows to: {merged_clean_file}\")\n",
    "df_clean.to_csv(merged_clean_file, sep=\"|\", index=False)\n",
    "\n",
    "print(f\"Saving Dropped Data ({len(df_dropped):,}) rows to: {dropped_log_file}\")\n",
    "if not df_dropped.empty:\n",
    "    df_dropped.to_csv(dropped_log_file, sep=\"|\", index=False)\n",
    "else:\n",
    "    with open(dropped_log_file, \"w\") as f:\n",
    "        f.write(\"|\".join(merged.columns) + \"\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# STEP 6: Final Stats\n",
    "# =====================================================================\n",
    "print(\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"Total Processed: {len(merged):,}\")\n",
    "print(f\"Clean Rows:      {len(df_clean):,}\")\n",
    "print(f\"Dropped Rows:    {len(df_dropped):,}\")\n",
    "print(f\"Unique IDs:      {df_clean['ID'].nunique():,}\")\n",
    "\n",
    "del merged, df_clean, df_dropped # removed map_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Replication Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved filtered data to:\n",
      "/home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/FF_Benchmark_Factors_Merged_Clean_Replication.txt\n",
      "\n",
      "--- Safety Check ---\n",
      "Unique 'Country' values in the output file:\n",
      "[840 124]\n",
      "Check Passed: Only countries 124 and 840 are present.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Setup paths\n",
    "# -----------------------------------------------------------------------------\n",
    "# Make sure Temp_file_path_A is defined in your environment before running this.\n",
    "# Example: Temp_file_path_A = '/content/drive/My Drive/Data' \n",
    "\n",
    "input_filename = \"FF_Benchmark_Factors_Merged_Clean.txt\"\n",
    "input_path = f\"{Temp_file_path_A}/{input_filename}\"\n",
    "\n",
    "# Create the output filename: \"FF_Benchmark_Factors_Merged_Clean_Replication.txt\"\n",
    "# We split the extension to insert the suffix correctly.\n",
    "output_path = f\"{Temp_file_path_A}/FF_Benchmark_Factors_Merged_Clean_Replication.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data Extraction\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    # Read the file using the pipe separator\n",
    "    df = pd.read_csv(input_path, sep='|')\n",
    "    \n",
    "    # Filter for Country = 124 or Country = 840\n",
    "    filtered_df = df[df['Country'].isin([124, 840])].copy()\n",
    "    \n",
    "    # Save the filtered data to the new path\n",
    "    # We use sep='|' here as well to maintain the original file format\n",
    "    filtered_df.to_csv(output_path, sep='|', index=False)\n",
    "    \n",
    "    print(f\"Successfully saved filtered data to:\\n{output_path}\")\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Safety Check\n",
    "    # -----------------------------------------------------------------------------\n",
    "    print(\"\\n--- Safety Check ---\")\n",
    "    print(\"Unique 'Country' values in the output file:\")\n",
    "    unique_countries = filtered_df['Country'].unique()\n",
    "    print(unique_countries)\n",
    "    \n",
    "    # Validation logic\n",
    "    if all(c in [124, 840] for c in unique_countries):\n",
    "        print(\"Check Passed: Only countries 124 and 840 are present.\")\n",
    "    else:\n",
    "        print(\"Check Failed: Unexpected countries found.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {input_path}\")\n",
    "except KeyError:\n",
    "    print(\"Error: The column 'Country' was not found in the input file. Check your column names.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne B/M Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input file: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/FF_Benchmark_Factors_Merged_Clean.txt...\n",
      "Calculating variables...\n",
      "Calculating Momentum (Forced Recalculation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning Portfolios...\n",
      "Total groups to process: 426248\n",
      "Processing Group 426248 / 426248 (100.0%)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/1714758991.py:175: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_tagged = df_clean.groupby(group_keys).apply(assign_daily_portfolios)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Portfolio Assignment Done.\n",
      "Calculating Base Portfolio Returns...\n",
      "   ... Size-OP Sorts\n",
      "   ... Size-Inv Sorts\n",
      "   ... Size-Mom Sorts\n",
      "Pivoting data...\n",
      "Constructing Factors...\n",
      "   ... Calculating Market Return\n",
      "Saving Final Factors to TXT: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/Factors_Daily_Country.txt\n",
      "--------------------------------------------------------------------------------\n",
      "VERIFICATION CHECK\n",
      "--------------------------------------------------------------------------------\n",
      "         DayDate Country       RMW       CMA       Mom       SMB       Mkt      RF    Mkt-RF\n",
      "74356 2000-07-31     280 -0.008872 -0.019262 -0.001383 -0.017977  0.014813  0.0002  0.014613\n",
      "74401 2000-08-01     280 -0.007279 -0.001813  0.001504  0.002332 -0.005353  0.0002 -0.005553\n",
      "74446 2000-08-02     280 -0.011713  0.005775 -0.010608  0.003091 -0.017643  0.0002 -0.017843\n",
      "74491 2000-08-03     280  0.001477  0.018914 -0.025176  0.004143 -0.024798  0.0002 -0.024998\n",
      "74536 2000-08-04     280  0.005786  0.000474  0.012774 -0.007289  0.009633  0.0002  0.009433\n",
      "74582 2000-08-07     280  0.014376  0.001430  0.014901 -0.010972  0.013812  0.0002  0.013612\n",
      "74628 2000-08-08     280 -0.016265 -0.000165 -0.008643  0.011588 -0.015020  0.0002 -0.015220\n",
      "74674 2000-08-09     280  0.002575 -0.021545  0.007081 -0.001360  0.013443  0.0002  0.013243\n",
      "\n",
      "Process Complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress \"All-NaN slice encountered\" warnings\n",
    "warnings.filterwarnings('ignore', r'All-NaN slice encountered')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION & SETUP\n",
    "# ==============================================================================\n",
    "INPUT_FILENAME = \"FF_Benchmark_Factors_Merged_Clean.txt\"  \n",
    "\n",
    "# ### CHANGE: Updated output filename\n",
    "OUTPUT_FILENAME = \"Factors_Daily_Country.txt\"\n",
    "\n",
    "try:\n",
    "    path_base = f\"{Temp_file_path_A}\"\n",
    "except NameError:\n",
    "    path_base = \".\" \n",
    "\n",
    "INPUT_FILE = os.path.join(path_base, INPUT_FILENAME)\n",
    "OUTPUT_FILE = os.path.join(path_base, OUTPUT_FILENAME)\n",
    "\n",
    "# Column Mapping\n",
    "COLS = {\n",
    "    'date': 'DayDate', 'id': 'ID', 'country': 'Country',\n",
    "    'pcur': 'PCUR', 'hist_curr': 'HistCurrency',\n",
    "    'ret': 'ret_bps', \n",
    "    'mv': 'MV_USD', # ### CHANGE: Switched from MV_LC to MV_USD\n",
    "    'rf': 'rf',\n",
    "    'be': 'be', 'op': 'op', 'inv': 'inv', 'mom': 'mom_signal',\n",
    "    # Ingredients\n",
    "    'rev': 'rev', 'cogs': 'cogs', 'sga': 'sga', 'int': 'int',\n",
    "    'at': 'at', 'at_lag1': 'at_lag1'\n",
    "}\n",
    "\n",
    "# Global counter for progress tracking\n",
    "group_counter = 0\n",
    "total_groups = 0\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "def assign_daily_portfolios(slice_df):\n",
    "    global group_counter\n",
    "    group_counter += 1\n",
    "    if group_counter % 100 == 0 or group_counter == total_groups:\n",
    "        sys.stdout.write(f\"\\rProcessing Group {group_counter} / {total_groups} ({(group_counter/total_groups)*100:.1f}%)\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    if len(slice_df) < 2:\n",
    "        return slice_df \n",
    "\n",
    "    # 1. SIZE ASSIGNMENT\n",
    "    slice_df = slice_df.sort_values(COLS['mv'], ascending=False)\n",
    "    cum_mv_pct = slice_df[COLS['mv']].cumsum() / slice_df[COLS['mv']].sum()\n",
    "    \n",
    "    is_big = cum_mv_pct <= 0.90\n",
    "    slice_df['Size_Port'] = np.where(is_big, 'B', 'S')\n",
    "    mask_b = is_big\n",
    "\n",
    "    # 2. STYLE ASSIGNMENTS\n",
    "    def assign_style(col_name, out_col_name, labels=['L', 'N', 'H']):\n",
    "        vals = slice_df[col_name].values\n",
    "        big_vals = vals[mask_b]\n",
    "        \n",
    "        if len(big_vals[~np.isnan(big_vals)]) == 0:\n",
    "            p30, p70 = np.nanpercentile(vals, 30), np.nanpercentile(vals, 70)\n",
    "        else:\n",
    "            p30, p70 = np.nanpercentile(big_vals, 30), np.nanpercentile(big_vals, 70)\n",
    "        \n",
    "        conditions = [vals <= p30, (vals > p30) & (vals < p70), vals >= p70]\n",
    "        slice_df[out_col_name] = np.select(conditions, labels, default=None)\n",
    "\n",
    "    # ### CHANGE: Removed B/M sorting block\n",
    "    # if 'bm' in slice_df.columns: \n",
    "    #     assign_style('bm', 'BM_Port', ['L', 'N', 'H'])\n",
    "\n",
    "    if 'op' in slice_df.columns: \n",
    "        assign_style('op', 'OP_Port', ['W', 'N', 'R'])\n",
    "    if 'inv' in slice_df.columns: \n",
    "        assign_style('inv', 'Inv_Port', ['C', 'N', 'A'])\n",
    "    if 'mom_signal' in slice_df.columns: \n",
    "        assign_style('mom_signal', 'Mom_Port', ['L', 'N', 'H'])\n",
    "        \n",
    "    return slice_df\n",
    "\n",
    "def calc_vw_ret(df_sub):\n",
    "    if len(df_sub) == 0: return np.nan\n",
    "    val = np.average(df_sub['ret_decimal'], weights=df_sub[COLS['mv']])\n",
    "    return val\n",
    "\n",
    "def pivot_portfolios(port_series, port_col_name, base_keys):\n",
    "    df_p = port_series.reset_index()\n",
    "    df_p['Port_Label'] = df_p['Size_Port'] + '_' + df_p[port_col_name]\n",
    "    return df_p.pivot_table(index=base_keys, columns='Port_Label', values=port_series.name)\n",
    "\n",
    "def calc_mkt_rf(df_sub):\n",
    "    if len(df_sub) == 0 or df_sub[COLS['mv']].sum() == 0:\n",
    "        return pd.Series({'Mkt': np.nan, 'RF': np.nan})\n",
    "    mkt = np.average(df_sub['ret_decimal'], weights=df_sub[COLS['mv']])\n",
    "    rf_col = COLS['rf']\n",
    "    if rf_col in df_sub.columns:\n",
    "        rf = df_sub[rf_col].iloc[0]\n",
    "    else:\n",
    "        rf = 0.0\n",
    "    return pd.Series({'Mkt': mkt, 'RF': rf})\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. EXECUTION: LOAD & PREP\n",
    "# ==============================================================================\n",
    "print(f\"Loading input file: {INPUT_FILE}...\")\n",
    "df = pd.read_csv(INPUT_FILE, sep='|', dtype={'ID': str, 'Country': str, 'PCUR': str, 'HistCurrency': str})\n",
    "df[COLS['date']] = pd.to_datetime(df[COLS['date']])\n",
    "\n",
    "if COLS['ret'] in df.columns:\n",
    "    df['ret_decimal'] = df[COLS['ret']] / 10000.0\n",
    "else:\n",
    "    raise ValueError(f\"Could not find Return column '{COLS['ret']}'\")\n",
    "\n",
    "for c in [COLS['mv'], COLS['rf'], COLS['be']]:\n",
    "    if c in df.columns: df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "print(\"Calculating variables...\")\n",
    "\n",
    "# A) Operating Profitability (OP)\n",
    "if 'op' not in df.columns:\n",
    "    mandatory_op_cols = [COLS['rev'], COLS['cogs'], COLS['sga'], COLS['be']]\n",
    "    if all(c in df.columns for c in mandatory_op_cols):\n",
    "        rev = df[COLS['rev']]\n",
    "        cogs = df[COLS['cogs']]\n",
    "        sga = df[COLS['sga']]\n",
    "        be = df[COLS['be']]\n",
    "        if COLS['int'] in df.columns:\n",
    "            interest = df[COLS['int']].fillna(0)\n",
    "        else:\n",
    "            interest = 0\n",
    "        df['op'] = (rev - cogs - sga - interest) / be\n",
    "\n",
    "# B) Investment (Inv)\n",
    "if 'inv' not in df.columns:\n",
    "    if all(c in df.columns for c in [COLS['at'], COLS['at_lag1']]):\n",
    "        df['inv'] = (df[COLS['at']] - df[COLS['at_lag1']]) / df[COLS['at_lag1']]\n",
    "\n",
    "# ### CHANGE: Removed B/M Calculation block\n",
    "# if 'bm' not in df.columns: ...\n",
    "\n",
    "# D) Momentum \n",
    "print(\"Calculating Momentum (Forced Recalculation)...\")\n",
    "df = df.sort_values([COLS['id'], COLS['pcur'], COLS['date']])\n",
    "df['log_ret'] = np.log1p(df['ret_decimal'].fillna(0))\n",
    "df['mom_signal'] = df.groupby([COLS['id'], COLS['pcur']])['log_ret'].transform(\n",
    "    lambda x: x.rolling(window=250, min_periods=100).sum().shift(21)\n",
    ")\n",
    "df['mom_signal'] = np.exp(df['mom_signal']) - 1\n",
    "\n",
    "# Prep for Assignment\n",
    "subset_cols = [c for c in [COLS['mv'], 'ret_decimal'] if c in df.columns]\n",
    "df_clean = df.dropna(subset=subset_cols).copy()\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. EXECUTION: ASSIGN PORTFOLIOS\n",
    "# ==============================================================================\n",
    "print(\"Assigning Portfolios...\")\n",
    "\n",
    "# ### CHANGE: Removed PCUR and HistCurrency from grouping keys\n",
    "group_keys = [COLS['date'], COLS['country']]\n",
    "\n",
    "unique_groups = df_clean[group_keys].drop_duplicates()\n",
    "total_groups = len(unique_groups)\n",
    "print(f\"Total groups to process: {total_groups}\")\n",
    "\n",
    "df_tagged = df_clean.groupby(group_keys).apply(assign_daily_portfolios)\n",
    "print(\"\\nPortfolio Assignment Done.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. EXECUTION: AGGREGATE RETURNS\n",
    "# ==============================================================================\n",
    "print(\"Calculating Base Portfolio Returns...\")\n",
    "\n",
    "df_tagged = df_tagged.reset_index(drop=True)\n",
    "# ### CHANGE: Updated base_keys to match group_keys\n",
    "base_keys = [COLS['date'], COLS['country']]\n",
    "\n",
    "# ### CHANGE: Removed Size-BM Portfolios calculation\n",
    "# print(\"   ... Size-BM Sorts\")\n",
    "# port_bm = ...\n",
    "\n",
    "# 2. Size-OP Portfolios\n",
    "print(\"   ... Size-OP Sorts\")\n",
    "port_op = df_tagged.dropna(subset=['Size_Port', 'OP_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'OP_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_op.name = 'Ret_OP'\n",
    "\n",
    "# 3. Size-Inv Portfolios\n",
    "print(\"   ... Size-Inv Sorts\")\n",
    "port_inv = df_tagged.dropna(subset=['Size_Port', 'Inv_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'Inv_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_inv.name = 'Ret_Inv'\n",
    "\n",
    "# 4. Size-Mom Portfolios\n",
    "print(\"   ... Size-Mom Sorts\")\n",
    "port_mom = df_tagged.dropna(subset=['Size_Port', 'Mom_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'Mom_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_mom.name = 'Ret_Mom'\n",
    "\n",
    "print(\"Pivoting data...\")\n",
    "# ### CHANGE: Removed BM pivot\n",
    "# df_p_bm = pivot_portfolios(port_bm, 'BM_Port', base_keys)\n",
    "df_p_op = pivot_portfolios(port_op, 'OP_Port', base_keys)\n",
    "df_p_inv = pivot_portfolios(port_inv, 'Inv_Port', base_keys)\n",
    "df_p_mom = pivot_portfolios(port_mom, 'Mom_Port', base_keys)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. EXECUTION: FACTOR CONSTRUCTION\n",
    "# ==============================================================================\n",
    "print(\"Constructing Factors...\")\n",
    "\n",
    "# ### CHANGE: Use OP index instead of BM index\n",
    "factors = pd.DataFrame(index=df_p_op.index)\n",
    "smb_components = pd.DataFrame(index=df_p_op.index)\n",
    "\n",
    "# ### CHANGE: Removed HML & SMB (BM) Construction\n",
    "# factors['HML'] = ...\n",
    "\n",
    "# 2. RMW & SMB (OP)\n",
    "if all(c in df_p_op.columns for c in ['S_R', 'B_R', 'S_W', 'B_W']):\n",
    "    factors['RMW'] = 0.5 * (df_p_op['S_R'] + df_p_op['B_R']) - \\\n",
    "                     0.5 * (df_p_op['S_W'] + df_p_op['B_W'])\n",
    "                     \n",
    "    smb_components['SMB_OP'] = (df_p_op['S_R'] + df_p_op['S_N'] + df_p_op['S_W']) / 3 - \\\n",
    "                               (df_p_op['B_R'] + df_p_op['B_N'] + df_p_op['B_W']) / 3\n",
    "else:\n",
    "    factors['RMW'] = np.nan\n",
    "    smb_components['SMB_OP'] = np.nan\n",
    "\n",
    "# 3. CMA & SMB (Inv)\n",
    "if all(c in df_p_inv.columns for c in ['S_C', 'B_C', 'S_A', 'B_A']):\n",
    "    factors['CMA'] = 0.5 * (df_p_inv['S_C'] + df_p_inv['B_C']) - \\\n",
    "                     0.5 * (df_p_inv['S_A'] + df_p_inv['B_A'])\n",
    "                     \n",
    "    smb_components['SMB_INV'] = (df_p_inv['S_C'] + df_p_inv['S_N'] + df_p_inv['S_A']) / 3 - \\\n",
    "                                (df_p_inv['B_C'] + df_p_inv['B_N'] + df_p_inv['B_A']) / 3\n",
    "else:\n",
    "    factors['CMA'] = np.nan\n",
    "    smb_components['SMB_INV'] = np.nan\n",
    "\n",
    "# 4. MOM\n",
    "if all(c in df_p_mom.columns for c in ['S_H', 'B_H', 'S_L', 'B_L']):\n",
    "    factors['Mom'] = 0.5 * (df_p_mom['S_H'] + df_p_mom['B_H']) - \\\n",
    "                     0.5 * (df_p_mom['S_L'] + df_p_mom['B_L'])\n",
    "else:\n",
    "    factors['Mom'] = np.nan\n",
    "\n",
    "# 5. CONSOLIDATE SMB\n",
    "# ### CHANGE: Updated to only average OP and INV (excluded BM)\n",
    "factors['SMB'] = smb_components[['SMB_OP', 'SMB_INV']].mean(axis=1)\n",
    "\n",
    "# 6. MARKET RETURN\n",
    "print(\"   ... Calculating Market Return\")\n",
    "mkt_info = df_tagged.groupby(base_keys).apply(calc_mkt_rf, include_groups=False)\n",
    "\n",
    "factors = factors.join(mkt_info)\n",
    "factors['Mkt-RF'] = factors['Mkt'] - factors['RF']\n",
    "\n",
    "# Finalize structure\n",
    "factors_final = factors.reset_index()\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. OUTPUT & VERIFICATION\n",
    "# ==============================================================================\n",
    "print(f\"Saving Final Factors to TXT: {OUTPUT_FILE}\")\n",
    "factors_final.to_csv(OUTPUT_FILE, sep='|', index=False)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"VERIFICATION CHECK\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Filter for the specific check (Reduced filters since PCUR/HistCurrency are gone)\n",
    "mask_ver = (\n",
    "    (factors_final[COLS['date']] >= '2000-07-31') &\n",
    "    (factors_final[COLS['date']] <= '2000-08-09') &\n",
    "    (factors_final['Country'] == '280') \n",
    ")\n",
    "\n",
    "verification_subset = factors_final[mask_ver]\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(verification_subset)\n",
    "\n",
    "print(\"\\nProcess Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne B/M Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input file: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/FF_Benchmark_Factors_Merged_Clean.txt...\n",
      "Calculating variables...\n",
      "Calculating Momentum (Forced Recalculation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning Portfolios...\n",
      "Total groups to process: 8465\n",
      "Processing Group 8465 / 8465 (100.0%)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/1467111800.py:163: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_tagged = df_clean.groupby(group_keys).apply(assign_daily_portfolios)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Portfolio Assignment Done.\n",
      "Calculating Base Portfolio Returns...\n",
      "   ... Size-OP Sorts\n",
      "   ... Size-Inv Sorts\n",
      "   ... Size-Mom Sorts\n",
      "Pivoting data...\n",
      "Constructing Factors...\n",
      "   ... Calculating Global Market Return\n",
      "Saving Final Factors to TXT: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/Factors_Daily_Global.txt\n",
      "--------------------------------------------------------------------------------\n",
      "VERIFICATION CHECK (Global)\n",
      "--------------------------------------------------------------------------------\n",
      "        DayDate       RMW       CMA       Mom       SMB       Mkt      RF    Mkt-RF\n",
      "2061 2000-07-31 -0.007841 -0.006880  0.011728 -0.011425  0.007171  0.0002  0.006971\n",
      "2062 2000-08-01  0.000808  0.012878 -0.009002  0.004667  0.003369  0.0002  0.003169\n",
      "2063 2000-08-02  0.001155  0.006207 -0.008191  0.001787 -0.000906  0.0002 -0.001106\n",
      "2064 2000-08-03  0.002361 -0.001242 -0.002767  0.000777 -0.001763  0.0002 -0.001963\n",
      "2065 2000-08-04 -0.008170 -0.010783  0.007716 -0.005725  0.005312  0.0002  0.005112\n",
      "2066 2000-08-07  0.010247  0.000308  0.008218 -0.007901  0.010071  0.0002  0.009871\n",
      "2067 2000-08-08 -0.010594 -0.003557  0.002541  0.001581  0.001015  0.0002  0.000815\n",
      "2068 2000-08-09 -0.005549 -0.000421  0.009039  0.001396  0.002649  0.0002  0.002449\n",
      "\n",
      "Process Complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress \"All-NaN slice encountered\" warnings\n",
    "warnings.filterwarnings('ignore', r'All-NaN slice encountered')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION & SETUP\n",
    "# ==============================================================================\n",
    "INPUT_FILENAME = \"FF_Benchmark_Factors_Merged_Clean.txt\"  \n",
    "\n",
    "# ### CHANGE: Filename for Global version\n",
    "OUTPUT_FILENAME = \"Factors_Daily_Global.txt\"\n",
    "\n",
    "try:\n",
    "    path_base = f\"{Temp_file_path_A}\"\n",
    "except NameError:\n",
    "    path_base = \".\" \n",
    "\n",
    "INPUT_FILE = os.path.join(path_base, INPUT_FILENAME)\n",
    "OUTPUT_FILE = os.path.join(path_base, OUTPUT_FILENAME)\n",
    "\n",
    "# Column Mapping\n",
    "COLS = {\n",
    "    'date': 'DayDate', 'id': 'ID', 'country': 'Country',\n",
    "    'pcur': 'PCUR', 'hist_curr': 'HistCurrency',\n",
    "    'ret': 'ret_bps', \n",
    "    'mv': 'MV_USD', # Keeping MV_USD as requested previously\n",
    "    'rf': 'rf',\n",
    "    'be': 'be', 'op': 'op', 'inv': 'inv', 'mom': 'mom_signal',\n",
    "    'rev': 'rev', 'cogs': 'cogs', 'sga': 'sga', 'int': 'int',\n",
    "    'at': 'at', 'at_lag1': 'at_lag1'\n",
    "}\n",
    "\n",
    "group_counter = 0\n",
    "total_groups = 0\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "def assign_daily_portfolios(slice_df):\n",
    "    global group_counter\n",
    "    group_counter += 1\n",
    "    if group_counter % 100 == 0 or group_counter == total_groups:\n",
    "        sys.stdout.write(f\"\\rProcessing Group {group_counter} / {total_groups} ({(group_counter/total_groups)*100:.1f}%)\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    if len(slice_df) < 2:\n",
    "        return slice_df \n",
    "\n",
    "    # 1. SIZE ASSIGNMENT (Global Sort)\n",
    "    slice_df = slice_df.sort_values(COLS['mv'], ascending=False)\n",
    "    cum_mv_pct = slice_df[COLS['mv']].cumsum() / slice_df[COLS['mv']].sum()\n",
    "    \n",
    "    is_big = cum_mv_pct <= 0.90\n",
    "    slice_df['Size_Port'] = np.where(is_big, 'B', 'S')\n",
    "    mask_b = is_big\n",
    "\n",
    "    # 2. STYLE ASSIGNMENTS\n",
    "    def assign_style(col_name, out_col_name, labels=['L', 'N', 'H']):\n",
    "        vals = slice_df[col_name].values\n",
    "        big_vals = vals[mask_b]\n",
    "        \n",
    "        if len(big_vals[~np.isnan(big_vals)]) == 0:\n",
    "            p30, p70 = np.nanpercentile(vals, 30), np.nanpercentile(vals, 70)\n",
    "        else:\n",
    "            p30, p70 = np.nanpercentile(big_vals, 30), np.nanpercentile(big_vals, 70)\n",
    "        \n",
    "        conditions = [vals <= p30, (vals > p30) & (vals < p70), vals >= p70]\n",
    "        slice_df[out_col_name] = np.select(conditions, labels, default=None)\n",
    "\n",
    "    # (B/M Removed)\n",
    "    if 'op' in slice_df.columns: \n",
    "        assign_style('op', 'OP_Port', ['W', 'N', 'R'])\n",
    "    if 'inv' in slice_df.columns: \n",
    "        assign_style('inv', 'Inv_Port', ['C', 'N', 'A'])\n",
    "    if 'mom_signal' in slice_df.columns: \n",
    "        assign_style('mom_signal', 'Mom_Port', ['L', 'N', 'H'])\n",
    "        \n",
    "    return slice_df\n",
    "\n",
    "def calc_vw_ret(df_sub):\n",
    "    if len(df_sub) == 0: return np.nan\n",
    "    val = np.average(df_sub['ret_decimal'], weights=df_sub[COLS['mv']])\n",
    "    return val\n",
    "\n",
    "def pivot_portfolios(port_series, port_col_name, base_keys):\n",
    "    df_p = port_series.reset_index()\n",
    "    df_p['Port_Label'] = df_p['Size_Port'] + '_' + df_p[port_col_name]\n",
    "    return df_p.pivot_table(index=base_keys, columns='Port_Label', values=port_series.name)\n",
    "\n",
    "def calc_mkt_rf(df_sub):\n",
    "    if len(df_sub) == 0 or df_sub[COLS['mv']].sum() == 0:\n",
    "        return pd.Series({'Mkt': np.nan, 'RF': np.nan})\n",
    "    mkt = np.average(df_sub['ret_decimal'], weights=df_sub[COLS['mv']])\n",
    "    rf_col = COLS['rf']\n",
    "    if rf_col in df_sub.columns:\n",
    "        rf = df_sub[rf_col].iloc[0]\n",
    "    else:\n",
    "        rf = 0.0\n",
    "    return pd.Series({'Mkt': mkt, 'RF': rf})\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. EXECUTION: LOAD & PREP\n",
    "# ==============================================================================\n",
    "print(f\"Loading input file: {INPUT_FILE}...\")\n",
    "df = pd.read_csv(INPUT_FILE, sep='|', dtype={'ID': str, 'Country': str, 'PCUR': str, 'HistCurrency': str})\n",
    "df[COLS['date']] = pd.to_datetime(df[COLS['date']])\n",
    "\n",
    "if COLS['ret'] in df.columns:\n",
    "    df['ret_decimal'] = df[COLS['ret']] / 10000.0\n",
    "else:\n",
    "    raise ValueError(f\"Could not find Return column '{COLS['ret']}'\")\n",
    "\n",
    "for c in [COLS['mv'], COLS['rf'], COLS['be']]:\n",
    "    if c in df.columns: df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "print(\"Calculating variables...\")\n",
    "# A) OP\n",
    "if 'op' not in df.columns:\n",
    "    mandatory_op_cols = [COLS['rev'], COLS['cogs'], COLS['sga'], COLS['be']]\n",
    "    if all(c in df.columns for c in mandatory_op_cols):\n",
    "        rev = df[COLS['rev']]\n",
    "        cogs = df[COLS['cogs']]\n",
    "        sga = df[COLS['sga']]\n",
    "        be = df[COLS['be']]\n",
    "        interest = df[COLS['int']].fillna(0) if COLS['int'] in df.columns else 0\n",
    "        df['op'] = (rev - cogs - sga - interest) / be\n",
    "\n",
    "# B) Inv\n",
    "if 'inv' not in df.columns:\n",
    "    if all(c in df.columns for c in [COLS['at'], COLS['at_lag1']]):\n",
    "        df['inv'] = (df[COLS['at']] - df[COLS['at_lag1']]) / df[COLS['at_lag1']]\n",
    "\n",
    "# D) Momentum \n",
    "print(\"Calculating Momentum (Forced Recalculation)...\")\n",
    "df = df.sort_values([COLS['id'], COLS['pcur'], COLS['date']])\n",
    "df['log_ret'] = np.log1p(df['ret_decimal'].fillna(0))\n",
    "df['mom_signal'] = df.groupby([COLS['id'], COLS['pcur']])['log_ret'].transform(\n",
    "    lambda x: x.rolling(window=250, min_periods=100).sum().shift(21)\n",
    ")\n",
    "df['mom_signal'] = np.exp(df['mom_signal']) - 1\n",
    "\n",
    "# Prep\n",
    "subset_cols = [c for c in [COLS['mv'], 'ret_decimal'] if c in df.columns]\n",
    "df_clean = df.dropna(subset=subset_cols).copy()\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. EXECUTION: ASSIGN PORTFOLIOS (GLOBAL)\n",
    "# ==============================================================================\n",
    "print(\"Assigning Portfolios...\")\n",
    "\n",
    "# ### CHANGE: Group ONLY by date (Global sort)\n",
    "group_keys = [COLS['date']]\n",
    "\n",
    "unique_groups = df_clean[group_keys].drop_duplicates()\n",
    "total_groups = len(unique_groups)\n",
    "print(f\"Total groups to process: {total_groups}\")\n",
    "\n",
    "df_tagged = df_clean.groupby(group_keys).apply(assign_daily_portfolios)\n",
    "print(\"\\nPortfolio Assignment Done.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. EXECUTION: AGGREGATE RETURNS (GLOBAL)\n",
    "# ==============================================================================\n",
    "print(\"Calculating Base Portfolio Returns...\")\n",
    "\n",
    "df_tagged = df_tagged.reset_index(drop=True)\n",
    "# ### CHANGE: Base keys are ONLY date\n",
    "base_keys = [COLS['date']]\n",
    "\n",
    "# 2. Size-OP\n",
    "print(\"   ... Size-OP Sorts\")\n",
    "port_op = df_tagged.dropna(subset=['Size_Port', 'OP_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'OP_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_op.name = 'Ret_OP'\n",
    "\n",
    "# 3. Size-Inv\n",
    "print(\"   ... Size-Inv Sorts\")\n",
    "port_inv = df_tagged.dropna(subset=['Size_Port', 'Inv_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'Inv_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_inv.name = 'Ret_Inv'\n",
    "\n",
    "# 4. Size-Mom\n",
    "print(\"   ... Size-Mom Sorts\")\n",
    "port_mom = df_tagged.dropna(subset=['Size_Port', 'Mom_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'Mom_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_mom.name = 'Ret_Mom'\n",
    "\n",
    "print(\"Pivoting data...\")\n",
    "df_p_op = pivot_portfolios(port_op, 'OP_Port', base_keys)\n",
    "df_p_inv = pivot_portfolios(port_inv, 'Inv_Port', base_keys)\n",
    "df_p_mom = pivot_portfolios(port_mom, 'Mom_Port', base_keys)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. EXECUTION: FACTOR CONSTRUCTION\n",
    "# ==============================================================================\n",
    "print(\"Constructing Factors...\")\n",
    "\n",
    "factors = pd.DataFrame(index=df_p_op.index)\n",
    "smb_components = pd.DataFrame(index=df_p_op.index)\n",
    "\n",
    "# 2. RMW & SMB (OP)\n",
    "if all(c in df_p_op.columns for c in ['S_R', 'B_R', 'S_W', 'B_W']):\n",
    "    factors['RMW'] = 0.5 * (df_p_op['S_R'] + df_p_op['B_R']) - \\\n",
    "                     0.5 * (df_p_op['S_W'] + df_p_op['B_W'])\n",
    "    smb_components['SMB_OP'] = (df_p_op['S_R'] + df_p_op['S_N'] + df_p_op['S_W']) / 3 - \\\n",
    "                               (df_p_op['B_R'] + df_p_op['B_N'] + df_p_op['B_W']) / 3\n",
    "else:\n",
    "    factors['RMW'] = np.nan\n",
    "    smb_components['SMB_OP'] = np.nan\n",
    "\n",
    "# 3. CMA & SMB (Inv)\n",
    "if all(c in df_p_inv.columns for c in ['S_C', 'B_C', 'S_A', 'B_A']):\n",
    "    factors['CMA'] = 0.5 * (df_p_inv['S_C'] + df_p_inv['B_C']) - \\\n",
    "                     0.5 * (df_p_inv['S_A'] + df_p_inv['B_A'])\n",
    "    smb_components['SMB_INV'] = (df_p_inv['S_C'] + df_p_inv['S_N'] + df_p_inv['S_A']) / 3 - \\\n",
    "                                (df_p_inv['B_C'] + df_p_inv['B_N'] + df_p_inv['B_A']) / 3\n",
    "else:\n",
    "    factors['CMA'] = np.nan\n",
    "    smb_components['SMB_INV'] = np.nan\n",
    "\n",
    "# 4. MOM\n",
    "if all(c in df_p_mom.columns for c in ['S_H', 'B_H', 'S_L', 'B_L']):\n",
    "    factors['Mom'] = 0.5 * (df_p_mom['S_H'] + df_p_mom['B_H']) - \\\n",
    "                     0.5 * (df_p_mom['S_L'] + df_p_mom['B_L'])\n",
    "else:\n",
    "    factors['Mom'] = np.nan\n",
    "\n",
    "# 5. CONSOLIDATE SMB\n",
    "factors['SMB'] = smb_components[['SMB_OP', 'SMB_INV']].mean(axis=1)\n",
    "\n",
    "# 6. MARKET RETURN (GLOBAL)\n",
    "print(\"   ... Calculating Global Market Return\")\n",
    "mkt_info = df_tagged.groupby(base_keys).apply(calc_mkt_rf, include_groups=False)\n",
    "\n",
    "factors = factors.join(mkt_info)\n",
    "factors['Mkt-RF'] = factors['Mkt'] - factors['RF']\n",
    "\n",
    "factors_final = factors.reset_index()\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. OUTPUT & VERIFICATION\n",
    "# ==============================================================================\n",
    "print(f\"Saving Final Factors to TXT: {OUTPUT_FILE}\")\n",
    "factors_final.to_csv(OUTPUT_FILE, sep='|', index=False)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"VERIFICATION CHECK (Global)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ### CHANGE: Removed Country filter for verification\n",
    "mask_ver = (\n",
    "    (factors_final[COLS['date']] >= '2000-07-31') &\n",
    "    (factors_final[COLS['date']] <= '2000-08-09')\n",
    ")\n",
    "\n",
    "verification_subset = factors_final[mask_ver]\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(verification_subset)\n",
    "\n",
    "print(\"\\nProcess Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne B/M Replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input file: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/FF_Benchmark_Factors_Merged_Clean_Replication.txt...\n",
      "Calculating variables...\n",
      "Calculating Momentum (Forced Recalculation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning Portfolios...\n",
      "Total groups to process: 8465\n",
      "Processing Group 8465 / 8465 (100.0%)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_847332/2860167856.py:163: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_tagged = df_clean.groupby(group_keys).apply(assign_daily_portfolios)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Portfolio Assignment Done.\n",
      "Calculating Base Portfolio Returns...\n",
      "   ... Size-OP Sorts\n",
      "   ... Size-Inv Sorts\n",
      "   ... Size-Mom Sorts\n",
      "Pivoting data...\n",
      "Constructing Factors...\n",
      "   ... Calculating Global Market Return\n",
      "Saving Final Factors to TXT: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/Factors_Daily_Replication.txt\n",
      "--------------------------------------------------------------------------------\n",
      "VERIFICATION CHECK (Global)\n",
      "--------------------------------------------------------------------------------\n",
      "        DayDate       RMW       CMA       Mom       SMB       Mkt      RF    Mkt-RF\n",
      "2061 2000-07-31  0.005752 -0.014189  0.017705 -0.004491  0.010342  0.0002  0.010142\n",
      "2062 2000-08-01  0.009035  0.011346 -0.020302 -0.002796  0.001457  0.0002  0.001257\n",
      "2063 2000-08-02 -0.008886  0.003379 -0.012408  0.008025  0.002911  0.0002  0.002711\n",
      "2064 2000-08-03  0.014573 -0.017572  0.011433 -0.012990  0.010520  0.0002  0.010320\n",
      "2065 2000-08-04 -0.010248 -0.003808  0.013296 -0.006872  0.004173  0.0002  0.003973\n",
      "2066 2000-08-07 -0.015185 -0.015766  0.013251 -0.007584  0.013828  0.0002  0.013628\n",
      "2067 2000-08-08 -0.003277  0.000301 -0.001653 -0.004997  0.005367  0.0002  0.005167\n",
      "2068 2000-08-09  0.005479 -0.006449  0.012394  0.005404 -0.003659  0.0002 -0.003859\n",
      "\n",
      "Process Complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress \"All-NaN slice encountered\" warnings\n",
    "warnings.filterwarnings('ignore', r'All-NaN slice encountered')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION & SETUP\n",
    "# ==============================================================================\n",
    "INPUT_FILENAME = \"FF_Benchmark_Factors_Merged_Clean_Replication.txt\"  \n",
    "\n",
    "# ### CHANGE: Filename for Replication version\n",
    "OUTPUT_FILENAME = \"Factors_Daily_Replication.txt\"\n",
    "\n",
    "try:\n",
    "    path_base = f\"{Temp_file_path_A}\"\n",
    "except NameError:\n",
    "    path_base = \".\" \n",
    "\n",
    "INPUT_FILE = os.path.join(path_base, INPUT_FILENAME)\n",
    "OUTPUT_FILE = os.path.join(path_base, OUTPUT_FILENAME)\n",
    "\n",
    "# Column Mapping\n",
    "COLS = {\n",
    "    'date': 'DayDate', 'id': 'ID', 'country': 'Country',\n",
    "    'pcur': 'PCUR', 'hist_curr': 'HistCurrency',\n",
    "    'ret': 'ret_bps', \n",
    "    'mv': 'MV_USD', # Keeping MV_USD as requested previously\n",
    "    'rf': 'rf',\n",
    "    'be': 'be', 'op': 'op', 'inv': 'inv', 'mom': 'mom_signal',\n",
    "    'rev': 'rev', 'cogs': 'cogs', 'sga': 'sga', 'int': 'int',\n",
    "    'at': 'at', 'at_lag1': 'at_lag1'\n",
    "}\n",
    "\n",
    "group_counter = 0\n",
    "total_groups = 0\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "def assign_daily_portfolios(slice_df):\n",
    "    global group_counter\n",
    "    group_counter += 1\n",
    "    if group_counter % 100 == 0 or group_counter == total_groups:\n",
    "        sys.stdout.write(f\"\\rProcessing Group {group_counter} / {total_groups} ({(group_counter/total_groups)*100:.1f}%)\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    if len(slice_df) < 2:\n",
    "        return slice_df \n",
    "\n",
    "    # 1. SIZE ASSIGNMENT (Global Sort)\n",
    "    slice_df = slice_df.sort_values(COLS['mv'], ascending=False)\n",
    "    cum_mv_pct = slice_df[COLS['mv']].cumsum() / slice_df[COLS['mv']].sum()\n",
    "    \n",
    "    is_big = cum_mv_pct <= 0.90\n",
    "    slice_df['Size_Port'] = np.where(is_big, 'B', 'S')\n",
    "    mask_b = is_big\n",
    "\n",
    "    # 2. STYLE ASSIGNMENTS\n",
    "    def assign_style(col_name, out_col_name, labels=['L', 'N', 'H']):\n",
    "        vals = slice_df[col_name].values\n",
    "        big_vals = vals[mask_b]\n",
    "        \n",
    "        if len(big_vals[~np.isnan(big_vals)]) == 0:\n",
    "            p30, p70 = np.nanpercentile(vals, 30), np.nanpercentile(vals, 70)\n",
    "        else:\n",
    "            p30, p70 = np.nanpercentile(big_vals, 30), np.nanpercentile(big_vals, 70)\n",
    "        \n",
    "        conditions = [vals <= p30, (vals > p30) & (vals < p70), vals >= p70]\n",
    "        slice_df[out_col_name] = np.select(conditions, labels, default=None)\n",
    "\n",
    "    # (B/M Removed)\n",
    "    if 'op' in slice_df.columns: \n",
    "        assign_style('op', 'OP_Port', ['W', 'N', 'R'])\n",
    "    if 'inv' in slice_df.columns: \n",
    "        assign_style('inv', 'Inv_Port', ['C', 'N', 'A'])\n",
    "    if 'mom_signal' in slice_df.columns: \n",
    "        assign_style('mom_signal', 'Mom_Port', ['L', 'N', 'H'])\n",
    "        \n",
    "    return slice_df\n",
    "\n",
    "def calc_vw_ret(df_sub):\n",
    "    if len(df_sub) == 0: return np.nan\n",
    "    val = np.average(df_sub['ret_decimal'], weights=df_sub[COLS['mv']])\n",
    "    return val\n",
    "\n",
    "def pivot_portfolios(port_series, port_col_name, base_keys):\n",
    "    df_p = port_series.reset_index()\n",
    "    df_p['Port_Label'] = df_p['Size_Port'] + '_' + df_p[port_col_name]\n",
    "    return df_p.pivot_table(index=base_keys, columns='Port_Label', values=port_series.name)\n",
    "\n",
    "def calc_mkt_rf(df_sub):\n",
    "    if len(df_sub) == 0 or df_sub[COLS['mv']].sum() == 0:\n",
    "        return pd.Series({'Mkt': np.nan, 'RF': np.nan})\n",
    "    mkt = np.average(df_sub['ret_decimal'], weights=df_sub[COLS['mv']])\n",
    "    rf_col = COLS['rf']\n",
    "    if rf_col in df_sub.columns:\n",
    "        rf = df_sub[rf_col].iloc[0]\n",
    "    else:\n",
    "        rf = 0.0\n",
    "    return pd.Series({'Mkt': mkt, 'RF': rf})\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. EXECUTION: LOAD & PREP\n",
    "# ==============================================================================\n",
    "print(f\"Loading input file: {INPUT_FILE}...\")\n",
    "df = pd.read_csv(INPUT_FILE, sep='|', dtype={'ID': str, 'Country': str, 'PCUR': str, 'HistCurrency': str})\n",
    "df[COLS['date']] = pd.to_datetime(df[COLS['date']])\n",
    "\n",
    "if COLS['ret'] in df.columns:\n",
    "    df['ret_decimal'] = df[COLS['ret']] / 10000.0\n",
    "else:\n",
    "    raise ValueError(f\"Could not find Return column '{COLS['ret']}'\")\n",
    "\n",
    "for c in [COLS['mv'], COLS['rf'], COLS['be']]:\n",
    "    if c in df.columns: df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "print(\"Calculating variables...\")\n",
    "# A) OP\n",
    "if 'op' not in df.columns:\n",
    "    mandatory_op_cols = [COLS['rev'], COLS['cogs'], COLS['sga'], COLS['be']]\n",
    "    if all(c in df.columns for c in mandatory_op_cols):\n",
    "        rev = df[COLS['rev']]\n",
    "        cogs = df[COLS['cogs']]\n",
    "        sga = df[COLS['sga']]\n",
    "        be = df[COLS['be']]\n",
    "        interest = df[COLS['int']].fillna(0) if COLS['int'] in df.columns else 0\n",
    "        df['op'] = (rev - cogs - sga - interest) / be\n",
    "\n",
    "# B) Inv\n",
    "if 'inv' not in df.columns:\n",
    "    if all(c in df.columns for c in [COLS['at'], COLS['at_lag1']]):\n",
    "        df['inv'] = (df[COLS['at']] - df[COLS['at_lag1']]) / df[COLS['at_lag1']]\n",
    "\n",
    "# D) Momentum \n",
    "print(\"Calculating Momentum (Forced Recalculation)...\")\n",
    "df = df.sort_values([COLS['id'], COLS['pcur'], COLS['date']])\n",
    "df['log_ret'] = np.log1p(df['ret_decimal'].fillna(0))\n",
    "df['mom_signal'] = df.groupby([COLS['id'], COLS['pcur']])['log_ret'].transform(\n",
    "    lambda x: x.rolling(window=250, min_periods=100).sum().shift(21)\n",
    ")\n",
    "df['mom_signal'] = np.exp(df['mom_signal']) - 1\n",
    "\n",
    "# Prep\n",
    "subset_cols = [c for c in [COLS['mv'], 'ret_decimal'] if c in df.columns]\n",
    "df_clean = df.dropna(subset=subset_cols).copy()\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. EXECUTION: ASSIGN PORTFOLIOS (GLOBAL)\n",
    "# ==============================================================================\n",
    "print(\"Assigning Portfolios...\")\n",
    "\n",
    "# ### CHANGE: Group ONLY by date (Global sort)\n",
    "group_keys = [COLS['date']]\n",
    "\n",
    "unique_groups = df_clean[group_keys].drop_duplicates()\n",
    "total_groups = len(unique_groups)\n",
    "print(f\"Total groups to process: {total_groups}\")\n",
    "\n",
    "df_tagged = df_clean.groupby(group_keys).apply(assign_daily_portfolios)\n",
    "print(\"\\nPortfolio Assignment Done.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. EXECUTION: AGGREGATE RETURNS (GLOBAL)\n",
    "# ==============================================================================\n",
    "print(\"Calculating Base Portfolio Returns...\")\n",
    "\n",
    "df_tagged = df_tagged.reset_index(drop=True)\n",
    "# ### CHANGE: Base keys are ONLY date\n",
    "base_keys = [COLS['date']]\n",
    "\n",
    "# 2. Size-OP\n",
    "print(\"   ... Size-OP Sorts\")\n",
    "port_op = df_tagged.dropna(subset=['Size_Port', 'OP_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'OP_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_op.name = 'Ret_OP'\n",
    "\n",
    "# 3. Size-Inv\n",
    "print(\"   ... Size-Inv Sorts\")\n",
    "port_inv = df_tagged.dropna(subset=['Size_Port', 'Inv_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'Inv_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_inv.name = 'Ret_Inv'\n",
    "\n",
    "# 4. Size-Mom\n",
    "print(\"   ... Size-Mom Sorts\")\n",
    "port_mom = df_tagged.dropna(subset=['Size_Port', 'Mom_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'Mom_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_mom.name = 'Ret_Mom'\n",
    "\n",
    "print(\"Pivoting data...\")\n",
    "df_p_op = pivot_portfolios(port_op, 'OP_Port', base_keys)\n",
    "df_p_inv = pivot_portfolios(port_inv, 'Inv_Port', base_keys)\n",
    "df_p_mom = pivot_portfolios(port_mom, 'Mom_Port', base_keys)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. EXECUTION: FACTOR CONSTRUCTION\n",
    "# ==============================================================================\n",
    "print(\"Constructing Factors...\")\n",
    "\n",
    "factors = pd.DataFrame(index=df_p_op.index)\n",
    "smb_components = pd.DataFrame(index=df_p_op.index)\n",
    "\n",
    "# 2. RMW & SMB (OP)\n",
    "if all(c in df_p_op.columns for c in ['S_R', 'B_R', 'S_W', 'B_W']):\n",
    "    factors['RMW'] = 0.5 * (df_p_op['S_R'] + df_p_op['B_R']) - \\\n",
    "                     0.5 * (df_p_op['S_W'] + df_p_op['B_W'])\n",
    "    smb_components['SMB_OP'] = (df_p_op['S_R'] + df_p_op['S_N'] + df_p_op['S_W']) / 3 - \\\n",
    "                               (df_p_op['B_R'] + df_p_op['B_N'] + df_p_op['B_W']) / 3\n",
    "else:\n",
    "    factors['RMW'] = np.nan\n",
    "    smb_components['SMB_OP'] = np.nan\n",
    "\n",
    "# 3. CMA & SMB (Inv)\n",
    "if all(c in df_p_inv.columns for c in ['S_C', 'B_C', 'S_A', 'B_A']):\n",
    "    factors['CMA'] = 0.5 * (df_p_inv['S_C'] + df_p_inv['B_C']) - \\\n",
    "                     0.5 * (df_p_inv['S_A'] + df_p_inv['B_A'])\n",
    "    smb_components['SMB_INV'] = (df_p_inv['S_C'] + df_p_inv['S_N'] + df_p_inv['S_A']) / 3 - \\\n",
    "                                (df_p_inv['B_C'] + df_p_inv['B_N'] + df_p_inv['B_A']) / 3\n",
    "else:\n",
    "    factors['CMA'] = np.nan\n",
    "    smb_components['SMB_INV'] = np.nan\n",
    "\n",
    "# 4. MOM\n",
    "if all(c in df_p_mom.columns for c in ['S_H', 'B_H', 'S_L', 'B_L']):\n",
    "    factors['Mom'] = 0.5 * (df_p_mom['S_H'] + df_p_mom['B_H']) - \\\n",
    "                     0.5 * (df_p_mom['S_L'] + df_p_mom['B_L'])\n",
    "else:\n",
    "    factors['Mom'] = np.nan\n",
    "\n",
    "# 5. CONSOLIDATE SMB\n",
    "factors['SMB'] = smb_components[['SMB_OP', 'SMB_INV']].mean(axis=1)\n",
    "\n",
    "# 6. MARKET RETURN (GLOBAL)\n",
    "print(\"   ... Calculating Global Market Return\")\n",
    "mkt_info = df_tagged.groupby(base_keys).apply(calc_mkt_rf, include_groups=False)\n",
    "\n",
    "factors = factors.join(mkt_info)\n",
    "factors['Mkt-RF'] = factors['Mkt'] - factors['RF']\n",
    "\n",
    "factors_final = factors.reset_index()\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. OUTPUT & VERIFICATION\n",
    "# ==============================================================================\n",
    "print(f\"Saving Final Factors to TXT: {OUTPUT_FILE}\")\n",
    "factors_final.to_csv(OUTPUT_FILE, sep='|', index=False)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"VERIFICATION CHECK (Global)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ### CHANGE: Removed Country filter for verification\n",
    "mask_ver = (\n",
    "    (factors_final[COLS['date']] >= '2000-07-31') &\n",
    "    (factors_final[COLS['date']] <= '2000-08-09')\n",
    ")\n",
    "\n",
    "verification_subset = factors_final[mask_ver]\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(verification_subset)\n",
    "\n",
    "print(\"\\nProcess Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne B/M Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input file: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/FF_Benchmark_Factors_Merged_Clean.txt...\n",
      "Calculating variables...\n",
      "Calculating Momentum...\n",
      "\n",
      "=== STEP 4: ANNUAL REBALANCING LOGIC ===\n",
      "Calculing Portfolio Year for Daily Data...\n",
      "Identifying Anchor Dates (Best data near June 30th)...\n",
      "Identified 488570 anchor rows.\n",
      "Assigning Portfolios on Anchor Dates...\n",
      "Rows with valid Market Cap: 488570\n",
      "Annual Assignment Done.\n",
      "\n",
      "Rolling Forward: Merging Anchors to Daily Data...\n",
      "Merge Complete.\n",
      "  Total Daily Rows: 128221039\n",
      "  Tagged Daily Rows: 120175968 (93.7%)\n",
      "\n",
      "Calculating Base Portfolio Returns...\n",
      "   ... Size-OP Sorts\n",
      "   ... Size-Inv Sorts\n",
      "   ... Size-Mom Sorts\n",
      "Pivoting data...\n",
      "Constructing Factors...\n",
      "   ... Calculating Market Return\n",
      "Saving Final Factors to TXT: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/Factors_Annual_Country.txt\n",
      "--------------------------------------------------------------------------------\n",
      "VERIFICATION CHECK (ANNUAL)\n",
      "--------------------------------------------------------------------------------\n",
      "         DayDate Country       RMW       CMA       Mom       SMB       Mkt      RF    Mkt-RF\n",
      "67765 2000-07-31     280 -0.010262 -0.008708 -0.000573 -0.015663  0.014813  0.0002  0.014613\n",
      "67811 2000-08-01     280 -0.006707 -0.004480 -0.002526  0.004105 -0.005353  0.0002 -0.005553\n",
      "67857 2000-08-02     280 -0.012998  0.002074 -0.011225  0.002893 -0.017643  0.0002 -0.017843\n",
      "67903 2000-08-03     280  0.000478  0.015470 -0.028949  0.004264 -0.024798  0.0002 -0.024998\n",
      "67949 2000-08-04     280  0.006026 -0.007074  0.011463 -0.003624  0.009633  0.0002  0.009433\n",
      "67995 2000-08-07     280  0.015595 -0.002518  0.013797 -0.011118  0.013812  0.0002  0.013612\n",
      "68041 2000-08-08     280 -0.015858  0.001925 -0.008504  0.009532 -0.015020  0.0002 -0.015220\n",
      "68087 2000-08-09     280  0.001984 -0.016773  0.005018 -0.002081  0.013443  0.0002  0.013243\n",
      "\n",
      "Process Complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Suppress \"All-NaN slice encountered\" and other noisy warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION & SETUP\n",
    "# ==============================================================================\n",
    "INPUT_FILENAME = \"FF_Benchmark_Factors_Merged_Clean.txt\"  \n",
    "\n",
    "# ### CHANGE: Updated Filename\n",
    "OUTPUT_FILENAME = \"Factors_Annual_Country.txt\"\n",
    "\n",
    "# Define Paths\n",
    "try:\n",
    "    path_base = f\"{Temp_file_path_A}\"\n",
    "except NameError:\n",
    "    path_base = \".\" \n",
    "\n",
    "INPUT_FILE = os.path.join(path_base, INPUT_FILENAME)\n",
    "OUTPUT_FILE = os.path.join(path_base, OUTPUT_FILENAME)\n",
    "\n",
    "# Column Mapping\n",
    "COLS = {\n",
    "    'date': 'DayDate', 'id': 'ID', 'country': 'Country',\n",
    "    'pcur': 'PCUR', 'hist_curr': 'HistCurrency',\n",
    "    'ret': 'ret_bps', \n",
    "    'mv': 'MV_USD', # ### CHANGE: Switched to MV_USD\n",
    "    'rf': 'rf',\n",
    "    'be': 'be', 'op': 'op', 'inv': 'inv', 'mom': 'mom_signal',\n",
    "    # Ingredients\n",
    "    'rev': 'rev', 'cogs': 'cogs', 'sga': 'sga', 'int': 'int',\n",
    "    'at': 'at', 'at_lag1': 'at_lag1'\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "def assign_portfolios_robust(slice_df):\n",
    "    \"\"\"\n",
    "    Assigns Size and Style groups. Handles missing columns gracefully.\n",
    "    \"\"\"\n",
    "    # ### CHANGE: Removed BM_Port from expected columns\n",
    "    expected_cols = ['Size_Port', 'OP_Port', 'Inv_Port', 'Mom_Port']\n",
    "    for c in expected_cols:\n",
    "        if c not in slice_df.columns:\n",
    "            slice_df[c] = np.nan\n",
    "\n",
    "    if len(slice_df) < 2:\n",
    "        return slice_df \n",
    "\n",
    "    # 1. SIZE ASSIGNMENT (Top 90% = Big)\n",
    "    if slice_df[COLS['mv']].isna().all():\n",
    "        return slice_df\n",
    "        \n",
    "    slice_df = slice_df.sort_values(COLS['mv'], ascending=False)\n",
    "    cum_mv_pct = slice_df[COLS['mv']].cumsum() / slice_df[COLS['mv']].sum()\n",
    "    \n",
    "    is_big = cum_mv_pct <= 0.90\n",
    "    slice_df['Size_Port'] = np.where(is_big, 'B', 'S')\n",
    "    mask_b = is_big\n",
    "\n",
    "    # 2. STYLE ASSIGNMENTS\n",
    "    def assign_style(col_name, out_col_name, labels=['L', 'N', 'H']):\n",
    "        vals = slice_df[col_name].values\n",
    "        big_vals = vals[mask_b]\n",
    "        \n",
    "        valid_big = big_vals[~np.isnan(big_vals)]\n",
    "        \n",
    "        if len(valid_big) == 0:\n",
    "             # Fallback\n",
    "             valid_all = vals[~np.isnan(vals)]\n",
    "             if len(valid_all) == 0:\n",
    "                 slice_df[out_col_name] = None\n",
    "                 return\n",
    "             p30, p70 = np.percentile(valid_all, [30, 70])\n",
    "        else:\n",
    "             p30, p70 = np.percentile(valid_big, [30, 70])\n",
    "        \n",
    "        conditions = [vals <= p30, (vals > p30) & (vals < p70), vals >= p70]\n",
    "        slice_df[out_col_name] = np.select(conditions, labels, default=None)\n",
    "\n",
    "    # ### CHANGE: Commented out BM assignment\n",
    "    # if 'bm' in slice_df.columns: assign_style('bm', 'BM_Port')\n",
    "    \n",
    "    if 'op' in slice_df.columns: assign_style('op', 'OP_Port', ['W', 'N', 'R'])\n",
    "    if 'inv' in slice_df.columns: assign_style('inv', 'Inv_Port', ['C', 'N', 'A'])\n",
    "    if 'mom_signal' in slice_df.columns: assign_style('mom_signal', 'Mom_Port')\n",
    "        \n",
    "    return slice_df\n",
    "\n",
    "def calc_vw_ret(df_sub):\n",
    "    if len(df_sub) == 0: return np.nan\n",
    "    val = np.average(df_sub['ret_decimal'], weights=df_sub[COLS['mv']])\n",
    "    return val\n",
    "\n",
    "def pivot_portfolios(port_series, port_col_name, base_keys):\n",
    "    if port_series.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_p = port_series.reset_index()\n",
    "    \n",
    "    # CAST TO STRING TO PREVENT CRASH\n",
    "    s_port = df_p['Size_Port'].astype(str)\n",
    "    style_port = df_p[port_col_name].astype(str)\n",
    "    \n",
    "    df_p['Port_Label'] = s_port + '_' + style_port\n",
    "    \n",
    "    return df_p.pivot_table(index=base_keys, columns='Port_Label', values=port_series.name)\n",
    "\n",
    "def calc_mkt_rf(df_sub):\n",
    "    if len(df_sub) == 0 or df_sub[COLS['mv']].sum() == 0:\n",
    "        return pd.Series({'Mkt': np.nan, 'RF': np.nan})\n",
    "    mkt = np.average(df_sub['ret_decimal'], weights=df_sub[COLS['mv']])\n",
    "    \n",
    "    rf_col = COLS['rf']\n",
    "    if rf_col in df_sub.columns:\n",
    "        rf = df_sub[rf_col].iloc[0]\n",
    "    else:\n",
    "        rf = 0.0\n",
    "    return pd.Series({'Mkt': mkt, 'RF': rf})\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. EXECUTION: LOAD & PREP\n",
    "# ==============================================================================\n",
    "print(f\"Loading input file: {INPUT_FILE}...\")\n",
    "df = pd.read_csv(INPUT_FILE, sep='|', dtype={'ID': str, 'Country': str, 'PCUR': str, 'HistCurrency': str})\n",
    "\n",
    "# Standardize Dates\n",
    "df[COLS['date']] = pd.to_datetime(df[COLS['date']])\n",
    "\n",
    "# Convert Returns: BPS -> Decimal\n",
    "if COLS['ret'] in df.columns:\n",
    "    df['ret_decimal'] = df[COLS['ret']] / 10000.0\n",
    "else:\n",
    "    raise ValueError(f\"Could not find Return column '{COLS['ret']}'. Check COLS mapping.\")\n",
    "\n",
    "# Prep numeric columns\n",
    "for c in [COLS['mv'], COLS['rf'], COLS['be']]:\n",
    "    if c in df.columns: df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CALCULATE VARIABLES\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"Calculating variables...\")\n",
    "\n",
    "# A) Operating Profitability (OP)\n",
    "if 'op' not in df.columns:\n",
    "    mandatory_op_cols = [COLS['rev'], COLS['cogs'], COLS['sga'], COLS['be']]\n",
    "    if all(c in df.columns for c in mandatory_op_cols):\n",
    "        rev = df[COLS['rev']]\n",
    "        cogs = df[COLS['cogs']]\n",
    "        sga = df[COLS['sga']]\n",
    "        be = df[COLS['be']]\n",
    "        interest = df[COLS['int']].fillna(0) if COLS['int'] in df.columns else 0\n",
    "        df['op'] = (rev - cogs - sga - interest) / be\n",
    "\n",
    "# B) Investment (Inv)\n",
    "if 'inv' not in df.columns:\n",
    "    if all(c in df.columns for c in [COLS['at'], COLS['at_lag1']]):\n",
    "        df['inv'] = (df[COLS['at']] - df[COLS['at_lag1']]) / df[COLS['at_lag1']]\n",
    "\n",
    "# ### CHANGE: Commented out B/M Calculation\n",
    "# # C) Book-to-Market (BM)\n",
    "# if 'bm' not in df.columns:\n",
    "#     if COLS['be'] in df.columns and COLS['mv'] in df.columns:\n",
    "#         df['bm'] = df[COLS['be']] / df[COLS['mv']]\n",
    "\n",
    "# D) Momentum (Forced Recalc)\n",
    "print(\"Calculating Momentum...\")\n",
    "df = df.sort_values([COLS['id'], COLS['pcur'], COLS['date']])\n",
    "df['log_ret'] = np.log1p(df['ret_decimal'].fillna(0))\n",
    "df['mom_signal'] = df.groupby([COLS['id'], COLS['pcur']])['log_ret'].transform(\n",
    "    lambda x: x.rolling(window=250, min_periods=100).sum().shift(21)\n",
    ")\n",
    "df['mom_signal'] = np.exp(df['mom_signal']) - 1\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. EXECUTION: ANNUAL PORTFOLIO ASSIGNMENT\n",
    "# ==============================================================================\n",
    "print(\"\\n=== STEP 4: ANNUAL REBALANCING LOGIC ===\")\n",
    "\n",
    "# 4A. DEFINE \"PORTFOLIO YEAR\"\n",
    "print(\"Calculing Portfolio Year for Daily Data...\")\n",
    "df['Port_Year'] = np.where(df[COLS['date']].dt.month >= 7, \n",
    "                           df[COLS['date']].dt.year, \n",
    "                           df[COLS['date']].dt.year - 1).astype(int)\n",
    "\n",
    "# 4B. FIND ANCHOR DATA\n",
    "print(\"Identifying Anchor Dates (Best data near June 30th)...\")\n",
    "candidates = df[df[COLS['date']].dt.month.isin([6, 7])].copy()\n",
    "candidates['Anchor_Year'] = candidates[COLS['date']].dt.year\n",
    "candidates['Target_Date'] = pd.to_datetime(candidates['Anchor_Year'].astype(str) + \"-06-30\")\n",
    "candidates['Diff'] = (candidates[COLS['date']] - candidates['Target_Date']).abs()\n",
    "candidates = candidates.sort_values([COLS['id'], 'Anchor_Year', 'Diff'])\n",
    "df_anchors = candidates.groupby([COLS['id'], 'Anchor_Year'], as_index=False).first()\n",
    "\n",
    "print(f\"Identified {len(df_anchors)} anchor rows.\")\n",
    "del candidates\n",
    "gc.collect()\n",
    "\n",
    "# 4C. ASSIGN PORTFOLIOS ON ANCHOR DATA\n",
    "print(\"Assigning Portfolios on Anchor Dates...\")\n",
    "\n",
    "# ### CHANGE: Removed Currency columns from grouping keys.\n",
    "# Now grouping only by Country + Year\n",
    "group_keys = [COLS['country'], 'Anchor_Year']\n",
    "\n",
    "df_anchors_clean = df_anchors.dropna(subset=[COLS['mv']]).copy()\n",
    "print(f\"Rows with valid Market Cap: {len(df_anchors_clean)}\")\n",
    "\n",
    "if df_anchors_clean.empty:\n",
    "    raise ValueError(\"CRITICAL: No valid anchor data (Market Cap is NaN).\")\n",
    "\n",
    "# Run Assignment\n",
    "df_tagged_anchors = df_anchors_clean.groupby(group_keys, group_keys=False).apply(assign_portfolios_robust)\n",
    "\n",
    "print(\"Annual Assignment Done.\")\n",
    "\n",
    "# 4D. THE MERGE (ROLL FORWARD)\n",
    "print(\"\\nRolling Forward: Merging Anchors to Daily Data...\")\n",
    "\n",
    "# ### CHANGE: Removed BM_Port from cols_to_keep\n",
    "cols_to_keep = [COLS['id'], 'Anchor_Year', 'Size_Port', 'OP_Port', 'Inv_Port', 'Mom_Port']\n",
    "cols_to_keep = [c for c in cols_to_keep if c in df_tagged_anchors.columns]\n",
    "\n",
    "df_merge_right = df_tagged_anchors[cols_to_keep].copy()\n",
    "df_merge_right = df_merge_right.rename(columns={'Anchor_Year': 'Port_Year'})\n",
    "\n",
    "# Perform Merge\n",
    "df_tagged = pd.merge(df, df_merge_right, on=[COLS['id'], 'Port_Year'], how='left')\n",
    "\n",
    "# Check results\n",
    "n_total = len(df_tagged)\n",
    "n_tagged = df_tagged['Size_Port'].notna().sum()\n",
    "print(f\"Merge Complete.\")\n",
    "print(f\"  Total Daily Rows: {n_total}\")\n",
    "print(f\"  Tagged Daily Rows: {n_tagged} ({(n_tagged/n_total)*100:.1f}%)\")\n",
    "\n",
    "df_tagged = df_tagged.dropna(subset=['Size_Port'])\n",
    "\n",
    "if df_tagged.empty:\n",
    "    raise ValueError(\"Zero tagged rows after merge.\")\n",
    "\n",
    "del df_merge_right, df_anchors, df_anchors_clean\n",
    "gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. EXECUTION: AGGREGATE RETURNS (DAILY)\n",
    "# ==============================================================================\n",
    "print(\"\\nCalculating Base Portfolio Returns...\")\n",
    "\n",
    "# ### CHANGE: Removed PCUR and HistCurrency\n",
    "base_keys = [COLS['date'], COLS['country']]\n",
    "\n",
    "# ### CHANGE: Removed Size-BM Portfolios calculation\n",
    "# print(\"   ... Size-BM Sorts\")\n",
    "# port_bm = ...\n",
    "\n",
    "# 2. Size-OP Portfolios\n",
    "print(\"   ... Size-OP Sorts\")\n",
    "port_op = df_tagged.dropna(subset=['Size_Port', 'OP_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'OP_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_op.name = 'Ret_OP'\n",
    "\n",
    "# 3. Size-Inv Portfolios\n",
    "print(\"   ... Size-Inv Sorts\")\n",
    "port_inv = df_tagged.dropna(subset=['Size_Port', 'Inv_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'Inv_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_inv.name = 'Ret_Inv'\n",
    "\n",
    "# 4. Size-Mom Portfolios\n",
    "print(\"   ... Size-Mom Sorts\")\n",
    "port_mom = df_tagged.dropna(subset=['Size_Port', 'Mom_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'Mom_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_mom.name = 'Ret_Mom'\n",
    "\n",
    "# Pivot Data\n",
    "print(\"Pivoting data...\")\n",
    "# ### CHANGE: Removed BM pivot\n",
    "# df_p_bm = pivot_portfolios(port_bm, 'BM_Port', base_keys)\n",
    "df_p_op = pivot_portfolios(port_op, 'OP_Port', base_keys)\n",
    "df_p_inv = pivot_portfolios(port_inv, 'Inv_Port', base_keys)\n",
    "df_p_mom = pivot_portfolios(port_mom, 'Mom_Port', base_keys)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. EXECUTION: FACTOR CONSTRUCTION\n",
    "# ==============================================================================\n",
    "print(\"Constructing Factors...\")\n",
    "\n",
    "if df_p_op.empty: # ### CHANGE: Checked OP instead of BM\n",
    "    print(\"WARNING: No Portfolio Returns calculated. Factors will be empty.\")\n",
    "    factors = pd.DataFrame()\n",
    "else:\n",
    "    # ### CHANGE: Use OP index instead of BM\n",
    "    factors = pd.DataFrame(index=df_p_op.index)\n",
    "    smb_components = pd.DataFrame(index=df_p_op.index)\n",
    "\n",
    "    # ### CHANGE: Removed HML Construction\n",
    "    # 1. HML & SMB (BM) ...\n",
    "\n",
    "    # 2. RMW & SMB (OP)\n",
    "    if all(c in df_p_op.columns for c in ['S_R', 'B_R', 'S_W', 'B_W']):\n",
    "        factors['RMW'] = 0.5 * (df_p_op['S_R'] + df_p_op['B_R']) - \\\n",
    "                        0.5 * (df_p_op['S_W'] + df_p_op['B_W'])\n",
    "        smb_components['SMB_OP'] = (df_p_op['S_R'] + df_p_op['S_N'] + df_p_op['S_W']) / 3 - \\\n",
    "                                (df_p_op['B_R'] + df_p_op['B_N'] + df_p_op['B_W']) / 3\n",
    "    else:\n",
    "        factors['RMW'] = np.nan\n",
    "        smb_components['SMB_OP'] = np.nan\n",
    "\n",
    "    # 3. CMA & SMB (Inv)\n",
    "    if all(c in df_p_inv.columns for c in ['S_C', 'B_C', 'S_A', 'B_A']):\n",
    "        factors['CMA'] = 0.5 * (df_p_inv['S_C'] + df_p_inv['B_C']) - \\\n",
    "                        0.5 * (df_p_inv['S_A'] + df_p_inv['B_A'])\n",
    "        smb_components['SMB_INV'] = (df_p_inv['S_C'] + df_p_inv['S_N'] + df_p_inv['S_A']) / 3 - \\\n",
    "                                    (df_p_inv['B_C'] + df_p_inv['B_N'] + df_p_inv['B_A']) / 3\n",
    "    else:\n",
    "        factors['CMA'] = np.nan\n",
    "        smb_components['SMB_INV'] = np.nan\n",
    "\n",
    "    # 4. MOM\n",
    "    if all(c in df_p_mom.columns for c in ['S_H', 'B_H', 'S_L', 'B_L']):\n",
    "        factors['Mom'] = 0.5 * (df_p_mom['S_H'] + df_p_mom['B_H']) - \\\n",
    "                        0.5 * (df_p_mom['S_L'] + df_p_mom['B_L'])\n",
    "    else:\n",
    "        factors['Mom'] = np.nan\n",
    "\n",
    "    # 5. CONSOLIDATE SMB\n",
    "    # ### CHANGE: Removed SMB_BM from average\n",
    "    factors['SMB'] = smb_components[['SMB_OP', 'SMB_INV']].mean(axis=1)\n",
    "\n",
    "    # 6. MARKET RETURN\n",
    "    print(\"   ... Calculating Market Return\")\n",
    "    mkt_info = df_tagged.groupby(base_keys).apply(calc_mkt_rf, include_groups=False)\n",
    "\n",
    "    factors = factors.join(mkt_info)\n",
    "    factors['Mkt-RF'] = factors['Mkt'] - factors['RF']\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. OUTPUT & VERIFICATION\n",
    "# ==============================================================================\n",
    "if not factors.empty:\n",
    "    factors_final = factors.reset_index()\n",
    "    print(f\"Saving Final Factors to TXT: {OUTPUT_FILE}\")\n",
    "    factors_final.to_csv(OUTPUT_FILE, sep='|', index=False)\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"VERIFICATION CHECK (ANNUAL)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # ### CHANGE: Removed PCUR/HistCurrency filter check\n",
    "    mask_ver = (\n",
    "        (factors_final[COLS['date']] >= '2000-07-31') &\n",
    "        (factors_final[COLS['date']] <= '2000-08-09') &\n",
    "        (factors_final['Country'] == '280') \n",
    "    )\n",
    "\n",
    "    verification_subset = factors_final[mask_ver]\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(verification_subset)\n",
    "else:\n",
    "    print(\"Factor DataFrame is empty. No output generated.\")\n",
    "\n",
    "print(\"\\nProcess Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne B/M Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input file: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/FF_Benchmark_Factors_Merged_Clean.txt...\n",
      "Calculating variables...\n",
      "Calculating Momentum...\n",
      "\n",
      "=== STEP 4: ANNUAL REBALANCING LOGIC ===\n",
      "Calculing Portfolio Year for Daily Data...\n",
      "Identifying Anchor Dates (Best data near June 30th)...\n",
      "Identified 488570 anchor rows.\n",
      "Assigning Portfolios on Anchor Dates...\n",
      "Rows with valid Market Cap: 488570\n",
      "Annual Assignment Done.\n",
      "\n",
      "Rolling Forward: Merging Anchors to Daily Data...\n",
      "Merge Complete.\n",
      "  Total Daily Rows: 128221039\n",
      "  Tagged Daily Rows: 120178335 (93.7%)\n",
      "\n",
      "Calculating Base Portfolio Returns...\n",
      "   ... Size-OP Sorts\n",
      "   ... Size-Inv Sorts\n",
      "   ... Size-Mom Sorts\n",
      "Pivoting data...\n",
      "Constructing Factors...\n",
      "   ... Calculating Global Market Return\n",
      "Saving Final Factors to TXT: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/Factors_Annual_Global.txt\n",
      "--------------------------------------------------------------------------------\n",
      "VERIFICATION CHECK (ANNUAL - GLOBAL)\n",
      "--------------------------------------------------------------------------------\n",
      "        DayDate       RMW       CMA       Mom       SMB       Mkt      RF    Mkt-RF\n",
      "1847 2000-07-31 -0.012455 -0.005349  0.012633 -0.009077  0.007171  0.0002  0.006971\n",
      "1848 2000-08-01  0.003528  0.011737 -0.009542  0.004160  0.003369  0.0002  0.003169\n",
      "1849 2000-08-02  0.002015  0.003858 -0.008397  0.002109 -0.000904  0.0002 -0.001104\n",
      "1850 2000-08-03 -0.001484 -0.003385 -0.003100  0.003378 -0.001762  0.0002 -0.001962\n",
      "1851 2000-08-04 -0.007420 -0.010740  0.006637 -0.003183  0.005319  0.0002  0.005119\n",
      "1852 2000-08-07  0.004722 -0.000175  0.007565 -0.012110  0.010076  0.0002  0.009876\n",
      "1853 2000-08-08 -0.005242 -0.004926  0.001310 -0.002065  0.001026  0.0002  0.000826\n",
      "1854 2000-08-09 -0.008240  0.003786  0.011662  0.000221  0.002653  0.0002  0.002453\n",
      "\n",
      "Process Complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Suppress \"All-NaN slice encountered\" and other noisy warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION & SETUP\n",
    "# ==============================================================================\n",
    "INPUT_FILENAME = \"FF_Benchmark_Factors_Merged_Clean.txt\"  \n",
    "\n",
    "# ### CHANGE: Updated Filename for Global\n",
    "OUTPUT_FILENAME = \"Factors_Annual_Global.txt\"\n",
    "\n",
    "# Define Paths\n",
    "try:\n",
    "    path_base = f\"{Temp_file_path_A}\"\n",
    "except NameError:\n",
    "    path_base = \".\" \n",
    "\n",
    "INPUT_FILE = os.path.join(path_base, INPUT_FILENAME)\n",
    "OUTPUT_FILE = os.path.join(path_base, OUTPUT_FILENAME)\n",
    "\n",
    "# Column Mapping\n",
    "COLS = {\n",
    "    'date': 'DayDate', 'id': 'ID', 'country': 'Country',\n",
    "    'pcur': 'PCUR', 'hist_curr': 'HistCurrency',\n",
    "    'ret': 'ret_bps', \n",
    "    'mv': 'MV_USD', # Keeping MV_USD\n",
    "    'rf': 'rf',\n",
    "    'be': 'be', 'op': 'op', 'inv': 'inv', 'mom': 'mom_signal',\n",
    "    # Ingredients\n",
    "    'rev': 'rev', 'cogs': 'cogs', 'sga': 'sga', 'int': 'int',\n",
    "    'at': 'at', 'at_lag1': 'at_lag1'\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "def assign_portfolios_robust(slice_df):\n",
    "    \"\"\"\n",
    "    Assigns Size and Style groups. Handles missing columns gracefully.\n",
    "    \"\"\"\n",
    "    expected_cols = ['Size_Port', 'OP_Port', 'Inv_Port', 'Mom_Port']\n",
    "    for c in expected_cols:\n",
    "        if c not in slice_df.columns:\n",
    "            slice_df[c] = np.nan\n",
    "\n",
    "    if len(slice_df) < 2:\n",
    "        return slice_df \n",
    "\n",
    "    # 1. SIZE ASSIGNMENT (Global Sort)\n",
    "    if slice_df[COLS['mv']].isna().all():\n",
    "        return slice_df\n",
    "        \n",
    "    slice_df = slice_df.sort_values(COLS['mv'], ascending=False)\n",
    "    cum_mv_pct = slice_df[COLS['mv']].cumsum() / slice_df[COLS['mv']].sum()\n",
    "    \n",
    "    is_big = cum_mv_pct <= 0.90\n",
    "    slice_df['Size_Port'] = np.where(is_big, 'B', 'S')\n",
    "    mask_b = is_big\n",
    "\n",
    "    # 2. STYLE ASSIGNMENTS\n",
    "    def assign_style(col_name, out_col_name, labels=['L', 'N', 'H']):\n",
    "        vals = slice_df[col_name].values\n",
    "        big_vals = vals[mask_b]\n",
    "        \n",
    "        valid_big = big_vals[~np.isnan(big_vals)]\n",
    "        \n",
    "        if len(valid_big) == 0:\n",
    "             # Fallback\n",
    "             valid_all = vals[~np.isnan(vals)]\n",
    "             if len(valid_all) == 0:\n",
    "                 slice_df[out_col_name] = None\n",
    "                 return\n",
    "             p30, p70 = np.percentile(valid_all, [30, 70])\n",
    "        else:\n",
    "             p30, p70 = np.percentile(valid_big, [30, 70])\n",
    "        \n",
    "        conditions = [vals <= p30, (vals > p30) & (vals < p70), vals >= p70]\n",
    "        slice_df[out_col_name] = np.select(conditions, labels, default=None)\n",
    "\n",
    "    # (BM Removed)\n",
    "    if 'op' in slice_df.columns: assign_style('op', 'OP_Port', ['W', 'N', 'R'])\n",
    "    if 'inv' in slice_df.columns: assign_style('inv', 'Inv_Port', ['C', 'N', 'A'])\n",
    "    if 'mom_signal' in slice_df.columns: assign_style('mom_signal', 'Mom_Port')\n",
    "        \n",
    "    return slice_df\n",
    "\n",
    "def calc_vw_ret(df_sub):\n",
    "    if len(df_sub) == 0: return np.nan\n",
    "    val = np.average(df_sub['ret_decimal'], weights=df_sub[COLS['mv']])\n",
    "    return val\n",
    "\n",
    "def pivot_portfolios(port_series, port_col_name, base_keys):\n",
    "    if port_series.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_p = port_series.reset_index()\n",
    "    s_port = df_p['Size_Port'].astype(str)\n",
    "    style_port = df_p[port_col_name].astype(str)\n",
    "    df_p['Port_Label'] = s_port + '_' + style_port\n",
    "    \n",
    "    return df_p.pivot_table(index=base_keys, columns='Port_Label', values=port_series.name)\n",
    "\n",
    "def calc_mkt_rf(df_sub):\n",
    "    if len(df_sub) == 0 or df_sub[COLS['mv']].sum() == 0:\n",
    "        return pd.Series({'Mkt': np.nan, 'RF': np.nan})\n",
    "    mkt = np.average(df_sub['ret_decimal'], weights=df_sub[COLS['mv']])\n",
    "    rf_col = COLS['rf']\n",
    "    if rf_col in df_sub.columns:\n",
    "        rf = df_sub[rf_col].iloc[0]\n",
    "    else:\n",
    "        rf = 0.0\n",
    "    return pd.Series({'Mkt': mkt, 'RF': rf})\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. EXECUTION: LOAD & PREP\n",
    "# ==============================================================================\n",
    "print(f\"Loading input file: {INPUT_FILE}...\")\n",
    "df = pd.read_csv(INPUT_FILE, sep='|', dtype={'ID': str, 'Country': str, 'PCUR': str, 'HistCurrency': str})\n",
    "df[COLS['date']] = pd.to_datetime(df[COLS['date']])\n",
    "\n",
    "if COLS['ret'] in df.columns:\n",
    "    df['ret_decimal'] = df[COLS['ret']] / 10000.0\n",
    "else:\n",
    "    raise ValueError(f\"Could not find Return column '{COLS['ret']}'\")\n",
    "\n",
    "for c in [COLS['mv'], COLS['rf'], COLS['be']]:\n",
    "    if c in df.columns: df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "print(\"Calculating variables...\")\n",
    "# A) OP\n",
    "if 'op' not in df.columns:\n",
    "    mandatory_op_cols = [COLS['rev'], COLS['cogs'], COLS['sga'], COLS['be']]\n",
    "    if all(c in df.columns for c in mandatory_op_cols):\n",
    "        rev = df[COLS['rev']]\n",
    "        cogs = df[COLS['cogs']]\n",
    "        sga = df[COLS['sga']]\n",
    "        be = df[COLS['be']]\n",
    "        interest = df[COLS['int']].fillna(0) if COLS['int'] in df.columns else 0\n",
    "        df['op'] = (rev - cogs - sga - interest) / be\n",
    "\n",
    "# B) Inv\n",
    "if 'inv' not in df.columns:\n",
    "    if all(c in df.columns for c in [COLS['at'], COLS['at_lag1']]):\n",
    "        df['inv'] = (df[COLS['at']] - df[COLS['at_lag1']]) / df[COLS['at_lag1']]\n",
    "\n",
    "# (BM Removed)\n",
    "\n",
    "# D) Momentum\n",
    "print(\"Calculating Momentum...\")\n",
    "df = df.sort_values([COLS['id'], COLS['pcur'], COLS['date']])\n",
    "df['log_ret'] = np.log1p(df['ret_decimal'].fillna(0))\n",
    "df['mom_signal'] = df.groupby([COLS['id'], COLS['pcur']])['log_ret'].transform(\n",
    "    lambda x: x.rolling(window=250, min_periods=100).sum().shift(21)\n",
    ")\n",
    "df['mom_signal'] = np.exp(df['mom_signal']) - 1\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. EXECUTION: ANNUAL PORTFOLIO ASSIGNMENT\n",
    "# ==============================================================================\n",
    "print(\"\\n=== STEP 4: ANNUAL REBALANCING LOGIC ===\")\n",
    "\n",
    "# 4A. DEFINE \"PORTFOLIO YEAR\"\n",
    "print(\"Calculing Portfolio Year for Daily Data...\")\n",
    "df['Port_Year'] = np.where(df[COLS['date']].dt.month >= 7, \n",
    "                           df[COLS['date']].dt.year, \n",
    "                           df[COLS['date']].dt.year - 1).astype(int)\n",
    "\n",
    "# 4B. FIND ANCHOR DATA\n",
    "print(\"Identifying Anchor Dates (Best data near June 30th)...\")\n",
    "candidates = df[df[COLS['date']].dt.month.isin([6, 7])].copy()\n",
    "candidates['Anchor_Year'] = candidates[COLS['date']].dt.year\n",
    "candidates['Target_Date'] = pd.to_datetime(candidates['Anchor_Year'].astype(str) + \"-06-30\")\n",
    "candidates['Diff'] = (candidates[COLS['date']] - candidates['Target_Date']).abs()\n",
    "candidates = candidates.sort_values([COLS['id'], 'Anchor_Year', 'Diff'])\n",
    "df_anchors = candidates.groupby([COLS['id'], 'Anchor_Year'], as_index=False).first()\n",
    "\n",
    "print(f\"Identified {len(df_anchors)} anchor rows.\")\n",
    "del candidates\n",
    "gc.collect()\n",
    "\n",
    "# 4C. ASSIGN PORTFOLIOS ON ANCHOR DATA\n",
    "print(\"Assigning Portfolios on Anchor Dates...\")\n",
    "\n",
    "# ### CHANGE: Group ONLY by Anchor_Year (Global Sort)\n",
    "group_keys = ['Anchor_Year']\n",
    "\n",
    "df_anchors_clean = df_anchors.dropna(subset=[COLS['mv']]).copy()\n",
    "print(f\"Rows with valid Market Cap: {len(df_anchors_clean)}\")\n",
    "\n",
    "if df_anchors_clean.empty:\n",
    "    raise ValueError(\"CRITICAL: No valid anchor data (Market Cap is NaN).\")\n",
    "\n",
    "# Run Assignment\n",
    "df_tagged_anchors = df_anchors_clean.groupby(group_keys, group_keys=False).apply(assign_portfolios_robust)\n",
    "\n",
    "print(\"Annual Assignment Done.\")\n",
    "\n",
    "# 4D. THE MERGE (ROLL FORWARD)\n",
    "print(\"\\nRolling Forward: Merging Anchors to Daily Data...\")\n",
    "\n",
    "cols_to_keep = [COLS['id'], 'Anchor_Year', 'Size_Port', 'OP_Port', 'Inv_Port', 'Mom_Port']\n",
    "cols_to_keep = [c for c in cols_to_keep if c in df_tagged_anchors.columns]\n",
    "\n",
    "df_merge_right = df_tagged_anchors[cols_to_keep].copy()\n",
    "df_merge_right = df_merge_right.rename(columns={'Anchor_Year': 'Port_Year'})\n",
    "\n",
    "# Perform Merge\n",
    "df_tagged = pd.merge(df, df_merge_right, on=[COLS['id'], 'Port_Year'], how='left')\n",
    "\n",
    "# Check results\n",
    "n_total = len(df_tagged)\n",
    "n_tagged = df_tagged['Size_Port'].notna().sum()\n",
    "print(f\"Merge Complete.\")\n",
    "print(f\"  Total Daily Rows: {n_total}\")\n",
    "print(f\"  Tagged Daily Rows: {n_tagged} ({(n_tagged/n_total)*100:.1f}%)\")\n",
    "\n",
    "df_tagged = df_tagged.dropna(subset=['Size_Port'])\n",
    "\n",
    "if df_tagged.empty:\n",
    "    raise ValueError(\"Zero tagged rows after merge.\")\n",
    "\n",
    "del df_merge_right, df_anchors, df_anchors_clean\n",
    "gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. EXECUTION: AGGREGATE RETURNS (DAILY)\n",
    "# ==============================================================================\n",
    "print(\"\\nCalculating Base Portfolio Returns...\")\n",
    "\n",
    "# ### CHANGE: Group ONLY by Date (Global Aggregation)\n",
    "base_keys = [COLS['date']]\n",
    "\n",
    "# 2. Size-OP\n",
    "print(\"   ... Size-OP Sorts\")\n",
    "port_op = df_tagged.dropna(subset=['Size_Port', 'OP_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'OP_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_op.name = 'Ret_OP'\n",
    "\n",
    "# 3. Size-Inv\n",
    "print(\"   ... Size-Inv Sorts\")\n",
    "port_inv = df_tagged.dropna(subset=['Size_Port', 'Inv_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'Inv_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_inv.name = 'Ret_Inv'\n",
    "\n",
    "# 4. Size-Mom\n",
    "print(\"   ... Size-Mom Sorts\")\n",
    "port_mom = df_tagged.dropna(subset=['Size_Port', 'Mom_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'Mom_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_mom.name = 'Ret_Mom'\n",
    "\n",
    "# Pivot Data\n",
    "print(\"Pivoting data...\")\n",
    "df_p_op = pivot_portfolios(port_op, 'OP_Port', base_keys)\n",
    "df_p_inv = pivot_portfolios(port_inv, 'Inv_Port', base_keys)\n",
    "df_p_mom = pivot_portfolios(port_mom, 'Mom_Port', base_keys)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. EXECUTION: FACTOR CONSTRUCTION\n",
    "# ==============================================================================\n",
    "print(\"Constructing Factors...\")\n",
    "\n",
    "if df_p_op.empty:\n",
    "    print(\"WARNING: No Portfolio Returns calculated. Factors will be empty.\")\n",
    "    factors = pd.DataFrame()\n",
    "else:\n",
    "    factors = pd.DataFrame(index=df_p_op.index)\n",
    "    smb_components = pd.DataFrame(index=df_p_op.index)\n",
    "\n",
    "    # (HML Removed)\n",
    "\n",
    "    # 2. RMW & SMB (OP)\n",
    "    if all(c in df_p_op.columns for c in ['S_R', 'B_R', 'S_W', 'B_W']):\n",
    "        factors['RMW'] = 0.5 * (df_p_op['S_R'] + df_p_op['B_R']) - \\\n",
    "                        0.5 * (df_p_op['S_W'] + df_p_op['B_W'])\n",
    "        smb_components['SMB_OP'] = (df_p_op['S_R'] + df_p_op['S_N'] + df_p_op['S_W']) / 3 - \\\n",
    "                                (df_p_op['B_R'] + df_p_op['B_N'] + df_p_op['B_W']) / 3\n",
    "    else:\n",
    "        factors['RMW'] = np.nan\n",
    "        smb_components['SMB_OP'] = np.nan\n",
    "\n",
    "    # 3. CMA & SMB (Inv)\n",
    "    if all(c in df_p_inv.columns for c in ['S_C', 'B_C', 'S_A', 'B_A']):\n",
    "        factors['CMA'] = 0.5 * (df_p_inv['S_C'] + df_p_inv['B_C']) - \\\n",
    "                        0.5 * (df_p_inv['S_A'] + df_p_inv['B_A'])\n",
    "        smb_components['SMB_INV'] = (df_p_inv['S_C'] + df_p_inv['S_N'] + df_p_inv['S_A']) / 3 - \\\n",
    "                                    (df_p_inv['B_C'] + df_p_inv['B_N'] + df_p_inv['B_A']) / 3\n",
    "    else:\n",
    "        factors['CMA'] = np.nan\n",
    "        smb_components['SMB_INV'] = np.nan\n",
    "\n",
    "    # 4. MOM\n",
    "    if all(c in df_p_mom.columns for c in ['S_H', 'B_H', 'S_L', 'B_L']):\n",
    "        factors['Mom'] = 0.5 * (df_p_mom['S_H'] + df_p_mom['B_H']) - \\\n",
    "                        0.5 * (df_p_mom['S_L'] + df_p_mom['B_L'])\n",
    "    else:\n",
    "        factors['Mom'] = np.nan\n",
    "\n",
    "    # 5. CONSOLIDATE SMB\n",
    "    factors['SMB'] = smb_components[['SMB_OP', 'SMB_INV']].mean(axis=1)\n",
    "\n",
    "    # 6. MARKET RETURN\n",
    "    print(\"   ... Calculating Global Market Return\")\n",
    "    mkt_info = df_tagged.groupby(base_keys).apply(calc_mkt_rf, include_groups=False)\n",
    "\n",
    "    factors = factors.join(mkt_info)\n",
    "    factors['Mkt-RF'] = factors['Mkt'] - factors['RF']\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. OUTPUT & VERIFICATION\n",
    "# ==============================================================================\n",
    "if not factors.empty:\n",
    "    factors_final = factors.reset_index()\n",
    "    print(f\"Saving Final Factors to TXT: {OUTPUT_FILE}\")\n",
    "    factors_final.to_csv(OUTPUT_FILE, sep='|', index=False)\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"VERIFICATION CHECK (ANNUAL - GLOBAL)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # ### CHANGE: Removed Country filter\n",
    "    mask_ver = (\n",
    "        (factors_final[COLS['date']] >= '2000-07-31') &\n",
    "        (factors_final[COLS['date']] <= '2000-08-09') \n",
    "    )\n",
    "\n",
    "    verification_subset = factors_final[mask_ver]\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(verification_subset)\n",
    "else:\n",
    "    print(\"Factor DataFrame is empty. No output generated.\")\n",
    "\n",
    "print(\"\\nProcess Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohne B/M Replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input file: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/FF_Benchmark_Factors_Merged_Clean_Replication.txt...\n",
      "Calculating variables...\n",
      "Calculating Momentum...\n",
      "\n",
      "=== STEP 4: ANNUAL REBALANCING LOGIC ===\n",
      "Calculing Portfolio Year for Daily Data...\n",
      "Identifying Anchor Dates (Best data near June 30th)...\n",
      "Identified 101256 anchor rows.\n",
      "Assigning Portfolios on Anchor Dates...\n",
      "Rows with valid Market Cap: 101256\n",
      "Annual Assignment Done.\n",
      "\n",
      "Rolling Forward: Merging Anchors to Daily Data...\n",
      "Merge Complete.\n",
      "  Total Daily Rows: 26598430\n",
      "  Tagged Daily Rows: 24883669 (93.6%)\n",
      "\n",
      "Calculating Base Portfolio Returns...\n",
      "   ... Size-OP Sorts\n",
      "   ... Size-Inv Sorts\n",
      "   ... Size-Mom Sorts\n",
      "Pivoting data...\n",
      "Constructing Factors...\n",
      "   ... Calculating Global Market Return\n",
      "Saving Final Factors to TXT: /home/jovyan/work/hpool1/pseidel/test/Temp/TempAnomalies/Factors_Annual_Replication.txt\n",
      "--------------------------------------------------------------------------------\n",
      "VERIFICATION CHECK (ANNUAL - GLOBAL)\n",
      "--------------------------------------------------------------------------------\n",
      "        DayDate       RMW       CMA       Mom       SMB       Mkt      RF    Mkt-RF\n",
      "1847 2000-07-31 -0.024694 -0.019108  0.019688  0.006208  0.010342  0.0002  0.010142\n",
      "1848 2000-08-01  0.013737  0.013390 -0.017796  0.003101  0.001457  0.0002  0.001257\n",
      "1849 2000-08-02 -0.001877  0.005773 -0.010344  0.004225  0.002913  0.0002  0.002713\n",
      "1850 2000-08-03 -0.012920 -0.023183  0.009214 -0.006180  0.010522  0.0002  0.010322\n",
      "1851 2000-08-04 -0.016301 -0.004863  0.012591 -0.000784  0.004189  0.0002  0.003989\n",
      "1852 2000-08-07 -0.011767 -0.013614  0.015693 -0.016500  0.013843  0.0002  0.013643\n",
      "1853 2000-08-08  0.017853  0.007678 -0.003738 -0.015263  0.005397  0.0002  0.005197\n",
      "1854 2000-08-09 -0.005511 -0.004183  0.015033  0.005415 -0.003652  0.0002 -0.003852\n",
      "\n",
      "Process Complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "# Suppress \"All-NaN slice encountered\" and other noisy warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION & SETUP\n",
    "# ==============================================================================\n",
    "INPUT_FILENAME = \"FF_Benchmark_Factors_Merged_Clean_Replication.txt\"  \n",
    "\n",
    "# ### CHANGE: Updated Filename for Replication\n",
    "OUTPUT_FILENAME = \"Factors_Annual_Replication.txt\"\n",
    "\n",
    "# Define Paths\n",
    "try:\n",
    "    path_base = f\"{Temp_file_path_A}\"\n",
    "except NameError:\n",
    "    path_base = \".\" \n",
    "\n",
    "INPUT_FILE = os.path.join(path_base, INPUT_FILENAME)\n",
    "OUTPUT_FILE = os.path.join(path_base, OUTPUT_FILENAME)\n",
    "\n",
    "# Column Mapping\n",
    "COLS = {\n",
    "    'date': 'DayDate', 'id': 'ID', 'country': 'Country',\n",
    "    'pcur': 'PCUR', 'hist_curr': 'HistCurrency',\n",
    "    'ret': 'ret_bps', \n",
    "    'mv': 'MV_USD', # Keeping MV_USD\n",
    "    'rf': 'rf',\n",
    "    'be': 'be', 'op': 'op', 'inv': 'inv', 'mom': 'mom_signal',\n",
    "    # Ingredients\n",
    "    'rev': 'rev', 'cogs': 'cogs', 'sga': 'sga', 'int': 'int',\n",
    "    'at': 'at', 'at_lag1': 'at_lag1'\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "def assign_portfolios_robust(slice_df):\n",
    "    \"\"\"\n",
    "    Assigns Size and Style groups. Handles missing columns gracefully.\n",
    "    \"\"\"\n",
    "    expected_cols = ['Size_Port', 'OP_Port', 'Inv_Port', 'Mom_Port']\n",
    "    for c in expected_cols:\n",
    "        if c not in slice_df.columns:\n",
    "            slice_df[c] = np.nan\n",
    "\n",
    "    if len(slice_df) < 2:\n",
    "        return slice_df \n",
    "\n",
    "    # 1. SIZE ASSIGNMENT (Global Sort)\n",
    "    if slice_df[COLS['mv']].isna().all():\n",
    "        return slice_df\n",
    "        \n",
    "    slice_df = slice_df.sort_values(COLS['mv'], ascending=False)\n",
    "    cum_mv_pct = slice_df[COLS['mv']].cumsum() / slice_df[COLS['mv']].sum()\n",
    "    \n",
    "    is_big = cum_mv_pct <= 0.90\n",
    "    slice_df['Size_Port'] = np.where(is_big, 'B', 'S')\n",
    "    mask_b = is_big\n",
    "\n",
    "    # 2. STYLE ASSIGNMENTS\n",
    "    def assign_style(col_name, out_col_name, labels=['L', 'N', 'H']):\n",
    "        vals = slice_df[col_name].values\n",
    "        big_vals = vals[mask_b]\n",
    "        \n",
    "        valid_big = big_vals[~np.isnan(big_vals)]\n",
    "        \n",
    "        if len(valid_big) == 0:\n",
    "             # Fallback\n",
    "             valid_all = vals[~np.isnan(vals)]\n",
    "             if len(valid_all) == 0:\n",
    "                 slice_df[out_col_name] = None\n",
    "                 return\n",
    "             p30, p70 = np.percentile(valid_all, [30, 70])\n",
    "        else:\n",
    "             p30, p70 = np.percentile(valid_big, [30, 70])\n",
    "        \n",
    "        conditions = [vals <= p30, (vals > p30) & (vals < p70), vals >= p70]\n",
    "        slice_df[out_col_name] = np.select(conditions, labels, default=None)\n",
    "\n",
    "    # (BM Removed)\n",
    "    if 'op' in slice_df.columns: assign_style('op', 'OP_Port', ['W', 'N', 'R'])\n",
    "    if 'inv' in slice_df.columns: assign_style('inv', 'Inv_Port', ['C', 'N', 'A'])\n",
    "    if 'mom_signal' in slice_df.columns: assign_style('mom_signal', 'Mom_Port')\n",
    "        \n",
    "    return slice_df\n",
    "\n",
    "def calc_vw_ret(df_sub):\n",
    "    if len(df_sub) == 0: return np.nan\n",
    "    val = np.average(df_sub['ret_decimal'], weights=df_sub[COLS['mv']])\n",
    "    return val\n",
    "\n",
    "def pivot_portfolios(port_series, port_col_name, base_keys):\n",
    "    if port_series.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_p = port_series.reset_index()\n",
    "    s_port = df_p['Size_Port'].astype(str)\n",
    "    style_port = df_p[port_col_name].astype(str)\n",
    "    df_p['Port_Label'] = s_port + '_' + style_port\n",
    "    \n",
    "    return df_p.pivot_table(index=base_keys, columns='Port_Label', values=port_series.name)\n",
    "\n",
    "def calc_mkt_rf(df_sub):\n",
    "    if len(df_sub) == 0 or df_sub[COLS['mv']].sum() == 0:\n",
    "        return pd.Series({'Mkt': np.nan, 'RF': np.nan})\n",
    "    mkt = np.average(df_sub['ret_decimal'], weights=df_sub[COLS['mv']])\n",
    "    rf_col = COLS['rf']\n",
    "    if rf_col in df_sub.columns:\n",
    "        rf = df_sub[rf_col].iloc[0]\n",
    "    else:\n",
    "        rf = 0.0\n",
    "    return pd.Series({'Mkt': mkt, 'RF': rf})\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. EXECUTION: LOAD & PREP\n",
    "# ==============================================================================\n",
    "print(f\"Loading input file: {INPUT_FILE}...\")\n",
    "df = pd.read_csv(INPUT_FILE, sep='|', dtype={'ID': str, 'Country': str, 'PCUR': str, 'HistCurrency': str})\n",
    "df[COLS['date']] = pd.to_datetime(df[COLS['date']])\n",
    "\n",
    "if COLS['ret'] in df.columns:\n",
    "    df['ret_decimal'] = df[COLS['ret']] / 10000.0\n",
    "else:\n",
    "    raise ValueError(f\"Could not find Return column '{COLS['ret']}'\")\n",
    "\n",
    "for c in [COLS['mv'], COLS['rf'], COLS['be']]:\n",
    "    if c in df.columns: df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "print(\"Calculating variables...\")\n",
    "# A) OP\n",
    "if 'op' not in df.columns:\n",
    "    mandatory_op_cols = [COLS['rev'], COLS['cogs'], COLS['sga'], COLS['be']]\n",
    "    if all(c in df.columns for c in mandatory_op_cols):\n",
    "        rev = df[COLS['rev']]\n",
    "        cogs = df[COLS['cogs']]\n",
    "        sga = df[COLS['sga']]\n",
    "        be = df[COLS['be']]\n",
    "        interest = df[COLS['int']].fillna(0) if COLS['int'] in df.columns else 0\n",
    "        df['op'] = (rev - cogs - sga - interest) / be\n",
    "\n",
    "# B) Inv\n",
    "if 'inv' not in df.columns:\n",
    "    if all(c in df.columns for c in [COLS['at'], COLS['at_lag1']]):\n",
    "        df['inv'] = (df[COLS['at']] - df[COLS['at_lag1']]) / df[COLS['at_lag1']]\n",
    "\n",
    "# (BM Removed)\n",
    "\n",
    "# D) Momentum\n",
    "print(\"Calculating Momentum...\")\n",
    "df = df.sort_values([COLS['id'], COLS['pcur'], COLS['date']])\n",
    "df['log_ret'] = np.log1p(df['ret_decimal'].fillna(0))\n",
    "df['mom_signal'] = df.groupby([COLS['id'], COLS['pcur']])['log_ret'].transform(\n",
    "    lambda x: x.rolling(window=250, min_periods=100).sum().shift(21)\n",
    ")\n",
    "df['mom_signal'] = np.exp(df['mom_signal']) - 1\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. EXECUTION: ANNUAL PORTFOLIO ASSIGNMENT\n",
    "# ==============================================================================\n",
    "print(\"\\n=== STEP 4: ANNUAL REBALANCING LOGIC ===\")\n",
    "\n",
    "# 4A. DEFINE \"PORTFOLIO YEAR\"\n",
    "print(\"Calculing Portfolio Year for Daily Data...\")\n",
    "df['Port_Year'] = np.where(df[COLS['date']].dt.month >= 7, \n",
    "                           df[COLS['date']].dt.year, \n",
    "                           df[COLS['date']].dt.year - 1).astype(int)\n",
    "\n",
    "# 4B. FIND ANCHOR DATA\n",
    "print(\"Identifying Anchor Dates (Best data near June 30th)...\")\n",
    "candidates = df[df[COLS['date']].dt.month.isin([6, 7])].copy()\n",
    "candidates['Anchor_Year'] = candidates[COLS['date']].dt.year\n",
    "candidates['Target_Date'] = pd.to_datetime(candidates['Anchor_Year'].astype(str) + \"-06-30\")\n",
    "candidates['Diff'] = (candidates[COLS['date']] - candidates['Target_Date']).abs()\n",
    "candidates = candidates.sort_values([COLS['id'], 'Anchor_Year', 'Diff'])\n",
    "df_anchors = candidates.groupby([COLS['id'], 'Anchor_Year'], as_index=False).first()\n",
    "\n",
    "print(f\"Identified {len(df_anchors)} anchor rows.\")\n",
    "del candidates\n",
    "gc.collect()\n",
    "\n",
    "# 4C. ASSIGN PORTFOLIOS ON ANCHOR DATA\n",
    "print(\"Assigning Portfolios on Anchor Dates...\")\n",
    "\n",
    "# ### CHANGE: Group ONLY by Anchor_Year (Global Sort)\n",
    "group_keys = ['Anchor_Year']\n",
    "\n",
    "df_anchors_clean = df_anchors.dropna(subset=[COLS['mv']]).copy()\n",
    "print(f\"Rows with valid Market Cap: {len(df_anchors_clean)}\")\n",
    "\n",
    "if df_anchors_clean.empty:\n",
    "    raise ValueError(\"CRITICAL: No valid anchor data (Market Cap is NaN).\")\n",
    "\n",
    "# Run Assignment\n",
    "df_tagged_anchors = df_anchors_clean.groupby(group_keys, group_keys=False).apply(assign_portfolios_robust)\n",
    "\n",
    "print(\"Annual Assignment Done.\")\n",
    "\n",
    "# 4D. THE MERGE (ROLL FORWARD)\n",
    "print(\"\\nRolling Forward: Merging Anchors to Daily Data...\")\n",
    "\n",
    "cols_to_keep = [COLS['id'], 'Anchor_Year', 'Size_Port', 'OP_Port', 'Inv_Port', 'Mom_Port']\n",
    "cols_to_keep = [c for c in cols_to_keep if c in df_tagged_anchors.columns]\n",
    "\n",
    "df_merge_right = df_tagged_anchors[cols_to_keep].copy()\n",
    "df_merge_right = df_merge_right.rename(columns={'Anchor_Year': 'Port_Year'})\n",
    "\n",
    "# Perform Merge\n",
    "df_tagged = pd.merge(df, df_merge_right, on=[COLS['id'], 'Port_Year'], how='left')\n",
    "\n",
    "# Check results\n",
    "n_total = len(df_tagged)\n",
    "n_tagged = df_tagged['Size_Port'].notna().sum()\n",
    "print(f\"Merge Complete.\")\n",
    "print(f\"  Total Daily Rows: {n_total}\")\n",
    "print(f\"  Tagged Daily Rows: {n_tagged} ({(n_tagged/n_total)*100:.1f}%)\")\n",
    "\n",
    "df_tagged = df_tagged.dropna(subset=['Size_Port'])\n",
    "\n",
    "if df_tagged.empty:\n",
    "    raise ValueError(\"Zero tagged rows after merge.\")\n",
    "\n",
    "del df_merge_right, df_anchors, df_anchors_clean\n",
    "gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. EXECUTION: AGGREGATE RETURNS (DAILY)\n",
    "# ==============================================================================\n",
    "print(\"\\nCalculating Base Portfolio Returns...\")\n",
    "\n",
    "# ### CHANGE: Group ONLY by Date (Global Aggregation)\n",
    "base_keys = [COLS['date']]\n",
    "\n",
    "# 2. Size-OP\n",
    "print(\"   ... Size-OP Sorts\")\n",
    "port_op = df_tagged.dropna(subset=['Size_Port', 'OP_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'OP_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_op.name = 'Ret_OP'\n",
    "\n",
    "# 3. Size-Inv\n",
    "print(\"   ... Size-Inv Sorts\")\n",
    "port_inv = df_tagged.dropna(subset=['Size_Port', 'Inv_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'Inv_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_inv.name = 'Ret_Inv'\n",
    "\n",
    "# 4. Size-Mom\n",
    "print(\"   ... Size-Mom Sorts\")\n",
    "port_mom = df_tagged.dropna(subset=['Size_Port', 'Mom_Port'])\\\n",
    "    .groupby(base_keys + ['Size_Port', 'Mom_Port'])\\\n",
    "    .apply(calc_vw_ret, include_groups=False)\n",
    "port_mom.name = 'Ret_Mom'\n",
    "\n",
    "# Pivot Data\n",
    "print(\"Pivoting data...\")\n",
    "df_p_op = pivot_portfolios(port_op, 'OP_Port', base_keys)\n",
    "df_p_inv = pivot_portfolios(port_inv, 'Inv_Port', base_keys)\n",
    "df_p_mom = pivot_portfolios(port_mom, 'Mom_Port', base_keys)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. EXECUTION: FACTOR CONSTRUCTION\n",
    "# ==============================================================================\n",
    "print(\"Constructing Factors...\")\n",
    "\n",
    "if df_p_op.empty:\n",
    "    print(\"WARNING: No Portfolio Returns calculated. Factors will be empty.\")\n",
    "    factors = pd.DataFrame()\n",
    "else:\n",
    "    factors = pd.DataFrame(index=df_p_op.index)\n",
    "    smb_components = pd.DataFrame(index=df_p_op.index)\n",
    "\n",
    "    # (HML Removed)\n",
    "\n",
    "    # 2. RMW & SMB (OP)\n",
    "    if all(c in df_p_op.columns for c in ['S_R', 'B_R', 'S_W', 'B_W']):\n",
    "        factors['RMW'] = 0.5 * (df_p_op['S_R'] + df_p_op['B_R']) - \\\n",
    "                        0.5 * (df_p_op['S_W'] + df_p_op['B_W'])\n",
    "        smb_components['SMB_OP'] = (df_p_op['S_R'] + df_p_op['S_N'] + df_p_op['S_W']) / 3 - \\\n",
    "                                (df_p_op['B_R'] + df_p_op['B_N'] + df_p_op['B_W']) / 3\n",
    "    else:\n",
    "        factors['RMW'] = np.nan\n",
    "        smb_components['SMB_OP'] = np.nan\n",
    "\n",
    "    # 3. CMA & SMB (Inv)\n",
    "    if all(c in df_p_inv.columns for c in ['S_C', 'B_C', 'S_A', 'B_A']):\n",
    "        factors['CMA'] = 0.5 * (df_p_inv['S_C'] + df_p_inv['B_C']) - \\\n",
    "                        0.5 * (df_p_inv['S_A'] + df_p_inv['B_A'])\n",
    "        smb_components['SMB_INV'] = (df_p_inv['S_C'] + df_p_inv['S_N'] + df_p_inv['S_A']) / 3 - \\\n",
    "                                    (df_p_inv['B_C'] + df_p_inv['B_N'] + df_p_inv['B_A']) / 3\n",
    "    else:\n",
    "        factors['CMA'] = np.nan\n",
    "        smb_components['SMB_INV'] = np.nan\n",
    "\n",
    "    # 4. MOM\n",
    "    if all(c in df_p_mom.columns for c in ['S_H', 'B_H', 'S_L', 'B_L']):\n",
    "        factors['Mom'] = 0.5 * (df_p_mom['S_H'] + df_p_mom['B_H']) - \\\n",
    "                        0.5 * (df_p_mom['S_L'] + df_p_mom['B_L'])\n",
    "    else:\n",
    "        factors['Mom'] = np.nan\n",
    "\n",
    "    # 5. CONSOLIDATE SMB\n",
    "    factors['SMB'] = smb_components[['SMB_OP', 'SMB_INV']].mean(axis=1)\n",
    "\n",
    "    # 6. MARKET RETURN\n",
    "    print(\"   ... Calculating Global Market Return\")\n",
    "    mkt_info = df_tagged.groupby(base_keys).apply(calc_mkt_rf, include_groups=False)\n",
    "\n",
    "    factors = factors.join(mkt_info)\n",
    "    factors['Mkt-RF'] = factors['Mkt'] - factors['RF']\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. OUTPUT & VERIFICATION\n",
    "# ==============================================================================\n",
    "if not factors.empty:\n",
    "    factors_final = factors.reset_index()\n",
    "    print(f\"Saving Final Factors to TXT: {OUTPUT_FILE}\")\n",
    "    factors_final.to_csv(OUTPUT_FILE, sep='|', index=False)\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"VERIFICATION CHECK (ANNUAL - GLOBAL)\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # ### CHANGE: Removed Country filter\n",
    "    mask_ver = (\n",
    "        (factors_final[COLS['date']] >= '2000-07-31') &\n",
    "        (factors_final[COLS['date']] <= '2000-08-09') \n",
    "    )\n",
    "\n",
    "    verification_subset = factors_final[mask_ver]\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(verification_subset)\n",
    "else:\n",
    "    print(\"Factor DataFrame is empty. No output generated.\")\n",
    "\n",
    "print(\"\\nProcess Complete.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPSbnONVLN/hNvx3gaV6bIs",
   "collapsed_sections": [
    "ctXZHVs-zurM",
    "fI91nEqImRuW",
    "1I-ohwwoiaOf",
    "beVmMnSWlujq",
    "G_LVUFFol4or",
    "5MVSADPQl_Im",
    "nNDLh9YeGTUd",
    "YA4wqdb-Gpmp",
    "2lMgXNkWn1so",
    "p9SCqWZhoHWV",
    "0sZeny4spkXe",
    "lJrElD7tpy6D",
    "RaT9qk-lqH2U",
    "kS_NAo1V_lDt",
    "3LdCsaYrqm2-",
    "j3GqzaH4KdvB",
    "ByeSFikWrNrS",
    "ymD2DOB0rjjp",
    "3RtjOlDfsQAs",
    "AdhQDQpNsvRQ",
    "Dl-xJ7YFlgZP",
    "n9RfF_uXtEWe",
    "ibx9sNQvtqPG",
    "FU2KBlF4w9Sg",
    "OPggIVBfxYKp",
    "yy8Td9o3x5OT",
    "qP_5O3nLyVP6",
    "Ee0G6keb0vx2",
    "OFOGWXQu2dLr",
    "w2dXuVHw7m9P",
    "XuLx5Umh8Hgh",
    "n6Ju2Ehq8tjC",
    "N6BMDicV9HnF",
    "4SrElfeZ9gNA",
    "X9OgqUzrIkpt",
    "nSJg6cgGIvA1",
    "n40a8yHIVMR6",
    "spmfdwMZVT0K",
    "Pjty6RWLVT0N",
    "peLARLfKV7Ve",
    "0cdh6V_CXYjy",
    "4VV_GHlnXali",
    "oujAtKVjXall",
    "o7t777qpXalm",
    "wIT0NremXpir",
    "UbGeEnwfUl7y",
    "10h4B-beVKZi",
    "-t5PG_BADyJa",
    "qW9qTRmDD1px",
    "d8duQiheEBE8",
    "5bb7Y8QOFOvp",
    "cX2o4ZVhGOe0",
    "BI6pdBLLMA3R",
    "W0wBZt9PMIM1",
    "SyBv0xKBMX6k",
    "qRh0r7RtVsBu",
    "UaGLNDHINGeX",
    "oxk_1LFLPSDn",
    "Itq-w3b3PYBM",
    "HQVQUYMZOqnY",
    "xcPJ4gLXPWQg",
    "XOj3twhL-2-4",
    "-zjJ6KWB6Gfr",
    "eNAl51r_RLRA",
    "90ijzZcYZX2r",
    "jD8lrPdReWP6",
    "fBonCCy_hfUP",
    "Ey20IFLChmnL",
    "h5PYjLZWANfm",
    "2dJywftRrUBr",
    "_mHXx1RSr4xI",
    "jfdzly-lsC0j",
    "O_ekkq7isOnj",
    "asxpQXwHsftR",
    "4FS-9QiQrUc6",
    "PPp60Qu_rlMi",
    "rRIDrpHqsP-3",
    "EYvpA1c8sYnf",
    "Z_U_zo4cstxB",
    "JnClUmvQs9tS",
    "GlLntIw1tJ_7",
    "D9FctGGaKNNQ",
    "SelT2RuFmB-V",
    "fmYxcS66m6ae",
    "APemjsQ1nSdP",
    "kv4yt7aspLpl",
    "yrnwfeeSpyYa",
    "f-rKHHxFp3LM",
    "3kk1fhEwtcnk",
    "WLXQuKSkvA0I",
    "v50n6z-Lvv9g",
    "VO7U975lv0Ru",
    "v4Kn6kKWwFUp",
    "ZNsVYlH1AWGR",
    "J391VaJk8o-E",
    "6MCpu1xg8o-K",
    "lldVuPr5_D-I",
    "cfJTpeNYw2CV",
    "62PDf8WZyI8k",
    "S1am1BoJygqf",
    "5e63md-nzTWN",
    "LgzlO8WbzWnX",
    "WNby3OAT5sok",
    "coI9PVy06Uvm",
    "1fVkSOIy6wWv",
    "6BITXbd_9F_f",
    "-p7KOFkD9gqG",
    "6niDGbEE91XQ",
    "FaPzAippM-If",
    "TrQTywxkNNCs",
    "vNzFRJnpN35N",
    "7y7F9u8mB7Ur",
    "UsQt0BezK93J",
    "fy1IrXy2_Lc7",
    "oJP3mjuM_Lc-",
    "Suolm-aZ_Mmi",
    "SnC2RuXoHjC4",
    "rESFxSfaIXWz",
    "dMSMkZBuIaQQ",
    "JnAh06oTJJsN",
    "cxoCn0ONJmaA",
    "kkRyP3irJ-CM",
    "mfGPN92zJ-CV",
    "d2kN05RdJ-CY",
    "yKfqt10DJ-CZ",
    "0I1BdtJQKeJ_",
    "PTtZneigLhwC",
    "sjb4NzSZPzsj"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
