{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ctXZHVs-zurM",
   "metadata": {
    "id": "ctXZHVs-zurM"
   },
   "source": [
    "### Mount Google Drive, Import Libraries and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef11b3f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19186,
     "status": "ok",
     "timestamp": 1765469239653,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ef11b3f9",
    "outputId": "be5ea6ba-0d23-4ca6-ad3c-5680ac4266af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: False\n",
      "BASE_PATH: /home/jovyan/work/hpool1/pseidel/test\n",
      "Importing numpy ...\n",
      "numpy OK\n",
      "Importing scipy ...\n",
      "scipy OK\n",
      "Importing pandas ...\n",
      "pandas OK\n",
      "Importing linearmodels ...\n",
      "linearmodels OK\n",
      "Importing xlsxwriter ...\n",
      "xlsxwriter OK\n",
      "Paths configured. Temp outputs -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview\n",
      "Example input path -> /home/jovyan/work/hpool1/pseidel/test/Input/WSFV_f_20250131.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT SETUP + PATH CONFIGURATION (SERVER / COLAB COMPATIBLE)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) HARD SAFETY: cap native thread usage (prevents pthread_create EAGAIN)\n",
    "#    MUST be set before importing numpy / scipy / pandas\n",
    "# -----------------------------------------------------------------------------\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"BLIS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Detect environment\n",
    "# -----------------------------------------------------------------------------\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) (Colab only) Mount Google Drive\n",
    "# -----------------------------------------------------------------------------\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "else:\n",
    "    # Server base path (your target)\n",
    "    BASE_PATH = \"/home/jovyan/work/hpool1/pseidel/test\"\n",
    "\n",
    "print(\"IN_COLAB:\", IN_COLAB)\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Sanity checks: path exists + write permission\n",
    "# -----------------------------------------------------------------------------\n",
    "BASE = Path(BASE_PATH)\n",
    "if not BASE.exists():\n",
    "    raise FileNotFoundError(f\"BASE_PATH does not exist: {BASE}\")\n",
    "\n",
    "# quick write test (fails fast if you don't have permissions)\n",
    "test_file = BASE / \".write_test_tmp\"\n",
    "try:\n",
    "    test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "    test_file.unlink()\n",
    "except Exception as e:\n",
    "    raise PermissionError(f\"No write permission in {BASE}. Error: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Environment check: ensure required packages import cleanly\n",
    "# -----------------------------------------------------------------------------\n",
    "required_packages = [\"numpy\", \"scipy\", \"pandas\", \"linearmodels\", \"xlsxwriter\"]\n",
    "\n",
    "for pkg in required_packages:\n",
    "    print(f\"Importing {pkg} ...\")\n",
    "    importlib.import_module(pkg)\n",
    "    print(f\"{pkg} OK\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Base paths and input/output locations\n",
    "# -----------------------------------------------------------------------------\n",
    "Input_file_path   = str(BASE / \"Input\")\n",
    "Temp_file_path    = str(BASE / \"Temp\")\n",
    "Output_file_path  = str(BASE / \"Output\")\n",
    "\n",
    "Fundamentals_file_path = f\"{Input_file_path}/WSFV_f_20250131.txt\"\n",
    "Current_file_path      = f\"{Input_file_path}/WSCurrent_f_20250131.txt\"\n",
    "Calendar_file_path     = f\"{Input_file_path}/WSCalendarPrd_f_20250131.txt\"\n",
    "Meta_file_path         = f\"{Input_file_path}/WSMetaData_f_20250131.txt\"\n",
    "Excel_file_path        = f\"{Input_file_path}/WS PIT Table Definitions V5 with start dates.xls\"\n",
    "\n",
    "MarketValues_file_path          = f\"{Input_file_path}/Daily MV USD\"\n",
    "MarketValues_file_path_LC       = f\"{Input_file_path}/Daily MV LC\"\n",
    "DailyTotalReturns_file_path     = f\"{Input_file_path}/Daily Returns USD\"\n",
    "DailyIndexReturns_file_path     = f\"{Input_file_path}/Daily Index Returns USD\"\n",
    "Constituents_file_path          = f\"{Input_file_path}/Constituents.01.csv\"\n",
    "UniversalMatching_file_path     = f\"{Input_file_path}/Universal Matching File\"\n",
    "\n",
    "Temp_file_path_GO  = f\"{Temp_file_path}/TempGeneralOverview\"\n",
    "Temp_file_path_EoC = f\"{Temp_file_path}/TempExtractionofCharacteristics\"\n",
    "Temp_file_path_DP  = f\"{Temp_file_path}/TempDataPreparation\"\n",
    "Temp_file_path_A   = f\"{Temp_file_path}/TempAnomalies\"\n",
    "Temp_file_path_R   = f\"{Temp_file_path}/TempRegressionModel\"\n",
    "\n",
    "Relevant_items_path   = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_A = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_B = f\"{Input_file_path}/RelevantItemsB.txt\"\n",
    "Relevant_items_path_C = f\"{Input_file_path}/RelevantItemsC.txt\"\n",
    "Relevant_items_path_D = f\"{Input_file_path}/RelevantItemsD.txt\"\n",
    "\n",
    "Subset_file_path = f\"{Temp_file_path_GO}/Subsets\"\n",
    "Fundamentals_clean_file_path = f\"{Temp_file_path_GO}/Fundamentals_clean.txt\"\n",
    "Current_clean_file_path      = f\"{Temp_file_path_GO}/Current_clean.txt\"\n",
    "Calendar_clean_file_path     = f\"{Temp_file_path_GO}/Input/Calendar_clean.txt\"\n",
    "Meta_clean_file_path         = f\"{Temp_file_path_GO}/Input/Meta_clean.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Ensure required directories exist\n",
    "# -----------------------------------------------------------------------------\n",
    "Path(Output_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_GO).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_EoC).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_A).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(Subset_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Path(Calendar_clean_file_path).parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Streaming / deduplication settings\n",
    "# -----------------------------------------------------------------------------\n",
    "CHUNK_SIZE = 2_000_000\n",
    "DATE_COL = \"PIT Date\"\n",
    "DEDUP_KEYS = [\"ID\", \"ItemCode\", DATE_COL]\n",
    "\n",
    "print(\"Paths configured. Temp outputs ->\", Temp_file_path_GO)\n",
    "print(\"Example input path ->\", Fundamentals_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nSGEkPAefY12",
   "metadata": {
    "id": "nSGEkPAefY12"
   },
   "source": [
    "# Worldscope PIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c5a3e",
   "metadata": {
    "id": "2b4c5a3e"
   },
   "source": [
    "## 1.0. Extraction of Relevant Data from the Different Input Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D_mJzRSiXgF0",
   "metadata": {
    "id": "D_mJzRSiXgF0"
   },
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0xu2ExcYXALH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15478,
     "status": "ok",
     "timestamp": 1764877849734,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "0xu2ExcYXALH",
    "outputId": "9f52c5dc-e92f-42fc-acab-57a301e392fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished imports from subset files for required codes only.\n",
      "  Codes processed: 9\n",
      "  OK: 9, Missing subset: 0, Errors: 0\n",
      "Per-code results:\n",
      "  05350: ok | rows_out=7162436 | out=/home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_FYE.txt | written\n",
      "  06001: ok | rows_out=231806 | out=/home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_CompanyName.txt | written\n",
      "  06026: ok | rows_out=131340 | out=/home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_Nation.txt | written\n",
      "  06027: ok | rows_out=121852 | out=/home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_NationCode.txt | written\n",
      "  06100: ok | rows_out=121531 | out=/home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_EntityType.txt | written\n",
      "  07021: ok | rows_out=227315 | out=/home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_SIC.txt | written\n",
      "  11503: ok | rows_out=2337 | out=/home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_ADRIndicator.txt | written\n",
      "  56027: ok | rows_out=128503 | out=/home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_CurrencyCode.txt | written\n",
      "  57034: ok | rows_out=8418018 | out=/home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_UpdateCode.txt | written\n",
      "\n",
      "All filtered datasets saved in:\n",
      "  /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORT AND FILTER REQUIRED CODES FROM SUBSET FILES INTO FILTERED OUTPUTS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   - Reads only a specific, predefined set of item codes from subset_XXXXX.txt\n",
    "#     files in the Subset_file_path directory.\n",
    "#   - Normalizes raw code strings to a consistent 5-digit numeric format so that\n",
    "#     route definitions and file names align (e.g., \"6027\" vs \"06027\").\n",
    "#   - For each required code, loads its corresponding subset file and:\n",
    "#       * Detects whether the file has named columns (ID, PIT DATE, ItemCode,\n",
    "#         Value, Frequency, FiscalPeriod) or uses positional columns.\n",
    "#       * Selects and reorders the relevant columns into standardized 4-column\n",
    "#         (Current/Meta-like) or 6-column (Calendar) structures.\n",
    "#       * Optionally transforms the value column to title case for certain\n",
    "#         attributes (e.g., Nation, EntityType) to standardize text formatting.\n",
    "#   - Writes the cleaned, standardized data into filtered_*.txt files with\n",
    "#     fixed headers in the Temp_file_path_EoC directory.\n",
    "#   - Collects detailed per-code logging on success, missing subset files, or\n",
    "#     errors, and reports summary statistics including rows written per code.\n",
    "#\n",
    "# Filtering / transformation focus:\n",
    "#   - Filters the universe of available subset_XXXXX.txt files down to a\n",
    "#     required_codes set derived from ROUTES and ROUTES_CALENDAR.\n",
    "#   - For each loaded subset, either:\n",
    "#       * Explicitly maps named columns (ID, PIT DATE, ItemCode, Value, etc.),\n",
    "#         or\n",
    "#       * Falls back to positional slicing of the first N columns to match\n",
    "#         the expected 4- or 6-column output structure.\n",
    "#   - Applies a text transformation (title casing) to selected value columns\n",
    "#     when configured (do_title=True) to normalize categorical string values.\n",
    "# =============================================================================\n",
    "\n",
    "# High-level imports needed in this cell\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Subset_file_path: directory containing subset_XXXXX.txt files (one per code)\n",
    "# Temp_file_path_EoC: directory where filtered_*.txt outputs will be written\n",
    "subset_dir = Path(Subset_file_path)   # Base directory for subset input files\n",
    "out_dir = Path(Temp_file_path_EoC)    # Output directory for filtered data\n",
    "out_dir.mkdir(parents=True, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "def normalize_code(code: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a raw code to a 5-digit numeric string.\n",
    "    - Strips whitespace.\n",
    "    - Removes all non-digit characters.\n",
    "    - If the remaining digits are 4 characters long, a leading '0' is added.\n",
    "    - Otherwise returns the digits unchanged.\n",
    "    \"\"\"\n",
    "    s = str(code).strip()                    # Convert to string and strip whitespace\n",
    "    s_digits = re.sub(r\"\\D+\", \"\", s)         # Keep only digits from the string\n",
    "    if not s_digits:                         # If no digits found, return original string\n",
    "        return s\n",
    "    # Return normalized code:\n",
    "    #   - exactly 5 digits if already length 5\n",
    "    #   - prepend '0' if length 4\n",
    "    #   - otherwise return as-is (e.g., longer codes)\n",
    "    return s_digits if len(s_digits) == 5 else (\"0\" + s_digits if len(s_digits) == 4 else s_digits)\n",
    "\n",
    "# --- ROUTES (Current/Meta-like; 4-column outputs) ---\n",
    "# Each entry maps a code to:\n",
    "#   (friendly output name, fixed header line, apply_title_case_to_value)\n",
    "ROUTES = {\n",
    "    \"6027\": (\"NationCode\",   \"ID|PIT DATE|ItemCode|NationCode\\n\",   True),\n",
    "    \"6026\": (\"Nation\",       \"ID|PIT DATE|ItemCode|Nation\\n\",       True),\n",
    "    \"6001\": (\"CompanyName\",  \"ID|PIT DATE|ItemCode|Name\\n\",         False),\n",
    "    \"6100\": (\"EntityType\",   \"ID|PIT DATE|ItemCode|EntityType\\n\",   True),\n",
    "    \"11503\":(\"ADRIndicator\", \"ID|PIT DATE|ItemCode|ADRIndicator\\n\", True),\n",
    "    \"56027\":(\"CurrencyCode\", \"ID|PIT DATE|ItemCode|CurrencyCode\\n\", True),\n",
    "    \"5350\": (\"FYE\",          \"ID|PIT DATE|ItemCode|FYE\\n\",          True),\n",
    "    \"7021\": (\"SIC\",          \"ID|PIT DATE|ItemCode|SIC\\n\",          True),\n",
    "}\n",
    "# Normalize all route keys so that they are 5-digit codes\n",
    "ROUTES = {normalize_code(k): v for k, v in ROUTES.items()}\n",
    "\n",
    "# --- Calendar route (6-column output) ---\n",
    "# Each entry maps a code to:\n",
    "#   (friendly output name, fixed header line, apply_title_case_to_value)\n",
    "ROUTES_CALENDAR = {\n",
    "    \"57034\": (\"UpdateCode\", \"ID|PIT Date|Frequency|FiscalPeriod|ItemCode|Value\\n\", True),\n",
    "}\n",
    "# Normalize calendar route keys as well\n",
    "ROUTES_CALENDAR = {normalize_code(k): v for k, v in ROUTES_CALENDAR.items()}\n",
    "\n",
    "# Combine all required codes from both four-column and calendar routes\n",
    "required_codes = set(ROUTES.keys()) | set(ROUTES_CALENDAR.keys())\n",
    "\n",
    "# Dictionaries to keep track of processing results and row counts per code\n",
    "attempt_log = {}   # code -> {status, message, rows_out, outfile}\n",
    "match_counts = {}  # code -> rows written\n",
    "\n",
    "def _write_df_with_fixed_header(df: pd.DataFrame, cols_out: list, header_text: str,\n",
    "                                out_path: Path, title_case_value: bool, value_col_name: str):\n",
    "    \"\"\"\n",
    "    Write a DataFrame to a pipe-separated text file with:\n",
    "      - A fixed header line (header_text) written manually.\n",
    "      - A subset of columns (cols_out) written without column names.\n",
    "    Optionally title-case the specified value column before writing.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Work on a copy to avoid modifying the original DataFrame\n",
    "    # If requested and the value column exists, convert its entries to title case\n",
    "    if title_case_value and value_col_name in df.columns:\n",
    "        df[value_col_name] = df[value_col_name].astype(str).map(\n",
    "            lambda x: x.title() if x is not None else x\n",
    "        )\n",
    "    # First write the fixed header, then append data rows without header\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(header_text)\n",
    "    df.to_csv(\n",
    "        out_path,\n",
    "        sep=\"|\",\n",
    "        index=False,\n",
    "        header=False,\n",
    "        mode=\"a\",\n",
    "        encoding=\"utf-8\",\n",
    "        lineterminator=\"\\n\",\n",
    "        columns=cols_out,\n",
    "    )\n",
    "\n",
    "def _import_route_fourcol(code: str, friendly_name: str, header_text: str, do_title: bool):\n",
    "    \"\"\"\n",
    "    Import a 4-column route (e.g., Current/Meta-like attributes).\n",
    "    Supports input subset files with:\n",
    "      - Named columns ID, PITDATE, ITEMCODE, VALUE, or\n",
    "      - Generic positional columns (uses either 4 or 6 source columns).\n",
    "    The function standardizes to a 4-column output and writes it to a\n",
    "    filtered_<friendly_name>.txt file in out_dir.\n",
    "    \"\"\"\n",
    "    subset_path = subset_dir / f\"subset_{code}.txt\"  # Expected input file path\n",
    "    out_path = out_dir / f\"filtered_{friendly_name}.txt\"  # Output file path\n",
    "    # If the subset file does not exist, log as missing and skip\n",
    "    if not subset_path.exists():\n",
    "        attempt_log[code] = {\n",
    "            \"status\": \"missing_subset\",\n",
    "            \"message\": f\"Missing {subset_path}\",\n",
    "            \"rows_out\": 0,\n",
    "            \"outfile\": str(out_path),\n",
    "        }\n",
    "        return\n",
    "    try:\n",
    "        # Read the subset file as a pipe-separated text file with all columns as strings\n",
    "        df = pd.read_csv(subset_path, sep=\"|\", dtype=str, engine=\"c\", encoding=\"latin1\")\n",
    "        # Normalize column names by stripping spaces\n",
    "        cols = [c.strip() for c in df.columns]\n",
    "        # Build a mapping from normalized (lowercased, no spaces) names to original names\n",
    "        colmap = {c.lower().replace(\" \", \"\"): c for c in cols}\n",
    "        # Check if the expected four logical columns are present\n",
    "        if {\"id\", \"pitdate\", \"itemcode\", \"value\"}.issubset(colmap.keys()):\n",
    "            # Use explicit named columns when available\n",
    "            wanted = [\n",
    "                colmap[\"id\"],\n",
    "                colmap[\"pitdate\"],\n",
    "                colmap[\"itemcode\"],\n",
    "                colmap[\"value\"],\n",
    "            ]\n",
    "            _write_df_with_fixed_header(\n",
    "                df,\n",
    "                wanted,\n",
    "                header_text,\n",
    "                out_path,\n",
    "                do_title,\n",
    "                value_col_name=colmap[\"value\"],\n",
    "            )\n",
    "            n = len(df)  # All rows are written out\n",
    "        else:\n",
    "            # Fallback: use positional selection when column names cannot be mapped\n",
    "            if df.shape[1] >= 6:\n",
    "                # If there are at least 6 columns, use columns [0,1,4,5]\n",
    "                wanted_pos = [0, 1, 4, 5]\n",
    "            else:\n",
    "                # Otherwise, default to the first four columns\n",
    "                wanted_pos = [0, 1, 2, 3]\n",
    "            # Slice the DataFrame to keep only the selected columns\n",
    "            df_pos = df.iloc[:, wanted_pos]\n",
    "            valname = df_pos.columns[-1]  # Last column treated as value column\n",
    "            _write_df_with_fixed_header(\n",
    "                df_pos,\n",
    "                list(df_pos.columns),\n",
    "                header_text,\n",
    "                out_path,\n",
    "                do_title,\n",
    "                value_col_name=valname,\n",
    "            )\n",
    "            n = len(df_pos)\n",
    "        # Log successful processing and number of rows written\n",
    "        attempt_log[code] = {\n",
    "            \"status\": \"ok\",\n",
    "            \"message\": \"written\",\n",
    "            \"rows_out\": n,\n",
    "            \"outfile\": str(out_path),\n",
    "        }\n",
    "        match_counts[code] = n\n",
    "    except Exception as e:\n",
    "        # Log any unexpected error during processing\n",
    "        attempt_log[code] = {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": str(e),\n",
    "            \"rows_out\": 0,\n",
    "            \"outfile\": str(out_path),\n",
    "        }\n",
    "\n",
    "def _import_route_sixcol(code: str, friendly_name: str, header_text: str, do_title: bool):\n",
    "    \"\"\"\n",
    "    Import a 6-column Calendar route.\n",
    "    Supports input subset files with:\n",
    "      - Named columns ID, PITDATE, FREQUENCY, FISCALPERIOD, ITEMCODE, VALUE, or\n",
    "      - Generic positional columns (first 6 columns used).\n",
    "    The function standardizes to a 6-column output and writes it to a\n",
    "    filtered_<friendly_name>.txt file in out_dir.\n",
    "    \"\"\"\n",
    "    subset_path = subset_dir / f\"subset_{code}.txt\"  # Expected input file path\n",
    "    out_path = out_dir / f\"filtered_{friendly_name}.txt\"  # Output file path\n",
    "    # If the subset file does not exist, log as missing and skip\n",
    "    if not subset_path.exists():\n",
    "        attempt_log[code] = {\n",
    "            \"status\": \"missing_subset\",\n",
    "            \"message\": f\"Missing {subset_path}\",\n",
    "            \"rows_out\": 0,\n",
    "            \"outfile\": str(out_path),\n",
    "        }\n",
    "        return\n",
    "    try:\n",
    "        # Read the subset file as a pipe-separated text file with all columns as strings\n",
    "        df = pd.read_csv(subset_path, sep=\"|\", dtype=str, engine=\"c\", encoding=\"latin1\")\n",
    "        # Normalize column names by stripping spaces\n",
    "        cols = [c.strip() for c in df.columns]\n",
    "        # Build a mapping from normalized (lowercased, no spaces) names to original names\n",
    "        colmap = {c.lower().replace(\" \", \"\"): c for c in cols}\n",
    "        # Check if the expected six logical columns are present\n",
    "        if {\n",
    "            \"id\",\n",
    "            \"pitdate\",\n",
    "            \"frequency\",\n",
    "            \"fiscalperiod\",\n",
    "            \"itemcode\",\n",
    "            \"value\",\n",
    "        }.issubset(colmap.keys()):\n",
    "            # Use explicit named columns when available\n",
    "            wanted = [\n",
    "                colmap[\"id\"],\n",
    "                colmap[\"pitdate\"],\n",
    "                colmap[\"frequency\"],\n",
    "                colmap[\"fiscalperiod\"],\n",
    "                colmap[\"itemcode\"],\n",
    "                colmap[\"value\"],\n",
    "            ]\n",
    "            _write_df_with_fixed_header(\n",
    "                df,\n",
    "                wanted,\n",
    "                header_text,\n",
    "                out_path,\n",
    "                do_title,\n",
    "                value_col_name=colmap[\"value\"],\n",
    "            )\n",
    "            n = len(df)\n",
    "        else:\n",
    "            # Fallback: require at least 6 columns to map positionally\n",
    "            if df.shape[1] < 6:\n",
    "                attempt_log[code] = {\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"Expected >=6 columns, found {df.shape[1]}\",\n",
    "                    \"rows_out\": 0,\n",
    "                    \"outfile\": str(out_path),\n",
    "                }\n",
    "                return\n",
    "            # Take the first six columns as the standardized output\n",
    "            wanted_pos = [0, 1, 2, 3, 4, 5]\n",
    "            df_pos = df.iloc[:, wanted_pos]\n",
    "            valname = df_pos.columns[-1]  # Last column is treated as value column\n",
    "            _write_df_with_fixed_header(\n",
    "                df_pos,\n",
    "                list(df_pos.columns),\n",
    "                header_text,\n",
    "                out_path,\n",
    "                do_title,\n",
    "                value_col_name=valname,\n",
    "            )\n",
    "            n = len(df_pos)\n",
    "        # Log successful processing and number of rows written\n",
    "        attempt_log[code] = {\n",
    "            \"status\": \"ok\",\n",
    "            \"message\": \"written\",\n",
    "            \"rows_out\": n,\n",
    "            \"outfile\": str(out_path),\n",
    "        }\n",
    "        match_counts[code] = n\n",
    "    except Exception as e:\n",
    "        # Log any unexpected error during processing\n",
    "        attempt_log[code] = {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": str(e),\n",
    "            \"rows_out\": 0,\n",
    "            \"outfile\": str(out_path),\n",
    "        }\n",
    "\n",
    "# Execute imports only for the required codes\n",
    "for code in sorted(required_codes):\n",
    "    # Decide whether to treat the code as a four-column or six-column route\n",
    "    if code in ROUTES:\n",
    "        friendly, header_text, do_title = ROUTES[code]\n",
    "        _import_route_fourcol(code, friendly, header_text, do_title)\n",
    "    elif code in ROUTES_CALENDAR:\n",
    "        friendly, header_text, do_title = ROUTES_CALENDAR[code]\n",
    "        _import_route_sixcol(code, friendly, header_text, do_title)\n",
    "    gc.collect()  # Explicitly invoke garbage collection to free memory between iterations\n",
    "\n",
    "# Summary statistics for all processed codes\n",
    "total_ok = sum(1 for v in attempt_log.values() if v[\"status\"] == \"ok\")\n",
    "total_missing = sum(1 for v in attempt_log.values() if v[\"status\"] == \"missing_subset\")\n",
    "total_error = sum(1 for v in attempt_log.values() if v[\"status\"] == \"error\")\n",
    "\n",
    "print(\"Finished imports from subset files for required codes only.\")\n",
    "print(f\"  Codes processed: {len(required_codes)}\")\n",
    "print(f\"  OK: {total_ok}, Missing subset: {total_missing}, Errors: {total_error}\")\n",
    "print(\"Per-code results:\")\n",
    "for code in sorted(required_codes):\n",
    "    info = attempt_log.get(code, {})\n",
    "    print(\n",
    "        f\"  {code}: {info.get('status','n/a')} | \"\n",
    "        f\"rows_out={info.get('rows_out',0)} | \"\n",
    "        f\"out={info.get('outfile','-')} | {info.get('message','')}\"\n",
    "    )\n",
    "print(\"\\nAll filtered datasets saved in:\")\n",
    "print(f\"  {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VtAy-TVVL-nW",
   "metadata": {
    "id": "VtAy-TVVL-nW"
   },
   "source": [
    "### Support Excel-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RtXy24nSMvdy",
   "metadata": {
    "id": "RtXy24nSMvdy"
   },
   "source": [
    "#### Current & Fundamentals tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "i3Z8JOyNMgk5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1764878176817,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "i3Z8JOyNMgk5",
    "outputId": "4ec79e08-cc30-4f2f-9ddf-aa6715996065"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 477 rows with headers [ItemCode|ItemName|Source] to /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/ValueCoding.txt\n",
      "\n",
      "=== Preview of the exported data ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>ItemName</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05006</td>\n",
       "      <td>Market Price Current</td>\n",
       "      <td>Market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05007</td>\n",
       "      <td>Market Price YTD High Current</td>\n",
       "      <td>Market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05008</td>\n",
       "      <td>Market Price YTD Low Current</td>\n",
       "      <td>Market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>05009</td>\n",
       "      <td>Date of Current Price</td>\n",
       "      <td>Market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>05091</td>\n",
       "      <td>Market Price 52 Week High Current</td>\n",
       "      <td>Market</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ItemCode                           ItemName  Source\n",
       "2    05006               Market Price Current  Market\n",
       "3    05007      Market Price YTD High Current  Market\n",
       "4    05008       Market Price YTD Low Current  Market\n",
       "5    05009              Date of Current Price  Market\n",
       "6    05091  Market Price 52 Week High Current  Market"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BUILD VALUE-CODING LOOKUP FROM EXCEL DEFINITIONS AND EXPORT TO TEXT\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   - Reads selected sheets (\"WS_Current\" and \"WS_FV\") from the PIT table\n",
    "#     definition Excel file.\n",
    "#   - Extracts only the relevant columns (ItemCode and ItemName) from each sheet\n",
    "#     and stacks them into a single combined DataFrame.\n",
    "#   - Filters the combined DataFrame so that only rows with purely numeric\n",
    "#     ItemCodes are retained (non-numeric codes are removed).\n",
    "#   - Normalizes ItemCodes by adding a leading zero to 4-digit codes so that all\n",
    "#     codes have a consistent 5-digit format.\n",
    "#   - Derives a new categorical column \"Source\" based on ItemCode prefixes\n",
    "#     (e.g., '01' -> IS, '02'/'03' -> BS, '04' -> CFS, '05' -> Market, '10' -> Other).\n",
    "#   - Exports the resulting standardized value-coding table to a pipe-separated\n",
    "#     text file with header [ItemCode|ItemName|Source] in Temp_file_path_EoC.\n",
    "#\n",
    "# Filtering / transformation focus:\n",
    "#   - Filters out any rows where ItemCode is not strictly digits using a regex.\n",
    "#   - Transforms 4-digit ItemCodes into 5-digit codes by prefixing with '0'.\n",
    "#   - Transforms ItemCodes into a categorical \"Source\" variable via prefix-based\n",
    "#     mapping, enabling downstream grouping or filtering by data source.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Inputs/outputs ---\n",
    "temp_file  = f\"{Temp_file_path_EoC}/ValueCoding.txt\"  # Target output text file path\n",
    "\n",
    "# --- Which sheets do you want? ---\n",
    "# List of sheet names from the Excel file that will be read and combined\n",
    "sheets_to_export = [\"WS_Current\", \"WS_FV\"]\n",
    "\n",
    "# --- Read the sheets into DataFrames (only col 1 + col 4) ---\n",
    "dfs = []  # List to hold DataFrames for each sheet\n",
    "for sh in sheets_to_export:\n",
    "    # Read only the first and fourth columns from the specified sheet\n",
    "    df = pd.read_excel(\n",
    "        Excel_file_path,\n",
    "        sheet_name=sh,\n",
    "        dtype=str,   # Read all values as strings to preserve leading zeros\n",
    "        usecols=[0, 3],  # Use column indices 0 and 3 (1st and 4th logical columns)\n",
    "        engine=\"xlrd\"\n",
    "    )\n",
    "    # Explicitly rename columns to standardized names\n",
    "    df.columns = [\"ItemCode\", \"ItemName\"]\n",
    "    dfs.append(df)  # Append each sheet's DataFrame to the list\n",
    "\n",
    "# --- Combine them ---\n",
    "# Vertically concatenate the DataFrames from all selected sheets into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# --- Filter out rows that do not have a number in the first column (ItemCode) ---\n",
    "# Keep only rows where ItemCode consists of digits only (no letters or symbols)\n",
    "combined_df = combined_df[\n",
    "    combined_df['ItemCode'].astype(str).str.match(r'^\\d+$')\n",
    "].copy()\n",
    "\n",
    "# --- Add leading zero to 4-digit ItemCodes ---\n",
    "# Normalize ItemCodes: if a code is a 4-character string, prepend a '0'\n",
    "combined_df['ItemCode'] = combined_df['ItemCode'].apply(\n",
    "    lambda x: f'0{x}' if isinstance(x, str) and len(x) == 4 else x\n",
    ")\n",
    "\n",
    "# --- Add a 'Source' column based on the first few characters of ItemCode ---\n",
    "def assign_source(item_code):\n",
    "    \"\"\"\n",
    "    Map an ItemCode to a high-level source category based on its prefix:\n",
    "      - '01' -> 'IS' (Income Statement)\n",
    "      - '02' or '03' -> 'BS' (Balance Sheet)\n",
    "      - '04' -> 'CFS' (Cash Flow Statement)\n",
    "      - '05' -> 'Market'\n",
    "      - '10' -> 'Other'\n",
    "      - Anything else or missing -> 'n.a.'\n",
    "    \"\"\"\n",
    "    if pd.isna(item_code):       # Handle missing values\n",
    "        return 'n.a.'\n",
    "    item_code_str = str(item_code).strip()  # Normalize to trimmed string\n",
    "    if item_code_str.startswith('01'):\n",
    "        return 'IS'\n",
    "    elif item_code_str.startswith('02') or item_code_str.startswith('03'):\n",
    "        return 'BS'\n",
    "    elif item_code_str.startswith('04'):\n",
    "        return 'CFS'\n",
    "    elif item_code_str.startswith('05'):\n",
    "        return 'Market'\n",
    "    elif item_code_str.startswith('10'):\n",
    "        return 'Other'  # ItemCodes starting with '10' are categorized as 'Other'\n",
    "    else:\n",
    "        return 'n.a.'   # All other prefixes default to 'n.a.'\n",
    "\n",
    "# Apply the source assignment function to derive the 'Source' column\n",
    "combined_df['Source'] = combined_df['ItemCode'].apply(assign_source)\n",
    "\n",
    "# --- Export to txt with header ---\n",
    "# Write the final DataFrame to a pipe-separated text file, including the header\n",
    "combined_df.to_csv(temp_file, sep=\"|\", index=False, header=True, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Exported {len(combined_df):,} rows with headers [ItemCode|ItemName|Source] to {temp_file}\")\n",
    "print(\"\\n=== Preview of the exported data ===\")\n",
    "display(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gyzaAqH3M67B",
   "metadata": {
    "id": "gyzaAqH3M67B"
   },
   "source": [
    "#### Only Fundamentals tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "GzR3CGxtMmND",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1764878176837,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "GzR3CGxtMmND",
    "outputId": "34913b7d-cb25-4307-8566-5ca403d7f5ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 380 rows with headers [ItemCode|ItemName] to /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/FundamentalsCoding.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BUILD FUNDAMENTALS-CODING LIST FROM EXCEL AND EXPORT TO TEXT\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   - Reads ItemCodes from the \"WS_FV\" sheet of the PIT table definition\n",
    "#     Excel file, using only the first column.\n",
    "#   - Combines all selected sheets (here only \"WS_FV\") into a single DataFrame.\n",
    "#   - Drops the first four rows, which typically contain metadata, titles, or\n",
    "#     non-code content that should not be part of the fundamentals code list.\n",
    "#   - Normalizes ItemCodes by adding a leading zero to 4-digit codes so that\n",
    "#     they follow a consistent 5-digit format.\n",
    "#   - Exports the resulting standardized fundamentals-coding list to a\n",
    "#     pipe-separated text file with a header in Temp_file_path_EoC.\n",
    "#\n",
    "# Filtering / transformation focus:\n",
    "#   - Structural filtering: removes the first four rows that are not part of\n",
    "#     the actual code list.\n",
    "#   - Data transformation: converts 4-character ItemCodes into 5-character\n",
    "#     codes by prefixing them with '0', ensuring a consistent code format for\n",
    "#     downstream joins, merges, or lookups.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Inputs/outputs ---\n",
    "temp_file  = f\"{Temp_file_path_EoC}/FundamentalsCoding.txt\"  # Target output text file path\n",
    "\n",
    "# --- Which sheets do you want? ---\n",
    "# List of sheet names from the Excel file that will be read and combined\n",
    "sheets_to_export = [\"WS_FV\"]\n",
    "\n",
    "# --- Read the sheets into DataFrames (only col 1) ---\n",
    "dfs = []  # List to collect DataFrames for each sheet\n",
    "for sh in sheets_to_export:\n",
    "    # Read only the first column from the specified sheet\n",
    "    df = pd.read_excel(\n",
    "        Excel_file_path,\n",
    "        sheet_name=sh,\n",
    "        dtype=str,   # Read values as strings to preserve leading zeros\n",
    "        usecols=[0],  # Only take column 1 (index 0)\n",
    "        engine=\"xlrd\"\n",
    "    )\n",
    "    # Explicitly rename the single column to a standardized name\n",
    "    df.columns = [\"ItemCode\"]\n",
    "    dfs.append(df)  # Append the sheet's DataFrame to the list\n",
    "\n",
    "# --- Combine them ---\n",
    "# Concatenate all DataFrames from selected sheets into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# --- Remove the first 4 rows ---\n",
    "# Drop the first four rows (e.g., header-like or metadata rows) and reset index\n",
    "combined_df = combined_df.iloc[4:].reset_index(drop=True)\n",
    "\n",
    "# --- Add leading zero to 4-digit ItemCodes ---\n",
    "# Normalize ItemCodes: if a code is a 4-character string, prepend a '0'\n",
    "combined_df['ItemCode'] = combined_df['ItemCode'].apply(\n",
    "    lambda x: f'0{x}' if isinstance(x, str) and len(x) == 4 else x\n",
    ")\n",
    "\n",
    "# --- Export to txt with header ---\n",
    "# Write the final list of fundamentals ItemCodes to a pipe-separated text file\n",
    "combined_df.to_csv(temp_file, sep=\"|\", index=False, header=True, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Exported {len(combined_df):,} rows with headers [ItemCode|ItemName] to {temp_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54314c0",
   "metadata": {
    "id": "f54314c0"
   },
   "source": [
    "## 2.0 General Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b759157",
   "metadata": {
    "id": "9b759157"
   },
   "source": [
    "### NationCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9mFMbJVIrm4-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1764880880473,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "9mFMbJVIrm4-",
    "outputId": "eb5c6e14-8610-46d8-98c9-389cf3c57b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_NationCode.txt ===\n",
      "          ID    PIT DATE ItemCode NationCode\n",
      "0  C00948205  2021-07-09     6027       S840\n",
      "1  C02500770  1995-12-29     6027        S25\n",
      "2  C0250077A  1999-10-01     6027        S25\n",
      "3  C0250077B  1999-10-01     6027        S25\n",
      "4  C0250077C  1999-10-01     6027        S25 \n",
      "\n",
      "File: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_NationCode.txt\n",
      "Number of Unique IDs: 121008\n",
      "Number of IDs used multiple times: 449\n",
      "Rows with error in \"NationCode\": 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUALITY CHECKS ON FILTERED NATION CODE DATA\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   - Reads a pre-filtered NationCode dataset from Temp_file_path_EoC.\n",
    "#   - Computes basic ID-level statistics:\n",
    "#       * Number of unique IDs in the file.\n",
    "#       * Number of IDs that appear more than once (indicating multiple records\n",
    "#         per entity).\n",
    "#   - Evaluates data quality for the NationCode column by:\n",
    "#       * Normalizing values to strings and stripping whitespace.\n",
    "#       * Flagging rows where NationCode is missing, empty, or contains\n",
    "#         placeholder/null-like values such as \"na\", \"nan\", \"null\", etc.\n",
    "#   - Reports the number of rows that fail this quality check so the extent of\n",
    "#     invalid NationCode entries is visible.\n",
    "#\n",
    "# Filtering / transformation focus:\n",
    "#   - Creates a boolean error_mask over rows based on transformed NationCode\n",
    "#     (whitespace-stripped, lowercased), identifying missing or invalid entries.\n",
    "#   - Uses this mask to count how many rows have problematic NationCode values,\n",
    "#     which can be used to filter those rows out in downstream processing.\n",
    "# =============================================================================\n",
    "\n",
    "# Paths\n",
    "FILE = f'{Temp_file_path_EoC}/filtered_NationCode.txt'  # Path to the filtered NationCode file\n",
    "\n",
    "VALUE_COL = \"NationCode\"  # Name of the value column to check for data quality\n",
    "\n",
    "# Read\n",
    "df = pd.read_csv(\n",
    "    FILE,\n",
    "    sep=\"|\",          # Use pipe as the column separator\n",
    "    encoding=\"utf-8\", # Assume UTF-8 encoding\n",
    "    dtype=str,        # Read all columns as strings for consistent processing\n",
    "    keep_default_na=True  # Let pandas interpret standard NA markers as NaN\n",
    ")\n",
    "\n",
    "# Quick preview of the loaded data for sanity check\n",
    "print(f\"\\n=== Preview of {FILE} ===\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Stats\n",
    "# Count how many distinct entities (IDs) are present\n",
    "unique_ids = df[\"ID\"].nunique()\n",
    "\n",
    "# Count how many IDs occur more than once (multiple records per ID)\n",
    "multi_used_ids = (df[\"ID\"].value_counts() > 1).sum()\n",
    "\n",
    "# Work with the value column as pandas StringDtype for robust string operations\n",
    "val = df[VALUE_COL].astype(\"string\")\n",
    "\n",
    "# Strip leading/trailing whitespace to normalize text values\n",
    "val_stripped = val.str.strip()\n",
    "\n",
    "# Build a mask of rows where NationCode is considered invalid:\n",
    "#   - missing (NaN),\n",
    "#   - empty after stripping,\n",
    "#   - or equal (case-insensitive) to typical placeholder/null strings\n",
    "error_mask = val.isna() | (\n",
    "    (val_stripped == \"\") |\n",
    "    val_stripped.str.lower().isin(\n",
    "        {\"na\", \"nan\", \"null\", \"none\", \"n/a\", \"#n/a\", \"n\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Total number of rows with invalid NationCode values\n",
    "error_rows = int(error_mask.sum())\n",
    "\n",
    "# Print summary statistics about ID usage and NationCode quality\n",
    "print(f\"File: {FILE}\")\n",
    "print(f\"Number of Unique IDs: {unique_ids}\")\n",
    "print(f\"Number of IDs used multiple times: {multi_used_ids}\")\n",
    "print(f'Rows with error in \"{VALUE_COL}\": {error_rows}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadec55",
   "metadata": {
    "id": "3cadec55"
   },
   "source": [
    "### Nation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4098944d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1764880880524,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "4098944d",
    "outputId": "789d1576-c58a-4ff9-b58f-2298a6dacf60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_Nation.txt ===\n",
      "          ID    PIT DATE ItemCode      Nation\n",
      "0  C00948205  2021-07-09     6026     'Canada\n",
      "1  C02500770  1999-10-01     6026  'Argentina\n",
      "2  C0250077A  1999-10-01     6026  'Argentina\n",
      "3  C0250077B  1999-10-01     6026  'Argentina\n",
      "4  C0250077C  1999-10-01     6026  'Argentina \n",
      "\n",
      "File: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_Nation.txt\n",
      "Number of Unique IDs: 120964\n",
      "Number of IDs used multiple times: 7282\n",
      "Rows with error in \"Nation\": 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUALITY CHECKS ON FILTERED NATION DATA\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   - Reads a pre-filtered Nation dataset from Temp_file_path_EoC.\n",
    "#   - Computes basic ID-level statistics:\n",
    "#       * Number of unique IDs present in the file.\n",
    "#       * Number of IDs that occur more than once (indicating multiple records\n",
    "#         per entity).\n",
    "#   - Evaluates data quality for the Nation column by:\n",
    "#       * Converting values to a pandas string type and stripping whitespace.\n",
    "#       * Flagging rows where Nation is missing, empty, or matches common\n",
    "#         placeholder/null-like strings (e.g., \"na\", \"null\", \"n/a\").\n",
    "#   - Reports how many rows have invalid Nation values, providing an overview\n",
    "#     of potential data quality issues in this attribute.\n",
    "#\n",
    "# Filtering / transformation focus:\n",
    "#   - Constructs a boolean error_mask over rows based on normalized Nation\n",
    "#     values (whitespace-stripped and lowercased).\n",
    "#   - Uses this mask to count rows with missing or invalid Nation entries,\n",
    "#     which can later be used to filter such records or impute values.\n",
    "# =============================================================================\n",
    "\n",
    "# Paths\n",
    "FILE = f'{Temp_file_path_EoC}/filtered_Nation.txt'  # Path to the filtered Nation file\n",
    "\n",
    "VALUE_COL = \"Nation\"  # Column whose content will be checked for quality issues\n",
    "\n",
    "# Read the filtered Nation file into a DataFrame\n",
    "df = pd.read_csv(\n",
    "    FILE,\n",
    "    sep=\"|\",           # Pipe-separated file\n",
    "    encoding=\"utf-8\",  # UTF-8 encoding\n",
    "    dtype=str,         # Read all columns as strings for consistent handling\n",
    "    keep_default_na=True  # Interpret standard NA markers as NaN\n",
    ")\n",
    "\n",
    "# Display a small preview of the loaded data\n",
    "print(f\"\\n=== Preview of {FILE} ===\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Compute statistics on ID usage\n",
    "unique_ids = df[\"ID\"].nunique()                 # Number of distinct IDs\n",
    "multi_used_ids = (df[\"ID\"].value_counts() > 1).sum()  # IDs that appear more than once\n",
    "\n",
    "# Work with the Nation column as a pandas StringDtype for robust string operations\n",
    "val = df[VALUE_COL].astype(\"string\")\n",
    "\n",
    "# Strip leading and trailing whitespace to normalize Nation values\n",
    "val_stripped = val.str.strip()\n",
    "\n",
    "# Build a mask of rows where Nation is considered invalid:\n",
    "#   - missing (NaN)\n",
    "#   - empty string after stripping\n",
    "#   - equal (case-insensitive) to typical placeholder/null markers\n",
    "error_mask = val.isna() | (\n",
    "    (val_stripped == \"\") |\n",
    "    val_stripped.str.lower().isin(\n",
    "        {\"na\", \"nan\", \"null\", \"none\", \"n/a\", \"#n/a\", \"n\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Count the number of rows with invalid Nation values\n",
    "error_rows = int(error_mask.sum())\n",
    "\n",
    "# Print summary statistics about ID usage and Nation data quality\n",
    "print(f\"File: {FILE}\")\n",
    "print(f\"Number of Unique IDs: {unique_ids}\")\n",
    "print(f\"Number of IDs used multiple times: {multi_used_ids}\")\n",
    "print(f'Rows with error in \"{VALUE_COL}\": {error_rows}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149764d",
   "metadata": {
    "id": "c149764d"
   },
   "source": [
    "### CompanyName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b1cc657",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1764880880573,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "2b1cc657",
    "outputId": "e497145e-d3b3-4a54-c766-1536bbe19649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_CompanyName.txt ===\n",
      "          ID    PIT DATE ItemCode                             Name\n",
      "0  C00948205  2021-07-09     6001   'AGRIFORCE GROWING SYSTEMS LTD\n",
      "1  C02500770  1995-12-29     6001            'SEVEL ARGENTINA S.A.\n",
      "2  C02500770  2000-06-02     6001  'PEUGEOT CITROEN ARGENTINA S.A.\n",
      "3  C0250077A  1999-10-01     6001            'SEVEL ARGENTINA S.A.\n",
      "4  C0250077A  2000-06-02     6001  'PEUGEOT CITROEN ARGENTINA S.A. \n",
      "\n",
      "File: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_CompanyName.txt\n",
      "Number of Unique IDs: 121008\n",
      "Number of IDs used multiple times: 54129\n",
      "Rows with error in \"Name\": 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUALITY CHECKS ON FILTERED COMPANY NAME DATA\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   - Loads the filtered company name dataset from Temp_file_path_EoC.\n",
    "#   - Calculates key ID-level statistics:\n",
    "#       * Total number of unique IDs.\n",
    "#       * Number of IDs that appear more than once (indicating multiple\n",
    "#         company name records per entity).\n",
    "#   - Performs a quality assessment of the \"Name\" column by:\n",
    "#       * Converting to pandas' string type, trimming whitespace, and\n",
    "#         standardizing text for comparison.\n",
    "#       * Identifying rows where Name is missing, empty, or equal to\n",
    "#         placeholder/null-like terms such as \"na\", \"none\", or \"n/a\".\n",
    "#   - Reports the number of rows in which Name values fail these checks,\n",
    "#     highlighting potential data quality issues for further cleaning.\n",
    "#\n",
    "# Filtering / transformation focus:\n",
    "#   - Creates an error_mask based on standardized Name values, allowing the\n",
    "#     dataset to be further filtered or remediated in subsequent steps.\n",
    "# =============================================================================\n",
    "\n",
    "# Paths\n",
    "FILE = f'{Temp_file_path_EoC}/filtered_CompanyName.txt'  # Path to filtered company name file\n",
    "\n",
    "VALUE_COL = \"Name\"  # Column to validate for completeness and correctness\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\n",
    "    FILE,\n",
    "    sep=\"|\",            # File is pipe-separated\n",
    "    encoding=\"utf-8\",   # Assume UTF-8 encoding\n",
    "    dtype=str,          # Read all fields as strings for consistent processing\n",
    "    keep_default_na=True  # Interpret standard NA-like tokens as NaN\n",
    ")\n",
    "\n",
    "# Display a small preview for verification\n",
    "print(f\"\\n=== Preview of {FILE} ===\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# ID-level stats\n",
    "unique_ids = df[\"ID\"].nunique()                    # Number of distinct IDs\n",
    "multi_used_ids = (df[\"ID\"].value_counts() > 1).sum()  # IDs appearing more than once\n",
    "\n",
    "# Prepare the Name column for validation\n",
    "val = df[VALUE_COL].astype(\"string\")       # Use pandas string type\n",
    "val_stripped = val.str.strip()             # Remove surrounding whitespace\n",
    "\n",
    "# Build a mask identifying rows with invalid or missing Name entries\n",
    "error_mask = val.isna() | (\n",
    "    (val_stripped == \"\") |\n",
    "    val_stripped.str.lower().isin(\n",
    "        {\"na\", \"nan\", \"null\", \"none\", \"n/a\", \"#n/a\", \"n\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Count problematic rows\n",
    "error_rows = int(error_mask.sum())\n",
    "\n",
    "# Output summary statistics\n",
    "print(f\"File: {FILE}\")\n",
    "print(f\"Number of Unique IDs: {unique_ids}\")\n",
    "print(f\"Number of IDs used multiple times: {multi_used_ids}\")\n",
    "print(f'Rows with error in \"{VALUE_COL}\": {error_rows}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992bb4d",
   "metadata": {
    "id": "f992bb4d"
   },
   "source": [
    "### EntityType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "596346fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1764880880612,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "596346fb",
    "outputId": "bb3fb4be-fcbd-434b-94cc-7bbaeae465a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_EntityType.txt ===\n",
      "          ID    PIT DATE ItemCode EntityType\n",
      "0  C00948205  2021-07-09     6100         'C\n",
      "1  C02500770  1999-10-01     6100         'C\n",
      "2  C0250077A  1999-10-01     6100         'S\n",
      "3  C0250077B  1999-10-01     6100         'S\n",
      "4  C0250077C  1999-10-01     6100         'S \n",
      "\n",
      "File: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_EntityType.txt\n",
      "Number of Unique IDs: 121000\n",
      "Number of IDs used multiple times: 401\n",
      "Rows with error in \"EntityType\": 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUALITY CHECKS ON FILTERED ENTITY TYPE DATA\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   - Loads the filtered EntityType dataset from Temp_file_path_EoC.\n",
    "#   - Computes two core ID-level metrics:\n",
    "#       * Count of unique IDs present.\n",
    "#       * Count of IDs that occur multiple times.\n",
    "#   - Performs quality validation of the EntityType field by:\n",
    "#       * Converting values to pandas' string type.\n",
    "#       * Stripping whitespace to normalize text input.\n",
    "#       * Marking rows as invalid if EntityType is missing, empty, or equal\n",
    "#         to common placeholder/null-like tokens (e.g., \"na\", \"null\", \"n/a\").\n",
    "#   - Reports the total number of rows where EntityType fails these checks.\n",
    "#\n",
    "# Filtering / transformation focus:\n",
    "#   - Constructs a boolean mask (error_mask) based on transformed EntityType\n",
    "#     values, enabling downstream filtering or remediation of invalid rows.\n",
    "# =============================================================================\n",
    "\n",
    "# Paths\n",
    "FILE = f'{Temp_file_path_EoC}/filtered_EntityType.txt'  # Input file path for EntityType data\n",
    "\n",
    "VALUE_COL = \"EntityType\"  # Column to evaluate for quality issues\n",
    "\n",
    "# Load the dataset containing ID, PIT Date, ItemCode, and EntityType\n",
    "df = pd.read_csv(\n",
    "    FILE,\n",
    "    sep=\"|\",            # File is pipe-delimited\n",
    "    encoding=\"utf-8\",   # Assume UTF-8 encoding\n",
    "    dtype=str,          # Load all fields as strings\n",
    "    keep_default_na=True  # Treat standard NA tokens as NaN\n",
    ")\n",
    "\n",
    "# Display a preview of the loaded data\n",
    "print(f\"\\n=== Preview of {FILE} ===\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Compute ID-level usage stats\n",
    "unique_ids = df[\"ID\"].nunique()                     # Number of unique IDs\n",
    "multi_used_ids = (df[\"ID\"].value_counts() > 1).sum()  # IDs with more than one record\n",
    "\n",
    "# Prepare the EntityType column for validation\n",
    "val = df[VALUE_COL].astype(\"string\")   # Convert to pandas string dtype\n",
    "val_stripped = val.str.strip()         # Remove leading/trailing whitespace\n",
    "\n",
    "# Build mask of invalid or missing EntityType entries\n",
    "error_mask = val.isna() | (\n",
    "    (val_stripped == \"\") |\n",
    "    val_stripped.str.lower().isin(\n",
    "        {\"na\", \"nan\", \"null\", \"none\", \"n/a\", \"#n/a\", \"n\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Count invalid rows\n",
    "error_rows = int(error_mask.sum())\n",
    "\n",
    "# Output summary statistics\n",
    "print(f\"File: {FILE}\")\n",
    "print(f\"Number of Unique IDs: {unique_ids}\")\n",
    "print(f\"Number of IDs used multiple times: {multi_used_ids}\")\n",
    "print(f'Rows with error in \"{VALUE_COL}\": {error_rows}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72c15c",
   "metadata": {
    "id": "ed72c15c"
   },
   "source": [
    "### ADRIndicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b96cad2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1764880880650,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "b96cad2d",
    "outputId": "be6df75f-87b2-4db7-a3f3-157c8d61dbf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_ADRIndicator.txt ===\n",
      "          ID    PIT DATE ItemCode ADRIndicator\n",
      "0  C036F63D0  2012-05-28    11503           'X\n",
      "1  C036F63D0  2014-01-15    11503           N'\n",
      "2  C056879S0  2018-04-23    11503           'X\n",
      "3  C2461T100  2004-04-23    11503           'X\n",
      "4  C2461T100  2005-06-10    11503           N' \n",
      "\n",
      "File: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_ADRIndicator.txt\n",
      "Number of Unique IDs: 2297\n",
      "Number of IDs used multiple times: 28\n",
      "Rows with error in \"ADRIndicator\": 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUALITY CHECKS ON FILTERED ADR INDICATOR DATA\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   - Loads the filtered ADRIndicator dataset from Temp_file_path_EoC.\n",
    "#   - Computes two key ID-based metrics:\n",
    "#       * Number of unique IDs present in the file.\n",
    "#       * Number of IDs appearing more than once (multiple PIT records).\n",
    "#   - Performs a quality validation of the ADRIndicator field by:\n",
    "#       * Converting values to pandas' string type.\n",
    "#       * Stripping whitespace to normalize the values.\n",
    "#       * Identifying rows where ADRIndicator is missing, blank, or matches\n",
    "#         common placeholder/null-like patterns such as \"na\", \"null\", \"none\".\n",
    "#   - Counts the total number of invalid ADRIndicator entries and reports all\n",
    "#     summary statistics.\n",
    "#\n",
    "# Filtering / transformation focus:\n",
    "#   - Builds a boolean mask (error_mask) to identify invalid ADRIndicator values.\n",
    "#   - Normalizes ADRIndicator through whitespace stripping and lowercasing\n",
    "#     before applying validation conditions.\n",
    "# =============================================================================\n",
    "\n",
    "# Paths\n",
    "FILE = f'{Temp_file_path_EoC}/filtered_ADRIndicator.txt'  # Path to the filtered ADRIndicator dataset\n",
    "\n",
    "VALUE_COL = \"ADRIndicator\"  # Column being validated\n",
    "\n",
    "# Load the dataset as a DataFrame\n",
    "df = pd.read_csv(\n",
    "    FILE,\n",
    "    sep=\"|\",            # Pipe-separated file\n",
    "    encoding=\"utf-8\",   # UTF-8 encoding\n",
    "    dtype=str,          # Treat all fields as strings\n",
    "    keep_default_na=True  # Convert standard NA tokens to NaN\n",
    ")\n",
    "\n",
    "# Display the first few rows for visual inspection\n",
    "print(f\"\\n=== Preview of {FILE} ===\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Compute ID-based usage statistics\n",
    "unique_ids = df[\"ID\"].nunique()                     # Number of distinct IDs\n",
    "multi_used_ids = (df[\"ID\"].value_counts() > 1).sum()  # Number of IDs with multiple rows\n",
    "\n",
    "# Prepare ADRIndicator column for quality checks\n",
    "val = df[VALUE_COL].astype(\"string\")   # Convert to string dtype for uniform handling\n",
    "val_stripped = val.str.strip()         # Strip whitespace for normalization\n",
    "\n",
    "# Construct mask to identify invalid or missing ADRIndicator entries\n",
    "error_mask = val.isna() | (\n",
    "    (val_stripped == \"\") |\n",
    "    val_stripped.str.lower().isin(\n",
    "        {\"na\", \"nan\", \"null\", \"none\", \"n/a\", \"#n/a\", \"n\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Total number of problematic rows\n",
    "error_rows = int(error_mask.sum())\n",
    "\n",
    "# Output summary information\n",
    "print(f\"File: {FILE}\")\n",
    "print(f\"Number of Unique IDs: {unique_ids}\")\n",
    "print(f\"Number of IDs used multiple times: {multi_used_ids}\")\n",
    "print(f'Rows with error in \"{VALUE_COL}\": {error_rows}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ce262",
   "metadata": {
    "id": "1e1ce262"
   },
   "source": [
    "### Currency Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a63a41ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1764880880689,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "a63a41ee",
    "outputId": "11154915-ef92-4e13-b9fe-54ec5f0586c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_CurrencyCode.txt ===\n",
      "          ID    PIT DATE ItemCode CurrencyCode\n",
      "0  C00948205  2021-07-09    56027         'Usd\n",
      "1  C02500770  1995-12-29    56027         'Ars\n",
      "2  C0250077A  1999-10-01    56027         'Ars\n",
      "3  C0250077B  1999-10-01    56027         'Ars\n",
      "4  C0250077C  1999-10-01    56027         'Ars \n",
      "\n",
      "File: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_CurrencyCode.txt\n",
      "Number of Unique IDs: 121007\n",
      "Number of IDs used multiple times: 7395\n",
      "Rows with error in \"CurrencyCode\": 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUALITY CHECKS ON FILTERED CURRENCY CODE DATA\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   - Loads the filtered CurrencyCode dataset from Temp_file_path_EoC.\n",
    "#   - Computes ID-level metrics:\n",
    "#       * Total number of unique IDs.\n",
    "#       * Number of IDs that appear in multiple rows.\n",
    "#   - Performs quality validation of the CurrencyCode column by:\n",
    "#       * Converting all values to pandas string type for consistent handling.\n",
    "#       * Stripping whitespace to normalize the values.\n",
    "#       * Flagging entries that are missing, empty, or equal to common\n",
    "#         placeholder/null-like terms (e.g., \"na\", \"null\", \"none\").\n",
    "#   - Reports how many CurrencyCode entries fail these validation rules.\n",
    "#\n",
    "# Filtering / transformation focus:\n",
    "#   - Constructs a boolean quality mask (error_mask) from normalized text values.\n",
    "#   - Enables downstream filtering or correction of invalid CurrencyCode entries.\n",
    "# =============================================================================\n",
    "\n",
    "# Paths\n",
    "FILE = f'{Temp_file_path_EoC}/filtered_CurrencyCode.txt'  # Path to currency code file\n",
    "\n",
    "VALUE_COL = \"CurrencyCode\"  # Column to evaluate\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\n",
    "    FILE,\n",
    "    sep=\"|\",            # Pipe-delimited file\n",
    "    encoding=\"utf-8\",   # UTF-8 encoding\n",
    "    dtype=str,          # Load all fields as strings\n",
    "    keep_default_na=True  # Handle standard NA-like values as NaN\n",
    ")\n",
    "\n",
    "# Preview first rows of the dataset\n",
    "print(f\"\\n=== Preview of {FILE} ===\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Compute ID-level metrics\n",
    "unique_ids = df[\"ID\"].nunique()                     # Distinct IDs\n",
    "multi_used_ids = (df[\"ID\"].value_counts() > 1).sum()  # IDs with multiple rows\n",
    "\n",
    "# Normalize value column for validation\n",
    "val = df[VALUE_COL].astype(\"string\")   # Convert to pandas string dtype\n",
    "val_stripped = val.str.strip()         # Remove leading/trailing whitespace\n",
    "\n",
    "# Identify missing or invalid CurrencyCode values\n",
    "error_mask = val.isna() | (\n",
    "    (val_stripped == \"\") |\n",
    "    val_stripped.str.lower().isin(\n",
    "        {\"na\", \"nan\", \"null\", \"none\", \"n/a\", \"#n/a\", \"n\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Count problematic rows\n",
    "error_rows = int(error_mask.sum())\n",
    "\n",
    "# Output summary\n",
    "print(f\"File: {FILE}\")\n",
    "print(f\"Number of Unique IDs: {unique_ids}\")\n",
    "print(f\"Number of IDs used multiple times: {multi_used_ids}\")\n",
    "print(f'Rows with error in \"{VALUE_COL}\": {error_rows}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba10b58",
   "metadata": {
    "id": "1ba10b58"
   },
   "source": [
    "### FYE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c57b383c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1764880880727,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "c57b383c",
    "outputId": "f6801b88-d22a-4e1d-9055-de1cf314debd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_FYE.txt ===\n",
      "          ID    PIT DATE ItemCode        FYE\n",
      "0  C00948205  2021-07-09     5350  D20181231\n",
      "1  C00948205  2021-07-09     5350  D20191231\n",
      "2  C00948205  2021-07-09     5350  D20201231\n",
      "3  C00948205  2021-07-09     5350  D20190930\n",
      "4  C00948205  2021-07-09     5350  D20191231 \n",
      "\n",
      "File: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_FYE.txt\n",
      "Number of Unique IDs: 120948\n",
      "Number of IDs used multiple times: 120564\n",
      "Rows with error in \"FYE\": 0\n",
      "\n",
      "Summary:\n",
      "Total rows: 7162436\n",
      "Unique IDs: 120948\n",
      "IDs with >1 row: 120564\n",
      "Rows with initial error in \"FYE\": 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUALITY CHECKS AND ERROR HANDLING ON FILTERED FYE DATA\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   - Attempts to read a pre-filtered FYE dataset from Temp_file_path_EoC.\n",
    "#   - Validates that the expected column (FYE) is present in the data.\n",
    "#   - If available, computes:\n",
    "#       * Number of unique IDs.\n",
    "#       * Number of IDs with multiple rows.\n",
    "#       * Number of rows where FYE is missing, empty, or a placeholder/null-like\n",
    "#         value.\n",
    "#   - Prints a concise summary of dataset size and FYE data quality.\n",
    "#   - Handles missing file and unexpected errors with informative messages.\n",
    "#\n",
    "# Filtering / transformation focus:\n",
    "#   - Builds an error_mask over rows based on normalized FYE values\n",
    "#     (whitespace-stripped and lowercased) to identify problematic entries.\n",
    "#   - Uses this mask to report how many rows have invalid FYE values, which\n",
    "#     supports later filtering or correction steps.\n",
    "# =============================================================================\n",
    "\n",
    "# Paths\n",
    "FILE = f'{Temp_file_path_EoC}/filtered_FYE.txt'  # Path to the filtered FYE file\n",
    "\n",
    "VALUE_COL = \"FYE\"  # Name of the FYE column to validate and analyze\n",
    "\n",
    "try:\n",
    "    # Read the filtered FYE file into a DataFrame\n",
    "    fye_df = pd.read_csv(\n",
    "        FILE,\n",
    "        sep=\"|\",           # Pipe-separated text file\n",
    "        encoding=\"utf-8\",  # UTF-8 encoding\n",
    "        dtype=str,         # Read all columns as strings\n",
    "        keep_default_na=True  # Interpret default NA markers as NaN\n",
    "    )\n",
    "\n",
    "    # Show a preview of the loaded data\n",
    "    print(f\"\\n=== Preview of {FILE} ===\")\n",
    "    print(fye_df.head(), \"\\n\")\n",
    "\n",
    "    # Check if the expected FYE column exists in the DataFrame\n",
    "    if VALUE_COL not in fye_df.columns:\n",
    "        print(f\"Error: Column '{VALUE_COL}' not found in {FILE}.\")\n",
    "        print(f\"Available columns are: {fye_df.columns.tolist()}\")\n",
    "        # No further processing if the required column is missing\n",
    "    else:\n",
    "        # Compute statistics on ID usage\n",
    "        unique_ids = fye_df[\"ID\"].nunique()\n",
    "        multi_used_ids = (fye_df[\"ID\"].value_counts() > 1).sum()\n",
    "\n",
    "        # Work with the FYE column as pandas StringDtype\n",
    "        val = fye_df[VALUE_COL].astype(\"string\")\n",
    "\n",
    "        # Strip leading and trailing whitespace from FYE values\n",
    "        val_stripped = val.str.strip()\n",
    "\n",
    "        # Build a mask for rows where FYE is considered invalid:\n",
    "        #   - missing (NaN),\n",
    "        #   - empty after stripping,\n",
    "        #   - or equal (case-insensitive) to typical placeholder/null markers\n",
    "        error_mask = val.isna() | (\n",
    "            (val_stripped == \"\") |\n",
    "            val_stripped.str.lower().isin(\n",
    "                {\"na\", \"nan\", \"null\", \"none\", \"n/a\", \"#n/a\", \"n\"}\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Count rows with invalid FYE values\n",
    "        error_rows = int(error_mask.sum())\n",
    "\n",
    "        # Print main statistics and FYE quality indicators\n",
    "        print(f\"File: {FILE}\")\n",
    "        print(f\"Number of Unique IDs: {unique_ids}\")\n",
    "        print(f\"Number of IDs used multiple times: {multi_used_ids}\")\n",
    "        print(f'Rows with error in \"{VALUE_COL}\": {error_rows}')\n",
    "\n",
    "        # --- Summary overview ---\n",
    "        # High-level summary of dataset size and FYE quality metrics\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"Total rows: {len(fye_df)}\")\n",
    "        print(f\"Unique IDs: {unique_ids}\")\n",
    "        print(f\"IDs with >1 row: {multi_used_ids}\")\n",
    "        print(f'Rows with initial error in \"{VALUE_COL}\": {error_rows}')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Handle scenario where the FYE file has not been created or is missing\n",
    "    print(f\"Error: File not found at {FILE}. Please ensure the file was created in the previous steps.\")\n",
    "except Exception as e:\n",
    "    # Catch any other unexpected exceptions and display the message\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5acfb",
   "metadata": {
    "id": "2ba5acfb"
   },
   "source": [
    "### Update Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f758fe29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1764880880765,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "f758fe29",
    "outputId": "8dcaceda-7730-49aa-d09b-8a56f1c02235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_UpdateCode.txt ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1985</td>\n",
       "      <td>57034</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1986</td>\n",
       "      <td>57034</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1987</td>\n",
       "      <td>57034</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1988</td>\n",
       "      <td>57034</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1989</td>\n",
       "      <td>57034</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT Date Frequency FiscalPeriod ItemCode Value\n",
       "0  C02500770  1995-12-29         A         1985    57034    S3\n",
       "1  C02500770  1995-12-29         A         1986    57034    S3\n",
       "2  C02500770  1995-12-29         A         1987    57034    S3\n",
       "3  C02500770  1995-12-29         A         1988    57034    S3\n",
       "4  C02500770  1995-12-29         A         1989    57034    S3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_UpdateCode.txt\n",
      "Number of Unique IDs: 111895\n",
      "Number of IDs used multiple times: 111464\n",
      "Rows with error in \"Value\": 0\n",
      "\n",
      "Unique values in 'Value' (9 total):\n",
      "['Ns', 'S0', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUALITY CHECKS AND UNIQUE VALUE OVERVIEW FOR UPDATE CODE DATA\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   - Loads the filtered UpdateCode dataset created earlier.\n",
    "#   - Validates that the expected Value column (containing update codes) exists.\n",
    "#   - Computes key dataset metrics:\n",
    "#       * Number of unique IDs.\n",
    "#       * Number of IDs appearing more than once.\n",
    "#       * Number of rows with invalid or missing update code values.\n",
    "#   - Extracts all unique update code values and produces a sorted list for\n",
    "#     manual inspection.\n",
    "#\n",
    "# Filtering / transformation focus:\n",
    "#   - Normalizes the Value column using pandas' string dtype and whitespace\n",
    "#     stripping.\n",
    "#   - Applies a validation mask that flags null-like or empty entries, enabling\n",
    "#     downstream filtering or quality checks.\n",
    "# =============================================================================\n",
    "\n",
    "# Paths\n",
    "FILE = f'{Temp_file_path_EoC}/filtered_UpdateCode.txt'  # Path to the filtered UpdateCode file\n",
    "\n",
    "VALUE_COL = \"Value\"  # Column expected to hold the update code values\n",
    "\n",
    "try:\n",
    "    # Load the filtered update code dataset\n",
    "    df = pd.read_csv(\n",
    "        FILE,\n",
    "        sep=\"|\",            # File is pipe-delimited\n",
    "        encoding=\"utf-8\",   # Use UTF-8 encoding\n",
    "        dtype=str,          # Load all columns as strings\n",
    "        keep_default_na=True  # Interpret standard NA-like tokens as NaN\n",
    "    )\n",
    "\n",
    "    # Display a preview of the first few rows\n",
    "    print(f\"\\n=== Preview of {FILE} ===\")\n",
    "    display(df.head(), \"\\n\")\n",
    "\n",
    "    # Ensure that the expected column exists\n",
    "    if VALUE_COL not in df.columns:\n",
    "        raise KeyError(\n",
    "            f\"Expected column '{VALUE_COL}' not found in {FILE}. \"\n",
    "            f\"Available columns: {df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    # Compute ID-level metrics\n",
    "    unique_ids = df[\"ID\"].nunique()                      # Number of unique entities\n",
    "    multi_used_ids = (df[\"ID\"].value_counts() > 1).sum() # Entities with multiple rows\n",
    "\n",
    "    # Normalize the Value column for quality testing\n",
    "    val = df[VALUE_COL].astype(\"string\")  # Convert to pandas string dtype\n",
    "    val_stripped = val.str.strip()        # Remove leading/trailing whitespace\n",
    "\n",
    "    # Identify invalid update code entries:\n",
    "    # - Missing values\n",
    "    # - Empty strings\n",
    "    # - Placeholder/null-like tokens\n",
    "    error_mask = val.isna() | (\n",
    "        (val_stripped == \"\") |\n",
    "        val_stripped.str.lower().isin(\n",
    "            {\"na\", \"nan\", \"null\", \"none\", \"n/a\", \"#n/a\", \"n\"}\n",
    "        )\n",
    "    )\n",
    "    error_rows = int(error_mask.sum())\n",
    "\n",
    "    # Output summary statistics\n",
    "    print(f\"File: {FILE}\")\n",
    "    print(f\"Number of Unique IDs: {unique_ids}\")\n",
    "    print(f\"Number of IDs used multiple times: {multi_used_ids}\")\n",
    "    print(f'Rows with error in \"{VALUE_COL}\": {error_rows}')\n",
    "\n",
    "    # Extract list of unique update code values (excluding NaN)\n",
    "    unique_values = df[VALUE_COL].dropna().unique().tolist()\n",
    "\n",
    "    # Sort unique values alphabetically, placing None/NaN at the end\n",
    "    unique_values.sort(key=lambda x: (x is None, x))\n",
    "\n",
    "    print(f\"\\nUnique values in '{VALUE_COL}' ({len(unique_values)} total):\")\n",
    "    print(unique_values)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # File does not exist or was not created earlier\n",
    "    print(f\"Error: File not found at {FILE}.\")\n",
    "except Exception as e:\n",
    "    # Handle any other unexpected errors\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53z7zaj_eh8a",
   "metadata": {
    "id": "53z7zaj_eh8a"
   },
   "source": [
    "### SIC Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "SEh34CwReh8i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1764880880804,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "SEh34CwReh8i",
    "outputId": "5ebf8f06-3525-4057-b78a-ed1f0d0b840e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_SIC.txt ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>SIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>7021</td>\n",
       "      <td>S3711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>7021</td>\n",
       "      <td>S3312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2001-11-30</td>\n",
       "      <td>7021</td>\n",
       "      <td>S3321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2002-11-15</td>\n",
       "      <td>7021</td>\n",
       "      <td>S3317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2005-04-15</td>\n",
       "      <td>7021</td>\n",
       "      <td>S3312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE ItemCode    SIC\n",
       "0  C02500770  1995-12-29     7021  S3711\n",
       "1  C02520200  1996-05-03     7021  S3312\n",
       "2  C02520200  2001-11-30     7021  S3321\n",
       "3  C02520200  2002-11-15     7021  S3317\n",
       "4  C02520200  2005-04-15     7021  S3312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Expected column 'Value' not found. Using 'SIC' as the value column instead.\n",
      "File: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_SIC.txt\n",
      "Using column 'SIC' as value column.\n",
      "Number of Unique IDs: 103693\n",
      "Number of IDs used multiple times: 53756\n",
      "Rows with error in \"SIC\": 0\n",
      "\n",
      "Unique values in 'SIC' (1379 total):\n",
      "['Ns', 'S0', 'S100', 'S1000', 'S1010', 'S1011', 'S1020', 'S1021', 'S1030', 'S1031', 'S1040', 'S1041', 'S1044', 'S1061', 'S1081', 'S1090', 'S1094', 'S1099', 'S110', 'S111', 'S112', 'S115', 'S116', 'S119', 'S1200', 'S1211', 'S1220', 'S1221', 'S1222', 'S1231', 'S1240', 'S1241', 'S1300', 'S131', 'S1310', 'S1311', 'S132', 'S1320', 'S1321', 'S133', 'S134', 'S1362', 'S1371', 'S1380', 'S1381', 'S1382', 'S1389', 'S139', 'S1400', 'S1410', 'S1411', 'S1420', 'S1422', 'S1423', 'S1429', 'S1440', 'S1442', 'S1446', 'S1450', 'S1455', 'S1459', 'S1470', 'S1474', 'S1475', 'S1476', 'S1479', 'S1481', 'S1490', 'S1491', 'S1499', 'S1500', 'S1520', 'S1521', 'S1522', 'S1530', 'S1531', 'S1540', 'S1541', 'S1542', 'S160', 'S1600', 'S161', 'S1610', 'S1611', 'S1620', 'S1622', 'S1623', 'S1629', 'S170', 'S1700', 'S171', 'S1710', 'S1711', 'S172', 'S1721', 'S173', 'S1730', 'S1731', 'S174', 'S1740', 'S1741', 'S1742', 'S1743', 'S175', 'S1751', 'S1752', 'S1761', 'S1771', 'S1781', 'S179', 'S1790', 'S1791', 'S1793', 'S1794', 'S1795', 'S1796', 'S1799', 'S180', 'S181', 'S182', 'S190', 'S191', 'S200', 'S2000', 'S2010', 'S2011', 'S2013', 'S2015', 'S2020', 'S2021', 'S2022', 'S2023', 'S2024', 'S2026', 'S2030', 'S2032', 'S2033', 'S2034', 'S2035', 'S2037', 'S2038', 'S2040', 'S2041', 'S2043', 'S2044', 'S2045', 'S2046', 'S2047', 'S2048', 'S2050', 'S2051', 'S2052', 'S2053', 'S2060', 'S2061', 'S2062', 'S2063', 'S2064', 'S2066', 'S2067', 'S2068', 'S2070', 'S2074', 'S2075', 'S2076', 'S2077', 'S2079', 'S2080', 'S2082', 'S2083', 'S2084', 'S2085', 'S2086', 'S2087', 'S2090', 'S2091', 'S2092', 'S2095', 'S2096', 'S2097', 'S2098', 'S2099', 'S210', 'S2100', 'S211', 'S2110', 'S2111', 'S212', 'S2121', 'S213', 'S2131', 'S214', 'S2141', 'S219', 'S2200', 'S2210', 'S2211', 'S2220', 'S2221', 'S2230', 'S2231', 'S2240', 'S2241', 'S2250', 'S2251', 'S2252', 'S2253', 'S2254', 'S2257', 'S2258', 'S2259', 'S2260', 'S2261', 'S2262', 'S2269', 'S2270', 'S2273', 'S2280', 'S2281', 'S2282', 'S2284', 'S229', 'S2290', 'S2295', 'S2296', 'S2297', 'S2298', 'S2299', 'S2300', 'S2310', 'S2311', 'S2320', 'S2321', 'S2322', 'S2323', 'S2325', 'S2326', 'S2329', 'S2330', 'S2331', 'S2335', 'S2337', 'S2339', 'S2340', 'S2341', 'S2342', 'S2353', 'S2360', 'S2361', 'S2369', 'S2371', 'S2380', 'S2381', 'S2384', 'S2385', 'S2386', 'S2387', 'S2389', 'S2390', 'S2391', 'S2392', 'S2393', 'S2394', 'S2395', 'S2396', 'S2399', 'S2400', 'S241', 'S2410', 'S2411', 'S2420', 'S2421', 'S2426', 'S2429', 'S2430', 'S2431', 'S2434', 'S2435', 'S2436', 'S2439', 'S2441', 'S2448', 'S2449', 'S2450', 'S2451', 'S2452', 'S2490', 'S2491', 'S2493', 'S2499', 'S250', 'S2500', 'S251', 'S2510', 'S2511', 'S2512', 'S2514', 'S2515', 'S2517', 'S2519', 'S252', 'S2520', 'S2521', 'S2522', 'S253', 'S2531', 'S254', 'S2540', 'S2541', 'S2542', 'S259', 'S2590', 'S2591', 'S2599', 'S2600', 'S2610', 'S2611', 'S2620', 'S2621', 'S2630', 'S2631', 'S2650', 'S2652', 'S2653', 'S2655', 'S2656', 'S2657', 'S2670', 'S2671', 'S2672', 'S2673', 'S2674', 'S2675', 'S2676', 'S2677', 'S2678', 'S2679', 'S2700', 'S271', 'S2710', 'S2711', 'S272', 'S2721', 'S273', 'S2730', 'S2731', 'S2732', 'S2740', 'S2741', 'S2742', 'S2750', 'S2752', 'S2754', 'S2759', 'S2760', 'S2761', 'S2771', 'S2780', 'S2782', 'S2789', 'S279', 'S2790', 'S2791', 'S2796', 'S2800', 'S2810', 'S2812', 'S2813', 'S2816', 'S2819', 'S2820', 'S2821', 'S2822', 'S2823', 'S2824', 'S2830', 'S2833', 'S2834', 'S2835', 'S2836', 'S2840', 'S2841', 'S2842', 'S2843', 'S2844', 'S2850', 'S2851', 'S2860', 'S2861', 'S2865', 'S2869', 'S2870', 'S2873', 'S2874', 'S2875', 'S2879', 'S2890', 'S2891', 'S2892', 'S2893', 'S2895', 'S2899', 'S2900', 'S291', 'S2910', 'S2911', 'S2950', 'S2951', 'S2952', 'S2990', 'S2992', 'S2999', 'S3000', 'S3010', 'S3011', 'S3021', 'S3052', 'S3053', 'S3060', 'S3061', 'S3069', 'S3079', 'S3080', 'S3081', 'S3082', 'S3083', 'S3084', 'S3085', 'S3086', 'S3087', 'S3088', 'S3089', 'S3100', 'S3110', 'S3111', 'S3130', 'S3131', 'S3140', 'S3142', 'S3143', 'S3144', 'S3149', 'S3151', 'S3161', 'S3170', 'S3171', 'S3172', 'S3190', 'S3199', 'S3210', 'S3211', 'S3220', 'S3221', 'S3229', 'S3230', 'S3231', 'S3240', 'S3241', 'S3250', 'S3251', 'S3253', 'S3255', 'S3259', 'S3260', 'S3261', 'S3262', 'S3263', 'S3264', 'S3269', 'S3270', 'S3271', 'S3272', 'S3273', 'S3274', 'S3275', 'S3281', 'S3290', 'S3291', 'S3292', 'S3295', 'S3296', 'S3297', 'S3299', 'S3300', 'S3310', 'S3312', 'S3313', 'S3315', 'S3316', 'S3317', 'S3320', 'S3321', 'S3322', 'S3324', 'S3325', 'S3330', 'S3331', 'S3334', 'S3339', 'S3341', 'S3350', 'S3351', 'S3353', 'S3354', 'S3355', 'S3356', 'S3357', 'S3360', 'S3363', 'S3364', 'S3365', 'S3366', 'S3369', 'S3390', 'S3398', 'S3399', 'S3400', 'S3410', 'S3411', 'S3412', 'S3420', 'S3421', 'S3423', 'S3425', 'S3429', 'S3430', 'S3431', 'S3432', 'S3433', 'S3440', 'S3441', 'S3442', 'S3443', 'S3444', 'S3446', 'S3448', 'S3449', 'S3450', 'S3451', 'S3452', 'S3460', 'S3462', 'S3463', 'S3465', 'S3466', 'S3469', 'S3470', 'S3471', 'S3479', 'S3480', 'S3482', 'S3483', 'S3484', 'S3489', 'S3490', 'S3491', 'S3492', 'S3493', 'S3494', 'S3495', 'S3496', 'S3497', 'S3498', 'S3499', 'S3500', 'S3510', 'S3511', 'S3519', 'S3520', 'S3523', 'S3524', 'S3525', 'S3530', 'S3531', 'S3532', 'S3533', 'S3534', 'S3535', 'S3536', 'S3537', 'S3540', 'S3541', 'S3542', 'S3543', 'S3544', 'S3545', 'S3546', 'S3547', 'S3548', 'S3549', 'S3550', 'S3552', 'S3553', 'S3554', 'S3555', 'S3556', 'S3559', 'S3560', 'S3561', 'S3562', 'S3563', 'S3564', 'S3565', 'S3566', 'S3567', 'S3568', 'S3569', 'S3570', 'S3571', 'S3572', 'S3575', 'S3577', 'S3578', 'S3579', 'S3580', 'S3581', 'S3582', 'S3585', 'S3586', 'S3589', 'S3590', 'S3592', 'S3593', 'S3594', 'S3596', 'S3599', 'S3600', 'S3610', 'S3612', 'S3613', 'S3620', 'S3621', 'S3624', 'S3625', 'S3629', 'S3630', 'S3631', 'S3632', 'S3633', 'S3634', 'S3635', 'S3639', 'S3640', 'S3641', 'S3643', 'S3644', 'S3645', 'S3646', 'S3647', 'S3648', 'S3650', 'S3651', 'S3652', 'S366', 'S3660', 'S3661', 'S3663', 'S3669', 'S3670', 'S3671', 'S3672', 'S3674', 'S3675', 'S3676', 'S3677', 'S3678', 'S3679', 'S3690', 'S3691', 'S3692', 'S3694', 'S3695', 'S3699', 'S3700', 'S3710', 'S3711', 'S3713', 'S3714', 'S3715', 'S3716', 'S3720', 'S3721', 'S3724', 'S3728', 'S3730', 'S3731', 'S3732', 'S3743', 'S3751', 'S3761', 'S3764', 'S3769', 'S3790', 'S3792', 'S3795', 'S3799', 'S3810', 'S3812', 'S3820', 'S3821', 'S3822', 'S3823', 'S3824', 'S3825', 'S3826', 'S3827', 'S3829', 'S3840', 'S3841', 'S3842', 'S3843', 'S3844', 'S3845', 'S3850', 'S3851', 'S3861', 'S3870', 'S3873', 'S3885', 'S3910', 'S3911', 'S3914', 'S3915', 'S3931', 'S3940', 'S3942', 'S3944', 'S3949', 'S3950', 'S3951', 'S3952', 'S3953', 'S3955', 'S3960', 'S3961', 'S3965', 'S3990', 'S3991', 'S3993', 'S3995', 'S3996', 'S3999', 'S4000', 'S4010', 'S4011', 'S4013', 'S4100', 'S4110', 'S4111', 'S4119', 'S4121', 'S4130', 'S4131', 'S4140', 'S4141', 'S4142', 'S4151', 'S4173', 'S4200', 'S421', 'S4210', 'S4212', 'S4213', 'S4214', 'S4215', 'S4220', 'S4221', 'S4222', 'S4225', 'S4226', 'S4230', 'S4231', 'S4310', 'S4311', 'S4320', 'S4391', 'S4400', 'S4410', 'S4412', 'S4420', 'S4424', 'S4432', 'S4440', 'S4449', 'S4480', 'S4481', 'S4482', 'S4489', 'S4490', 'S4491', 'S4492', 'S4493', 'S4499', 'S4510', 'S4512', 'S4513', 'S4522', 'S4540', 'S4580', 'S4581', 'S4593', 'S4610', 'S4612', 'S4613', 'S4619', 'S4700', 'S4720', 'S4724', 'S4725', 'S4729', 'S4730', 'S4731', 'S4740', 'S4741', 'S4780', 'S4783', 'S4785', 'S4789', 'S4800', 'S4810', 'S4812', 'S4813', 'S4820', 'S4821', 'S4822', 'S4830', 'S4832', 'S4833', 'S4840', 'S4841', 'S4890', 'S4899', 'S4900', 'S4910', 'S4911', 'S4920', 'S4922', 'S4923', 'S4924', 'S4925', 'S4930', 'S4931', 'S4932', 'S4939', 'S4940', 'S4941', 'S4950', 'S4952', 'S4953', 'S4959', 'S4961', 'S4971', 'S4992', 'S5000', 'S5010', 'S5012', 'S5013', 'S5014', 'S5015', 'S5020', 'S5021', 'S5023', 'S5030', 'S5031', 'S5032', 'S5033', 'S5039', 'S5040', 'S5043', 'S5044', 'S5045', 'S5046', 'S5047', 'S5048', 'S5049', 'S5050', 'S5051', 'S5052', 'S5060', 'S5063', 'S5064', 'S5065', 'S5070', 'S5072', 'S5074', 'S5075', 'S5078', 'S5080', 'S5082', 'S5083', 'S5084', 'S5085', 'S5087', 'S5088', 'S5090', 'S5091', 'S5092', 'S5093', 'S5094', 'S5099', 'S5100', 'S5110', 'S5111', 'S5112', 'S5113', 'S5120', 'S5122', 'S5130', 'S5131', 'S5136', 'S5137', 'S5139', 'S5140', 'S5141', 'S5142', 'S5143', 'S5144', 'S5145', 'S5146', 'S5147', 'S5148', 'S5149', 'S5150', 'S5153', 'S5154', 'S5159', 'S5160', 'S5161', 'S5162', 'S5169', 'S5170', 'S5171', 'S5172', 'S5180', 'S5181', 'S5182', 'S5190', 'S5191', 'S5192', 'S5193', 'S5194', 'S5198', 'S5199', 'S5200', 'S5210', 'S5211', 'S5231', 'S5251', 'S5261', 'S5271', 'S53', 'S5300', 'S5310', 'S5311', 'S5330', 'S5331', 'S5390', 'S5399', 'S5400', 'S5410', 'S5411', 'S5421', 'S5431', 'S5441', 'S5451', 'S5461', 'S5499', 'S5500', 'S5510', 'S5511', 'S5521', 'S5530', 'S5531', 'S5541', 'S5551', 'S5560', 'S5561', 'S5571', 'S5590', 'S5599', 'S5600', 'S5610', 'S5611', 'S5621', 'S5632', 'S5641', 'S5650', 'S5651', 'S5661', 'S5699', 'S5710', 'S5712', 'S5713', 'S5714', 'S5719', 'S5720', 'S5722', 'S5728', 'S5730', 'S5731', 'S5734', 'S5735', 'S5736', 'S5800', 'S5810', 'S5812', 'S5813', 'S5900', 'S5912', 'S5921', 'S5932', 'S5940', 'S5941', 'S5942', 'S5943', 'S5944', 'S5945', 'S5946', 'S5947', 'S5948', 'S5949', 'S5960', 'S5961', 'S5962', 'S5963', 'S5980', 'S5983', 'S5984', 'S5989', 'S5990', 'S5992', 'S5993', 'S5994', 'S5995', 'S5999', 'S6000', 'S6010', 'S6011', 'S6019', 'S6020', 'S6021', 'S6022', 'S6029', 'S6030', 'S6033', 'S6035', 'S6036', 'S6061', 'S6062', 'S6080', 'S6081', 'S6082', 'S6090', 'S6091', 'S6099', 'S6100', 'S6111', 'S6120', 'S6140', 'S6141', 'S6150', 'S6152', 'S6153', 'S6159', 'S6160', 'S6162', 'S6163', 'S6200', 'S6210', 'S6211', 'S6221', 'S6230', 'S6231', 'S6280', 'S6282', 'S6289', 'S6300', 'S6310', 'S6311', 'S6316', 'S6320', 'S6321', 'S6324', 'S6330', 'S6331', 'S6351', 'S6360', 'S6361', 'S6371', 'S6390', 'S6399', 'S6410', 'S6411', 'S6500', 'S6510', 'S6511', 'S6512', 'S6513', 'S6514', 'S6515', 'S6517', 'S6519', 'S6530', 'S6531', 'S6541', 'S6550', 'S6551', 'S6552', 'S6553', 'S6700', 'S6710', 'S6711', 'S6712', 'S6719', 'S6720', 'S6722', 'S6726', 'S6730', 'S6732', 'S6733', 'S6790', 'S6792', 'S6794', 'S6798', 'S6799', 'S700', 'S7000', 'S7010', 'S7011', 'S7020', 'S7021', 'S7032', 'S7033', 'S7041', 'S711', 'S7200', 'S721', 'S7210', 'S7211', 'S7212', 'S7213', 'S7215', 'S7216', 'S7217', 'S7218', 'S7219', 'S722', 'S7221', 'S723', 'S7231', 'S724', 'S7241', 'S7251', 'S7260', 'S7261', 'S7291', 'S7299', 'S7300', 'S7310', 'S7311', 'S7312', 'S7313', 'S7319', 'S7320', 'S7322', 'S7323', 'S7330', 'S7331', 'S7334', 'S7335', 'S7336', 'S7338', 'S7340', 'S7342', 'S7349', 'S7350', 'S7352', 'S7353', 'S7359', 'S7360', 'S7361', 'S7363', 'S7370', 'S7371', 'S7372', 'S7373', 'S7374', 'S7375', 'S7376', 'S7377', 'S7378', 'S7379', 'S7380', 'S7381', 'S7382', 'S7383', 'S7384', 'S7389', 'S7390', 'S740', 'S741', 'S742', 'S7500', 'S751', 'S7510', 'S7513', 'S7514', 'S7515', 'S7519', 'S752', 'S7521', 'S7530', 'S7532', 'S7533', 'S7534', 'S7536', 'S7538', 'S7539', 'S7542', 'S7549', 'S7603', 'S761', 'S762', 'S7622', 'S7623', 'S7629', 'S7630', 'S7631', 'S7640', 'S7641', 'S7651', 'S7692', 'S7694', 'S7699', 'S7800', 'S781', 'S7810', 'S7812', 'S7819', 'S782', 'S7820', 'S7822', 'S7829', 'S783', 'S7830', 'S7832', 'S7833', 'S7841', 'S7900', 'S7911', 'S7922', 'S7929', 'S7933', 'S7941', 'S7948', 'S7990', 'S7991', 'S7992', 'S7993', 'S7996', 'S7997', 'S7999', 'S8000', 'S8010', 'S8011', 'S8021', 'S8031', 'S8041', 'S8042', 'S8043', 'S8049', 'S8050', 'S8051', 'S8052', 'S8059', 'S8060', 'S8062', 'S8063', 'S8069', 'S8070', 'S8071', 'S8072', 'S8080', 'S8082', 'S8090', 'S8092', 'S8093', 'S8099', 'S810', 'S811', 'S8110', 'S8111', 'S8200', 'S8210', 'S8211', 'S8220', 'S8221', 'S8222', 'S8231', 'S8243', 'S8244', 'S8249', 'S8290', 'S8299', 'S830', 'S8300', 'S831', 'S8310', 'S8322', 'S8330', 'S8331', 'S8351', 'S8361', 'S8390', 'S8399', 'S8412', 'S8420', 'S8422', 'S851', 'S8611', 'S8621', 'S8631', 'S8641', 'S8699', 'S8700', 'S8710', 'S8711', 'S8712', 'S8713', 'S8720', 'S8721', 'S8730', 'S8731', 'S8732', 'S8733', 'S8734', 'S8740', 'S8741', 'S8742', 'S8743', 'S8744', 'S8748', 'S8811', 'S8900', 'S8990', 'S8999', 'S910', 'S9111', 'S912', 'S9120', 'S913', 'S919', 'S9199', 'S921', 'S9222', 'S9223', 'S9224', 'S9229', 'S9300', 'S9310', 'S9311', 'S9411', 'S9431', 'S9441', 'S9510', 'S9511', 'S9512', 'S9531', 'S9532', 'S9611', 'S9621', 'S9631', 'S9641', 'S9651', 'S9661', 'S971', 'S9711', 'S9721', 'S9999']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# QUALITY CHECKS AND UNIQUE VALUE OVERVIEW FOR UPDATE CODE DATA\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   - Loads the filtered UpdateCode dataset created earlier.\n",
    "#   - Validates that the expected Value column (containing update codes) exists,\n",
    "#     or falls back to a suitable alternative (e.g., 'SIC').\n",
    "#   - Computes key dataset metrics:\n",
    "#       * Number of unique IDs.\n",
    "#       * Number of IDs appearing more than once.\n",
    "#       * Number of rows with invalid or missing update code values.\n",
    "#   - Extracts all unique update code values and produces a sorted list for\n",
    "#     manual inspection.\n",
    "#\n",
    "# Filtering / transformation focus:\n",
    "#   - Normalizes the Value column using pandas' string dtype and whitespace\n",
    "#     stripping.\n",
    "#   - Applies a validation mask that flags null-like or empty entries, enabling\n",
    "#     downstream filtering or quality checks.\n",
    "# =============================================================================\n",
    "\n",
    "# Paths\n",
    "FILE = f'{Temp_file_path_EoC}/filtered_SIC.txt'  # Path to the filtered UpdateCode file\n",
    "\n",
    "PREFERRED_VALUE_COL = \"Value\"  # Original expected column name\n",
    "FALLBACK_VALUE_COLS = [\"SIC\", \"ItemCode\"]  # Fallbacks if 'Value' is not present\n",
    "\n",
    "try:\n",
    "    # Load the filtered update code dataset\n",
    "    df = pd.read_csv(\n",
    "        FILE,\n",
    "        sep=\"|\",              # File is pipe-delimited\n",
    "        encoding=\"utf-8\",     # Use UTF-8 encoding\n",
    "        dtype=str,            # Load all columns as strings\n",
    "        keep_default_na=True  # Interpret standard NA-like tokens as NaN\n",
    "    )\n",
    "\n",
    "    # Display a preview of the first few rows\n",
    "    print(f\"\\n=== Preview of {FILE} ===\")\n",
    "    display(df.head(), \"\\n\")\n",
    "\n",
    "    # Determine which column to use as the \"value\" column\n",
    "    if PREFERRED_VALUE_COL in df.columns:\n",
    "        VALUE_COL = PREFERRED_VALUE_COL\n",
    "    else:\n",
    "        # Try fallbacks\n",
    "        VALUE_COL = None\n",
    "        for col in FALLBACK_VALUE_COLS:\n",
    "            if col in df.columns:\n",
    "                VALUE_COL = col\n",
    "                print(\n",
    "                    f\"Info: Expected column '{PREFERRED_VALUE_COL}' not found. \"\n",
    "                    f\"Using '{VALUE_COL}' as the value column instead.\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "        if VALUE_COL is None:\n",
    "            # Nothing suitable found → raise a clear error\n",
    "            raise KeyError(\n",
    "                f\"None of the candidate value columns \"\n",
    "                f\"{[PREFERRED_VALUE_COL] + FALLBACK_VALUE_COLS} \"\n",
    "                f\"were found in {FILE}. Available columns: {df.columns.tolist()}\"\n",
    "            )\n",
    "\n",
    "    # Compute ID-level metrics\n",
    "    unique_ids = df[\"ID\"].nunique()                      # Number of unique entities\n",
    "    multi_used_ids = (df[\"ID\"].value_counts() > 1).sum() # Entities with multiple rows\n",
    "\n",
    "    # Normalize the chosen value column for quality testing\n",
    "    val = df[VALUE_COL].astype(\"string\")  # Convert to pandas string dtype\n",
    "    val_stripped = val.str.strip()        # Remove leading/trailing whitespace\n",
    "\n",
    "    # Identify invalid update code entries:\n",
    "    # - Missing values\n",
    "    # - Empty strings\n",
    "    # - Placeholder/null-like tokens\n",
    "    error_mask = val.isna() | (\n",
    "        (val_stripped == \"\") |\n",
    "        val_stripped.str.lower().isin(\n",
    "            {\"na\", \"nan\", \"null\", \"none\", \"n/a\", \"#n/a\", \"n\"}\n",
    "        )\n",
    "    )\n",
    "    error_rows = int(error_mask.sum())\n",
    "\n",
    "    # Output summary statistics\n",
    "    print(f\"File: {FILE}\")\n",
    "    print(f\"Using column '{VALUE_COL}' as value column.\")\n",
    "    print(f\"Number of Unique IDs: {unique_ids}\")\n",
    "    print(f\"Number of IDs used multiple times: {multi_used_ids}\")\n",
    "    print(f'Rows with error in \"{VALUE_COL}\": {error_rows}')\n",
    "\n",
    "    # Extract list of unique update code values (excluding NaN)\n",
    "    unique_values = df[VALUE_COL].dropna().unique().tolist()\n",
    "\n",
    "    # Sort unique values alphabetically, placing None/NaN at the end\n",
    "    unique_values.sort(key=lambda x: (x is None, x))\n",
    "\n",
    "    print(f\"\\nUnique values in '{VALUE_COL}' ({len(unique_values)} total):\")\n",
    "    print(unique_values)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # File does not exist or was not created earlier\n",
    "    print(f\"Error: File not found at {FILE}.\")\n",
    "except Exception as e:\n",
    "    # Handle any other unexpected errors\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9148a15",
   "metadata": {
    "id": "f9148a15"
   },
   "source": [
    "## 3.0 Adjustments and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489cf18",
   "metadata": {
    "id": "8489cf18"
   },
   "source": [
    "### NationCode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b024437",
   "metadata": {
    "id": "3b024437"
   },
   "source": [
    "#### Drop ItemCode Column and Ensure Correct Spelling of NationCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "gUCtPP7-f_Vm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1764880885135,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "gUCtPP7-f_Vm",
    "outputId": "31978ea2-1a43-4832-e3f3-1ee0eb4256c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and sorted file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_NationCode_v2.txt\n",
      "          ID    PIT DATE NatCo\n",
      "0  C00948205  2021-07-09   840\n",
      "1  C02500770  1995-12-29   025\n",
      "2  C0250077A  1999-10-01   025\n",
      "3  C0250077B  1999-10-01   025\n",
      "4  C0250077C  1999-10-01   025\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell loads a pipe-delimited text file, removes and renames specific columns,\n",
    "# applies targeted string cleaning to the 'NatCo' column (removing a leading 'S'\n",
    "# and zero-padding when needed), sorts the dataset by ID and PIT DATE to ensure\n",
    "# deterministic ordering, and finally writes the cleaned result to a new file.\n",
    "# The focus of the transformation is on filtering out an unused column, renaming\n",
    "# a key identifier, and standardizing the 'NatCo' values.\n",
    "\n",
    "input_path  = f'{Temp_file_path_EoC}/filtered_NationCode.txt'\n",
    "output_path_v2 = f'{Temp_file_path_EoC}/filtered_NationCode_v2.txt'\n",
    "\n",
    "# Load the pipe-delimited input file into a DataFrame\n",
    "df = pd.read_csv(input_path, sep=\"|\", dtype=str, encoding=\"utf-8\")\n",
    "\n",
    "# Drop the ItemCode column if it exists, ignoring errors if it does not\n",
    "if \"ItemCode\" in df.columns:\n",
    "    df = df.drop(columns=[\"ItemCode\"], errors=\"ignore\")\n",
    "\n",
    "# Rename 'NationCode' to 'NatCo' to standardize column naming,\n",
    "# raising an error if the source column does not exist\n",
    "if \"NationCode\" in df.columns:\n",
    "    df = df.rename(columns={\"NationCode\": \"NatCo\"})\n",
    "else:\n",
    "    raise KeyError(\"Expected column 'NationCode' not found in input file.\")\n",
    "\n",
    "# Clean the 'NatCo' column by removing a leading 'S'\n",
    "# and adding a leading zero only when the resulting string has length 2\n",
    "df[\"NatCo\"] = (\n",
    "    df[\"NatCo\"]\n",
    "    .astype(\"string\")\n",
    "    .str.replace(r\"^S\", \"\", regex=True)               # Remove leading S\n",
    "    .apply(lambda x: f\"0{x}\" if isinstance(x, str) and len(x) == 2 else x)  # Conditional zero-padding\n",
    ")\n",
    "\n",
    "# Sort the dataset by ID and PIT DATE using a stable merge sort for consistent ordering\n",
    "df = df.sort_values(by=[\"ID\", \"PIT DATE\"], ascending=[True, True], kind=\"mergesort\")\n",
    "\n",
    "# Write the cleaned and sorted DataFrame back to disk using pipe delimiters\n",
    "df.to_csv(output_path_v2, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Display confirmation and preview of the resulting data\n",
    "print(f\"Cleaned and sorted file saved to: {output_path_v2}\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa9bff",
   "metadata": {
    "id": "32aa9bff"
   },
   "source": [
    "#### Get Overview of the Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "n5X3HiBzhcMK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1764880885183,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "n5X3HiBzhcMK",
    "outputId": "be6685f1-c86b-48c3-8938-265b0ae73d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs with >1 unique NatCo: 432\n",
      "             ID  UniqueNatCos\n",
      "255   C025L21C0             2\n",
      "2825  C036F43X0             2\n",
      "5584  C060E0000             2\n",
      "5832  C07209430             2\n",
      "5835  C07222660             2\n",
      "\n",
      "IDs with missing/invalid NatCo: 0\n",
      "\n",
      "Unique NatCo values (126 total):\n",
      "['025', '036', '040', '044', '048', '050', '052', '056', '060', '068', '070', '072', '076', '092', '0Ns', '100', '116', '120', '124', '136', '152', '156', '175', '178', '182', '191', '196', '203', '208', '218', '220', '222', '233', '234', '242', '246', '250', '268', '275', '280', '288', '300', '320', '328', '340', '344', '350', '352', '356', '366', '369', '372', '376', '380', '388', '392', '398', '400', '404', '410', '414', '422', '428', '440', '442', '446', '454', '458', '470', '480', '484', '496', '499', '504', '516', '528', '554', '562', '566', '578', '582', '586', '591', '593', '597', '608', '617', '620', '634', '642', '643', '646', '682', '686', '688', '702', '703', '704', '705', '710', '724', '730', '736', '748', '752', '756', '759', '760', '764', '780', '784', '788', '796', '800', '804', '807', '826', '831', '832', '833', '834', '840', '860', '862', '894', '897']\n",
      "\n",
      "Summary:\n",
      "Total rows: 121852\n",
      "Unique IDs: 121008\n",
      "IDs with >1 NatCo: 432\n",
      "IDs with missing NatCo: 0\n",
      "Unique NatCo values: 126\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell reads the cleaned NationCode file and performs several analytical checks:\n",
    "#   - calculates how many distinct NatCo values each ID has and identifies IDs with >1 NatCo\n",
    "#   - detects IDs where NatCo is missing or contains typical invalid placeholders\n",
    "#   - inspects and prints the set of unique NatCo values in the dataset\n",
    "#   - prints an overall summary of row counts, unique IDs, multi-NatCo IDs, and missing NatCo IDs\n",
    "\n",
    "input_path = f'{Temp_file_path_EoC}/filtered_NationCode_v2.txt'\n",
    "\n",
    "# Load the cleaned, pipe-delimited file into a DataFrame, keeping all columns as strings\n",
    "df = pd.read_csv(input_path, sep=\"|\", dtype=str)\n",
    "\n",
    "# Group by 'ID' and count how many distinct non-null NatCo values each ID has\n",
    "NatCo_counts = (\n",
    "    df.groupby(\"ID\")[\"NatCo\"]\n",
    "    .nunique(dropna=True)  # count unique non-NaN NatCo values per ID\n",
    "    .reset_index(name=\"UniqueNatCos\")\n",
    ")\n",
    "\n",
    "# Filter to IDs that have more than one unique NatCo value\n",
    "ids_with_multiple_countries = NatCo_counts[NatCo_counts[\"UniqueNatCos\"] > 1]\n",
    "num_multiple_NatCo_ids = len(ids_with_multiple_countries)\n",
    "\n",
    "# Report how many IDs have multiple NatCo values and show a preview of these IDs\n",
    "print(f\"IDs with >1 unique NatCo: {num_multiple_NatCo_ids}\")\n",
    "print(ids_with_multiple_countries.head())\n",
    "\n",
    "# Build a boolean mask to identify rows with missing or invalid NatCo values.\n",
    "# The mask covers:\n",
    "#   - NaN values\n",
    "#   - empty strings (after stripping whitespace)\n",
    "#   - common placeholder tokens like 'NA', 'NAN', 'NONE', and \"n'\"\n",
    "missing_mask = (\n",
    "    df[\"NatCo\"].isna() |\n",
    "    df[\"NatCo\"].str.strip().eq(\"\") |\n",
    "    df[\"NatCo\"].str.upper().isin([\"NA\", \"NAN\", \"NONE\", \"n'\"])\n",
    ")\n",
    "\n",
    "# Extract the unique IDs that have at least one row with missing or invalid NatCo\n",
    "ids_with_missing_NatCo = df[missing_mask][\"ID\"].unique()\n",
    "num_missing_NatCo_ids = len(ids_with_missing_NatCo)\n",
    "\n",
    "# Report how many IDs are affected by missing or invalid NatCo values\n",
    "print(f\"\\nIDs with missing/invalid NatCo: {num_missing_NatCo_ids}\")\n",
    "\n",
    "# Optionally, show some example rows where NatCo is missing or invalid\n",
    "if num_missing_NatCo_ids > 0:\n",
    "    print(\"\\nExample missing NatCo entries:\")\n",
    "    print(df[missing_mask].head())\n",
    "\n",
    "# Collect the set of unique non-null NatCo values in the dataset\n",
    "unique_NatCo = df[\"NatCo\"].dropna().unique()\n",
    "print(f\"\\nUnique NatCo values ({len(unique_NatCo)} total):\")\n",
    "print(sorted(unique_NatCo))\n",
    "\n",
    "# Print a compact summary of the dataset and the previous analyses\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Unique IDs: {df['ID'].nunique()}\")\n",
    "print(f\"IDs with >1 NatCo: {num_multiple_NatCo_ids}\")\n",
    "print(f\"IDs with missing NatCo: {num_missing_NatCo_ids}\")\n",
    "print(f\"Unique NatCo values: {len(unique_NatCo)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47068e49",
   "metadata": {
    "id": "47068e49"
   },
   "source": [
    "#### Get Summary Statistics of Frequency of Nations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "MP2Qpdj6n0Lv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1764880885229,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "MP2Qpdj6n0Lv",
    "outputId": "256306cf-01df-46ae-efe5-89c770adbfc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country summary saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/NationCode_summary.txt\n",
      "\n",
      "Summary statistics (top 10):\n",
      "  NatCo  Count     ImplCountry\n",
      "0   840  33054   United States\n",
      "1   124   8077          Canada\n",
      "2   156   7701           China\n",
      "3   826   6678  United Kingdom\n",
      "4   392   6446           Japan\n",
      "5   356   5843           India\n",
      "6   036   3974       Australia\n",
      "7   410   3759     South Korea\n",
      "8   344   3285       Hong Kong\n",
      "9   760   2905          Taiwan\n",
      "\n",
      "Total unique country codes: 126\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell loads the cleaned NationCode dataset and a reference table of country codes.\n",
    "# It computes how frequently each NatCo value appears, merges these counts with descriptive\n",
    "# country names, sorts the results in descending order of frequency, and writes a summary\n",
    "# table to disk. The transformation steps focus on: counting occurrences, enriching the\n",
    "# counts with metadata, and producing a sorted output for inspection.\n",
    "\n",
    "data_path = f'{Temp_file_path_EoC}/filtered_NationCode_v2.txt'\n",
    "codes_path = f'{Input_file_path}/CountryCodes.txt'\n",
    "output_path = f'{Temp_file_path_EoC}/NationCode_summary.txt'\n",
    "\n",
    "# Load the cleaned dataset containing NatCo and related fields\n",
    "df = pd.read_csv(data_path, sep=\"|\", dtype=str)\n",
    "\n",
    "# Load the country code reference file providing descriptive names for each NatCo\n",
    "df_codes = pd.read_csv(codes_path, sep=\"|\", dtype=str)\n",
    "\n",
    "# Count how often each NatCo value appears in the dataset,\n",
    "# including NaN and empty values if present\n",
    "summary = (\n",
    "    df[\"NatCo\"]\n",
    "    .value_counts(dropna=False)       # count occurrences of each NatCo value\n",
    "    .reset_index()                    # convert to DataFrame with column names\n",
    ")\n",
    "summary.columns = [\"NatCo\", \"Count\"]  # rename columns for clarity\n",
    "\n",
    "# Merge the frequency table with the country codes reference data\n",
    "# to append country names or descriptions for each NatCo\n",
    "summary = summary.merge(df_codes, on=\"NatCo\", how=\"left\")\n",
    "\n",
    "# Sort the merged summary so that the most frequent NatCo codes appear first\n",
    "summary = summary.sort_values(by=\"Count\", ascending=False)\n",
    "\n",
    "# Save the summarized frequency table to disk as a pipe-delimited file\n",
    "summary.to_csv(output_path, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Display the output path and a preview (top 10) of the summary\n",
    "print(\"Country summary saved to:\", output_path)\n",
    "print(\"\\nSummary statistics (top 10):\")\n",
    "print(summary.head(10))\n",
    "\n",
    "# Show how many distinct NatCo values exist in the summary table\n",
    "print(\"\\nTotal unique country codes:\", len(summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db8e04",
   "metadata": {
    "id": "77db8e04"
   },
   "source": [
    "#### Keep Only Newest Nation and Remove \"0N\" Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "gAv7fKL90r2T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1764880885265,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "gAv7fKL90r2T",
    "outputId": "99f29b0b-34bd-48e5-a982-0ada04559052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned NatCo file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/NationCodes_clean.txt\n",
      "\n",
      "=== Filtering Statistics ===\n",
      "Total rows before filtering:   121,852\n",
      "Rows kept as valid NatCo:      121,008\n",
      "Rows kept as fallback '0Ns':   0\n",
      "Total rows in final output:    121,008\n",
      "Total rows removed:            844\n",
      "\n",
      "          ID NatCo\n",
      "0  C00948205   840\n",
      "1  C02500770   025\n",
      "2  C0250077A   025\n",
      "3  C0250077B   025\n",
      "4  C0250077C   025\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell loads the cleaned NationCode file, converts PIT DATE into datetime,\n",
    "# sorts records so the newest entry per ID appears first, and selects exactly one\n",
    "# record per ID according to the rule:\n",
    "#   - Prefer the newest record where NatCo != \"0Ns\"\n",
    "#   - If none exist, keep the newest \"0Ns\" record\n",
    "# The cell also reports how many rows were filtered out during the selection.\n",
    "\n",
    "input_path  = f'{Temp_file_path_EoC}/filtered_NationCode_v2.txt'\n",
    "output_path = f'{Temp_file_path_EoC}/NationCodes_clean.txt'\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(input_path, sep=\"|\", dtype=str)\n",
    "\n",
    "# Convert PIT DATE to datetime for reliable sorting\n",
    "df[\"PIT DATE\"] = pd.to_datetime(df[\"PIT DATE\"], errors=\"coerce\")\n",
    "\n",
    "# Sort newest→oldest within each ID\n",
    "df = df.sort_values(by=[\"ID\", \"PIT DATE\"], ascending=[True, False])\n",
    "\n",
    "# Track initial row count\n",
    "initial_row_count = len(df)\n",
    "\n",
    "# Flag rows where NatCo is considered valid (anything except \"0Ns\")\n",
    "df[\"_is_valid_natco\"] = df[\"NatCo\"] != \"0Ns\"\n",
    "\n",
    "# Newest valid record per ID (if exists)\n",
    "valid = df[df[\"_is_valid_natco\"]]\n",
    "valid_latest = (\n",
    "    valid.groupby(\"ID\", group_keys=False)\n",
    "         .head(1)  # newest per ID\n",
    ")\n",
    "\n",
    "# Newest fallback \"0Ns\" record per ID (only those IDs that had no valid record)\n",
    "fallback = df[~df[\"_is_valid_natco\"]]\n",
    "fallback = fallback[~fallback[\"ID\"].isin(valid_latest[\"ID\"])]\n",
    "fallback_latest = (\n",
    "    fallback.groupby(\"ID\", group_keys=False)\n",
    "            .head(1)\n",
    ")\n",
    "\n",
    "# Combine valid + fallback\n",
    "df_clean = pd.concat([valid_latest, fallback_latest], ignore_index=True)\n",
    "\n",
    "# Track counts\n",
    "rows_after_selection = len(df_clean)\n",
    "rows_removed = initial_row_count - rows_after_selection\n",
    "valid_kept = len(valid_latest)\n",
    "fallback_kept = len(fallback_latest)\n",
    "\n",
    "# Drop helper columns\n",
    "df_clean = df_clean.drop(columns=[\"PIT DATE\", \"_is_valid_natco\"], errors=\"ignore\")\n",
    "\n",
    "# Sort output for readability\n",
    "df_clean = df_clean.sort_values(\"ID\").reset_index(drop=True)\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_clean.to_csv(output_path, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Display stats\n",
    "print(f\"Cleaned NatCo file saved to: {output_path}\\n\")\n",
    "print(\"=== Filtering Statistics ===\")\n",
    "print(f\"Total rows before filtering:   {initial_row_count:,}\")\n",
    "print(f\"Rows kept as valid NatCo:      {valid_kept:,}\")\n",
    "print(f\"Rows kept as fallback '0Ns':   {fallback_kept:,}\")\n",
    "print(f\"Total rows in final output:    {rows_after_selection:,}\")\n",
    "print(f\"Total rows removed:            {rows_removed:,}\\n\")\n",
    "\n",
    "# Show sample output\n",
    "print(df_clean.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f9da7",
   "metadata": {
    "id": "ff8f9da7"
   },
   "source": [
    "### CompanyName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a2e519",
   "metadata": {
    "id": "48a2e519"
   },
   "source": [
    "#### Save only the most recent company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a628dc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1764880885305,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "5a628dc3",
    "outputId": "613640c5-c4ad-44e5-f9e8-b34fdc08b3b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial unique IDs in /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_CompanyName.txt: 121,008\n",
      "Unique IDs after keeping latest valid name per ID: 121,008\n",
      "Rows excluded due to ID ending with a letter: 16,947\n",
      "          ID                               CompanyName\n",
      "0  C00948205             AGRIFORCE GROWING SYSTEMS LTD\n",
      "1  C02500770            PEUGEOT CITROEN ARGENTINA S.A.\n",
      "5  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA\n",
      "8  C02520220                       ALPARGATAS S.A.I.C.\n",
      "9  C02520230               ALUAR ALUMINIO ARGENTINO SA\n",
      "Saved cleaned names to /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/CompanyName_clean.txt (rows: 104,061)\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell loads company name data, validates required columns, and selects the most\n",
    "# recent valid company name per ID. A name is considered invalid if it equals \"n'\"\n",
    "# (case-insensitive, whitespace ignored). The logic keeps the latest non-invalid name\n",
    "# based on PIT DATE, drops IDs where all names are invalid, cleans the company names,\n",
    "# filters out IDs that end with a letter, and finally writes the cleaned result to file.\n",
    "\n",
    "# Define input and output file paths\n",
    "names_file       = f'{Temp_file_path_EoC}/filtered_CompanyName.txt'\n",
    "names_clean_file = f'{Temp_file_path_EoC}/CompanyName_clean.txt'\n",
    "\n",
    "# Load the names dataset using pipe delimiter; skip problematic lines instead of failing\n",
    "names_df = pd.read_csv(names_file, sep=\"|\", dtype=str, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "\n",
    "# Validate that required columns exist; raise an error if any are missing\n",
    "expected_cols = {\"ID\", \"PIT DATE\", \"Name\"}\n",
    "missing = expected_cols - set(names_df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing expected columns: {missing}. Found columns: {list(names_df.columns)}\")\n",
    "\n",
    "# Print the number of distinct IDs present in the raw data\n",
    "initial_unique_ids = names_df[\"ID\"].nunique()\n",
    "print(f\"Initial unique IDs in {names_file}: {initial_unique_ids:,}\")\n",
    "\n",
    "# Convert PIT DATE to a sortable numeric format (YYYYMMDD → integer); invalid values become NaN\n",
    "names_df[\"PIT_DATE_num\"] = pd.to_numeric(names_df[\"PIT DATE\"], errors=\"coerce\")\n",
    "\n",
    "# Mark rows where Name is exactly \"n'\" (ignoring case and whitespace)\n",
    "names_df[\"_is_bad_name\"] = names_df[\"Name\"].astype(str).str.strip().str.lower().eq(\"n'\")\n",
    "\n",
    "# Sort by ID and PIT_DATE_num so that within each ID the newest rows are last\n",
    "names_df_sorted = names_df.sort_values([\"ID\", \"PIT_DATE_num\"], kind=\"mergesort\")\n",
    "\n",
    "# Keep only rows with valid names (i.e., Name != \"n'\")\n",
    "valid_names = names_df_sorted[~names_df_sorted[\"_is_bad_name\"]]\n",
    "\n",
    "# For each ID, select the newest valid name (last row in each ID group)\n",
    "names_latest = (\n",
    "    valid_names\n",
    "    .groupby(\"ID\", as_index=False)\n",
    "    .tail(1)\n",
    "    .drop(columns=[\"PIT_DATE_num\", \"_is_bad_name\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Print unique ID count after dropping invalid-only IDs and keeping newest valid names\n",
    "unique_ids_after_latest_valid = names_latest[\"ID\"].nunique()\n",
    "print(f\"Unique IDs after keeping latest valid name per ID: {unique_ids_after_latest_valid:,}\")\n",
    "\n",
    "# Rename the Name column for clarity\n",
    "names_latest = names_latest.rename(columns={\"Name\": \"CompanyName\"})\n",
    "\n",
    "# Clean the CompanyName column by removing leading apostrophes and stripping whitespace\n",
    "names_latest[\"CompanyName\"] = (\n",
    "    names_latest[\"CompanyName\"]\n",
    "    .astype(\"string\")\n",
    "    .str.replace(r\"^'+\", \"\", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Retain only ID and CompanyName for the output dataset\n",
    "names_latest_out = names_latest[[\"ID\", \"CompanyName\"]].copy()\n",
    "\n",
    "# Identify IDs that end with a letter and exclude them\n",
    "initial_rows_before_id_filter = len(names_latest_out)\n",
    "mask_ids_ending_with_letter = names_latest_out[\"ID\"].astype(str).str[-1].str.isalpha()\n",
    "names_latest_filtered_id = names_latest_out[~mask_ids_ending_with_letter].copy()\n",
    "excluded_rows_id_filter = initial_rows_before_id_filter - len(names_latest_filtered_id)\n",
    "\n",
    "print(f\"Rows excluded due to ID ending with a letter: {excluded_rows_id_filter:,}\")\n",
    "\n",
    "# Write the cleaned and filtered dataset to file\n",
    "names_latest_filtered_id.to_csv(names_clean_file, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Show a preview and confirm save\n",
    "print(names_latest_filtered_id.head())\n",
    "print(f\"Saved cleaned names to {names_clean_file} (rows: {len(names_latest_filtered_id):,})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361295dd",
   "metadata": {
    "id": "361295dd"
   },
   "source": [
    "### ADR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae7b40b",
   "metadata": {
    "id": "aae7b40b"
   },
   "source": [
    "#### Merge ADRs to the Company Names to Understand DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "RhM7l1dXm81o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1764880885356,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "RhM7l1dXm81o",
    "outputId": "99a1587b-2177-49fe-d806-a43c4ee438c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ADR IDs: 2,297\n",
      "Merged test dataframe saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_ADRIndicator_v2.txt (rows: 2,337)\n",
      "=== Head of merged DataFrame ===\n",
      "          ID    PIT DATE ADRIndicator                  CompanyName\n",
      "0  C036F63D0  2012-05-28           'X  BNK BANKING CORPORATION LTD\n",
      "1  C036F63D0  2014-01-15           N'  BNK BANKING CORPORATION LTD\n",
      "2  C056879S0  2018-04-23           'X      ACACIA PHARMA GROUP PLC\n",
      "3  C2461T100  2004-04-23           'X        BIOTIE THERAPIES CORP\n",
      "4  C2461T100  2005-06-10           N'        BIOTIE THERAPIES CORP\n",
      "5  C2504O500  2010-06-30           'X   FRANCE LOCATION EQUIPEMENT\n",
      "6  C2504O500  2011-06-29           N'   FRANCE LOCATION EQUIPEMENT\n",
      "7  C250C9180  2010-10-20           'X                FASHION B AIR\n",
      "8  C250C9180  2011-06-29           N'                FASHION B AIR\n",
      "9  C344BU720  2002-08-23           'X            VIVA GOODS CO LTD\n"
     ]
    }
   ],
   "source": [
    "# This cell merges ADR indicator data with cleaned company names on ID.\n",
    "# It also counts how many unique IDs exist inside the ADR dataset before merging.\n",
    "# The full workflow includes:\n",
    "# 1) Define input and output file paths for ADR indicator data and company name data.\n",
    "# 2) Load both datasets as pipe-delimited text files with all columns as strings.\n",
    "# 3) Compute how many unique IDs appear in the ADR dataset.\n",
    "# 4) Perform a left join on \"ID\" to attach company names to each ADR record.\n",
    "# 5) Remove the optional technical column \"ItemCode\" if present.\n",
    "# 6) Export the cleaned merged dataset.\n",
    "# 7) Print summary information including unique ADR ID count, row count, and preview.\n",
    "\n",
    "# --- Paths ---\n",
    "company_file = f'{Temp_file_path_EoC}/CompanyName_clean.txt'\n",
    "adr_file     = f'{Temp_file_path_EoC}/filtered_ADRIndicator.txt'\n",
    "merged_out   = f'{Temp_file_path_EoC}/filtered_ADRIndicator_v2.txt'\n",
    "\n",
    "# --- Load datasets ---\n",
    "company_df = pd.read_csv(company_file, sep=\"|\", dtype=str, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "adr_df     = pd.read_csv(adr_file,     sep=\"|\", dtype=str, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "\n",
    "# --- Count unique IDs in the ADR dataset ---\n",
    "unique_adr_ids = adr_df['ID'].nunique()              # Number of distinct IDs in ADR data\n",
    "print(f\"Number of unique ADR IDs: {unique_adr_ids:,}\")\n",
    "\n",
    "# --- Merge on ID ---\n",
    "test_merged_df = pd.merge(adr_df, company_df, on=\"ID\", how=\"left\")\n",
    "\n",
    "# --- Drop unnecessary technical columns if they exist ---\n",
    "cols_to_drop = [\"ItemCode\"]\n",
    "existing_to_drop = [c for c in cols_to_drop if c in test_merged_df.columns]\n",
    "if existing_to_drop:\n",
    "    test_merged_df = test_merged_df.drop(columns=existing_to_drop)\n",
    "\n",
    "# --- Export merged DataFrame ---\n",
    "test_merged_df.to_csv(merged_out, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# --- Display result summary ---\n",
    "print(f\"Merged test dataframe saved to: {merged_out} (rows: {len(test_merged_df):,})\")\n",
    "print(\"=== Head of merged DataFrame ===\")\n",
    "print(test_merged_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e6dca",
   "metadata": {
    "id": "b48e6dca"
   },
   "source": [
    "#### Keep only the newest row per ID in the merged df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "kKoz7cZ6Uf_0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1764880885401,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "kKoz7cZ6Uf_0",
    "outputId": "494a7164-2c8a-4758-c2b7-031fa5024021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved newest-per-ID merged dataset to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_ADRIndicator_v3.txt (rows: 2,297)\n",
      "          ID    PIT DATE ADRIndicator                  CompanyName\n",
      "0  C036F63D0  2014-01-15           N'  BNK BANKING CORPORATION LTD\n",
      "1  C056879S0  2018-04-23           'X      ACACIA PHARMA GROUP PLC\n",
      "2  C2461T100  2005-06-10           N'        BIOTIE THERAPIES CORP\n",
      "3  C2504O500  2011-06-29           N'   FRANCE LOCATION EQUIPEMENT\n",
      "4  C250C9180  2011-06-29           N'                FASHION B AIR\n",
      "5  C344BU720  2002-10-04           N'            VIVA GOODS CO LTD\n",
      "6  C344G3820  2002-10-04           N'       CHINA GAS HOLDINGS LTD\n",
      "7  C392WZ700  2014-01-15           N'               MEDICINOVA INC\n",
      "8  C410FVF00  2011-06-29           N'               SOFTCEN CO LTD\n",
      "9  C484B3960  2009-07-23           N'      GRUPO ELEKTRA SAB DE CV\n"
     ]
    }
   ],
   "source": [
    "# This cell loads a merged ADR/company dataset, parses the PIT DATE column into a comparable\n",
    "# datetime representation using a flexible parser, and then selects the newest record per ID\n",
    "# based on this parsed date. The result is a filtered dataset where each ID appears only once,\n",
    "# representing its most recent PIT DATE entry, which is then saved to an output file.\n",
    "\n",
    "# Paths\n",
    "merged_in  = f'{Temp_file_path_EoC}/filtered_ADRIndicator_v2.txt'\n",
    "latest_out = f'{Temp_file_path_EoC}/filtered_ADRIndicator_v3.txt'\n",
    "\n",
    "# Load merged dataset as strings to avoid unwanted type inference\n",
    "df = pd.read_csv(merged_in, sep=\"|\", dtype=str, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "\n",
    "# Function to parse PIT DATE values into datetime, handling multiple possible formats\n",
    "def parse_pit_date(s: pd.Series) -> pd.Series:\n",
    "    # Convert to string, strip whitespace to normalize the raw date values\n",
    "    s = s.astype(str).str.strip()\n",
    "    try:\n",
    "        # Try using a flexible datetime parser that can handle mixed formats directly\n",
    "        return pd.to_datetime(s, errors=\"coerce\", format=\"mixed\")\n",
    "    except TypeError:\n",
    "        # Fallback branch for environments/pandas versions without 'format=\"mixed\"'\n",
    "        # Create a mask for values in strict YYYYMMDD format\n",
    "        mask_8 = s.str.fullmatch(r\"\\d{8}\", na=False)\n",
    "        # Initialize a datetime Series with NaT values\n",
    "        dt = pd.to_datetime(pd.Series([None]*len(s)), errors=\"coerce\")\n",
    "        # Parse strictly formatted YYYYMMDD values\n",
    "        dt.loc[mask_8] = pd.to_datetime(s.loc[mask_8], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "        # For remaining entries, try a more generic parser with inference\n",
    "        rem = ~mask_8\n",
    "        if rem.any():\n",
    "            dt.loc[rem] = pd.to_datetime(s.loc[rem], errors=\"coerce\", infer_datetime_format=True)\n",
    "        # Return the combined datetime Series used for ordering by PIT DATE\n",
    "        return dt\n",
    "\n",
    "# Compute newest per ID:\n",
    "# 1) Parse PIT DATE into a datetime helper column\n",
    "df[\"_PIT_dt\"] = parse_pit_date(df[\"PIT DATE\"])\n",
    "\n",
    "# 2) Sort by ID and PIT DATE so that within each ID, rows are ordered chronologically\n",
    "df_sorted = df.sort_values([\"ID\", \"_PIT_dt\"], ascending=[True, True], kind=\"mergesort\")\n",
    "\n",
    "# 3) For each ID, keep the last row (newest PIT DATE) and drop the helper column\n",
    "test_merged_latest_df = (\n",
    "    df_sorted\n",
    "    .groupby(\"ID\", as_index=False)\n",
    "    .tail(1)                          # newest row per ID due to ascending sort\n",
    "    .drop(columns=[\"_PIT_dt\"])        # remove helper datetime column\n",
    "    .reset_index(drop=True)           # clean up index after filtering\n",
    ")\n",
    "\n",
    "# Save the per-ID newest-record dataset and print a small preview\n",
    "test_merged_latest_df.to_csv(latest_out, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved newest-per-ID merged dataset to: {latest_out} (rows: {len(test_merged_latest_df):,})\")\n",
    "print(test_merged_latest_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207697a8",
   "metadata": {
    "id": "207697a8"
   },
   "source": [
    "#### Clean \"ADRIndicator\" column and drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ICjKiYN5WGw8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1764880885447,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ICjKiYN5WGw8",
    "outputId": "50153648-da43-45be-b97f-112c5725b0a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned ADRIndicator values saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/ADR_clean.txt\n",
      "          ID ADRIndicator\n",
      "0  C036F63D0            N\n",
      "1  C056879S0            X\n",
      "2  C2461T100            N\n",
      "3  C2504O500            N\n",
      "4  C250C9180            N\n",
      "5  C344BU720            N\n",
      "6  C344G3820            N\n",
      "7  C392WZ700            N\n",
      "8  C410FVF00            N\n",
      "9  C484B3960            N\n",
      "Count IDs: 2,297\n",
      "Count unique IDs: 2,297\n"
     ]
    }
   ],
   "source": [
    "# This cell loads the latest ADRIndicator data, cleans and normalizes the ADRIndicator values,\n",
    "# removes columns that are no longer needed, and then writes a compact ADR-only dataset.\n",
    "#\n",
    "# Main transformations and checks:\n",
    "# - Reads the ADR file (pipe-delimited) as strings to preserve formats and avoid type coercion.\n",
    "# - Verifies that the expected 'ADRIndicator' column exists and raises an error if missing.\n",
    "# - Cleans the ADRIndicator column by:\n",
    "#     * Stripping surrounding whitespace.\n",
    "#     * Removing a leading apostrophe directly before an 'X' (e.g., \"'X\" → \"X\").\n",
    "#     * Removing a trailing apostrophe directly after an 'N' (e.g., \"N'\" → \"N\").\n",
    "# - Drops auxiliary columns ('PIT DATE', 'CompanyName') if present, keeping only the fields\n",
    "#   necessary for the downstream ADR analysis.\n",
    "# - Saves the cleaned dataset and prints basic diagnostics:\n",
    "#     * First rows of the cleaned frame.\n",
    "#     * Total count of IDs and the number of unique IDs.\n",
    "\n",
    "# --- Paths ---\n",
    "adr_in  = f'{Temp_file_path_EoC}/filtered_ADRIndicator_v3.txt'\n",
    "adr_out = f'{Temp_file_path_EoC}/ADR_clean.txt'\n",
    "\n",
    "# --- Load ADR dataset ---\n",
    "# Read the ADRIndicator dataset from the pipe-delimited file, keeping all columns as strings.\n",
    "# on_bad_lines=\"skip\" ensures that malformed lines are skipped instead of raising an error.\n",
    "df = pd.read_csv(adr_in, sep=\"|\", dtype=str, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "\n",
    "# --- Clean ADRIndicator column ---\n",
    "# Ensure the required ADRIndicator column is available; abort with a clear error if not.\n",
    "if \"ADRIndicator\" not in df.columns:\n",
    "    raise KeyError(\"Expected column 'ADRIndicator' not found in the input file.\")\n",
    "\n",
    "# Normalize ADRIndicator values by fixing common formatting artifacts.\n",
    "df[\"ADRIndicator\"] = (\n",
    "    df[\"ADRIndicator\"]\n",
    "    .astype(\"string\")                     # work with pandas' string dtype for vectorized operations\n",
    "    .str.strip()                          # remove leading and trailing whitespace\n",
    "    .str.replace(r\"^'+X\", \"X\", regex=True)   # remove one or more leading apostrophes directly before X (e.g. \"'X\" -> \"X\")\n",
    "    .str.replace(r\"N'+$\", \"N\", regex=True)   # remove one or more trailing apostrophes directly after N (e.g. \"N'\" -> \"N\")\n",
    ")\n",
    "\n",
    "# --- Drop unnecessary columns if they exist ---\n",
    "# Define columns that are not needed anymore for the ADR-specific output.\n",
    "cols_to_drop = [\"PIT DATE\", \"CompanyName\"]\n",
    "# Keep only those columns from the list that are actually present in the DataFrame.\n",
    "existing_to_drop = [c for c in cols_to_drop if c in df.columns]\n",
    "# Drop the identified columns if there are any to remove.\n",
    "if existing_to_drop:\n",
    "    df = df.drop(columns=existing_to_drop)\n",
    "\n",
    "# --- Save cleaned dataset ---\n",
    "# Write the cleaned ADR dataset back to disk as a pipe-delimited file without the index column.\n",
    "df.to_csv(adr_out, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Cleaned ADRIndicator values saved to: {adr_out}\")\n",
    "print(df.head(10))\n",
    "\n",
    "# --- Count IDs ---\n",
    "# Compute the total number of ID records and the number of distinct IDs in the cleaned dataset.\n",
    "count_ids = len(df[\"ID\"])\n",
    "count_unique_ids = df[\"ID\"].nunique()\n",
    "\n",
    "# --- Print results ---\n",
    "# Provide a brief summary of the ID distribution after cleaning.\n",
    "print(f\"Count IDs: {count_ids:,}\")\n",
    "print(f\"Count unique IDs: {count_unique_ids:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T9bJaXBsesvy",
   "metadata": {
    "id": "T9bJaXBsesvy"
   },
   "source": [
    "### SIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z9m9ya4o9Fpb",
   "metadata": {
    "id": "z9m9ya4o9Fpb"
   },
   "source": [
    "#### Clean SIC Row, i.e., Remove Ns and the \"S\" Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fT5qpYL79Eqs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1764880885496,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "fT5qpYL79Eqs",
    "outputId": "62494477-eae2-4c25-a8e3-21219fd03a75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Original Preview ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>SIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>7021</td>\n",
       "      <td>S3711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>7021</td>\n",
       "      <td>S3312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2001-11-30</td>\n",
       "      <td>7021</td>\n",
       "      <td>S3321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2002-11-15</td>\n",
       "      <td>7021</td>\n",
       "      <td>S3317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2005-04-15</td>\n",
       "      <td>7021</td>\n",
       "      <td>S3312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE ItemCode    SIC\n",
       "0  C02500770  1995-12-29     7021  S3711\n",
       "1  C02520200  1996-05-03     7021  S3312\n",
       "2  C02520200  2001-11-30     7021  S3321\n",
       "3  C02520200  2002-11-15     7021  S3317\n",
       "4  C02520200  2005-04-15     7021  S3312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'ItemCode' dropped.\n",
      "\n",
      "Rows before cleaning: 227315\n",
      "Rows after cleaning:  226463\n",
      "Removed rows:         852\n",
      "Unique IDs (final):   103693\n",
      "Saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_SIC_v2.txt\n",
      "\n",
      "=== Cleaned Preview ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>SIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>3711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2001-11-30</td>\n",
       "      <td>3321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2002-11-15</td>\n",
       "      <td>3317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2005-04-15</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE   SIC\n",
       "0  C02500770  1995-12-29  3711\n",
       "1  C02520200  1996-05-03  3312\n",
       "2  C02520200  2001-11-30  3321\n",
       "3  C02520200  2002-11-15  3317\n",
       "4  C02520200  2005-04-15  3312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY — FILTERED SIC PROCESSING\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   1. Loads the file: filtered_SIC.txt\n",
    "#   2. Cleans the SIC column by:\n",
    "#        - Removing leading 'S' (e.g., S1000 → 1000)\n",
    "#        - Removing rows where SIC is:\n",
    "#            * \"Ns\"\n",
    "#            * Empty\n",
    "#            * Non-numeric\n",
    "#   3. Drops the ItemCode column entirely\n",
    "#   4. Saves the cleaned dataset as: filtered_SIC_v2.txt\n",
    "#   5. Prints:\n",
    "#        - Before/after row counts\n",
    "#        - Number of removed rows\n",
    "#        - Number of unique IDs in the final dataset\n",
    "#        - A preview of the cleaned data\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Input / Output paths\n",
    "INPUT_FILE = f'{Temp_file_path_EoC}/filtered_SIC.txt'\n",
    "OUTPUT_FILE = f'{Temp_file_path_EoC}/filtered_SIC_v2.txt'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load the file\n",
    "# -----------------------------------------------------------------------------\n",
    "df = pd.read_csv(\n",
    "    INPUT_FILE,\n",
    "    sep=\"|\",\n",
    "    encoding=\"utf-8\",\n",
    "    dtype=str,\n",
    "    keep_default_na=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== Original Preview ===\")\n",
    "display(df.head())\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Remove leading 'S' from SIC column\n",
    "# -----------------------------------------------------------------------------\n",
    "df[\"SIC\"] = df[\"SIC\"].astype(\"string\").str.strip()\n",
    "df[\"SIC\"] = df[\"SIC\"].str.lstrip(\"S\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Remove invalid SIC values:\n",
    "#    - \"Ns\"\n",
    "#    - Empty\n",
    "#    - Non-numeric\n",
    "# -----------------------------------------------------------------------------\n",
    "valid_mask = (\n",
    "    df[\"SIC\"].notna() &\n",
    "    (df[\"SIC\"].str.strip() != \"\") &\n",
    "    (df[\"SIC\"].str.lower() != \"ns\") &\n",
    "    (df[\"SIC\"].str.isnumeric())\n",
    ")\n",
    "\n",
    "df_clean = df[valid_mask].copy()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Drop ItemCode column\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"ItemCode\" in df_clean.columns:\n",
    "    df_clean.drop(columns=[\"ItemCode\"], inplace=True)\n",
    "    print(\"Column 'ItemCode' dropped.\")\n",
    "else:\n",
    "    print(\"Column 'ItemCode' not found — nothing to drop.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Save cleaned data\n",
    "# -----------------------------------------------------------------------------\n",
    "df_clean.to_csv(\n",
    "    OUTPUT_FILE,\n",
    "    sep=\"|\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Summary + Unique ID count\n",
    "# -----------------------------------------------------------------------------\n",
    "rows_before = len(df)\n",
    "rows_after = len(df_clean)\n",
    "removed_rows = rows_before - rows_after\n",
    "unique_ids_final = df_clean[\"ID\"].nunique()\n",
    "\n",
    "print(f\"\\nRows before cleaning: {rows_before}\")\n",
    "print(f\"Rows after cleaning:  {rows_after}\")\n",
    "print(f\"Removed rows:         {removed_rows}\")\n",
    "print(f\"Unique IDs (final):   {unique_ids_final}\")\n",
    "print(f\"Saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "print(\"\\n=== Cleaned Preview ===\")\n",
    "display(df_clean.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w4n9lwpIAU3c",
   "metadata": {
    "id": "w4n9lwpIAU3c"
   },
   "source": [
    "#### Remove Duplicates Entirely When SIC Between 6000-7000 (Financial Firms) or >= 9000 (Public Administration & Non-classifiable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5PdlyG90C45m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1764880885535,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "5PdlyG90C45m",
    "outputId": "6faf76b7-0882-49b8-bac6-a3a3b597f79e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of filtered_SIC_v2.txt ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>SIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>3711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2001-11-30</td>\n",
       "      <td>3321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2002-11-15</td>\n",
       "      <td>3317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2005-04-15</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE   SIC\n",
       "0  C02500770  1995-12-29  3711\n",
       "1  C02520200  1996-05-03  3312\n",
       "2  C02520200  2001-11-30  3321\n",
       "3  C02520200  2002-11-15  3317\n",
       "4  C02520200  2005-04-15  3312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FILTERING SUMMARY (V3) ===\n",
      "Rows before filtering:              226463\n",
      "Rows after filtering:               160946\n",
      "Rows removed:                       65517\n",
      "Unique IDs before filtering:        103693\n",
      "Unique IDs after filtering:         77573\n",
      "Unique IDs dropped (SIC rule):      26120\n",
      "Saved cleaned file to:              /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_SIC_v3.txt\n",
      "Saved dropped IDs file to:          /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/Dropped_SIC.txt\n",
      "\n",
      "=== Preview of filtered_SIC_v3.txt ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>SIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>3711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2001-11-30</td>\n",
       "      <td>3321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2002-11-15</td>\n",
       "      <td>3317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2005-04-15</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE   SIC\n",
       "0  C02500770  1995-12-29  3711\n",
       "1  C02520200  1996-05-03  3312\n",
       "2  C02520200  2001-11-30  3321\n",
       "3  C02520200  2002-11-15  3317\n",
       "4  C02520200  2005-04-15  3312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of Dropped_SIC.txt ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>SIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C02520250</td>\n",
       "      <td>1997-10-31</td>\n",
       "      <td>6021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C02520250</td>\n",
       "      <td>2010-08-12</td>\n",
       "      <td>6029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C02520260</td>\n",
       "      <td>1997-08-29</td>\n",
       "      <td>6021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C02520260</td>\n",
       "      <td>2010-06-18</td>\n",
       "      <td>6029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>C02520290</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>2911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID    PIT DATE   SIC\n",
       "12  C02520250  1997-10-31  6021\n",
       "13  C02520250  2010-08-12  6029\n",
       "14  C02520260  1997-08-29  6021\n",
       "15  C02520260  2010-06-18  6029\n",
       "23  C02520290  1995-12-29  2911"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY — FILTERED SIC PROCESSING (VERSION 3)\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   1. Loads the file: filtered_SIC_v2.txt\n",
    "#   2. Converts SIC to numeric (safely).\n",
    "#   3. Identifies \"bad\" SIC values:\n",
    "#        - 6000 <= SIC <= 7000  (inclusive)\n",
    "#        - SIC >= 9000\n",
    "#   4. Finds all IDs that have at least one \"bad\" SIC value.\n",
    "#   5. Removes *all rows* for those IDs from the dataset.\n",
    "#   6. Saves:\n",
    "#        - Cleaned dataset as: filtered_SIC_v3.txt\n",
    "#        - Dropped IDs + their rows as: Dropped_SIC.txt\n",
    "#   7. Prints:\n",
    "#        - Rows before/after\n",
    "#        - Unique IDs before/after\n",
    "#        - Number of unique IDs dropped due to the SIC rule\n",
    "# =============================================================================\n",
    "\n",
    "# Input / Output paths\n",
    "INPUT_FILE = f'{Temp_file_path_EoC}/filtered_SIC_v2.txt'\n",
    "OUTPUT_FILE = f'{Temp_file_path_EoC}/filtered_SIC_v3.txt'\n",
    "DROPPED_FILE = f'{Temp_file_path_EoC}/Dropped_SIC.txt'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Load v2\n",
    "# -----------------------------------------------------------------------------\n",
    "df_v2 = pd.read_csv(\n",
    "    INPUT_FILE,\n",
    "    sep=\"|\",\n",
    "    encoding=\"utf-8\",\n",
    "    dtype=str,\n",
    "    keep_default_na=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== Preview of filtered_SIC_v2.txt ===\")\n",
    "display(df_v2.head())\n",
    "\n",
    "rows_before = len(df_v2)\n",
    "unique_ids_before = df_v2[\"ID\"].nunique()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Ensure SIC is numeric\n",
    "# -----------------------------------------------------------------------------\n",
    "sic_numeric = pd.to_numeric(df_v2[\"SIC\"], errors=\"coerce\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Row-level mask: identify \"bad\" SIC values\n",
    "#    - 6000 <= SIC <= 7000\n",
    "#    - SIC >= 9000\n",
    "# -----------------------------------------------------------------------------\n",
    "bad_sic_mask = ((sic_numeric >= 6000) & (sic_numeric <= 7000)) | (sic_numeric >= 9000)\n",
    "\n",
    "# IDs that have at least one \"bad\" SIC\n",
    "ids_with_bad_sic = df_v2.loc[bad_sic_mask, \"ID\"].unique()\n",
    "num_ids_dropped_due_to_sic = len(ids_with_bad_sic)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Create dropped dataset (FULL rows for dropped IDs)\n",
    "# -----------------------------------------------------------------------------\n",
    "df_dropped = df_v2[df_v2[\"ID\"].isin(ids_with_bad_sic)].copy()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Remove ALL rows for those IDs from main dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "df_v3 = df_v2[~df_v2[\"ID\"].isin(ids_with_bad_sic)].copy()\n",
    "\n",
    "rows_after = len(df_v3)\n",
    "unique_ids_after = df_v3[\"ID\"].nunique()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. Save outputs\n",
    "# -----------------------------------------------------------------------------\n",
    "df_v3.to_csv(\n",
    "    OUTPUT_FILE,\n",
    "    sep=\"|\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "df_dropped.to_csv(\n",
    "    DROPPED_FILE,\n",
    "    sep=\"|\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. Summary\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n=== FILTERING SUMMARY (V3) ===\")\n",
    "print(f\"Rows before filtering:              {rows_before}\")\n",
    "print(f\"Rows after filtering:               {rows_after}\")\n",
    "print(f\"Rows removed:                       {rows_before - rows_after}\")\n",
    "print(f\"Unique IDs before filtering:        {unique_ids_before}\")\n",
    "print(f\"Unique IDs after filtering:         {unique_ids_after}\")\n",
    "print(f\"Unique IDs dropped (SIC rule):      {num_ids_dropped_due_to_sic}\")\n",
    "print(f\"Saved cleaned file to:              {OUTPUT_FILE}\")\n",
    "print(f\"Saved dropped IDs file to:          {DROPPED_FILE}\")\n",
    "\n",
    "print(\"\\n=== Preview of filtered_SIC_v3.txt ===\")\n",
    "display(df_v3.head())\n",
    "\n",
    "print(\"\\n=== Preview of Dropped_SIC.txt ===\")\n",
    "display(df_dropped.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xONxHzcTD8EO",
   "metadata": {
    "id": "xONxHzcTD8EO"
   },
   "source": [
    "#### Get Unique IDs from SIC Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "A0KN1rISEDvV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1764880885576,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "A0KN1rISEDvV",
    "outputId": "d1b49a13-5e47-427b-d292-773388a6dde3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of filtered_SIC_v3.txt ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>SIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>3711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2001-11-30</td>\n",
       "      <td>3321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2002-11-15</td>\n",
       "      <td>3317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2005-04-15</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE   SIC\n",
       "0  C02500770  1995-12-29  3711\n",
       "1  C02520200  1996-05-03  3312\n",
       "2  C02520200  2001-11-30  3321\n",
       "3  C02520200  2002-11-15  3317\n",
       "4  C02520200  2005-04-15  3312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SIC CLEAN FILE SUMMARY ===\n",
      "Rows in v3 file:          160946\n",
      "Unique IDs in v3 file:   77573\n",
      "Rows written to clean:   77573\n",
      "Saved clean ID file to:  /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/SIC_clean.txt\n",
      "\n",
      "=== Preview of SIC_clean.txt ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02520220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID\n",
       "0  C02500770\n",
       "1  C02520200\n",
       "2  C02520220\n",
       "3  C02520230\n",
       "4  C02520240"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY — CREATE CLEAN UNIQUE SIC ID FILE\n",
    "# =============================================================================\n",
    "# This script:\n",
    "#   1. Loads the file: filtered_SIC_v3.txt\n",
    "#   2. Extracts all UNIQUE IDs only\n",
    "#   3. Sorts the IDs alphabetically\n",
    "#   4. Drops all other columns\n",
    "#   5. Saves the result as: SIC_clean.txt\n",
    "#   6. Prints a short summary and preview\n",
    "# =============================================================================\n",
    "\n",
    "# Input / Output paths\n",
    "INPUT_FILE = f'{Temp_file_path_EoC}/filtered_SIC_v3.txt'\n",
    "OUTPUT_FILE = f'{Temp_file_path_EoC}/SIC_clean.txt'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Load v3\n",
    "# -----------------------------------------------------------------------------\n",
    "df_v3 = pd.read_csv(\n",
    "    INPUT_FILE,\n",
    "    sep=\"|\",\n",
    "    encoding=\"utf-8\",\n",
    "    dtype=str,\n",
    "    keep_default_na=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== Preview of filtered_SIC_v3.txt ===\")\n",
    "display(df_v3.head())\n",
    "\n",
    "rows_before = len(df_v3)\n",
    "unique_ids_before = df_v3[\"ID\"].nunique()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Extract, deduplicate, and sort unique IDs\n",
    "# -----------------------------------------------------------------------------\n",
    "df_clean_ids = (\n",
    "    df_v3[[\"ID\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(by=\"ID\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "rows_after = len(df_clean_ids)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Save clean ID file\n",
    "# -----------------------------------------------------------------------------\n",
    "df_clean_ids.to_csv(\n",
    "    OUTPUT_FILE,\n",
    "    sep=\"|\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Summary\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n=== SIC CLEAN FILE SUMMARY ===\")\n",
    "print(f\"Rows in v3 file:          {rows_before}\")\n",
    "print(f\"Unique IDs in v3 file:   {unique_ids_before}\")\n",
    "print(f\"Rows written to clean:   {rows_after}\")\n",
    "print(f\"Saved clean ID file to:  {OUTPUT_FILE}\")\n",
    "\n",
    "print(\"\\n=== Preview of SIC_clean.txt ===\")\n",
    "display(df_clean_ids.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6iPI0AmBMhPI",
   "metadata": {
    "id": "6iPI0AmBMhPI"
   },
   "source": [
    "### ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LxgZ08CnMwGe",
   "metadata": {
    "id": "LxgZ08CnMwGe"
   },
   "source": [
    "#### Overview of Unique IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "A_tXjHJ0M2AC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6127,
     "status": "ok",
     "timestamp": 1764880891705,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "A_tXjHJ0M2AC",
    "outputId": "1e08b054-f8a3-4584-e6f3-1ca63f700fd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique IDs: 120815\n",
      "Unique IDs saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_ids.txt\n"
     ]
    }
   ],
   "source": [
    "# This cell reads the large fundamentals file in chunks and extracts all unique IDs.\n",
    "# It performs a filtering operation by selecting only the \"ID\" column during loading,\n",
    "# reducing memory footprint and ensuring the code processes only relevant data.\n",
    "# The transformation involves accumulating unique IDs across all chunks and\n",
    "# writing the deduplicated list into a text file for downstream processing.\n",
    "\n",
    "output_path = f'{Temp_file_path_EoC}/filtered_ids.txt'\n",
    "\n",
    "# Define column names so the CSV parser assigns correct labels\n",
    "column_names = [\"ID\", \"PIT Date\", \"Frequency\", \"FiscalPeriod\", \"ItemCode\", \"Value\"]\n",
    "\n",
    "# Number of rows to load per chunk. This helps handle large files efficiently.\n",
    "chunk_size = 1_000_000\n",
    "\n",
    "# Set to store unique IDs without duplicates\n",
    "unique_ids = set()\n",
    "\n",
    "# Read the large dataset in chunks, selecting only the ID column for efficiency\n",
    "for chunk in pd.read_csv(\n",
    "    Fundamentals_clean_file_path,\n",
    "    sep=\"|\",                   # Pipe-separated file\n",
    "    header=0,                 # Use the first row as header\n",
    "    names=column_names,       # Assign predefined column names\n",
    "    usecols=[\"ID\"],           # Load only the ID column to reduce memory usage\n",
    "    dtype={\"ID\": \"string\"},   # Ensure IDs are treated as strings\n",
    "    chunksize=chunk_size,     # Load in controlled chunk sizes\n",
    "    engine=\"python\"           # Use Python engine for flexibility with separators\n",
    "):\n",
    "    # Extract unique IDs in the current chunk, drop missing values, and update the global set\n",
    "    unique_ids.update(chunk[\"ID\"].dropna().unique())\n",
    "\n",
    "# Print the number of unique IDs collected\n",
    "print(\"Unique IDs:\", len(unique_ids))\n",
    "\n",
    "# Write all unique IDs into the output text file in sorted order\n",
    "with open(output_path, \"w\") as f:\n",
    "    for uid in sorted(unique_ids):\n",
    "        f.write(f\"{uid}\\n\")\n",
    "\n",
    "print(f\"Unique IDs saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rlk28gvLNCro",
   "metadata": {
    "id": "rlk28gvLNCro"
   },
   "source": [
    "#### Check Invalid IDs => Only 1 => Fixed Manually and Considered as Company (WSID = C00948205, i.e., CUSIP of the Company for Some Reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "_7-ihM5uN3y6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1764880891739,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "_7-ihM5uN3y6",
    "outputId": "d2ca5d07-546a-4e8a-b5fc-223560599ede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total IDs: 120815\n",
      "Non-alphanumeric: 0\n",
      "Wrong length (!= 9): 0\n",
      "Not starting with 'C': 0\n",
      "Ending with 1–9: 1\n",
      "Total error IDs: 1\n",
      "Percentage: 0.00%\n",
      "Saved 1 error IDs to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/error_ids.txt\n",
      "Example → C00948205 : ends with 1–9\n"
     ]
    }
   ],
   "source": [
    "# This cell validates structural quality of previously extracted IDs.\n",
    "# It performs filtering logic by checking each ID against multiple rules:\n",
    "# - must be alphanumeric\n",
    "# - must be exactly 9 characters long\n",
    "# - must start with the letter 'C'\n",
    "# - must not end with digits 1–9\n",
    "# The transformation step assigns error categories to IDs that fail any rule,\n",
    "# aggregates these issues, produces summary statistics, and writes all invalid\n",
    "# IDs with error reasons to a text file for downstream analysis.\n",
    "\n",
    "# Define the path to the file containing unique IDs and the output file for invalid IDs\n",
    "input_path = f'{Temp_file_path_EoC}/filtered_ids.txt'\n",
    "output_path = f\"{Temp_file_path_EoC}/error_ids.txt\"\n",
    "\n",
    "# Initialize counters for different types of errors found during validation\n",
    "count_ending_1to9 = 0        # IDs ending with digits 1–9\n",
    "count_wrong_length = 0       # IDs not exactly 9 characters long\n",
    "count_not_starting_C = 0     # IDs not starting with 'C'\n",
    "count_non_alnum = 0          # IDs containing non-alphanumeric characters\n",
    "total_count = 0              # Total number of IDs processed\n",
    "\n",
    "# List to store invalid IDs together with the reason(s) they failed validation\n",
    "error_ids = []\n",
    "\n",
    "# Read and validate each ID from the input file\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        id_str = line.strip()  # Remove newline and surrounding whitespace\n",
    "\n",
    "        # Skip empty or whitespace-only lines\n",
    "        if not id_str:\n",
    "            continue\n",
    "\n",
    "        # Remove potential UTF-8 BOM prefix if present\n",
    "        if id_str.startswith(\"\\ufeff\"):\n",
    "            id_str = id_str.lstrip(\"\\ufeff\")\n",
    "\n",
    "        # Count every processed ID\n",
    "        total_count += 1\n",
    "\n",
    "        # Apply validation checks to the current ID\n",
    "        has_non_alnum = not id_str.isalnum()                    # Contains characters other than letters/digits\n",
    "        wrong_length = (len(id_str) != 9)                       # Must be exactly 9 characters long\n",
    "        not_starting_C = not id_str.startswith(\"C\")             # Must begin with 'C'\n",
    "        ends_with_1to9 = id_str[-1].isdigit() and id_str[-1] != \"0\"  # Ending digit must not be 1–9\n",
    "\n",
    "        # Collect error reasons for this ID\n",
    "        reasons = []\n",
    "\n",
    "        # Record each failed rule and increment corresponding count\n",
    "        if has_non_alnum:\n",
    "            reasons.append(\"non-alphanumeric chars\")\n",
    "            count_non_alnum += 1\n",
    "        if wrong_length:\n",
    "            reasons.append(f\"length {len(id_str)} ≠ 9\")\n",
    "            count_wrong_length += 1\n",
    "        if not_starting_C:\n",
    "            reasons.append(\"does not start with 'C'\")\n",
    "            count_not_starting_C += 1\n",
    "        if ends_with_1to9:\n",
    "            reasons.append(\"ends with 1–9\")\n",
    "            count_ending_1to9 += 1\n",
    "\n",
    "        # If there are any validation issues, store this ID and its reason(s)\n",
    "        if reasons:\n",
    "            error_ids.append((id_str, \", \".join(reasons)))\n",
    "\n",
    "# Print aggregated validation statistics for quick overview\n",
    "print(f\"Total IDs: {total_count}\")\n",
    "print(f\"Non-alphanumeric: {count_non_alnum}\")\n",
    "print(f\"Wrong length (!= 9): {count_wrong_length}\")\n",
    "print(f\"Not starting with 'C': {count_not_starting_C}\")\n",
    "print(f\"Ending with 1–9: {count_ending_1to9}\")\n",
    "print(f\"Total error IDs: {len(error_ids)}\")\n",
    "print(f\"Percentage: {len(error_ids) / total_count * 100:.2f}%\")\n",
    "\n",
    "# Save all invalid IDs including error reasons into a separate output file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for uid, reason in error_ids:\n",
    "        out_file.write(f\"{uid}\\t{reason}\\n\")\n",
    "\n",
    "print(f\"Saved {len(error_ids)} error IDs to: {output_path}\")\n",
    "\n",
    "# Show a few example invalid IDs for inspection\n",
    "for uid, reason in error_ids[:10]:\n",
    "    print(f\"Example → {uid} : {reason}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OxPbDZndOSVC",
   "metadata": {
    "id": "OxPbDZndOSVC"
   },
   "source": [
    "#### Get Number of Companies (Ending with 0 or 5 - 5 is an Error Name but Data is True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "qSNo6V8COgSJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1764880891774,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "qSNo6V8COgSJ",
    "outputId": "98cc90b5-196e-4e17-c6c3-82b96bad69f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total IDs: 120815\n",
      "IDs ending with '0' or '5': 103963\n",
      "Percentage: 86.05%\n"
     ]
    }
   ],
   "source": [
    "# This cell filters the list of previously extracted IDs based on a specific pattern:\n",
    "# it selects only those IDs that end with either '0' or '5'. This acts as a\n",
    "# targeted filtering step to isolate a subset of IDs with a particular suffix rule.\n",
    "# The transformation consists of collecting matching IDs, counting them, reporting\n",
    "# proportions, and writing the filtered list to a new text file.\n",
    "\n",
    "# Define input file containing all unique IDs and output file for filtered results\n",
    "input_path = f'{Temp_file_path_EoC}/filtered_ids.txt'\n",
    "output_path = f'{Temp_file_path_EoC}/filtered_company_ids.txt'\n",
    "\n",
    "# Initialize counters\n",
    "count_ending_0_or_5 = 0     # Number of IDs ending with 0 or 5\n",
    "total_count = 0             # Total number of IDs processed\n",
    "\n",
    "# List to store IDs that match the filter condition\n",
    "ids_with_0_or_5 = []\n",
    "\n",
    "# Read and evaluate each ID from the input file\n",
    "with open(input_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        id_str = line.strip()          # Remove whitespace and newline\n",
    "\n",
    "        # Skip empty lines\n",
    "        if id_str:\n",
    "            total_count += 1           # Count each valid line\n",
    "\n",
    "            # Check whether the ID ends with '0' or '5'\n",
    "            if id_str.endswith((\"0\", \"5\")):\n",
    "                count_ending_0_or_5 += 1\n",
    "                ids_with_0_or_5.append(id_str)\n",
    "\n",
    "# Print summary statistics of the filtering operation\n",
    "print(f\"Total IDs: {total_count}\")\n",
    "print(f\"IDs ending with '0' or '5': {count_ending_0_or_5}\")\n",
    "print(f\"Percentage: {count_ending_0_or_5 / total_count * 100:.2f}%\")\n",
    "\n",
    "# Save the filtered set of IDs into the output file\n",
    "with open(output_path, \"w\") as out_file:\n",
    "    for uid in ids_with_0_or_5:\n",
    "        out_file.write(uid + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m6kUSXzaPZln",
   "metadata": {
    "id": "m6kUSXzaPZln"
   },
   "source": [
    "#### Get Number of Securities (Ending with Letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "QuljADIJPfoI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1764880891810,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "QuljADIJPfoI",
    "outputId": "88d8f581-bb1d-4d78-dd24-729bdcae2519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total IDs: 120815\n",
      "IDs ending with a letter: 16852\n",
      "Percentage: 13.95%\n"
     ]
    }
   ],
   "source": [
    "# This cell filters the list of IDs by selecting only those whose final character\n",
    "# is an alphabetic letter (A–Z or a–z). This filtering step isolates IDs that\n",
    "# follow a letter-suffix pattern and separates them into a dedicated output file.\n",
    "# The transformation includes scanning each ID, checking its ending character,\n",
    "# counting how many match the rule, calculating proportions, and writing all\n",
    "# matching IDs to disk for downstream categorization.\n",
    "\n",
    "# Define input path with all IDs and output path for the filtered subset\n",
    "input_path = f'{Temp_file_path_EoC}/filtered_ids.txt'\n",
    "output_path = f\"{Temp_file_path_EoC}/filtered_security_ids.txt\"\n",
    "\n",
    "# Precompute the full set of alphabetic characters for quick membership checks\n",
    "letters = set(string.ascii_letters)\n",
    "\n",
    "# Initialize counters for summary statistics\n",
    "total_count = 0               # Total number of IDs processed\n",
    "count_ending_letter = 0       # Number of IDs ending with a letter\n",
    "\n",
    "# Store IDs that meet the condition\n",
    "ids_with_letter = []\n",
    "\n",
    "# Process the input file line by line\n",
    "with open(input_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        id_str = line.strip()             # Remove surrounding whitespace and newline\n",
    "\n",
    "        # Skip blank or whitespace-only lines\n",
    "        if id_str:\n",
    "            total_count += 1              # Count each valid ID\n",
    "\n",
    "            # Check whether the last character is a letter\n",
    "            if id_str[-1] in letters:\n",
    "                count_ending_letter += 1\n",
    "                ids_with_letter.append(id_str)\n",
    "\n",
    "# Print summary of the filtering operation\n",
    "print(f\"Total IDs: {total_count}\")\n",
    "print(f\"IDs ending with a letter: {count_ending_letter}\")\n",
    "print(f\"Percentage: {count_ending_letter / total_count * 100:.2f}%\")\n",
    "\n",
    "# Write all matching IDs to the designated output file\n",
    "with open(output_path, \"w\") as out_file:\n",
    "    for uid in ids_with_letter:\n",
    "        out_file.write(uid + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wD0VdQzJPqv2",
   "metadata": {
    "id": "wD0VdQzJPqv2"
   },
   "source": [
    "#### Check, for Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "uYzL-84eP2WB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1764880891847,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "uYzL-84eP2WB",
    "outputId": "16bd279a-e6dd-47ba-c3b3-647296f98a75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line performs a consistency check ensuring that all IDs have been\n",
    "# fully partitioned into three categories:\n",
    "# - IDs with validation errors\n",
    "# - IDs ending with letters\n",
    "# - IDs ending with 0 or 5\n",
    "# The transformation here is a logical verification: it checks whether the sum\n",
    "# of these three groups equals the total number of processed IDs. If the result\n",
    "# is True, all IDs were accounted for without overlap or omission.\n",
    "\n",
    "# Compare total items in all categorized groups against the total processed count\n",
    "len(error_ids) + count_ending_letter + count_ending_0_or_5 == total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "KyANw8poPhy8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1764880891883,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "KyANw8poPhy8",
    "outputId": "3767ed76-e609-4dbc-ecdc-243600cb5758"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line performs a consistency check ensuring that all IDs have been\n",
    "# fully partitioned into three categories:\n",
    "# - IDs with validation errors\n",
    "# - IDs ending with letters\n",
    "# - IDs ending with 0 or 5\n",
    "# The transformation here is a logical verification: it checks whether the sum\n",
    "# of these three groups equals the total number of processed IDs. If the result\n",
    "# is True, all IDs were accounted for without overlap or omission.\n",
    "\n",
    "count_ending_letter + count_ending_0_or_5 == total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "REwRiMkbP781",
   "metadata": {
    "id": "REwRiMkbP781"
   },
   "source": [
    "#### Verification of Implied Nation Code from the WSID through Match with True Country. Nation Code used for True Country as Country Name from the Current File is not Clean, i.e., Values like \"Germany\" and \"Germayn\" are Mixed up. Securities are Excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fd8Gry81Rtk5",
   "metadata": {
    "id": "Fd8Gry81Rtk5"
   },
   "source": [
    "##### Extract Implied Nation Code (ImplNatCo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1jsgCKtRQWM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1764880891932,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "d1jsgCKtRQWM",
    "outputId": "1a6f8f9d-9b84-46c7-fc4a-36b0a7136339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_company_ids_v2.txt\n",
      "Preview:\n",
      "          ID ImplNatCo\n",
      "0  C00948205       009\n",
      "1  C02500770       025\n",
      "2  C02520200       025\n",
      "3  C02520220       025\n",
      "4  C02520230       025\n"
     ]
    }
   ],
   "source": [
    "# This cell enriches the list of company-related IDs by extracting a specific\n",
    "# substring (characters 2–4) from each ID. This acts as a transformation step\n",
    "# that derives a new feature called \"ImplNatCo\". The code reads the IDs,\n",
    "# performs substring extraction, builds a DataFrame pairing each ID with its\n",
    "# derived value, and writes the result to a delimited text file for further\n",
    "# analysis or categorization.\n",
    "\n",
    "# Paths for input file (raw IDs) and output file (enhanced with substring field)\n",
    "FILE = f'{Temp_file_path_EoC}/filtered_company_ids.txt'\n",
    "OUTPUT = f'{Temp_file_path_EoC}/filtered_company_ids_v2.txt'\n",
    "\n",
    "# Read all non-empty lines from the input file, stripping whitespace\n",
    "with open(FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    ids = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Extract characters at index positions 1–3 (2nd to 4th character)\n",
    "# If the ID is too short, return an empty string as fallback\n",
    "impl_nat_co = [id_str[1:4] if len(id_str) >= 4 else \"\" for id_str in ids]\n",
    "\n",
    "# Create DataFrame linking each ID with its derived substring\n",
    "df = pd.DataFrame({\n",
    "    \"ID\": ids,\n",
    "    \"ImplNatCo\": impl_nat_co\n",
    "})\n",
    "\n",
    "# Save the result as a pipe-separated file for clarity and compatibility\n",
    "df.to_csv(OUTPUT, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Print output location and show the first few rows for inspection\n",
    "print(f\"File saved to: {OUTPUT}\")\n",
    "print(\"Preview:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ifOtcIUiSOaG",
   "metadata": {
    "id": "ifOtcIUiSOaG"
   },
   "source": [
    "#### Merge with Official Nation Codes from Current File\n",
    "#### Comment:\n",
    "#### NatCo 009 Wrong => Checked and Changed to Canada (Place of Listing)\n",
    "#### NatCo 288 Wrong => Checked and Changed to Ghana (Place of Listing)\n",
    "#### NatCo 466 Wrong => Checked and Changed to Macau (Place of Listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2YtKPQIQSXRF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1764880891981,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "2YtKPQIQSXRF",
    "outputId": "ecff5f1d-d3ea-490f-d7cb-a31433d5083e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge complete.\n",
      "Matching NatCo values: 103962\n",
      "Non-matching NatCo values: 1\n",
      "Total rows checked: 103963\n",
      "Mismatched NatCo IDs saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_company_NatCoMissmatch_ids.txt\n",
      "\n",
      "Full merged file saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_company_ids_v3.txt\n",
      "          ID ImplNatCo NatCo NatCo_Match\n",
      "0  C00948205       009   840          No\n",
      "1  C02500770       025   025         Yes\n",
      "2  C02520200       025   025         Yes\n",
      "3  C02520220       025   025         Yes\n",
      "4  C02520230       025   025         Yes\n"
     ]
    }
   ],
   "source": [
    "# This cell combines two datasets to validate whether the derived ImplNatCo codes\n",
    "# match the official NatCo values. It performs several data transformations:\n",
    "# - Cleans column names and ID fields\n",
    "# - Merges the two datasets on the ID column\n",
    "# - Compares the ImplNatCo substring with the NatCo column\n",
    "# - Classifies each row as match or mismatch\n",
    "# - Outputs both the full merged data and a separate file containing only mismatches\n",
    "# This workflow applies filtering by selecting mismatched rows and transformation\n",
    "# by deriving new comparison fields.\n",
    "\n",
    "# Define input and output file paths\n",
    "unique_ids_path = f'{Temp_file_path_EoC}/filtered_company_ids_v2.txt'\n",
    "natco_path = f\"{Temp_file_path_EoC}/NationCodes_clean.txt\"\n",
    "output_path = f'{Temp_file_path_EoC}/filtered_company_ids_v3.txt'\n",
    "mismatch_output = f'{Temp_file_path_EoC}/filtered_company_NatCoMissmatch_ids.txt'\n",
    "\n",
    "# Load the ID dataset and the NatCo reference data\n",
    "df_unique = pd.read_csv(unique_ids_path, sep=\"|\", dtype=str)\n",
    "df_natco_country = pd.read_csv(natco_path, sep=\"|\", dtype=str)\n",
    "\n",
    "# Clean column names and values in both DataFrames\n",
    "for df in [df_unique, df_natco_country]:\n",
    "    df.columns = df.columns.str.strip().str.replace(\"\\ufeff\", \"\")   # Normalize column names\n",
    "    df[\"ID\"] = df[\"ID\"].astype(str).str.strip()                     # Ensure clean, string-based IDs\n",
    "\n",
    "# Merge the two datasets on the ID column using a left join\n",
    "df_merged = df_unique.merge(df_natco_country, on=\"ID\", how=\"left\")\n",
    "\n",
    "# Add a column indicating whether ImplNatCo matches NatCo after trimming\n",
    "df_merged[\"NatCo_Match\"] = df_merged.apply(\n",
    "    lambda row: \"Yes\" if str(row.get(\"ImplNatCo\", \"\")).strip() == str(row.get(\"NatCo\", \"\")).strip() else \"No\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compute summary counts for matching and non-matching NatCo values\n",
    "match_count = (df_merged[\"NatCo_Match\"] == \"Yes\").sum()\n",
    "mismatch_count = (df_merged[\"NatCo_Match\"] == \"No\").sum()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Merge complete.\")\n",
    "print(f\"Matching NatCo values: {match_count}\")\n",
    "print(f\"Non-matching NatCo values: {mismatch_count}\")\n",
    "print(f\"Total rows checked: {len(df_merged)}\")\n",
    "\n",
    "# Filter rows where the ImplNatCo and NatCo do not match\n",
    "df_mismatch = df_merged[df_merged[\"NatCo_Match\"] == \"No\"][[\"ID\", \"ImplNatCo\", \"NatCo\"]]\n",
    "\n",
    "# Save mismatched IDs to a separate file\n",
    "df_mismatch.to_csv(mismatch_output, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "print(f\"Mismatched NatCo IDs saved to: {mismatch_output}\")\n",
    "\n",
    "# Save the full merged DataFrame including comparison results\n",
    "df_merged.to_csv(output_path, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nFull merged file saved to: {output_path}\")\n",
    "\n",
    "# Show the first few rows of the merged dataset for review\n",
    "print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cSG1QnFiU6Lk",
   "metadata": {
    "id": "cSG1QnFiU6Lk"
   },
   "source": [
    "#### Drop Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "lf15qpOaVsZ9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1764880892031,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "lf15qpOaVsZ9",
    "outputId": "af5db5c7-61d1-4f5f-87e5-e42a523fdd96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_company_ids_v4.txt (rows: 103,963, columns dropped: ['ImplNatCo', 'NatCo', 'NatCo_Match'])\n",
      "=== Head of DF ===\n",
      "          ID\n",
      "0  C00948205\n",
      "1  C02500770\n",
      "2  C02520200\n",
      "3  C02520220\n",
      "4  C02520230\n",
      "5  C02520240\n",
      "6  C02520250\n",
      "7  C02520260\n",
      "8  C02520280\n",
      "9  C02520290\n"
     ]
    }
   ],
   "source": [
    "# This cell cleans the merged dataset by removing columns related to the\n",
    "# ImplNatCo–NatCo comparison. It performs a filtering operation that drops\n",
    "# the columns \"ImplNatCo\", \"NatCo\", and \"NatCo_Match\" if they exist.\n",
    "# The transformation step produces a simplified DataFrame containing only\n",
    "# the essential ID fields, which is then written to a new file.\n",
    "\n",
    "# Paths for the input dataset (v3) and the cleaned output dataset (v4)\n",
    "unique_ids_file = f'{Temp_file_path_EoC}/filtered_company_ids_v3.txt'\n",
    "output_file     = f'{Temp_file_path_EoC}/filtered_company_ids_v4.txt'\n",
    "\n",
    "# Load the dataset containing enriched ID information\n",
    "unique_ids_v3 = pd.read_csv(unique_ids_file, sep=\"|\", dtype=str, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "\n",
    "# Columns intended for removal, typically derived fields from earlier processing\n",
    "cols_to_drop = [\"ImplNatCo\", \"NatCo\", \"NatCo_Match\"]\n",
    "\n",
    "# Identify which of these columns are actually present in the DataFrame\n",
    "existing_cols = [c for c in cols_to_drop if c in unique_ids_v3.columns]\n",
    "\n",
    "# Drop the existing unwanted columns and create a clean copy\n",
    "ID_clean = unique_ids_v3.drop(columns=existing_cols, errors=\"ignore\").copy()\n",
    "\n",
    "# Save the cleaned DataFrame as a pipe-separated file\n",
    "ID_clean.to_csv(output_file, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Print summary information and preview of the cleaned dataset\n",
    "print(f\"DataFrame saved to: {output_file} (rows: {len(ID_clean):,}, columns dropped: {existing_cols})\")\n",
    "print(\"=== Head of DF ===\")\n",
    "print(ID_clean.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u8zSneDaBWbg",
   "metadata": {
    "id": "u8zSneDaBWbg"
   },
   "source": [
    "#### Extract IDs from Datastream to Identify Overlap/Feasible Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "E0XnJ2KeBfzp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5111,
     "status": "ok",
     "timestamp": 1764880897143,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "E0XnJ2KeBfzp",
    "outputId": "a579e16b-bf9b-4aec-9838-c13e6950b445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique IDs (mapping): 95,104\n",
      "Unique IDs (MV):      72,079\n",
      "Unique IDs (TRI):     85,918\n",
      "\n",
      "Total unique IDs IN ALL THREE files (intersection): 71,972\n",
      "\n",
      "Saved intersection IDs to:\n",
      "  /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/Mappaple_DS_IDs.txt\n",
      "\n",
      "Preview:\n",
      "  C41009760\n",
      "  C39237370\n",
      "  C840R5140\n",
      "  C344GB500\n",
      "  C380K8100\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell extracts all unique IDs found across three cleaned datasets located\n",
    "# in Temp_file_path_GO:\n",
    "#   - ID_mapping_clean.txt\n",
    "#   - MV_clean.txt\n",
    "#   - TRI_clean.txt\n",
    "#\n",
    "# It computes the INTERSECTION (only IDs present in ALL THREE FILES),\n",
    "# not the union, and writes the result as Mappaple_DS_IDs.txt\n",
    "# into Temp_file_path_EoC.\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Use your existing paths without overwriting them\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Input files in GO\n",
    "id_mapping_path = Path(Temp_file_path_GO) / \"ID_mapping_clean.txt\"\n",
    "mv_clean_path   = Path(Temp_file_path_GO) / \"MV_clean.txt\"\n",
    "tri_clean_path  = Path(Temp_file_path_GO) / \"TRI_clean.txt\"\n",
    "\n",
    "# Output file goes to EoC\n",
    "mappable_ids_path = Path(Temp_file_path_EoC) / \"Mappaple_DS_IDs.txt\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Helper (RAM-light)\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_unique_ids(file_path: Path) -> set:\n",
    "    if not file_path.exists():\n",
    "        print(f\"Warning: missing file {file_path}\")\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            file_path,\n",
    "            sep=\"|\",\n",
    "            usecols=[\"ID\"],\n",
    "            dtype=str,\n",
    "            low_memory=True\n",
    "        )\n",
    "        return set(df[\"ID\"].dropna())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Load ID sets from the three files\n",
    "# -----------------------------------------------------------------------------\n",
    "ids_mapping = get_unique_ids(id_mapping_path)\n",
    "ids_mv      = get_unique_ids(mv_clean_path)\n",
    "ids_tri     = get_unique_ids(tri_clean_path)\n",
    "\n",
    "print(f\"Unique IDs (mapping): {len(ids_mapping):,}\")\n",
    "print(f\"Unique IDs (MV):      {len(ids_mv):,}\")\n",
    "print(f\"Unique IDs (TRI):     {len(ids_tri):,}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Compute the INTERSECTION instead of the union\n",
    "# -----------------------------------------------------------------------------\n",
    "all_ids = ids_mapping & ids_mv & ids_tri\n",
    "\n",
    "print(f\"\\nTotal unique IDs IN ALL THREE files (intersection): {len(all_ids):,}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Save without creating a big DataFrame (RAM friendly)\n",
    "# -----------------------------------------------------------------------------\n",
    "with mappable_ids_path.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "    f_out.write(\"ID\\n\")\n",
    "    for id_val in all_ids:\n",
    "        if pd.notna(id_val):\n",
    "            f_out.write(f\"{id_val}\\n\")\n",
    "\n",
    "print(f\"\\nSaved intersection IDs to:\\n  {mappable_ids_path}\")\n",
    "\n",
    "print(\"\\nPreview:\")\n",
    "for sample in list(all_ids)[:5]:\n",
    "    print(\" \", sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lLwatKcLWqV4",
   "metadata": {
    "id": "lLwatKcLWqV4"
   },
   "source": [
    "#### Remove ADR IDs, Non-Mappable & False SIC IDs and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "qE1VvMroWwW8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1764880897586,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "qE1VvMroWwW8",
    "outputId": "43518e13-320b-452d-b83e-cadcbb280533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in source file:                     103,963\n",
      "Unique IDs in source file:               103,963\n",
      "Unique ADR IDs loaded:                   2,298\n",
      "Unique IDs in Mappaple_DS_IDs:                  71,972\n",
      "Unique IDs in SIC_clean:                  77,573\n",
      "\n",
      "IDs present in source ∩ mapping:         71,933\n",
      "IDs dropped (not in mapping):            32,030\n",
      "IDs present in source ∩ mapping ∩ SIC:   55,389\n",
      "IDs dropped (not in SIC_clean):          16,544\n",
      "Rows after intersection filtering:       55,389\n",
      "IDs dropped because ADR:                 22\n",
      "Final unique IDs:                        55,367\n",
      "Final row count:                         55,367\n",
      "\n",
      "Cleaned ID file saved to:\n",
      "  /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/ID_clean.txt\n",
      "=== Head of df_cleaned ===\n",
      "          ID\n",
      "1  C02500770\n",
      "2  C02520200\n",
      "3  C02520220\n",
      "4  C02520230\n",
      "5  C02520240\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# CELL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "# This cell filters the company IDs in Temp_file_path_EoC/filtered_company_ids_v4.txt\n",
    "# using three constraints based on four files in Temp_file_path_EoC:\n",
    "#\n",
    "#   1) Keep only IDs that are present in ALL of:\n",
    "#        - filtered_company_ids_v4.txt   (this file, \"source\")\n",
    "#        - Mappaple_DS_IDs.txt           (intersection from the GO files)\n",
    "#        - SIC_clean.txt                 (intersection from the SIC pipeline)\n",
    "#\n",
    "#   2) Remove all ADR IDs found in:\n",
    "#        - ADR_clean.txt\n",
    "#\n",
    "# It also tracks how many unique IDs are removed at each step:\n",
    "#   - Dropped because they are not in Mappaple_DS_IDs.txt\n",
    "#   - Dropped because they are not in SIC_clean.txt\n",
    "#   - Dropped because they are ADRs\n",
    "#\n",
    "# The final filtered IDs are saved as Path(Temp_file_path_EoC)/ID_clean.txt.\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Configure folder (EDIT this to your actual EoC path before running)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Main input file: in EoC\n",
    "source_file = Path(Temp_file_path_EoC) / \"filtered_company_ids_v4.txt\"\n",
    "\n",
    "# ADR list (to remove): in EoC\n",
    "adr_file = Path(Temp_file_path_EoC) / \"ADR_clean.txt\"\n",
    "\n",
    "# Mapping list from GO files: in EoC\n",
    "mappable_file = Path(Temp_file_path_EoC) / \"Mappaple_DS_IDs.txt\"\n",
    "\n",
    "# SIC-based ID list: in EoC\n",
    "sic_file = Path(Temp_file_path_EoC) / \"SIC_clean.txt\"\n",
    "\n",
    "# Final output: ID_clean.txt in EoC\n",
    "output_file = Path(Temp_file_path_EoC) / \"ID_clean.txt\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Load main dataset (filtered_company_ids_v4.txt)\n",
    "# -----------------------------------------------------------------------------\n",
    "df = pd.read_csv(\n",
    "    source_file,\n",
    "    sep=\"|\",\n",
    "    dtype=str,\n",
    "    encoding=\"utf-8\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "source_ids = set(df[\"ID\"].dropna().tolist())\n",
    "\n",
    "print(f\"Rows in source file:                     {len(df):,}\")\n",
    "print(f\"Unique IDs in source file:               {len(source_ids):,}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Load ADR IDs (ADR_clean.txt)\n",
    "# -----------------------------------------------------------------------------\n",
    "ids_adr = set()\n",
    "if adr_file.exists():\n",
    "    with open(adr_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            id_str = line.strip().split(\"|\")[0]\n",
    "            if id_str:\n",
    "                ids_adr.add(id_str)\n",
    "    print(f\"Unique ADR IDs loaded:                   {len(ids_adr):,}\")\n",
    "else:\n",
    "    print(f\"Warning: ADR file not found:             {adr_file}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Helper: load ID sets (Mappaple_DS_IDs, SIC_clean)\n",
    "# -----------------------------------------------------------------------------\n",
    "def load_id_set(path: Path, label: str) -> set:\n",
    "    if not path.exists():\n",
    "        print(f\"Warning: {label} not found:              {path}\")\n",
    "        return set()\n",
    "    try:\n",
    "        df_ids = pd.read_csv(path, sep=\"|\", usecols=[\"ID\"], dtype=str)\n",
    "        out = set(df_ids[\"ID\"].dropna().tolist())\n",
    "        print(f\"Unique IDs in {label}:                  {len(out):,}\")\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {label} ({path}): {e}\")\n",
    "        return set()\n",
    "\n",
    "ids_mappable = load_id_set(mappable_file, \"Mappaple_DS_IDs\")\n",
    "ids_sic      = load_id_set(sic_file, \"SIC_clean\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Intersection filtering step 1:\n",
    "#    Keep only IDs that are in BOTH source and Mappaple_DS_IDs\n",
    "# -----------------------------------------------------------------------------\n",
    "ids_in_source_and_mapping = source_ids & ids_mappable\n",
    "dropped_not_in_mapping = len(source_ids) - len(ids_in_source_and_mapping)\n",
    "\n",
    "print(f\"\\nIDs present in source ∩ mapping:         {len(ids_in_source_and_mapping):,}\")\n",
    "print(f\"IDs dropped (not in mapping):            {dropped_not_in_mapping:,}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Intersection filtering step 2:\n",
    "#    From that set, keep only IDs that are also in SIC_clean\n",
    "# -----------------------------------------------------------------------------\n",
    "ids_in_all = ids_in_source_and_mapping & ids_sic\n",
    "dropped_not_in_sic = len(ids_in_source_and_mapping) - len(ids_in_all)\n",
    "\n",
    "print(f\"IDs present in source ∩ mapping ∩ SIC:   {len(ids_in_all):,}\")\n",
    "print(f\"IDs dropped (not in SIC_clean):          {dropped_not_in_sic:,}\")\n",
    "\n",
    "df_step = df[df[\"ID\"].isin(ids_in_all)].copy()\n",
    "print(f\"Rows after intersection filtering:       {len(df_step):,}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Remove ADRs\n",
    "# -----------------------------------------------------------------------------\n",
    "adr_in_step = ids_in_all & ids_adr\n",
    "print(f\"IDs dropped because ADR:                 {len(adr_in_step):,}\")\n",
    "\n",
    "df_cleaned = df_step[~df_step[\"ID\"].isin(ids_adr)].copy()\n",
    "\n",
    "final_ids = set(df_cleaned[\"ID\"].dropna().tolist())\n",
    "print(f\"Final unique IDs:                        {len(final_ids):,}\")\n",
    "print(f\"Final row count:                         {len(df_cleaned):,}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8) Save output — overwrite ID_clean.txt in EoC\n",
    "# -----------------------------------------------------------------------------\n",
    "df_cleaned.to_csv(output_file, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\nCleaned ID file saved to:\\n  {output_file}\")\n",
    "print(\"=== Head of df_cleaned ===\")\n",
    "print(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df13a2",
   "metadata": {
    "id": "a8df13a2"
   },
   "source": [
    "### Currency Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff8fae",
   "metadata": {
    "id": "bdff8fae"
   },
   "source": [
    "#### Load Initial Currency Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f408b9ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1764880897667,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "f408b9ac",
    "outputId": "f3086dd7-416e-4cf0-fb76-9e5f3121b93b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_CurrencyCode.txt.\n",
      "\n",
      "=== Preview of loaded data ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>CurrencyCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>56027</td>\n",
       "      <td>'Usd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>56027</td>\n",
       "      <td>'Ars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0250077A</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>56027</td>\n",
       "      <td>'Ars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0250077B</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>56027</td>\n",
       "      <td>'Ars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0250077C</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>56027</td>\n",
       "      <td>'Ars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE ItemCode CurrencyCode\n",
       "0  C00948205  2021-07-09    56027         'Usd\n",
       "1  C02500770  1995-12-29    56027         'Ars\n",
       "2  C0250077A  1999-10-01    56027         'Ars\n",
       "3  C0250077B  1999-10-01    56027         'Ars\n",
       "4  C0250077C  1999-10-01    56027         'Ars"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned 'CurrencyCode' column (removed leading single quote and stripped whitespace).\n",
      "Converted 'PIT DATE' to datetime objects.\n",
      "\n",
      "=== Preview after initial cleaning ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>CurrencyCode</th>\n",
       "      <th>PIT DATE_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>56027</td>\n",
       "      <td>Usd</td>\n",
       "      <td>2021-07-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>56027</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0250077A</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>56027</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0250077B</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>56027</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0250077C</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>56027</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE ItemCode CurrencyCode PIT DATE_dt\n",
       "0  C00948205  2021-07-09    56027          Usd  2021-07-09\n",
       "1  C02500770  1995-12-29    56027          Ars  1995-12-29\n",
       "2  C0250077A  1999-10-01    56027          Ars  1999-10-01\n",
       "3  C0250077B  1999-10-01    56027          Ars  1999-10-01\n",
       "4  C0250077C  1999-10-01    56027          Ars  1999-10-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell loads a currency code file, performs initial cleanup, and prepares data for further processing.\n",
    "Key operations include:\n",
    "- Reading a pipe-separated text file into a DataFrame.\n",
    "- Cleaning and transforming the 'CurrencyCode' column by removing leading characters and trimming whitespace.\n",
    "- Converting the 'PIT DATE' column into datetime objects to enable downstream filtering or sorting.\n",
    "- Displaying previews before and after transformations to validate data integrity.\n",
    "\"\"\"\n",
    "\n",
    "# Construct full file path for the input dataset\n",
    "file_path = f'{Temp_file_path_EoC}/filtered_CurrencyCode.txt'\n",
    "\n",
    "# Load the file into a DataFrame, ensuring all columns are read as strings for consistent cleaning\n",
    "df_currency = pd.read_csv(file_path, sep=\"|\", dtype=str)\n",
    "\n",
    "# Print confirmation that the file was successfully read\n",
    "print(f\"Successfully loaded data from {file_path}.\")\n",
    "\n",
    "# Display a preview of the raw, uncleaned data\n",
    "print(\"\\n=== Preview of loaded data ===\")\n",
    "display(df_currency.head())\n",
    "\n",
    "# Initial Cleaning Section\n",
    "\n",
    "# Check whether the 'CurrencyCode' column exists before applying string cleaning\n",
    "if 'CurrencyCode' in df_currency.columns:\n",
    "    # Convert to string, remove leading single quotes using regex, and trim surrounding whitespace\n",
    "    df_currency['CurrencyCode'] = (\n",
    "        df_currency['CurrencyCode']\n",
    "        .astype(str)\n",
    "        .str.replace(r\"^'\", \"\", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "    print(\"\\nCleaned 'CurrencyCode' column (removed leading single quote and stripped whitespace).\")\n",
    "else:\n",
    "    # Notify if the expected column is missing\n",
    "    print(\"\\nWarning: 'CurrencyCode' column not found for cleaning.\")\n",
    "\n",
    "# Convert 'PIT DATE' column to datetime if present\n",
    "if 'PIT DATE' in df_currency.columns:\n",
    "    # Convert strings to datetime objects, coercing invalid entries into NaT\n",
    "    df_currency['PIT DATE_dt'] = pd.to_datetime(\n",
    "        df_currency['PIT DATE'],\n",
    "        errors='coerce'\n",
    "    )\n",
    "    print(\"Converted 'PIT DATE' to datetime objects.\")\n",
    "else:\n",
    "    # Notify when the column is missing\n",
    "    print(\"Warning: 'PIT DATE' column not found. Cannot convert to datetime.\")\n",
    "\n",
    "# Display preview after cleaning and conversion steps\n",
    "print(\"\\n=== Preview after initial cleaning ===\")\n",
    "display(df_currency.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d9bf3",
   "metadata": {
    "id": "310d9bf3"
   },
   "source": [
    "#### Sort Currency Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d40910b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1764880897707,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "4d40910b",
    "outputId": "be02c479-c7e2-44e0-87e9-450af16c958f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sorting data by ID and PIT DATE ---\n",
      "\n",
      "Successfully sorted data.\n",
      "\n",
      "=== Preview of sorted data ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>CurrencyCode</th>\n",
       "      <th>PIT DATE_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>56027</td>\n",
       "      <td>Usd</td>\n",
       "      <td>2021-07-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>56027</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0250077A</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>56027</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0250077B</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>56027</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0250077C</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>56027</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE ItemCode CurrencyCode PIT DATE_dt\n",
       "0  C00948205  2021-07-09    56027          Usd  2021-07-09\n",
       "1  C02500770  1995-12-29    56027          Ars  1995-12-29\n",
       "2  C0250077A  1999-10-01    56027          Ars  1999-10-01\n",
       "3  C0250077B  1999-10-01    56027          Ars  1999-10-01\n",
       "4  C0250077C  1999-10-01    56027          Ars  1999-10-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell validates the presence of required data, then sorts the currency DataFrame by two key fields.\n",
    "Main operations:\n",
    "- Verifying that the DataFrame and necessary columns ('ID' and 'PIT DATE_dt') exist.\n",
    "- Sorting the data to ensure chronological ordering within each ID group.\n",
    "- Creating a sorted copy for downstream transformations or filtering.\n",
    "- Displaying a preview to confirm correct ordering.\n",
    "\"\"\"\n",
    "\n",
    "# Check whether the DataFrame and the required columns exist before proceeding\n",
    "if 'df_currency' in locals() and 'ID' in df_currency.columns and 'PIT DATE_dt' in df_currency.columns:\n",
    "\n",
    "    # Print status message indicating the start of the sorting process\n",
    "    print(\"--- Sorting data by ID and PIT DATE ---\")\n",
    "\n",
    "    # Sort DataFrame first by ID, then by the cleaned datetime column to ensure chronological ordering\n",
    "    df_currency_sorted = df_currency.sort_values(\n",
    "        by=['ID', 'PIT DATE_dt'],\n",
    "        ascending=[True, True]\n",
    "    ).copy()\n",
    "\n",
    "    # Confirmation message after sorting completes\n",
    "    print(\"\\nSuccessfully sorted data.\")\n",
    "\n",
    "    # Display a preview of the sorted DataFrame for validation\n",
    "    print(\"\\n=== Preview of sorted data ===\")\n",
    "    display(df_currency_sorted.head())\n",
    "\n",
    "else:\n",
    "    # Inform the user that the required data or columns are missing\n",
    "    print(\"Error: `df_currency` DataFrame or required columns ('ID', 'PIT DATE_dt') not found. Please ensure previous steps were executed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe257f9",
   "metadata": {
    "id": "0fe257f9"
   },
   "source": [
    "#### Identify Currency Switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f114e98e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1764880897743,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "f114e98e",
    "outputId": "b6d47ac5-7cf7-4c93-9506-e28f569b9d0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Identifying IDs with Currency Switches ---\n",
      "\n",
      "Found 7,395 IDs with currency switches.\n",
      "\n",
      "=== Preview of IDs with Currency Switches ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UniqueCurrencyCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C02520240</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C02520300</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>C02520310</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>C02520330</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4303</th>\n",
       "      <td>C04000090</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  UniqueCurrencyCount\n",
       "10    C02520240                    2\n",
       "19    C02520300                    2\n",
       "22    C02520310                    2\n",
       "26    C02520330                    2\n",
       "4303  C04000090                    2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell analyzes the sorted currency data to detect IDs whose currency code changes over time.\n",
    "Key operations:\n",
    "- Validating that the sorted DataFrame and required columns exist.\n",
    "- Grouping records by ID to count how many distinct currency codes appear for each entity.\n",
    "- Identifying IDs that have more than one unique currency code, indicating a currency switch.\n",
    "- Creating both a DataFrame and a Python list of these IDs for downstream filtering or investigation.\n",
    "\"\"\"\n",
    "\n",
    "# Ensure required data and columns are available before processing\n",
    "if 'df_currency_sorted' in locals() and 'ID' in df_currency_sorted.columns and 'CurrencyCode' in df_currency_sorted.columns:\n",
    "    print(\"--- Identifying IDs with Currency Switches ---\")\n",
    "\n",
    "    # Group by ID and compute the number of unique currency codes for each ID\n",
    "    currency_counts = (\n",
    "        df_currency_sorted.groupby('ID')['CurrencyCode']\n",
    "        .nunique(dropna=True)  # Count unique non-null currency codes\n",
    "        .reset_index(name='UniqueCurrencyCount')\n",
    "    )\n",
    "\n",
    "    # Select IDs that have more than one distinct currency code\n",
    "    ids_with_switches_df = currency_counts[currency_counts['UniqueCurrencyCount'] > 1].copy()\n",
    "\n",
    "    # Print the number of IDs detected with currency changes\n",
    "    print(f\"\\nFound {len(ids_with_switches_df):,} IDs with currency switches.\")\n",
    "\n",
    "    # Display a preview of IDs exhibiting currency transitions\n",
    "    print(\"\\n=== Preview of IDs with Currency Switches ===\")\n",
    "    display(ids_with_switches_df.head())\n",
    "\n",
    "    # Extract a list of the affected IDs for later use\n",
    "    ids_with_switches_list = ids_with_switches_df['ID'].tolist()\n",
    "\n",
    "else:\n",
    "    # Provide an error message if required data or columns are missing\n",
    "    print(\"Error: `df_currency_sorted` DataFrame or required columns ('ID', 'CurrencyCode') not found. Please ensure previous steps were executed successfully.\")\n",
    "\n",
    "    # Create an empty list to avoid errors in subsequent steps\n",
    "    ids_with_switches_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19004e5",
   "metadata": {
    "id": "e19004e5"
   },
   "source": [
    "#### Extract Currency Switch Information\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2db8bbbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1764880897970,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "2db8bbbc",
    "outputId": "944d1683-eee3-4479-b5ac-484db38cf803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Extracting Currency Information for All IDs ---\n",
      "\n",
      "Extracted currency information for 121,007 IDs (including those without switches).\n",
      "\n",
      "=== Preview of Extracted Currency Information (including non-switchers) ===\n",
      "ID: C00948205, Info: [{'Currency': 'Usd', 'SwitchDate': Timestamp('2021-07-09 00:00:00')}]\n",
      "ID: C02500770, Info: [{'Currency': 'Ars', 'SwitchDate': Timestamp('1995-12-29 00:00:00')}]\n",
      "ID: C0250077A, Info: [{'Currency': 'Ars', 'SwitchDate': Timestamp('1999-10-01 00:00:00')}]\n",
      "ID: C0250077B, Info: [{'Currency': 'Ars', 'SwitchDate': Timestamp('1999-10-01 00:00:00')}]\n",
      "ID: C0250077C, Info: [{'Currency': 'Ars', 'SwitchDate': Timestamp('1999-10-01 00:00:00')}]\n",
      "ID: C02520200, Info: [{'Currency': 'Ars', 'SwitchDate': Timestamp('1996-05-03 00:00:00')}]\n",
      "ID: C0252020A, Info: [{'Currency': 'Ars', 'SwitchDate': Timestamp('1999-10-01 00:00:00')}]\n",
      "ID: C0252020B, Info: [{'Currency': 'Ars', 'SwitchDate': Timestamp('1999-10-01 00:00:00')}]\n",
      "ID: C02520220, Info: [{'Currency': 'Ars', 'SwitchDate': Timestamp('1995-12-29 00:00:00')}]\n",
      "ID: C02520230, Info: [{'Currency': 'Ars', 'SwitchDate': Timestamp('1995-12-29 00:00:00')}]\n"
     ]
    }
   ],
   "source": [
    "# This cell constructs a detailed currency-change history for every ID in the sorted dataset.\n",
    "# It iterates over all IDs in df_currency_sorted and, within each group, ensures the rows are\n",
    "# ordered chronologically by PIT DATE_dt. For each ID, it builds an ordered sequence of\n",
    "# currency \"states\": it records the first occurrence of a currency and any subsequent points\n",
    "# in time where the currency changes. The result is a list of dictionaries, where each entry\n",
    "# contains an ID and the sequence of currency states and their corresponding switch dates.\n",
    "# This captures both IDs that switch currency and those that remain constant over time.\n",
    "\n",
    "if 'df_currency_sorted' in locals() and 'ID' in df_currency_sorted.columns and 'CurrencyCode' in df_currency_sorted.columns:\n",
    "    # Indicate the start of the process for extracting currency information for all IDs\n",
    "    print(\"--- Extracting Currency Information for All IDs ---\")\n",
    "\n",
    "    # Work on a copy of the sorted DataFrame so the original remains unchanged\n",
    "    df_all_ids = df_currency_sorted.copy()\n",
    "\n",
    "    # Initialize a list to hold the currency-switch sequences for each ID\n",
    "    currency_info_all_ids = []\n",
    "\n",
    "    # Iterate over the data grouped by ID to process each entity separately\n",
    "    for id, group in df_all_ids.groupby('ID'):\n",
    "        # Ensure that, within each ID, rows are ordered chronologically by PIT DATE_dt\n",
    "        group_sorted = group.sort_values(by='PIT DATE_dt', ascending=True)\n",
    "\n",
    "        # Initialize a list to store the ordered sequence of currency states for the current ID\n",
    "        switch_sequence = []\n",
    "        # Track the last seen currency to detect changes over time\n",
    "        last_currency = None\n",
    "\n",
    "        # Iterate through each row of the sorted group to build the currency-change sequence\n",
    "        for index, row in group_sorted.iterrows():\n",
    "            # Current row's currency code\n",
    "            current_currency = row['CurrencyCode']\n",
    "            # Current row's PIT DATE as datetime\n",
    "            current_date = row['PIT DATE_dt']\n",
    "\n",
    "            # Record the first currency for this ID, and then only record entries when the currency changes\n",
    "            if current_currency != last_currency:\n",
    "                switch_sequence.append({'Currency': current_currency, 'SwitchDate': current_date})\n",
    "                # Update the last seen currency to the current one\n",
    "                last_currency = current_currency\n",
    "\n",
    "        # Append the constructed sequence for this ID to the overall list,\n",
    "        # even if there were no changes (i.e., only one currency throughout)\n",
    "        currency_info_all_ids.append({'ID': id, 'Switches': switch_sequence})\n",
    "\n",
    "    # Report how many IDs have been processed and included in the result\n",
    "    print(f\"\\nExtracted currency information for {len(currency_info_all_ids):,} IDs (including those without switches).\")\n",
    "    print(\"\\n=== Preview of Extracted Currency Information (including non-switchers) ===\")\n",
    "    # Print a small sample of the resulting structures to inspect the format\n",
    "    for i, info in enumerate(currency_info_all_ids[:10]):  # Show up to the first 10 IDs\n",
    "        print(f\"ID: {info['ID']}, Info: {info['Switches']}\")\n",
    "\n",
    "else:\n",
    "    # If the required DataFrame or columns are missing, log a message and initialize an empty result list\n",
    "    print(\"Required data (df_currency_sorted) not available. Skipping extraction.\")\n",
    "    currency_info_all_ids = []  # Initialize empty list when prerequisites are not met\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb3615b",
   "metadata": {
    "id": "beb3615b"
   },
   "source": [
    "#### Restructure Currency Switch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "148a248d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1764880898006,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "148a248d",
    "outputId": "d8aeaf7b-aa90-4858-d767-9d9aef2c2771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Restructuring Currency Information for All IDs ---\n",
      "\n",
      "Successfully restructured data for all IDs.\n",
      "\n",
      "=== Preview of Restructured Data (All IDs) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CurrencyCode1</th>\n",
       "      <th>SwitchDate1</th>\n",
       "      <th>CurrencyCode2</th>\n",
       "      <th>SwitchDate2</th>\n",
       "      <th>CurrencyCode3</th>\n",
       "      <th>SwitchDate3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>Usd</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0250077A</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0250077B</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0250077C</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID CurrencyCode1 SwitchDate1 CurrencyCode2 SwitchDate2  \\\n",
       "0  C00948205           Usd  2021-07-09          None         NaT   \n",
       "1  C02500770           Ars  1995-12-29          None         NaT   \n",
       "2  C0250077A           Ars  1999-10-01          None         NaT   \n",
       "3  C0250077B           Ars  1999-10-01          None         NaT   \n",
       "4  C0250077C           Ars  1999-10-01          None         NaT   \n",
       "\n",
       "  CurrencyCode3 SwitchDate3  \n",
       "0          None         NaT  \n",
       "1          None         NaT  \n",
       "2          None         NaT  \n",
       "3          None         NaT  \n",
       "4          None         NaT  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell restructures the extracted currency-switch sequence data for every ID into a wide,\n",
    "table-friendly DataFrame. Each ID becomes one row. For each possible switch position, the\n",
    "currency and the date of its first appearance are placed into dedicated columns\n",
    "(CurrencyCode1, SwitchDate1, CurrencyCode2, SwitchDate2, etc.). IDs with fewer switches\n",
    "than the maximum are padded with None / NaT. This transformation allows consistent comparison,\n",
    "filtering, exporting, or merging with other datasets.\n",
    "\"\"\"\n",
    "\n",
    "# Desired fixed number of switch positions (based on full dataset)\n",
    "FIXED_MAX_SWITCHES = 3\n",
    "\n",
    "# Check that the currency_info_all_ids list exists and contains data before processing\n",
    "if 'currency_info_all_ids' in locals() and currency_info_all_ids:\n",
    "    print(\"--- Restructuring Currency Information for All IDs ---\")\n",
    "\n",
    "    # List that will store row-wise dictionaries representing the final table\n",
    "    restructured_data_all_ids = []\n",
    "\n",
    "    # Determine how many switch positions exist at most across all IDs\n",
    "    # and enforce a minimum of FIXED_MAX_SWITCHES\n",
    "    max_switches = 0\n",
    "    if currency_info_all_ids:\n",
    "        detected_max = max(len(item['Switches']) for item in currency_info_all_ids)\n",
    "        max_switches = max(detected_max, FIXED_MAX_SWITCHES)\n",
    "\n",
    "    # Iterate over each ID’s switch sequence data\n",
    "    for item in currency_info_all_ids:\n",
    "        id = item['ID']\n",
    "        switches = item['Switches']\n",
    "\n",
    "        # Initialize row with the ID\n",
    "        row_data = {'ID': id}\n",
    "\n",
    "        # Add currency and date columns for each switch position\n",
    "        for i in range(max_switches):\n",
    "            if i < len(switches):\n",
    "                # Populate with actual switch data if available\n",
    "                row_data[f'CurrencyCode{i+1}'] = switches[i]['Currency']\n",
    "                row_data[f'SwitchDate{i+1}'] = switches[i]['SwitchDate']\n",
    "            else:\n",
    "                # Fill with proper empty values\n",
    "                row_data[f'CurrencyCode{i+1}'] = None\n",
    "                row_data[f'SwitchDate{i+1}'] = pd.NaT   # <-- important fix\n",
    "\n",
    "        # Add the completed row dictionary to the final list\n",
    "        restructured_data_all_ids.append(row_data)\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df_restructured_all_ids = pd.DataFrame(restructured_data_all_ids)\n",
    "\n",
    "    print(\"\\nSuccessfully restructured data for all IDs.\")\n",
    "    print(\"\\n=== Preview of Restructured Data (All IDs) ===\")\n",
    "    display(df_restructured_all_ids.head())\n",
    "\n",
    "else:\n",
    "    # Fallback when prerequisite data is not available\n",
    "    print(\"No currency information found or required data not available. Skipping restructuring.\")\n",
    "    df_restructured_all_ids = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ee6b5",
   "metadata": {
    "id": "744ee6b5"
   },
   "source": [
    "#### Add CurrentCurrency Column and Finalize Data + Add missing currency (Pat in Macau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "10g1l21bO02J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1764880898043,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "10g1l21bO02J",
    "outputId": "74dac0f7-e019-4d2b-bccd-231eb8adc9bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Adding 'CurrentCurrency' Column and Finalizing Data (using df_restructured_all_ids) ---\n",
      "\n",
      "Successfully added 'CurrentCurrency' column.\n",
      "\n",
      "=== Preview of Finalized Data ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CurrencyCode1</th>\n",
       "      <th>SwitchDate1</th>\n",
       "      <th>CurrencyCode2</th>\n",
       "      <th>SwitchDate2</th>\n",
       "      <th>CurrencyCode3</th>\n",
       "      <th>SwitchDate3</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>Usd</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Usd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Ars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0250077A</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Ars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0250077B</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Ars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0250077C</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Ars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID CurrencyCode1 SwitchDate1 CurrencyCode2 SwitchDate2  \\\n",
       "0  C00948205           Usd  2021-07-09          None         NaT   \n",
       "1  C02500770           Ars  1995-12-29          None         NaT   \n",
       "2  C0250077A           Ars  1999-10-01          None         NaT   \n",
       "3  C0250077B           Ars  1999-10-01          None         NaT   \n",
       "4  C0250077C           Ars  1999-10-01          None         NaT   \n",
       "\n",
       "  CurrencyCode3 SwitchDate3 CurrentCurrency  \n",
       "0          None         NaT             Usd  \n",
       "1          None         NaT             Ars  \n",
       "2          None         NaT             Ars  \n",
       "3          None         NaT             Ars  \n",
       "4          None         NaT             Ars  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final currency switch data saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/CurrencyCodes_clean.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell finalizes the restructured currency-switch dataset.\n",
    "It determines each ID's most recent currency using the CurrencyCode columns\n",
    "and adds a 'CurrentCurrency' column to the DataFrame. The finalized dataset\n",
    "is then saved to disk.\n",
    "\"\"\"\n",
    "\n",
    "output_path_final = f'{Temp_file_path_EoC}/CurrencyCodes_clean.txt'\n",
    "\n",
    "# Proceed only if the restructured DataFrame exists and is not empty\n",
    "if 'df_restructured_all_ids' in locals() and not df_restructured_all_ids.empty:\n",
    "    print(\"--- Adding 'CurrentCurrency' Column and Finalizing Data (using df_restructured_all_ids) ---\")\n",
    "\n",
    "    # Identify all CurrencyCode columns (e.g., CurrencyCode1, CurrencyCode2, ...)\n",
    "    currency_code_cols = [\n",
    "        col for col in df_restructured_all_ids.columns\n",
    "        if col.startswith('CurrencyCode')\n",
    "    ]\n",
    "\n",
    "    if currency_code_cols:\n",
    "\n",
    "        # Function to determine the last non-null currency for each row\n",
    "        def get_last_currency(row):\n",
    "            # Iterate columns in reverse so the first non-null value is the most recent currency\n",
    "            for col in reversed(currency_code_cols):\n",
    "                if pd.notna(row[col]):\n",
    "                    return row[col]\n",
    "            return None\n",
    "\n",
    "        # Add the 'CurrentCurrency' column using the helper function\n",
    "        df_restructured_all_ids['CurrentCurrency'] = (\n",
    "            df_restructured_all_ids.apply(get_last_currency, axis=1)\n",
    "        )\n",
    "\n",
    "        # Make a final copy for clarity\n",
    "        df_final_currency = df_restructured_all_ids.copy()\n",
    "\n",
    "        # Display a preview of the finalized data\n",
    "        print(\"\\nSuccessfully added 'CurrentCurrency' column.\")\n",
    "        print(\"\\n=== Preview of Finalized Data ===\")\n",
    "        display(df_final_currency.head())\n",
    "\n",
    "        # Save finalized dataset\n",
    "        df_final_currency.to_csv(output_path_final, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "        print(f\"\\nFinal currency switch data saved to: {output_path_final}\")\n",
    "\n",
    "    else:\n",
    "        # When no CurrencyCode columns are found, continue with the DataFrame as-is\n",
    "        print(\"Warning: No 'CurrencyCode' columns found to process.\")\n",
    "        df_final_currency = df_restructured_all_ids.copy()\n",
    "\n",
    "        fallback_path = Path(Temp_file_path) / \"CurrencyCodes_clean.txt\"\n",
    "        df_final_currency.to_csv(fallback_path, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "        print(f\"\\nFinal currency data (without CurrentCurrency) saved to: {fallback_path}\")\n",
    "\n",
    "else:\n",
    "    # If the DataFrame is missing or empty, skip processing\n",
    "    print(\"DataFrame from previous step (df_restructured_all_ids) not found or is empty. Skipping finalization.\")\n",
    "    df_final_currency = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PHcposo3L_rH",
   "metadata": {
    "id": "PHcposo3L_rH"
   },
   "source": [
    "### FYE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41bfc45",
   "metadata": {
    "id": "e41bfc45"
   },
   "source": [
    "#### Checking \"D\" Prefix, FYE Month Distribution, and Switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "03b69a88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1764880898152,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "03b69a88",
    "outputId": "5b8023ca-e0b4-411a-af0b-fc47539d847d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Additional FYE Analysis ---\n",
      "\n",
      "Does the \"FYE\" column start with \"D\" for all non-empty rows? False\n",
      "\n",
      "Company count by FYE Month:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>December</td>\n",
       "      <td>2540373</td>\n",
       "      <td>35.468003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>June</td>\n",
       "      <td>1406872</td>\n",
       "      <td>19.642367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>1382692</td>\n",
       "      <td>19.304773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>September</td>\n",
       "      <td>1131818</td>\n",
       "      <td>15.802138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Invalid/Missing FYE Date</td>\n",
       "      <td>154599</td>\n",
       "      <td>2.158470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>January</td>\n",
       "      <td>74396</td>\n",
       "      <td>1.038697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>April</td>\n",
       "      <td>73159</td>\n",
       "      <td>1.021426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>February</td>\n",
       "      <td>71803</td>\n",
       "      <td>1.002494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>October</td>\n",
       "      <td>70763</td>\n",
       "      <td>0.987974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>July</td>\n",
       "      <td>69398</td>\n",
       "      <td>0.968916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>August</td>\n",
       "      <td>69340</td>\n",
       "      <td>0.968106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>May</td>\n",
       "      <td>61127</td>\n",
       "      <td>0.853439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>November</td>\n",
       "      <td>56096</td>\n",
       "      <td>0.783197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FYE Month    Count  Percentage\n",
       "0                   December  2540373   35.468003\n",
       "1                       June  1406872   19.642367\n",
       "2                      March  1382692   19.304773\n",
       "3                  September  1131818   15.802138\n",
       "4   Invalid/Missing FYE Date   154599    2.158470\n",
       "5                    January    74396    1.038697\n",
       "6                      April    73159    1.021426\n",
       "7                   February    71803    1.002494\n",
       "8                    October    70763    0.987974\n",
       "9                       July    69398    0.968916\n",
       "10                    August    69340    0.968106\n",
       "11                       May    61127    0.853439\n",
       "12                  November    56096    0.783197"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IDs with >1 unique FYE month (indicating a potential switch): 111891 (92.51%)\n",
      "Example IDs with multiple FYE months:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UniqueFYEMonths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C0250077A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C0250077B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C0250077C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  UniqueFYEMonths\n",
       "0  C00948205                4\n",
       "1  C02500770                2\n",
       "2  C0250077A                2\n",
       "3  C0250077B                2\n",
       "4  C0250077C                2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional Analysis Summary:\n",
      "Rows with invalid date format in \"FYE\": 154599\n",
      "IDs with >1 unique FYE month: 111891 (92.51%)\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell loads a filtered financial year-end (FYE) dataset from disk, validates that\n",
    "# required columns (\"FYE\" and \"ID\") are present and non-empty, and reconstructs\n",
    "# several derived variables from the freshly loaded data so the analysis is independent\n",
    "# of any previous cell state.\n",
    "# It then:\n",
    "# - Cleans and standardizes the FYE column (string conversion and whitespace stripping).\n",
    "# - Builds an initial error mask that flags missing, empty, and placeholder FYE values\n",
    "#   (e.g., \"na\", \"null\", etc.).\n",
    "# - Parses the cleaned FYE values (assumed format DYYYYMMDD) into actual dates and\n",
    "#   derives the corresponding month names, marking invalid or unparsable dates.\n",
    "# - Aggregates FYE by month to show the distribution of firms across months (absolute\n",
    "#   counts and percentages).\n",
    "# - Groups data by firm ID to detect how many firms have more than one unique FYE month,\n",
    "#   which indicates potential FYE switches over time.\n",
    "# - Summarizes invalid date formats and the fraction of firms that appear to have\n",
    "#   changed their FYE month.\n",
    "\n",
    "# Paths\n",
    "FILE = f'{Temp_file_path_EoC}/filtered_FYE.txt'  # Construct full path to the filtered FYE file\n",
    "VALUE_COL = \"FYE\"  # Name of the column that stores financial year-end information\n",
    "\n",
    "# Load the file and perform initial checks to ensure necessary data is available\n",
    "# This reloads the data into fye_df, making it independent of prior cell execution state\n",
    "try:\n",
    "    # Read the FYE file as a pipe-separated text file, keeping all values as strings\n",
    "    # and preserving NA-like values instead of automatically converting them\n",
    "    fye_df = pd.read_csv(FILE, sep=\"|\", encoding=\"utf-8\", dtype=str, keep_default_na=True)\n",
    "\n",
    "    # Check that the dataframe is non-empty and contains the required columns\n",
    "    if fye_df.empty or VALUE_COL not in fye_df.columns or 'ID' not in fye_df.columns:\n",
    "         raise ValueError(f\"Could not load data or missing required columns in {FILE}\")\n",
    "\n",
    "    # Recompute the FYE value series as a string-based Series\n",
    "    val = fye_df[VALUE_COL].astype(\"string\")\n",
    "\n",
    "    # Strip leading and trailing whitespace from FYE values to standardize formatting\n",
    "    val_stripped = val.str.strip()\n",
    "\n",
    "    # Build the initial error mask, flagging rows where FYE is missing, empty,\n",
    "    # or contains placeholder values indicating missingness or non-applicability\n",
    "    error_mask_initial = val.isna() | (val_stripped == \"\") | val_stripped.str.lower().isin(\n",
    "        {\"na\", \"nan\", \"null\", \"none\", \"n/a\", \"#n/a\", \"n\"}\n",
    "    )\n",
    "\n",
    "    # Compute the number of unique firm IDs to use later when calculating percentages\n",
    "    unique_ids = fye_df[\"ID\"].nunique()\n",
    "\n",
    "# Handle file not found or validation errors\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    print(f\"Error loading or processing data: {e}\")\n",
    "    # Re-raise the exception to stop execution of this cell if loading fails\n",
    "    raise  # Re-raise the exception to stop execution\n",
    "\n",
    "# Proceed with additional analysis after successful data loading and validation\n",
    "print(\"\\n--- Additional FYE Analysis ---\")\n",
    "\n",
    "# 1. Check if all FYE values start with \"D\"\n",
    "# This verifies a structural expectation on the FYE format (e.g., \"DYYYYMMDD\")\n",
    "starts_with_D = val_stripped.str.startswith(\"D\").all()\n",
    "print(f'\\nDoes the \"{VALUE_COL}\" column start with \"D\" for all non-empty rows? {starts_with_D}')\n",
    "\n",
    "# Extract month from FYE (assuming YYYYMMDD format after removing the leading 'D')\n",
    "# First remove the 'D' prefix, then parse the remaining string as a date\n",
    "fye_dates = pd.to_datetime(val_stripped.str.replace('D', '', regex=False), format='%Y%m%d', errors='coerce')\n",
    "\n",
    "# Convert parsed dates to month names; mark invalid or missing dates with a placeholder label\n",
    "fye_months = fye_dates.dt.month_name().fillna(\"Invalid/Missing FYE Date\")\n",
    "\n",
    "# Identify rows where the date parsing failed but which are not already flagged as initial errors\n",
    "invalid_date_mask = fye_dates.isna() & ~error_mask_initial  # Exclude those already captured by the initial error mask\n",
    "\n",
    "# 2.i) How many firms have their FYE in which month (absolute and in %)\n",
    "# Count how often each month label occurs across all rows\n",
    "fye_month_counts = fye_months.value_counts().reset_index()\n",
    "\n",
    "# Rename the columns to meaningful labels for reporting\n",
    "fye_month_counts.columns = [\"FYE Month\", \"Count\"]\n",
    "\n",
    "# Compute the percentage share of each month relative to all rows\n",
    "fye_month_counts[\"Percentage\"] = (fye_month_counts[\"Count\"] / len(fye_months)) * 100\n",
    "\n",
    "# Sort months by frequency in descending order to highlight the most common FYE months\n",
    "fye_month_counts = fye_month_counts.sort_values(by=\"Count\", ascending=False)\n",
    "\n",
    "print(f\"\\nCompany count by {VALUE_COL} Month:\")\n",
    "display(fye_month_counts)\n",
    "\n",
    "# 2.ii) How many firms switched their FYE (unique ID-Month combinations per company)\n",
    "# Construct a temporary DataFrame that pairs each row's ID with its derived FYE month\n",
    "id_month_counts = (\n",
    "    pd.DataFrame({'ID': fye_df['ID'], 'FYE_Month': fye_months})\n",
    "    # Group by firm ID and count how many distinct FYE months each firm has\n",
    "    .groupby(\"ID\")[\"FYE_Month\"]\n",
    "    .nunique(dropna=True)  # Count unique non-NaN months per ID\n",
    "    .reset_index(name=\"UniqueFYEMonths\")  # Store the count in a new column\n",
    ")\n",
    "\n",
    "# Filter to firms that have more than one unique FYE month, indicating a potential FYE switch\n",
    "ids_with_multiple_fye_months = id_month_counts[id_month_counts[\"UniqueFYEMonths\"] > 1]\n",
    "\n",
    "# Count how many firms exhibit FYE switching behavior\n",
    "num_multiple_fye_months_ids = len(ids_with_multiple_fye_months)\n",
    "\n",
    "# Compute the percentage of such firms relative to all unique IDs\n",
    "percent_multiple_fye_months_ids = (num_multiple_fye_months_ids / unique_ids) * 100 if unique_ids > 0 else 0\n",
    "\n",
    "print(f\"\\nIDs with >1 unique {VALUE_COL} month (indicating a potential switch): {num_multiple_fye_months_ids} ({percent_multiple_fye_months_ids:.2f}%)\")\n",
    "\n",
    "# Show a sample of IDs with multiple FYE months, if any exist, for manual inspection\n",
    "if num_multiple_fye_months_ids > 0:\n",
    "    print(f\"Example IDs with multiple {VALUE_COL} months:\")\n",
    "    display(ids_with_multiple_fye_months.head())\n",
    "\n",
    "# --- Summary overview of additional analysis ---\n",
    "print(\"\\nAdditional Analysis Summary:\")\n",
    "\n",
    "# Report the number of rows with invalid date formats in the FYE column (excluding initially flagged errors)\n",
    "print(f'Rows with invalid date format in \"{VALUE_COL}\": {invalid_date_mask.sum()}')\n",
    "\n",
    "# Report how many firms appear to have changed their FYE month and the corresponding percentage\n",
    "print(f\"IDs with >1 unique FYE month: {num_multiple_fye_months_ids} ({percent_multiple_fye_months_ids:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f1da0",
   "metadata": {
    "id": "402f1da0"
   },
   "source": [
    "#### Displaying Rows with Invalid FYE Date Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a2b98e70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1764880898176,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "a2b98e70",
    "outputId": "9040c7ae-1178-448e-a4f3-117f1c84d7e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rows with Invalid Date Format in FYE ===\n",
      "Number of rows with invalid date format: 154599\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>FYE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2009-09-29</td>\n",
       "      <td>5350</td>\n",
       "      <td>Nd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2009-09-29</td>\n",
       "      <td>5350</td>\n",
       "      <td>Nd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2009-09-29</td>\n",
       "      <td>5350</td>\n",
       "      <td>Nd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>2009-09-29</td>\n",
       "      <td>5350</td>\n",
       "      <td>Nd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>C0252020A</td>\n",
       "      <td>2009-09-29</td>\n",
       "      <td>5350</td>\n",
       "      <td>Nd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID    PIT DATE ItemCode FYE\n",
       "129  C02520200  2009-09-29     5350  Nd\n",
       "130  C02520200  2009-09-29     5350  Nd\n",
       "131  C02520200  2009-09-29     5350  Nd\n",
       "132  C02520200  2009-09-29     5350  Nd\n",
       "195  C0252020A  2009-09-29     5350  Nd"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell extracts and displays all rows whose FYE values could not be parsed into\n",
    "# valid dates, based on the `invalid_date_mask` produced earlier. It ensures that the\n",
    "# required variables (`fye_df` and `invalid_date_mask`) exist, initializes an empty\n",
    "# fallback DataFrame, and then filters the original dataset to produce a subset\n",
    "# containing only invalid FYE date-format rows. If such rows exist, the cell prints\n",
    "# their count and shows the first records for inspection; otherwise, it prints that\n",
    "# none were found.\n",
    "\n",
    "# Initialize invalid_fye_format_rows as an empty DataFrame in case dependencies are missing\n",
    "invalid_fye_format_rows = pd.DataFrame(columns=fye_df.columns if 'fye_df' in locals() else [])\n",
    "\n",
    "# Check that required variables exist before proceeding\n",
    "if 'fye_df' in locals() and 'invalid_date_mask' in locals():\n",
    "\n",
    "    # Apply the mask to filter rows where the parsed FYE date was invalid\n",
    "    invalid_fye_format_rows = fye_df[invalid_date_mask]\n",
    "\n",
    "    print(\"\\n=== Rows with Invalid Date Format in FYE ===\")\n",
    "\n",
    "    # If invalid rows exist, print their count and show examples\n",
    "    if not invalid_fye_format_rows.empty:\n",
    "        print(f\"Number of rows with invalid date format: {len(invalid_fye_format_rows)}\")\n",
    "        display(invalid_fye_format_rows.head())  # Show first few rows for inspection\n",
    "\n",
    "    # If no invalid rows were found, inform the user\n",
    "    else:\n",
    "        print(\"No rows found with invalid date format.\")\n",
    "\n",
    "# If prerequisites for analysis are missing, notify the user\n",
    "else:\n",
    "    print(\"Required data not found. Ensure the FYE summary analysis has been executed first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c893a",
   "metadata": {
    "id": "0d2c893a"
   },
   "source": [
    "#### Filtering and Analyzing FYE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f5fad762",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1764880898489,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "f5fad762",
    "outputId": "38613ecd-2fd4-4614-e084-1ae8f7081774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After filtering 'Nd' from FYE column ---\n",
      "Initial rows: 7,162,436\n",
      "Rows after filtering 'Nd': 7,007,837\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>FYE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20181231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20201231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20190930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20191231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE ItemCode        FYE\n",
       "0  C00948205  2021-07-09     5350  D20181231\n",
       "1  C00948205  2021-07-09     5350  D20191231\n",
       "2  C00948205  2021-07-09     5350  D20201231\n",
       "3  C00948205  2021-07-09     5350  D20190930\n",
       "4  C00948205  2021-07-09     5350  D20191231"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After cleaning and parsing dates ---\n",
      "Rows before parsing: 7,007,837\n",
      "Rows after parsing and dropping NaT: 7,007,837\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>FYE</th>\n",
       "      <th>FYE_cleaned</th>\n",
       "      <th>FYE_dt</th>\n",
       "      <th>PIT_DATE_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20181231</td>\n",
       "      <td>20181231</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2021-07-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20191231</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>2021-07-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20201231</td>\n",
       "      <td>20201231</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>2021-07-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20190930</td>\n",
       "      <td>20190930</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>2021-07-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20191231</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>2021-07-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE ItemCode        FYE FYE_cleaned     FYE_dt  \\\n",
       "0  C00948205  2021-07-09     5350  D20181231    20181231 2018-12-31   \n",
       "1  C00948205  2021-07-09     5350  D20191231    20191231 2019-12-31   \n",
       "2  C00948205  2021-07-09     5350  D20201231    20201231 2020-12-31   \n",
       "3  C00948205  2021-07-09     5350  D20190930    20190930 2019-09-30   \n",
       "4  C00948205  2021-07-09     5350  D20191231    20191231 2019-12-31   \n",
       "\n",
       "  PIT_DATE_dt  \n",
       "0  2021-07-09  \n",
       "1  2021-07-09  \n",
       "2  2021-07-09  \n",
       "3  2021-07-09  \n",
       "4  2021-07-09  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After selecting latest entry per ID and FYE Year ---\n",
      "Rows before selection: 7,007,837\n",
      "Rows after selection: 1,842,027\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>FYE</th>\n",
       "      <th>FYE_cleaned</th>\n",
       "      <th>FYE_dt</th>\n",
       "      <th>PIT_DATE_dt</th>\n",
       "      <th>FYE_Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20181231</td>\n",
       "      <td>20181231</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20191231</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20200331</td>\n",
       "      <td>20200331</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20211231</td>\n",
       "      <td>20211231</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20221231</td>\n",
       "      <td>20221231</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE ItemCode        FYE FYE_cleaned     FYE_dt  \\\n",
       "0  C00948205  2021-07-09     5350  D20181231    20181231 2018-12-31   \n",
       "1  C00948205  2021-07-09     5350  D20191231    20191231 2019-12-31   \n",
       "2  C00948205  2021-07-28     5350  D20200331    20200331 2020-03-31   \n",
       "3  C00948205  2022-04-06     5350  D20211231    20211231 2021-12-31   \n",
       "4  C00948205  2023-03-21     5350  D20221231    20221231 2022-12-31   \n",
       "\n",
       "  PIT_DATE_dt  FYE_Year  \n",
       "0  2021-07-09      2018  \n",
       "1  2021-07-09      2019  \n",
       "2  2021-07-28      2020  \n",
       "3  2022-04-06      2021  \n",
       "4  2023-03-21      2022  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of data with FYE Month and Year ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>FYE</th>\n",
       "      <th>FYE_cleaned</th>\n",
       "      <th>FYE_dt</th>\n",
       "      <th>PIT_DATE_dt</th>\n",
       "      <th>FYE_Year</th>\n",
       "      <th>FYE_Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20181231</td>\n",
       "      <td>20181231</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>2018</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20191231</td>\n",
       "      <td>20191231</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>2019</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20200331</td>\n",
       "      <td>20200331</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>2020</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20211231</td>\n",
       "      <td>20211231</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2021</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20221231</td>\n",
       "      <td>20221231</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE ItemCode        FYE FYE_cleaned     FYE_dt  \\\n",
       "0  C00948205  2021-07-09     5350  D20181231    20181231 2018-12-31   \n",
       "1  C00948205  2021-07-09     5350  D20191231    20191231 2019-12-31   \n",
       "2  C00948205  2021-07-28     5350  D20200331    20200331 2020-03-31   \n",
       "3  C00948205  2022-04-06     5350  D20211231    20211231 2021-12-31   \n",
       "4  C00948205  2023-03-21     5350  D20221231    20221231 2022-12-31   \n",
       "\n",
       "  PIT_DATE_dt  FYE_Year FYE_Month  \n",
       "0  2021-07-09      2018  December  \n",
       "1  2021-07-09      2019  December  \n",
       "2  2021-07-28      2020     March  \n",
       "3  2022-04-06      2021  December  \n",
       "4  2023-03-21      2022  December  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FYE Month Distribution per Year ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FYE_Year</th>\n",
       "      <th>FYE_Month</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1973</td>\n",
       "      <td>September</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1974</td>\n",
       "      <td>September</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1975</td>\n",
       "      <td>February</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1975</td>\n",
       "      <td>July</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1975</td>\n",
       "      <td>September</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>2024</td>\n",
       "      <td>September</td>\n",
       "      <td>39819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>2024</td>\n",
       "      <td>October</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>2024</td>\n",
       "      <td>November</td>\n",
       "      <td>1077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>2024</td>\n",
       "      <td>December</td>\n",
       "      <td>3474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>2025</td>\n",
       "      <td>January</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>593 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FYE_Year  FYE_Month  Count\n",
       "0        1973  September      1\n",
       "1        1974  September      1\n",
       "2        1975   February      1\n",
       "3        1975       July      1\n",
       "4        1975  September      1\n",
       "..        ...        ...    ...\n",
       "588      2024  September  39819\n",
       "589      2024    October   1127\n",
       "590      2024   November   1077\n",
       "591      2024   December   3474\n",
       "592      2025    January      1\n",
       "\n",
       "[593 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Does the original \"FYE\" column in the filtered data start with \"D\" for all non-empty rows? True\n",
      "\n",
      "Filtered FYE data saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_FYE_v2.txt (rows: 1,842,027)\n",
      "\n",
      "--- Task Summary ---\n",
      "- Filtered out rows where 'FYE' was 'Nd'.\n",
      "- Parsed FYE into datetime format.\n",
      "- Parsed PIT DATE into datetime format.\n",
      "- Selected latest PIT DATE per (ID, FYE_Year).\n",
      "- Derived FYE month distribution.\n",
      "- Verified whether original FYE values start with 'D'.\n",
      "- Saved cleaned subset to /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_FYE_v2.txt.\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell performs a complete filtering, cleaning, and restructuring of FYE data.\n",
    "# Key filtering steps include:\n",
    "# - Removing rows where the FYE value equals \"Nd\".\n",
    "# - Stripping whitespace, standardizing to string, and removing a leading \"D\".\n",
    "# - Parsing cleaned FYE strings as dates in YYYYMMDD format.\n",
    "# - Parsing PIT DATE in YYYY-MM-DD format.\n",
    "# - Dropping rows where either parsed date is invalid.\n",
    "# - Grouping by (ID, FYE_Year) and selecting the entry with the latest PIT DATE.\n",
    "# Key transformations include:\n",
    "# - Deriving the FYE_Year and FYE_Month.\n",
    "# - Computing distributions of FYE months per year.\n",
    "# - Saving a subset of cleaned columns to a pipe-separated file.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Paths ---\n",
    "output_path_v2 = f'{Temp_file_path_EoC}/filtered_FYE_v2.txt'  # Output file for final cleaned data\n",
    "\n",
    "# --- Main Logic ---\n",
    "if 'fye_df' in locals() and VALUE_COL in fye_df.columns:\n",
    "\n",
    "    # 1) Filter out rows where FYE equals \"Nd\"\n",
    "    initial_rows = len(fye_df)  # Track dataset size before filtering\n",
    "    fye_df_filtered = (\n",
    "        fye_df[\n",
    "            fye_df[VALUE_COL]\n",
    "            .astype(str)            # Ensure string type\n",
    "            .str.strip()            # Remove whitespace\n",
    "            .str.lower() != 'nd'    # Exclude placeholder value \"nd\"\n",
    "        ].copy()\n",
    "    )\n",
    "    rows_after_filtering_nd = len(fye_df_filtered)  # Size after filtering\n",
    "\n",
    "    print(f\"\\n--- After filtering 'Nd' from {VALUE_COL} column ---\")\n",
    "    print(f\"Initial rows: {initial_rows:,}\")\n",
    "    print(f\"Rows after filtering 'Nd': {rows_after_filtering_nd:,}\")\n",
    "    try:\n",
    "        display(fye_df_filtered.head())  # Preview filtered data\n",
    "    except NameError:\n",
    "        print(fye_df_filtered.head())\n",
    "\n",
    "    # 2) Clean FYE by removing a leading \"D\" if present\n",
    "    fye_df_filtered['FYE_cleaned'] = (\n",
    "        fye_df_filtered[VALUE_COL]\n",
    "        .astype(str)                # Ensure string type\n",
    "        .str.strip()                # Remove whitespace\n",
    "        .str.replace(r'^D', '', regex=True)  # Remove leading \"D\"\n",
    "    )\n",
    "\n",
    "    # 3) Parse FYE_cleaned as datetime (YYYYMMDD)\n",
    "    fye_df_filtered['FYE_dt'] = pd.to_datetime(\n",
    "        fye_df_filtered['FYE_cleaned'],\n",
    "        format='%Y%m%d',\n",
    "        errors='coerce'             # Invalid formats become NaT\n",
    "    )\n",
    "\n",
    "    # Parse PIT DATE column (YYYY-MM-DD)\n",
    "    if 'PIT DATE' in fye_df_filtered.columns:\n",
    "        fye_df_filtered['PIT_DATE_dt'] = pd.to_datetime(\n",
    "            fye_df_filtered['PIT DATE'],\n",
    "            format='%Y-%m-%d',\n",
    "            errors='coerce'         # Invalid formats become NaT\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nError: 'PIT DATE' column not found in fye_df.\")\n",
    "        raise KeyError(\"Missing 'PIT DATE' column\")\n",
    "\n",
    "    # 4) Drop rows where parsed dates are invalid\n",
    "    initial_filtered_rows = len(fye_df_filtered)  # Count before date-cleaning\n",
    "    fye_df_cleaned_dates = (\n",
    "        fye_df_filtered\n",
    "        .dropna(subset=['FYE_dt', 'PIT_DATE_dt'])  # Require both valid dates\n",
    "        .copy()\n",
    "    )\n",
    "    rows_after_date_parsing = len(fye_df_cleaned_dates)  # Count after dropping invalids\n",
    "\n",
    "    print(f\"\\n--- After cleaning and parsing dates ---\")\n",
    "    print(f\"Rows before parsing: {initial_filtered_rows:,}\")\n",
    "    print(f\"Rows after parsing and dropping NaT: {rows_after_date_parsing:,}\")\n",
    "    try:\n",
    "        display(fye_df_cleaned_dates.head())  # Preview valid rows\n",
    "    except NameError:\n",
    "        print(fye_df_cleaned_dates.head())\n",
    "\n",
    "    # 5) Extract FYE_Year and select latest PIT DATE per (ID, FYE_Year)\n",
    "    fye_df_cleaned_dates['FYE_Year'] = fye_df_cleaned_dates['FYE_dt'].dt.year  # Extract year\n",
    "\n",
    "    # Sort to ensure tail(1) selects the latest PIT DATE\n",
    "    fye_df_sorted = fye_df_cleaned_dates.sort_values(\n",
    "        by=['ID', 'FYE_Year', 'PIT_DATE_dt'],\n",
    "        ascending=[True, True, True]\n",
    "    )\n",
    "\n",
    "    # Group by ID and FYE_Year, keeping only the latest entry\n",
    "    fye_df_latest_per_year = (\n",
    "        fye_df_sorted\n",
    "        .groupby(['ID', 'FYE_Year'], as_index=False)\n",
    "        .tail(1)\n",
    "        .reset_index(drop=True)\n",
    "        .copy()\n",
    "    )\n",
    "    rows_after_latest_selection = len(fye_df_latest_per_year)\n",
    "\n",
    "    print(f\"\\n--- After selecting latest entry per ID and FYE Year ---\")\n",
    "    print(f\"Rows before selection: {rows_after_date_parsing:,}\")\n",
    "    print(f\"Rows after selection: {rows_after_latest_selection:,}\")\n",
    "    try:\n",
    "        display(fye_df_latest_per_year.head())  # Preview final firm-year dataset\n",
    "    except NameError:\n",
    "        print(fye_df_latest_per_year.head())\n",
    "\n",
    "    # 6) Add FYE_Month from the parsed FYE date\n",
    "    fye_df_latest_per_year['FYE_Month'] = (\n",
    "        fye_df_latest_per_year['FYE_dt'].dt.month_name()\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Preview of data with FYE Month and Year ===\")\n",
    "    try:\n",
    "        display(fye_df_latest_per_year.head())\n",
    "    except NameError:\n",
    "        print(fye_df_latest_per_year.head())\n",
    "\n",
    "    # 7) Compute distribution of FYE months per year\n",
    "    fye_monthly_distribution_per_year = (\n",
    "        fye_df_latest_per_year\n",
    "        .groupby(['FYE_Year', 'FYE_Month'])\n",
    "        .size()\n",
    "        .reset_index(name='Count')\n",
    "    )\n",
    "\n",
    "    # Define month ordering for sorted output\n",
    "    month_order = [\n",
    "        'January', 'February', 'March', 'April', 'May', 'June',\n",
    "        'July', 'August', 'September', 'October', 'November', 'December'\n",
    "    ]\n",
    "\n",
    "    # Enforce categorical ordering\n",
    "    fye_monthly_distribution_per_year['FYE_Month'] = pd.Categorical(\n",
    "        fye_monthly_distribution_per_year['FYE_Month'],\n",
    "        categories=month_order,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    # Sort by year, then month\n",
    "    fye_monthly_distribution_per_year = (\n",
    "        fye_monthly_distribution_per_year\n",
    "        .sort_values(['FYE_Year', 'FYE_Month'])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- FYE Month Distribution per Year ---\")\n",
    "    try:\n",
    "        display(fye_monthly_distribution_per_year)\n",
    "    except NameError:\n",
    "        print(fye_monthly_distribution_per_year)\n",
    "\n",
    "    # 8) Check: do remaining original (non-empty) FYE values start with \"D\"?\n",
    "    col = fye_df_filtered[VALUE_COL].astype(str).str.strip()  # Standardize formatting\n",
    "    non_empty = col[col != \"\"]                               # Exclude empty strings\n",
    "    starts_with_D_filtered = non_empty.str.startswith(\"D\").all()  # Check prefix condition\n",
    "\n",
    "    print(\n",
    "        f'\\nDoes the original \"{VALUE_COL}\" column in the filtered data '\n",
    "        f'start with \"D\" for all non-empty rows? {starts_with_D_filtered}'\n",
    "    )\n",
    "\n",
    "    # 9) Save selected subset with original FYE values\n",
    "    output_cols = ['ID', 'PIT DATE', 'ItemCode', VALUE_COL]  # Columns to export\n",
    "    missing_cols = [c for c in output_cols if c not in fye_df_latest_per_year.columns]\n",
    "\n",
    "    if missing_cols:\n",
    "        print(f\"\\nMissing columns in final DataFrame, cannot save: {missing_cols}\")\n",
    "    else:\n",
    "        fye_df_latest_per_year[output_cols].to_csv(\n",
    "            output_path_v2,\n",
    "            sep=\"|\",\n",
    "            index=False,\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\nFiltered FYE data saved to: {output_path_v2} \"\n",
    "            f\"(rows: {len(fye_df_latest_per_year):,})\"\n",
    "        )\n",
    "\n",
    "    # 10) Summary of cleaning steps applied\n",
    "    print(\"\\n--- Task Summary ---\")\n",
    "    print(f\"- Filtered out rows where '{VALUE_COL}' was 'Nd'.\")\n",
    "    print(\"- Parsed FYE into datetime format.\")\n",
    "    print(\"- Parsed PIT DATE into datetime format.\")\n",
    "    print(\"- Selected latest PIT DATE per (ID, FYE_Year).\")\n",
    "    print(\"- Derived FYE month distribution.\")\n",
    "    print(\"- Verified whether original FYE values start with 'D'.\")\n",
    "    print(f\"- Saved cleaned subset to {output_path_v2}.\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Error: 'fye_df' DataFrame or '{VALUE_COL}' column not found. \"\n",
    "        f\"Please ensure the loading/preparation cell was executed.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96478669",
   "metadata": {
    "id": "96478669"
   },
   "source": [
    "#### Pivoted FYE Month Distribution Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f6b318b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1764880899018,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "f6b318b0",
    "outputId": "03967a59-d96e-4a5b-951b-6a2010a3e39f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pivoted FYE Month Distribution (Years as Columns, Months as Rows) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>FYE_Year</th>\n",
       "      <th>1973</th>\n",
       "      <th>1974</th>\n",
       "      <th>1975</th>\n",
       "      <th>1976</th>\n",
       "      <th>1977</th>\n",
       "      <th>1978</th>\n",
       "      <th>1979</th>\n",
       "      <th>1980</th>\n",
       "      <th>1981</th>\n",
       "      <th>1982</th>\n",
       "      <th>...</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "      <th>2024</th>\n",
       "      <th>2025</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FYE_Month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>January</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>...</td>\n",
       "      <td>173.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>February</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>...</td>\n",
       "      <td>132.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>March</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4057.0</td>\n",
       "      <td>4575.0</td>\n",
       "      <td>4871.0</td>\n",
       "      <td>4270.0</td>\n",
       "      <td>3896.0</td>\n",
       "      <td>3699.0</td>\n",
       "      <td>3590.0</td>\n",
       "      <td>3445.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>April</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>...</td>\n",
       "      <td>187.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>May</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>...</td>\n",
       "      <td>112.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>June</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2928.0</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>3195.0</td>\n",
       "      <td>2925.0</td>\n",
       "      <td>3021.0</td>\n",
       "      <td>2892.0</td>\n",
       "      <td>2842.0</td>\n",
       "      <td>3255.0</td>\n",
       "      <td>10125.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>July</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>...</td>\n",
       "      <td>290.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>August</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>...</td>\n",
       "      <td>344.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>September</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>568.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3860.0</td>\n",
       "      <td>4639.0</td>\n",
       "      <td>4571.0</td>\n",
       "      <td>4290.0</td>\n",
       "      <td>3839.0</td>\n",
       "      <td>3678.0</td>\n",
       "      <td>4250.0</td>\n",
       "      <td>5067.0</td>\n",
       "      <td>39819.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>October</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1331.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>1544.0</td>\n",
       "      <td>1478.0</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>1127.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>November</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1238.0</td>\n",
       "      <td>1211.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>1229.0</td>\n",
       "      <td>1262.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1307.0</td>\n",
       "      <td>1246.0</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>December</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>2723.0</td>\n",
       "      <td>3378.0</td>\n",
       "      <td>5263.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44361.0</td>\n",
       "      <td>43977.0</td>\n",
       "      <td>45075.0</td>\n",
       "      <td>47747.0</td>\n",
       "      <td>49684.0</td>\n",
       "      <td>51242.0</td>\n",
       "      <td>50065.0</td>\n",
       "      <td>46287.0</td>\n",
       "      <td>3474.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>814.0</td>\n",
       "      <td>4567.0</td>\n",
       "      <td>5507.0</td>\n",
       "      <td>9065.0</td>\n",
       "      <td>...</td>\n",
       "      <td>59013.0</td>\n",
       "      <td>59676.0</td>\n",
       "      <td>61309.0</td>\n",
       "      <td>62841.0</td>\n",
       "      <td>64109.0</td>\n",
       "      <td>65378.0</td>\n",
       "      <td>64497.0</td>\n",
       "      <td>61548.0</td>\n",
       "      <td>57561.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "FYE_Year   1973  1974  1975   1976   1977   1978   1979    1980    1981  \\\n",
       "FYE_Month                                                                 \n",
       "January     0.0   0.0   0.0    0.0    8.0   15.0   34.0    79.0   179.0   \n",
       "February    0.0   0.0   1.0    2.0    7.0   10.0   13.0    91.0    94.0   \n",
       "March       0.0   0.0   0.0    5.0   24.0   41.0   55.0   536.0   560.0   \n",
       "April       0.0   0.0   0.0    0.0    5.0    6.0   10.0    93.0    95.0   \n",
       "May         0.0   0.0   0.0    5.0    7.0    9.0   16.0    82.0    90.0   \n",
       "June        0.0   0.0   0.0    8.0   23.0   42.0   64.0   311.0   370.0   \n",
       "July        0.0   0.0   1.0    5.0   11.0   19.0   23.0    75.0    86.0   \n",
       "August      0.0   0.0   0.0    7.0    9.0   16.0   23.0    88.0   103.0   \n",
       "September   1.0   1.0   1.0    9.0   27.0   44.0   57.0   287.0   316.0   \n",
       "October     0.0   0.0   0.0    2.0    7.0   12.0   16.0   113.0   145.0   \n",
       "November    0.0   0.0   0.0    4.0    6.0   15.0   18.0    89.0    91.0   \n",
       "December    0.0   0.0   3.0   89.0  205.0  336.0  485.0  2723.0  3378.0   \n",
       "Sum         1.0   1.0   6.0  136.0  339.0  565.0  814.0  4567.0  5507.0   \n",
       "\n",
       "FYE_Year     1982  ...     2016     2017     2018     2019     2020     2021  \\\n",
       "FYE_Month          ...                                                         \n",
       "January     250.0  ...    173.0    119.0    133.0    128.0    100.0    101.0   \n",
       "February    123.0  ...    132.0    149.0    156.0    246.0    120.0     81.0   \n",
       "March      1118.0  ...   4057.0   4575.0   4871.0   4270.0   3896.0   3699.0   \n",
       "April       227.0  ...    187.0    160.0    159.0    116.0    103.0    111.0   \n",
       "May         160.0  ...    112.0    109.0    185.0    100.0    110.0    107.0   \n",
       "June        624.0  ...   2928.0   2810.0   3195.0   2925.0   3021.0   2892.0   \n",
       "July        167.0  ...    290.0    310.0    295.0    221.0    240.0    274.0   \n",
       "August      154.0  ...    344.0    325.0    422.0    333.0    335.0    329.0   \n",
       "September   568.0  ...   3860.0   4639.0   4571.0   4290.0   3839.0   3678.0   \n",
       "October     251.0  ...   1331.0   1292.0   1129.0   1236.0   1399.0   1544.0   \n",
       "November    160.0  ...   1238.0   1211.0   1118.0   1229.0   1262.0   1320.0   \n",
       "December   5263.0  ...  44361.0  43977.0  45075.0  47747.0  49684.0  51242.0   \n",
       "Sum        9065.0  ...  59013.0  59676.0  61309.0  62841.0  64109.0  65378.0   \n",
       "\n",
       "FYE_Year      2022     2023     2024  2025  \n",
       "FYE_Month                                   \n",
       "January      102.0     82.0     37.0   1.0  \n",
       "February     103.0    101.0     45.0   0.0  \n",
       "March       3590.0   3445.0    887.0   0.0  \n",
       "April        112.0    113.0     90.0   0.0  \n",
       "May           89.0     83.0    103.0   0.0  \n",
       "June        2842.0   3255.0  10125.0   0.0  \n",
       "July         272.0    259.0    298.0   0.0  \n",
       "August       287.0    282.0    479.0   0.0  \n",
       "September   4250.0   5067.0  39819.0   0.0  \n",
       "October     1478.0   1328.0   1127.0   0.0  \n",
       "November    1307.0   1246.0   1077.0   0.0  \n",
       "December   50065.0  46287.0   3474.0   0.0  \n",
       "Sum        64497.0  61548.0  57561.0   1.0  \n",
       "\n",
       "[13 rows x 53 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivoted FYE month distribution table saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/Overview-Company_per_year_FYE_per_month_per_year.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell reshapes the FYE month distribution data into a pivot table where months\n",
    "# form the rows and years form the columns, with values representing counts of firms.\n",
    "# Missing month-year combinations are filled with zero, and a total row is added.\n",
    "# The resulting pivot table is displayed and exported to an Excel file.\n",
    "\n",
    "output_path_pivot_excel = f'{Temp_file_path_EoC}/Overview-Company_per_year_FYE_per_month_per_year.xlsx'  # Output file path\n",
    "\n",
    "# Create a pivot table with months as rows, years as columns, and counts as values\n",
    "fye_monthly_distribution_pivot = fye_monthly_distribution_per_year.pivot(\n",
    "    index='FYE_Month',     # Row axis: month names\n",
    "    columns='FYE_Year',    # Column axis: years\n",
    "    values='Count'         # Cell values: number of entries per month-year\n",
    ").fillna(0)                # Replace missing combinations with 0\n",
    "\n",
    "# Add a summary row that sums counts across all years for each month\n",
    "fye_monthly_distribution_pivot.loc['Sum'] = fye_monthly_distribution_pivot.sum()\n",
    "\n",
    "print(\"\\n--- Pivoted FYE Month Distribution (Years as Columns, Months as Rows) ---\")\n",
    "display(fye_monthly_distribution_pivot)  # Show the pivot table\n",
    "\n",
    "# Save the pivot table to an Excel file\n",
    "fye_monthly_distribution_pivot.to_excel(output_path_pivot_excel)\n",
    "\n",
    "print(f\"\\nPivoted FYE month distribution table saved to: {output_path_pivot_excel}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab25ade",
   "metadata": {
    "id": "eab25ade"
   },
   "source": [
    "#### Check for Multiple FYE Entries per Company per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1752df7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1764880899056,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "1752df7d",
    "outputId": "11f426f0-2450-49d9-adf2-b5536c145c87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Check for multiple entries per ID per FYE Year ---\n",
      "Each company (ID) appears only once per FYE Year in the fye_df_latest_per_year DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell verifies that the dataset containing the latest PIT DATE per (ID, FYE_Year)\n",
    "# truly has only one row per company per year. It groups the data by ID and FYE_Year,\n",
    "# counts the number of rows in each group, and checks whether any company appears\n",
    "# more than once in a given year. If duplicates are found, examples are shown.\n",
    "\n",
    "# Proceed only if the final per-year dataset exists\n",
    "if 'fye_df_latest_per_year' in locals():\n",
    "\n",
    "    # Group by company ID and FYE year, counting how many entries exist per group\n",
    "    id_year_counts = (\n",
    "        fye_df_latest_per_year\n",
    "        .groupby(['ID', 'FYE_Year'])\n",
    "        .size()\n",
    "        .reset_index(name='Count')\n",
    "    )\n",
    "\n",
    "    # Identify groups where a company appears more than once in the same year\n",
    "    multiple_entries = id_year_counts[id_year_counts['Count'] > 1]\n",
    "\n",
    "    print(\"\\n--- Check for multiple entries per ID per FYE Year ---\")\n",
    "\n",
    "    # Report whether duplicates exist\n",
    "    if multiple_entries.empty:\n",
    "        print(\"Each company (ID) appears only once per FYE Year in the fye_df_latest_per_year DataFrame.\")\n",
    "    else:\n",
    "        print(\"Found IDs with multiple entries for the same FYE Year in the fye_df_latest_per_year DataFrame.\")\n",
    "        print(\"Examples of IDs with multiple entries:\")\n",
    "        display(multiple_entries.head())\n",
    "\n",
    "# Handle missing dataset case\n",
    "else:\n",
    "    print(\"Error: fye_df_latest_per_year DataFrame not found. Ensure previous processing steps were executed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0759ec5",
   "metadata": {
    "id": "d0759ec5"
   },
   "source": [
    "#### Further Processing and Cleaning of Filtered FYE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "18ad1a1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "executionInfo": {
     "elapsed": 1165,
     "status": "ok",
     "timestamp": 1765469310415,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "18ad1a1e",
    "outputId": "180fdf4e-b82c-46f3-8ae2-c766b4d6522f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_FYE_v2.txt. Initial rows: 1,842,027\n",
      "\n",
      "=== Preview of loaded data ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT DATE</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>FYE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20181231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20191231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20200331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20211231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>5350</td>\n",
       "      <td>D20221231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT DATE ItemCode        FYE\n",
       "0  C00948205  2021-07-09     5350  D20181231\n",
       "1  C00948205  2021-07-09     5350  D20191231\n",
       "2  C00948205  2021-07-28     5350  D20200331\n",
       "3  C00948205  2022-04-06     5350  D20211231\n",
       "4  C00948205  2023-03-21     5350  D20221231"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped columns: ['ItemCode', 'PIT DATE']\n",
      "\n",
      "Extracted Year ('FY') and created:\n",
      " - 'FYE Month' for FYE_clean\n",
      " - 'FYE Date' for FYED_clean\n",
      "Dropped original 'FYE' column in both outputs.\n",
      "\n",
      "Sorted both FYE_clean and FYED_clean DataFrames by ID and FY.\n",
      "\n",
      "=== Preview of processed data for FYE_clean (FY + FYE Month) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>FY</th>\n",
       "      <th>FYE Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2018</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2019</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2020</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2022</td>\n",
       "      <td>December</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    FY FYE Month\n",
       "0  C00948205  2018  December\n",
       "1  C00948205  2019  December\n",
       "2  C00948205  2020     March\n",
       "3  C00948205  2021  December\n",
       "4  C00948205  2022  December"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of processed data for FYED_clean (FY + FYE Date) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>FY</th>\n",
       "      <th>FYE Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00948205</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    FY    FYE Date\n",
       "0  C00948205  2018  2018-12-31\n",
       "1  C00948205  2019  2019-12-31\n",
       "2  C00948205  2020  2020-03-31\n",
       "3  C00948205  2021  2021-12-31\n",
       "4  C00948205  2022  2022-12-31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data with FY and FYE Month saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/FYE_clean.txt\n",
      "Processed data with FY and full FYE Date saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/FYED_clean.txt\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell loads a pre-filtered FYE dataset from disk, performs column-level cleanup,\n",
    "# derives year and date/month information from the FYE variable, and outputs two cleaned files.\n",
    "# Concretely, it:\n",
    "# - Reads the input file containing FYE data into a DataFrame.\n",
    "# - Optionally drops the \"ItemCode\" and \"PIT DATE\" columns if they are present.\n",
    "# - Cleans and transforms the \"FYE\" column by removing a leading \"D\", parsing the\n",
    "#   remaining value as a date, and extracting:\n",
    "#     * the corresponding fiscal year (FY)\n",
    "#     * the month name (\"FYE Month\") for FYE_clean\n",
    "#     * the full date (\"FYE Date\") for FYED_clean\n",
    "# - Removes the original \"FYE\" column once derived fields are created.\n",
    "# - Sorts the resulting data by company identifier (\"ID\") and fiscal year (\"FY\")\n",
    "#   for a chronological view per company.\n",
    "# - Saves:\n",
    "#     * FYE_clean.txt  – with FY and FYE Month\n",
    "#     * FYED_clean.txt – with FY and FYE Date\n",
    "# - Handles missing files and unexpected errors via try/except.\n",
    "\n",
    "# Paths\n",
    "input_path = f'{Temp_file_path_EoC}/filtered_FYE_v2.txt'   # Path to the pre-filtered FYE input file\n",
    "output_path = f'{Temp_file_path_EoC}/FYE_clean.txt'        # Path where the cleaned FYE (month) output will be saved\n",
    "output_path_fyed = f'{Temp_file_path_EoC}/FYED_clean.txt'  # Path where the cleaned FYE (full date) output will be saved\n",
    "\n",
    "try:\n",
    "    # Load the data from the input file into a DataFrame\n",
    "    # All columns are read as strings to preserve formatting and avoid unintended type coercion\n",
    "    df_fye_v2 = pd.read_csv(input_path, sep=\"|\", dtype=str, encoding=\"utf-8\")\n",
    "\n",
    "    # Print basic information about the loaded dataset, including row count\n",
    "    print(f\"Successfully loaded data from {input_path}. Initial rows: {len(df_fye_v2):,}\")\n",
    "    print(\"\\n=== Preview of loaded data ===\")\n",
    "    display(df_fye_v2.head())\n",
    "\n",
    "    # Define which columns should be dropped from the dataset\n",
    "    cols_to_drop = [\"ItemCode\", \"PIT DATE\"]\n",
    "\n",
    "    # Identify which of the specified columns actually exist in the loaded DataFrame\n",
    "    existing_cols_to_drop = [col for col in cols_to_drop if col in df_fye_v2.columns]\n",
    "\n",
    "    # Drop only those columns that are present, and create a working copy of the DataFrame\n",
    "    if existing_cols_to_drop:\n",
    "        df_fye_base = df_fye_v2.drop(columns=existing_cols_to_drop, errors=\"ignore\").copy()\n",
    "        print(f\"\\nDropped columns: {existing_cols_to_drop}\")\n",
    "    else:\n",
    "        # If none of the specified columns exist, simply continue with a copy of the original DataFrame\n",
    "        df_fye_base = df_fye_v2.copy()\n",
    "        print(\"\\nNo columns to drop found among the specified list.\")\n",
    "\n",
    "    # Initialize the two output DataFrames to None\n",
    "    df_fye_processed = None       # For FYE_clean (with FY + FYE Month)\n",
    "    df_fyed_processed = None      # For FYED_clean (with FY + FYE Date)\n",
    "\n",
    "    # Extract year, month, and full date information from the FYE column, if it exists\n",
    "    if \"FYE\" in df_fye_base.columns:\n",
    "        # Clean the 'FYE' column by:\n",
    "        # - Converting to string\n",
    "        # - Removing a leading 'D' (if present)\n",
    "        # - Stripping surrounding whitespace\n",
    "        # Then parse the resulting string as a date with format YYYYMMDD\n",
    "        fye_dates = pd.to_datetime(\n",
    "            df_fye_base[\"FYE\"]\n",
    "            .astype(str)\n",
    "            .str.replace('D', '', regex=False)\n",
    "            .str.strip(),\n",
    "            format='%Y%m%d',\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "        # 1) DataFrame for FYE_clean: FY + FYE Month\n",
    "        df_fye_processed = df_fye_base.copy()\n",
    "        df_fye_processed[\"FY\"] = fye_dates.dt.year\n",
    "        df_fye_processed[\"FYE Month\"] = fye_dates.dt.month_name().fillna(\"Invalid/Missing FYE Date\")\n",
    "        df_fye_processed = df_fye_processed.drop(columns=[\"FYE\"], errors=\"ignore\")\n",
    "\n",
    "        # 2) DataFrame for FYED_clean: FY + full FYE Date\n",
    "        df_fyed_processed = df_fye_base.copy()\n",
    "        df_fyed_processed[\"FY\"] = fye_dates.dt.year\n",
    "        # Store the full date as a string YYYY-MM-DD; invalid dates become NaN\n",
    "        df_fyed_processed[\"FYE Date\"] = fye_dates.dt.strftime('%Y-%m-%d')\n",
    "        df_fyed_processed = df_fyed_processed.drop(columns=[\"FYE\"], errors=\"ignore\")\n",
    "\n",
    "        print(\"\\nExtracted Year ('FY') and created:\")\n",
    "        print(\" - 'FYE Month' for FYE_clean\")\n",
    "        print(\" - 'FYE Date' for FYED_clean\")\n",
    "        print(\"Dropped original 'FYE' column in both outputs.\")\n",
    "\n",
    "    else:\n",
    "        # If 'FYE' is not available, the date-based transformation cannot be performed\n",
    "        print(\"\\nWarning: 'FYE' column not found. Cannot extract year, month, or date.\")\n",
    "        # In this case, both processed DataFrames are simply the base DataFrame copies\n",
    "        df_fye_processed = df_fye_base.copy()\n",
    "        df_fyed_processed = df_fye_base.copy()\n",
    "\n",
    "    # Sort both processed DataFrames by company ID and fiscal year if both columns are present\n",
    "    if \"ID\" in df_fye_processed.columns and \"FY\" in df_fye_processed.columns:\n",
    "        df_fye_processed = df_fye_processed.sort_values(\n",
    "            by=[\"ID\", \"FY\"],\n",
    "            ascending=[True, True],\n",
    "            kind=\"mergesort\"  # Stable sort to preserve existing order within groups\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        df_fyed_processed = df_fyed_processed.sort_values(\n",
    "            by=[\"ID\", \"FY\"],\n",
    "            ascending=[True, True],\n",
    "            kind=\"mergesort\"\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        print(\"\\nSorted both FYE_clean and FYED_clean DataFrames by ID and FY.\")\n",
    "    else:\n",
    "        print(\"\\nWarning: 'ID' or 'FY' column not found. Cannot sort DataFrames.\")\n",
    "\n",
    "    # Show a preview of the transformed DataFrames after cleaning and sorting\n",
    "    print(\"\\n=== Preview of processed data for FYE_clean (FY + FYE Month) ===\")\n",
    "    display(df_fye_processed.head())\n",
    "\n",
    "    print(\"\\n=== Preview of processed data for FYED_clean (FY + FYE Date) ===\")\n",
    "    display(df_fyed_processed.head())\n",
    "\n",
    "    # Save the processed DataFrames to the specified output files in pipe-separated format\n",
    "    df_fye_processed.to_csv(output_path, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nProcessed data with FY and FYE Month saved to: {output_path}\")\n",
    "\n",
    "    df_fyed_processed.to_csv(output_path_fyed, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"Processed data with FY and full FYE Date saved to: {output_path_fyed}\")\n",
    "\n",
    "# Handle the case where the input file is missing\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at {input_path}. Please ensure 'filtered_FYE_v2.txt' was created in the previous steps.\")\n",
    "\n",
    "# Handle any other unexpected errors during processing\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during processing: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96838a13",
   "metadata": {
    "id": "96838a13"
   },
   "source": [
    "### Update Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q0tAs4JIgR6y",
   "metadata": {
    "id": "q0tAs4JIgR6y"
   },
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "609791ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1764880899275,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "609791ee",
    "outputId": "ffd9a4bc-9c6c-4a4b-f057-b3a94a63ad16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_UpdateCode.txt. Initial rows: 8,418,018\n",
      "\n",
      "=== Preview of loaded data ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1985</td>\n",
       "      <td>57034</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1986</td>\n",
       "      <td>57034</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1987</td>\n",
       "      <td>57034</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1988</td>\n",
       "      <td>57034</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1989</td>\n",
       "      <td>57034</td>\n",
       "      <td>S3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT Date Frequency FiscalPeriod ItemCode Value\n",
       "0  C02500770  1995-12-29         A         1985    57034    S3\n",
       "1  C02500770  1995-12-29         A         1986    57034    S3\n",
       "2  C02500770  1995-12-29         A         1987    57034    S3\n",
       "3  C02500770  1995-12-29         A         1988    57034    S3\n",
       "4  C02500770  1995-12-29         A         1989    57034    S3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns read by pandas: ['ID', 'PIT Date', 'Frequency', 'FiscalPeriod', 'ItemCode', 'Value']\n",
      "\n",
      "Number of missing values (NaN, empty, 'Ns', 'N') in 'Value': 2,873\n",
      "\n",
      "--- After removing leading 'S' from 'Value' column ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1985</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1986</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1987</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1988</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1989</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT Date Frequency FiscalPeriod ItemCode Value\n",
       "0  C02500770  1995-12-29         A         1985    57034     3\n",
       "1  C02500770  1995-12-29         A         1986    57034     3\n",
       "2  C02500770  1995-12-29         A         1987    57034     3\n",
       "3  C02500770  1995-12-29         A         1988    57034     3\n",
       "4  C02500770  1995-12-29         A         1989    57034     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Renamed 'Value' column to 'UpdateCode'.\n",
      "\n",
      "=== Preview after renaming column ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>UpdateCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1985</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1986</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1987</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1988</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1989</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT Date Frequency FiscalPeriod ItemCode UpdateCode\n",
       "0  C02500770  1995-12-29         A         1985    57034          3\n",
       "1  C02500770  1995-12-29         A         1986    57034          3\n",
       "2  C02500770  1995-12-29         A         1987    57034          3\n",
       "3  C02500770  1995-12-29         A         1988    57034          3\n",
       "4  C02500770  1995-12-29         A         1989    57034          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed data saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_UpdateCode_v2.txt\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell loads an Update Code dataset from a pipe-separated file, validates that the\n",
    "# expected \"Value\" column is present, and performs several cleaning and transformation steps.\n",
    "# Specifically, it:\n",
    "# - Reads the dataset with string-typed columns and confirms the header row structure.\n",
    "# - Identifies and counts missing or invalid entries in the \"Value\" column, treating NaN,\n",
    "#   empty strings, and specific placeholder values (\"Ns\", \"N\") as missing.\n",
    "# - Cleans the \"Value\" column by removing a leading \"S\" (if present) and trimming whitespace.\n",
    "# - Renames the cleaned \"Value\" column to \"UpdateCode\" to reflect its semantic meaning.\n",
    "# - Saves the transformed dataset to a new file, preserving all other columns.\n",
    "# The cell includes basic error handling for missing input files and unexpected issues.\n",
    "\n",
    "# Paths\n",
    "input_path = f'{Temp_file_path_EoC}/filtered_UpdateCode.txt'  # Input file containing raw Update Code data\n",
    "output_path_v2 = f'{Temp_file_path_EoC}/filtered_UpdateCode_v2.txt'  # Output path for the cleaned and renamed data\n",
    "\n",
    "# The column containing Update Code, will be renamed to \"UpdateCode\"\n",
    "VALUE_COL_ORIGINAL = \"Value\"       # Name of the original column containing the raw update codes\n",
    "VALUE_COL_RENAMED = \"UpdateCode\"   # Desired name for the cleaned update code column\n",
    "\n",
    "try:\n",
    "    # Load the data, expecting the first row to be the header\n",
    "    df_updatecode = pd.read_csv(\n",
    "        input_path,\n",
    "        sep=\"|\",           # Use pipe as the column separator\n",
    "        encoding=\"utf-8\",  # Assume UTF-8 encoding for the text file\n",
    "        dtype=str,         # Read all columns as strings to avoid unintended type inference\n",
    "        header=0,          # Explicitly specify that the first row contains column names\n",
    "        keep_default_na=True  # Let pandas assign NaN to recognized missing value patterns\n",
    "    )\n",
    "\n",
    "    # Report successful load and basic row count for validation\n",
    "    print(f\"Successfully loaded data from {input_path}. Initial rows: {len(df_updatecode):,}\")\n",
    "    print(\"\\n=== Preview of loaded data ===\")\n",
    "    display(df_updatecode.head())\n",
    "\n",
    "    # Print columns read by pandas to verify header and column names\n",
    "    print(f\"Columns read by pandas: {df_updatecode.columns.tolist()}\")\n",
    "\n",
    "    # Ensure the original value column exists\n",
    "    if VALUE_COL_ORIGINAL not in df_updatecode.columns:\n",
    "        # If the expected \"Value\" column is missing, raise a clear error\n",
    "        raise KeyError(\n",
    "            f\"Expected column '{VALUE_COL_ORIGINAL}' not found in DataFrame after reading. \"\n",
    "            f\"Available columns: {df_updatecode.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    # --- Explicitly count missing values including \"Ns\" and \"N\" in the original value column ---\n",
    "    # Normalize the original value column to stripped lowercase strings for missingness checks\n",
    "    val_original = df_updatecode[VALUE_COL_ORIGINAL].astype(str).str.strip()\n",
    "    # Build a mask that flags NaN, empty strings, and specific placeholder values as missing\n",
    "    missing_mask_original = (\n",
    "        val_original.isna()\n",
    "        | (val_original == \"\")\n",
    "        | val_original.str.lower().isin([\"ns\", \"n\"])\n",
    "    )\n",
    "\n",
    "    # Count how many entries in the original value column are considered missing\n",
    "    missing_values_count = missing_mask_original.sum()\n",
    "    print(f\"\\nNumber of missing values (NaN, empty, 'Ns', 'N') in '{VALUE_COL_ORIGINAL}': {missing_values_count:,}\")\n",
    "\n",
    "    # Remove the leading \"S\" from the original Value column\n",
    "    df_updatecode[VALUE_COL_ORIGINAL] = (\n",
    "        df_updatecode[VALUE_COL_ORIGINAL]\n",
    "        .astype(str)                         # Ensure string type for consistent string operations\n",
    "        .str.replace(r\"^S\", \"\", regex=True)  # Remove a leading \"S\" character using a regex pattern\n",
    "        .str.strip()                         # Strip leading and trailing whitespace\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- After removing leading 'S' from '{VALUE_COL_ORIGINAL}' column ---\")\n",
    "    display(df_updatecode.head())\n",
    "\n",
    "    # --- Rename the original 'Value' column to 'UpdateCode' ---\n",
    "    df_updatecode = df_updatecode.rename(columns={VALUE_COL_ORIGINAL: VALUE_COL_RENAMED})\n",
    "    print(f\"\\nRenamed '{VALUE_COL_ORIGINAL}' column to '{VALUE_COL_RENAMED}'.\")\n",
    "\n",
    "    print(\"\\n=== Preview after renaming column ===\")\n",
    "    display(df_updatecode.head())\n",
    "\n",
    "    # Save the resulting dataset as v2 (header=True by default)\n",
    "    df_updatecode.to_csv(output_path_v2, sep=\"|\", index=False)\n",
    "\n",
    "    # Confirm where the processed file has been written\n",
    "    print(f\"\\nProcessed data saved to: {output_path_v2}\")\n",
    "\n",
    "# Handle case where the input file cannot be found at the specified path\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Error: Input file not found at {input_path}. \"\n",
    "        f\"Please ensure the file was created in the previous steps.\"\n",
    "    )\n",
    "\n",
    "# Catch any other unexpected exceptions and report them\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during processing: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751c8e4c",
   "metadata": {
    "id": "751c8e4c"
   },
   "source": [
    "#### Filtering Empty Values and Final Cleaning of Update Code Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "508fa868",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1764880899565,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "508fa868",
    "outputId": "8d00dd0b-ca88-4063-8310-ac1fe6157a5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/filtered_UpdateCode_v2.txt. Initial rows: 8,418,018\n",
      "\n",
      "=== Preview of loaded data ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>UpdateCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1985</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1986</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1987</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1988</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1989</td>\n",
       "      <td>57034</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT Date Frequency FiscalPeriod ItemCode UpdateCode\n",
       "0  C02500770  1995-12-29         A         1985    57034          3\n",
       "1  C02500770  1995-12-29         A         1986    57034          3\n",
       "2  C02500770  1995-12-29         A         1987    57034          3\n",
       "3  C02500770  1995-12-29         A         1988    57034          3\n",
       "4  C02500770  1995-12-29         A         1989    57034          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Filtering empty/missing values, removing 'ItemCode' column, and saving ---\n",
      "Initial rows: 8,418,018\n",
      "Rows after filtering missing values: 8,415,145\n",
      "Dropped 'ItemCode' column.\n",
      "\n",
      "=== Preview of cleaned data ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>UpdateCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1985</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1986</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1987</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1988</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>1989</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    PIT Date Frequency FiscalPeriod UpdateCode\n",
       "0  C02500770  1995-12-29         A         1985          3\n",
       "1  C02500770  1995-12-29         A         1986          3\n",
       "2  C02500770  1995-12-29         A         1987          3\n",
       "3  C02500770  1995-12-29         A         1988          3\n",
       "4  C02500770  1995-12-29         A         1989          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Update Code data saved to: /home/jovyan/work/hpool1/pseidel/test/Temp/TempExtractionofCharacteristics/UpdateCodes_clean.txt\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "# This cell loads a preprocessed Update Code dataset from disk, validates that the\n",
    "# expected \"UpdateCode\" column is present, and then performs a series of cleaning\n",
    "# and filtering steps before saving a final, cleaned version:\n",
    "# - It identifies and removes rows where the UpdateCode value is missing, empty,\n",
    "#   or represents missingness via special codes such as \"Ns\" or \"N\".\n",
    "# - It drops the \"ItemCode\" column (if present), as it is not needed for the final\n",
    "#   output structure.\n",
    "# - It preserves all other columns and writes the cleaned, filtered data to a new\n",
    "#   pipe-separated text file.\n",
    "# The focus is on ensuring that only valid, non-missing UpdateCode entries remain\n",
    "# and that the resulting file is ready for downstream analysis or merging.\n",
    "\n",
    "# Paths\n",
    "input_path = f'{Temp_file_path_EoC}/filtered_UpdateCode_v2.txt'  # Path to the intermediate UpdateCode file\n",
    "output_path = f'{Temp_file_path_EoC}/UpdateCodes_clean.txt'      # Path for the final cleaned UpdateCode output\n",
    "\n",
    "# Assuming the v2 file has a header row and the column is named 'UpdateCode'\n",
    "\n",
    "try:\n",
    "    # Load the v2 data as a pipe-separated text file.\n",
    "    # The first row is treated as a header and all columns are read as strings.\n",
    "    df_updatecode_v2 = pd.read_csv(\n",
    "        input_path,\n",
    "        sep=\"|\",\n",
    "        encoding=\"utf-8\",\n",
    "        dtype=str,\n",
    "        header=0,          # Interpret the first row as column names\n",
    "        keep_default_na=True  # Use pandas' default NA parsing for standard missing markers\n",
    "    )\n",
    "\n",
    "    # Report successful load and show basic information about the dataset size\n",
    "    print(f\"Successfully loaded data from {input_path}. Initial rows: {len(df_updatecode_v2):,}\")\n",
    "\n",
    "    # Display a preview of the loaded data for visual inspection of structure and values\n",
    "    print(\"\\n=== Preview of loaded data ===\")\n",
    "    display(df_updatecode_v2.head())\n",
    "\n",
    "    # Ensure the 'UpdateCode' column exists before continuing with transformations\n",
    "    VALUE_COL_CLEANED = \"UpdateCode\"\n",
    "    if VALUE_COL_CLEANED not in df_updatecode_v2.columns:\n",
    "         raise KeyError(\n",
    "             f\"Expected column '{VALUE_COL_CLEANED}' not found in DataFrame. \"\n",
    "             f\"Available columns: {df_updatecode_v2.columns.tolist()}\"\n",
    "         )\n",
    "\n",
    "    print(\"\\n--- Filtering empty/missing values, removing 'ItemCode' column, and saving ---\")\n",
    "\n",
    "    # Standardize the UpdateCode values as strings and strip leading/trailing whitespace\n",
    "    val_cleaned = df_updatecode_v2[VALUE_COL_CLEANED].astype(str).str.strip()\n",
    "\n",
    "    # Build a mask for rows where UpdateCode is considered missing:\n",
    "    # - NaN values\n",
    "    # - Empty strings\n",
    "    # - Special codes \"Ns\" or \"N\" in a case-insensitive manner\n",
    "    missing_mask_cleaned = (\n",
    "        val_cleaned.isna()\n",
    "        | (val_cleaned == \"\")\n",
    "        | val_cleaned.str.lower().isin([\"ns\", \"n\"])\n",
    "    )\n",
    "\n",
    "    # Filter out rows with missing UpdateCode by inverting the missing mask\n",
    "    initial_rows = len(df_updatecode_v2)\n",
    "    df_updatecode_filtered = df_updatecode_v2[~missing_mask_cleaned].copy()\n",
    "    rows_after_filtering = len(df_updatecode_filtered)\n",
    "\n",
    "    # Report how many rows remain after removing missing or invalid UpdateCode entries\n",
    "    print(f\"Initial rows: {initial_rows:,}\")\n",
    "    print(f\"Rows after filtering missing values: {rows_after_filtering:,}\")\n",
    "\n",
    "    # Drop the ItemCode column, if present, since it is not needed in the final output\n",
    "    if \"ItemCode\" in df_updatecode_filtered.columns:\n",
    "        df_updatecode_clean = df_updatecode_filtered.drop(columns=[\"ItemCode\"], errors=\"ignore\").copy()\n",
    "        print(\"Dropped 'ItemCode' column.\")\n",
    "    else:\n",
    "        # If ItemCode does not exist, just propagate the filtered DataFrame unchanged\n",
    "        df_updatecode_clean = df_updatecode_filtered.copy()\n",
    "        print(\"Warning: 'ItemCode' column not found to drop.\")\n",
    "\n",
    "    # Show a small sample of the cleaned data to verify the structure and content\n",
    "    print(\"\\n=== Preview of cleaned data ===\")\n",
    "    display(df_updatecode_clean.head())\n",
    "\n",
    "    # Save the cleaned DataFrame as a pipe-separated text file without an index\n",
    "    df_updatecode_clean.to_csv(output_path, sep=\"|\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # Confirm successful save and report the output path\n",
    "    print(f\"\\nCleaned Update Code data saved to: {output_path}\")\n",
    "\n",
    "# Handle the case where the expected input file does not exist\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Error: Input file not found at {input_path}. \"\n",
    "        f\"Please ensure the previous processing steps were executed successfully.\"\n",
    "    )\n",
    "\n",
    "# Handle any other unexpected exceptions and print the error message\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during processing: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ctXZHVs-zurM",
    "D_mJzRSiXgF0",
    "VtAy-TVVL-nW",
    "RtXy24nSMvdy",
    "gyzaAqH3M67B",
    "f54314c0",
    "9b759157",
    "3cadec55",
    "c149764d",
    "f992bb4d",
    "ed72c15c",
    "1e1ce262",
    "1ba10b58",
    "2ba5acfb",
    "53z7zaj_eh8a",
    "8489cf18",
    "3b024437",
    "32aa9bff",
    "47068e49",
    "77db8e04",
    "ff8f9da7",
    "48a2e519",
    "361295dd",
    "aae7b40b",
    "b48e6dca",
    "207697a8",
    "T9bJaXBsesvy",
    "z9m9ya4o9Fpb",
    "w4n9lwpIAU3c",
    "xONxHzcTD8EO",
    "6iPI0AmBMhPI",
    "LxgZ08CnMwGe",
    "rlk28gvLNCro",
    "OxPbDZndOSVC",
    "m6kUSXzaPZln",
    "wD0VdQzJPqv2",
    "REwRiMkbP781",
    "Fd8Gry81Rtk5",
    "ifOtcIUiSOaG",
    "cSG1QnFiU6Lk",
    "u8zSneDaBWbg",
    "lLwatKcLWqV4",
    "a8df13a2",
    "bdff8fae",
    "310d9bf3",
    "0fe257f9",
    "e19004e5",
    "beb3615b",
    "744ee6b5",
    "e41bfc45",
    "402f1da0",
    "0d2c893a",
    "96478669",
    "eab25ade",
    "d0759ec5",
    "96838a13",
    "q0tAs4JIgR6y"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
