{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctXZHVs-zurM"
   },
   "source": [
    "### Mount Google Drive, Import Libraries and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24306,
     "status": "ok",
     "timestamp": 1765204790335,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "DVbbtM4ztajO",
    "outputId": "d26ad847-3427-4742-a1f0-84ac24c752ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: False\n",
      "BASE_PATH: /home/jovyan/work/hpool1/pseidel/test\n",
      "Importing numpy ...\n",
      "numpy OK\n",
      "Importing scipy ...\n",
      "scipy OK\n",
      "Importing pandas ...\n",
      "pandas OK\n",
      "Importing linearmodels ...\n",
      "linearmodels OK\n",
      "Importing xlsxwriter ...\n",
      "xlsxwriter OK\n",
      "Paths configured. Temp outputs -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview\n",
      "Example input path -> /home/jovyan/work/hpool1/pseidel/test/Input/WSFV_f_20250131.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT SETUP + PATH CONFIGURATION (SERVER / COLAB COMPATIBLE)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) HARD SAFETY: cap native thread usage (prevents pthread_create EAGAIN)\n",
    "#    MUST be set before importing numpy / scipy / pandas\n",
    "# -----------------------------------------------------------------------------\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"BLIS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Detect environment\n",
    "# -----------------------------------------------------------------------------\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) (Colab only) Mount Google Drive\n",
    "# -----------------------------------------------------------------------------\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "else:\n",
    "    # Server base path (your target)\n",
    "    BASE_PATH = \"/home/jovyan/work/hpool1/pseidel/test\"\n",
    "\n",
    "print(\"IN_COLAB:\", IN_COLAB)\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Sanity checks: path exists + write permission\n",
    "# -----------------------------------------------------------------------------\n",
    "BASE = Path(BASE_PATH)\n",
    "if not BASE.exists():\n",
    "    raise FileNotFoundError(f\"BASE_PATH does not exist: {BASE}\")\n",
    "\n",
    "# quick write test (fails fast if you don't have permissions)\n",
    "test_file = BASE / \".write_test_tmp\"\n",
    "try:\n",
    "    test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "    test_file.unlink()\n",
    "except Exception as e:\n",
    "    raise PermissionError(f\"No write permission in {BASE}. Error: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Environment check: ensure required packages import cleanly\n",
    "# -----------------------------------------------------------------------------\n",
    "required_packages = [\"numpy\", \"scipy\", \"pandas\", \"linearmodels\", \"xlsxwriter\"]\n",
    "\n",
    "for pkg in required_packages:\n",
    "    print(f\"Importing {pkg} ...\")\n",
    "    importlib.import_module(pkg)\n",
    "    print(f\"{pkg} OK\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Base paths and input/output locations\n",
    "# -----------------------------------------------------------------------------\n",
    "Input_file_path   = str(BASE / \"Input\")\n",
    "Temp_file_path    = str(BASE / \"Temp\")\n",
    "Output_file_path  = str(BASE / \"Output\")\n",
    "\n",
    "Fundamentals_file_path = f\"{Input_file_path}/WSFV_f_20250131.txt\"\n",
    "Current_file_path      = f\"{Input_file_path}/WSCurrent_f_20250131.txt\"\n",
    "Calendar_file_path     = f\"{Input_file_path}/WSCalendarPrd_f_20250131.txt\"\n",
    "Meta_file_path         = f\"{Input_file_path}/WSMetaData_f_20250131.txt\"\n",
    "Excel_file_path        = f\"{Input_file_path}/WS PIT Table Definitions V5 with start dates.xls\"\n",
    "\n",
    "MarketValues_file_path          = f\"{Input_file_path}/Daily MV USD\"\n",
    "MarketValues_file_path_LC       = f\"{Input_file_path}/Daily MV LC\"\n",
    "DailyTotalReturns_file_path     = f\"{Input_file_path}/Daily Returns USD\"\n",
    "DailyIndexReturns_file_path     = f\"{Input_file_path}/Daily Index Returns USD\"\n",
    "Constituents_file_path          = f\"{Input_file_path}/Constituents.01.csv\"\n",
    "UniversalMatching_file_path     = f\"{Input_file_path}/Universal Matching File\"\n",
    "\n",
    "Temp_file_path_GO  = f\"{Temp_file_path}/TempGeneralOverview\"\n",
    "Temp_file_path_EoC = f\"{Temp_file_path}/TempExtractionofCharacteristics\"\n",
    "Temp_file_path_DP  = f\"{Temp_file_path}/TempDataPreparation\"\n",
    "Temp_file_path_A   = f\"{Temp_file_path}/TempAnomalies\"\n",
    "Temp_file_path_R   = f\"{Temp_file_path}/TempRegressionModel\"\n",
    "\n",
    "Relevant_items_path   = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_A = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_B = f\"{Input_file_path}/RelevantItemsB.txt\"\n",
    "Relevant_items_path_C = f\"{Input_file_path}/RelevantItemsC.txt\"\n",
    "Relevant_items_path_D = f\"{Input_file_path}/RelevantItemsD.txt\"\n",
    "\n",
    "Subset_file_path = f\"{Temp_file_path_GO}/Subsets\"\n",
    "Fundamentals_clean_file_path = f\"{Temp_file_path_GO}/Fundamentals_clean.txt\"\n",
    "Current_clean_file_path      = f\"{Temp_file_path_GO}/Current_clean.txt\"\n",
    "Calendar_clean_file_path     = f\"{Temp_file_path_GO}/Input/Calendar_clean.txt\"\n",
    "Meta_clean_file_path         = f\"{Temp_file_path_GO}/Input/Meta_clean.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Ensure required directories exist\n",
    "# -----------------------------------------------------------------------------\n",
    "Path(Output_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_GO).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_EoC).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_A).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(Subset_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Path(Calendar_clean_file_path).parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Streaming / deduplication settings\n",
    "# -----------------------------------------------------------------------------\n",
    "CHUNK_SIZE = 2_000_000\n",
    "DATE_COL = \"PIT Date\"\n",
    "DEDUP_KEYS = [\"ID\", \"ItemCode\", DATE_COL]\n",
    "\n",
    "print(\"Paths configured. Temp outputs ->\", Temp_file_path_GO)\n",
    "print(\"Example input path ->\", Fundamentals_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:           754Gi       241Gi       167Gi        55Mi       354Gi       512Gi\n",
      "Swap:             0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQMCPR6XzurR"
   },
   "source": [
    "### Import Data Files to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7786,
     "status": "ok",
     "timestamp": 1765204798129,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "zW-FBOO8zurR",
    "outputId": "8f28b10d-6a5e-4ce2-fdd1-dea7547af218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imported RelevantItems.txt as DataFrame 'RelevantItems'\n",
      "Preview of 'RelevantItems':\n",
      "  ItemCode\n",
      "0    01001\n",
      "1    01051\n",
      "2    01075\n",
      "3    01101\n",
      "4    01151 \n",
      "\n",
      "\n",
      "Imported CountryCodes.txt as DataFrame 'CountryCodes'\n",
      "Preview of 'CountryCodes':\n",
      "  NatCo ImplCountry\n",
      "0   012     Algeria\n",
      "1   440   Lithuania\n",
      "2   025   Argentina\n",
      "3   442  Luxembourg\n",
      "4   036   Australia \n",
      "\n",
      "\n",
      "Imported ADR_clean.txt as DataFrame 'ADR_clean'\n",
      "Preview of 'ADR_clean':\n",
      "          ID ADRIndicator\n",
      "0  C036F63D0            N\n",
      "1  C056879S0            X\n",
      "2  C2461T100            N\n",
      "3  C2504O500            N\n",
      "4  C250C9180            N \n",
      "\n",
      "\n",
      "Imported CompanyName_clean.txt as DataFrame 'CompanyName_clean'\n",
      "Preview of 'CompanyName_clean':\n",
      "          ID                               CompanyName\n",
      "0  C00948205             AGRIFORCE GROWING SYSTEMS LTD\n",
      "1  C02500770            PEUGEOT CITROEN ARGENTINA S.A.\n",
      "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA\n",
      "3  C02520220                       ALPARGATAS S.A.I.C.\n",
      "4  C02520230               ALUAR ALUMINIO ARGENTINO SA \n",
      "\n",
      "\n",
      "Imported CurrencyCodes_clean.txt as DataFrame 'CurrencyCodes_clean'\n",
      "Preview of 'CurrencyCodes_clean':\n",
      "          ID CurrencyCode1 SwitchDate1 CurrencyCode2 SwitchDate2  \\\n",
      "0  C00948205           Usd  2021-07-09           NaN         NaN   \n",
      "1  C02500770           Ars  1995-12-29           NaN         NaN   \n",
      "2  C0250077A           Ars  1999-10-01           NaN         NaN   \n",
      "3  C0250077B           Ars  1999-10-01           NaN         NaN   \n",
      "4  C0250077C           Ars  1999-10-01           NaN         NaN   \n",
      "\n",
      "  CurrencyCode3 SwitchDate3 CurrentCurrency  \n",
      "0           NaN         NaN             Usd  \n",
      "1           NaN         NaN             Ars  \n",
      "2           NaN         NaN             Ars  \n",
      "3           NaN         NaN             Ars  \n",
      "4           NaN         NaN             Ars   \n",
      "\n",
      "\n",
      "Imported FYE_clean.txt as DataFrame 'FYE_clean'\n",
      "Preview of 'FYE_clean':\n",
      "          ID    FY FYE Month\n",
      "0  C00948205  2018  December\n",
      "1  C00948205  2019  December\n",
      "2  C00948205  2020     March\n",
      "3  C00948205  2021  December\n",
      "4  C00948205  2022  December \n",
      "\n",
      "\n",
      "Imported ID_clean.txt as DataFrame 'ID_clean'\n",
      "Preview of 'ID_clean':\n",
      "          ID\n",
      "0  C02500770\n",
      "1  C02520200\n",
      "2  C02520220\n",
      "3  C02520230\n",
      "4  C02520240 \n",
      "\n",
      "\n",
      "Imported UpdateCodes_clean.txt as DataFrame 'UpdateCodes_clean'\n",
      "Preview of 'UpdateCodes_clean':\n",
      "          ID    PIT Date Frequency FiscalPeriod UpdateCode\n",
      "0  C02500770  1995-12-29         A         1985          3\n",
      "1  C02500770  1995-12-29         A         1986          3\n",
      "2  C02500770  1995-12-29         A         1987          3\n",
      "3  C02500770  1995-12-29         A         1988          3\n",
      "4  C02500770  1995-12-29         A         1989          3 \n",
      "\n",
      "\n",
      "Imported ValueCoding.txt as DataFrame 'ValueCoding'\n",
      "Preview of 'ValueCoding':\n",
      "  ItemCode                           ItemName  Source\n",
      "0    05006               Market Price Current  Market\n",
      "1    05007      Market Price YTD High Current  Market\n",
      "2    05008       Market Price YTD Low Current  Market\n",
      "3    05009              Date of Current Price  Market\n",
      "4    05091  Market Price 52 Week High Current  Market \n",
      "\n",
      "\n",
      "Identifying subset files to process...\n",
      "  Found subset_01001.txt\n",
      "  Found subset_01051.txt\n",
      "  Found subset_01075.txt\n",
      "  Found subset_01101.txt\n",
      "  Found subset_01151.txt\n",
      "  Found subset_01250.txt\n",
      "  Found subset_01451.txt\n",
      "  Found subset_01551.txt\n",
      "  Found subset_01706.txt\n",
      "  Found subset_02001.txt\n",
      "  Found subset_02051.txt\n",
      "  Found subset_02101.txt\n",
      "  Found subset_02149.txt\n",
      "  Found subset_02201.txt\n",
      "  Found subset_02250.txt\n",
      "  Found subset_02256.txt\n",
      "  Found subset_02257.txt\n",
      "  Found subset_02258.txt\n",
      "  Found subset_02263.txt\n",
      "  Found subset_02501.txt\n",
      "  Found subset_02652.txt\n",
      "  Found subset_02999.txt\n",
      "  Found subset_03040.txt\n",
      "  Found subset_03051.txt\n",
      "  Found subset_03063.txt\n",
      "  Found subset_03066.txt\n",
      "  Found subset_03101.txt\n",
      "  Found subset_03251.txt\n",
      "  Found subset_03263.txt\n",
      "  Found subset_03273.txt\n",
      "  Found subset_03351.txt\n",
      "  Found subset_03426.txt\n",
      "  Found subset_03451.txt\n",
      "  Found subset_03501.txt\n",
      "  Found subset_04251.txt\n",
      "  Found subset_04401.txt\n",
      "  Found subset_04551.txt\n",
      "  Found subset_04601.txt\n",
      "  Found subset_04701.txt\n",
      "  Found subset_04751.txt\n",
      "  Found subset_04860.txt\n",
      "  Found subset_04870.txt\n",
      "  Found subset_04890.txt\n",
      "  Found subset_05202.txt\n",
      "  Found subset_04201.txt\n",
      "  Found subset_04225.txt\n",
      "  Found subset_04831.txt\n",
      "  Found subset_04351.txt\n",
      "  Found subset_05508.txt\n",
      "\n",
      "Identified 49 subset files for processing.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell:\n",
    "#\n",
    "#   1. Defines a helper function `import_file_to_dataframe` that reads a pipe-delimited\n",
    "#      text file into a pandas DataFrame (all columns as string; returns None on error).\n",
    "#   2. Imports a list of \"input\" files from Input_file_path into DataFrames\n",
    "#      (RelevantItems, CountryCodes, ...), storing them in globals() by filename.\n",
    "#   3. Imports a list of \"temp\" files from Temp_file_path_EoC into DataFrames\n",
    "#      (ADR_clean, CompanyName_clean, CurrencyCodes_clean, FYE_clean, ID_clean,\n",
    "#       UpdateCodes_clean, ValueCoding), also stored in globals().\n",
    "#   4. Identifies which subset_*.txt files exist in Subset_file_path based on the IDs\n",
    "#      listed in RelevantItems.txt, and records their names (without .txt) in\n",
    "#      `successful_subset_names`.\n",
    "#\n",
    "# No actual subset data is loaded here; that is deferred to later steps to keep\n",
    "# memory usage under control.\n",
    "\n",
    "\n",
    "# Function to import a file and return a pandas DataFrame\n",
    "def import_file_to_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Import a pipe-separated text file as a pandas DataFrame.\n",
    "\n",
    "    - Uses sep='|' to read pipe-delimited files.\n",
    "    - Reads all columns as strings (dtype=str), which helps preserve things like\n",
    "      leading zeros in codes (e.g., NatCo, ItemCode).\n",
    "    - Returns None on failure and prints a brief error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='|', dtype=str)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error importing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Import files from Input directory\n",
    "# -------------------------------------------------------------------------\n",
    "input_files_to_import = [\"RelevantItems.txt\", \"CountryCodes.txt\"]\n",
    "\n",
    "for file_name in input_files_to_import:\n",
    "    file_path = os.path.join(Input_file_path, file_name)\n",
    "    var_name = file_name.replace(\".txt\", \"\")  # e.g. \"RelevantItems\"\n",
    "    globals()[var_name] = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if globals()[var_name] is not None:\n",
    "        print(f\"\\nImported {file_name} as DataFrame '{var_name}'\")\n",
    "        print(f\"Preview of '{var_name}':\")\n",
    "        print(globals()[var_name].head(), \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Import files from Temp directory (end-of-cleaning stage)\n",
    "# -------------------------------------------------------------------------\n",
    "temp_files_to_import = [\n",
    "    \"ADR_clean.txt\",\n",
    "    \"CompanyName_clean.txt\",\n",
    "    \"CurrencyCodes_clean.txt\",\n",
    "    \"FYE_clean.txt\",\n",
    "    \"ID_clean.txt\",\n",
    "    \"UpdateCodes_clean.txt\",\n",
    "    \"ValueCoding.txt\"\n",
    "]\n",
    "\n",
    "for file_name in temp_files_to_import:\n",
    "    file_path = os.path.join(Temp_file_path_EoC, file_name)\n",
    "    var_name = file_name.replace(\".txt\", \"\")  # e.g. \"ADR_clean\"\n",
    "    globals()[var_name] = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if globals()[var_name] is not None:\n",
    "        print(f\"\\nImported {file_name} as DataFrame '{var_name}'\")\n",
    "        print(f\"Preview of '{var_name}':\")\n",
    "        print(globals()[var_name].head(), \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Identify subset files that exist for the relevant items\n",
    "# -------------------------------------------------------------------------\n",
    "successful_subset_names = []\n",
    "\n",
    "if 'RelevantItems' in globals() and RelevantItems is not None:\n",
    "    # Assume first column of RelevantItems holds the item IDs used in subset filenames\n",
    "    relevant_ids = RelevantItems.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "    print(\"\\nIdentifying subset files to process...\")\n",
    "    for item_id in relevant_ids:\n",
    "        file_name = f\"subset_{item_id}.txt\"\n",
    "        file_path = os.path.join(Subset_file_path, file_name)\n",
    "\n",
    "        # Check the existence of each candidate subset file\n",
    "        if os.path.exists(file_path):\n",
    "            successful_subset_names.append(f\"subset_{item_id}\")\n",
    "            print(f\"  Found {file_name}\")\n",
    "        else:\n",
    "            print(f\"  File not found: {file_name}. Skipping.\")\n",
    "\n",
    "    print(f\"\\nIdentified {len(successful_subset_names)} subset files for processing.\")\n",
    "else:\n",
    "    print(\"RelevantItems DataFrame not found or is empty. Cannot identify subset files.\")\n",
    "\n",
    "# Note: actual loading and processing of subset files happens later, in\n",
    "# batch-based steps, to manage memory usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmcsWZB0oDMl"
   },
   "source": [
    "# 4.0. Extracting the most recent, annualized values per PIT Date (incl. Plausibility checks for the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcll5N-9z4yZ"
   },
   "source": [
    "## 4.1. Split according to source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1765204798291,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "DWNOVA9Iz4ya",
    "outputId": "5d417175-782f-4a3d-da0f-407586d43474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ValueCoding DataFrame...\n",
      "\n",
      "Processed ValueCoding DataFrame (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>ItemName</th>\n",
       "      <th>Source</th>\n",
       "      <th>ItemName_Sanitized</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05006</td>\n",
       "      <td>Market Price Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05007</td>\n",
       "      <td>Market Price YTD High Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_YTD_High_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05008</td>\n",
       "      <td>Market Price YTD Low Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_YTD_Low_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05009</td>\n",
       "      <td>Date of Current Price</td>\n",
       "      <td>Market</td>\n",
       "      <td>Date_of_Current_Price</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05091</td>\n",
       "      <td>Market Price 52 Week High Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_52_Week_High_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ItemCode                           ItemName  Source  \\\n",
       "0    05006               Market Price Current  Market   \n",
       "1    05007      Market Price YTD High Current  Market   \n",
       "2    05008       Market Price YTD Low Current  Market   \n",
       "3    05009              Date of Current Price  Market   \n",
       "4    05091  Market Price 52 Week High Current  Market   \n",
       "\n",
       "                  ItemName_Sanitized Category  \n",
       "0               Market_Price_Current    Mixed  \n",
       "1      Market_Price_YTD_High_Current    Mixed  \n",
       "2       Market_Price_YTD_Low_Current    Mixed  \n",
       "3              Date_of_Current_Price    Mixed  \n",
       "4  Market_Price_52_Week_High_Current    Mixed  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Annualized items: 148\n",
      "Number of Mixed items: 141\n",
      "Number of Special items: 62\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell processes a ValueCoding DataFrame and assigns a Category to each item\n",
    "# (per sanitized item name), based on its data sources:\n",
    "#\n",
    "#   1. Validates that `ValueCoding` exists and is non-empty.\n",
    "#   2. Sanitizes `ItemName` to a filesystem-safe `ItemName_Sanitized` (same rules as\n",
    "#      used for filenames).\n",
    "#   3. Normalizes the `Source` column (string type, trimmed).\n",
    "#   4. Groups all distinct sources per `ItemName_Sanitized`.\n",
    "#   5. Uses `decide_category` to map each sanitized name to a Category:\n",
    "#        - Hardcoded overrides for certain items.\n",
    "#        - Generic rules:\n",
    "#             * presence of IS / Other  -> \"Mixed\"\n",
    "#             * presence of Market / BS -> \"Annualized\"\n",
    "#             * presence of CFS         -> \"Special\"\n",
    "#        - otherwise                    -> None\n",
    "#   6. Attaches the Category back to each row based on `ItemName_Sanitized`.\n",
    "#   7. Creates three unique-item DataFrames:\n",
    "#        - `annualized_items`\n",
    "#        - `mixed_items`\n",
    "#        - `special_items`\n",
    "#   8. Exposes the processed objects in `globals()` for use in later cells.\n",
    "#   9. Shows a sample and prints counts of each category.\n",
    "#\n",
    "# If `ValueCoding` is not present or is empty, processing is skipped.\n",
    "\n",
    "# CELL 1 — Process ValueCoding and assign Category per ItemName_Sanitized\n",
    "\n",
    "if 'ValueCoding' in globals() and ValueCoding is not None and not ValueCoding.empty:\n",
    "    # Inform that processing of ValueCoding is starting\n",
    "    print(\"Processing ValueCoding DataFrame...\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original ValueCoding\n",
    "    value_coding_processed = ValueCoding.copy()\n",
    "\n",
    "    # --- Sanitize ItemName ---\n",
    "    # Ensure ItemName is treated as string (avoid issues with numbers / NaNs)\n",
    "    value_coding_processed['ItemName'] = value_coding_processed['ItemName'].astype(str)\n",
    "\n",
    "    # First pass: replace spaces and certain filesystem-unsafe characters with underscores\n",
    "    # Same rule set as used for building filenames elsewhere\n",
    "    value_coding_processed['ItemName_Sanitized'] = value_coding_processed['ItemName'].str.replace(\n",
    "        r'[ \\-/\\:\\\\*\\?\"<>|]', '_', regex=True\n",
    "    )\n",
    "    # Second pass: strip any remaining characters not in [word chars, dot, hyphen]\n",
    "    value_coding_processed['ItemName_Sanitized'] = value_coding_processed['ItemName_Sanitized'].str.replace(\n",
    "        r'[^\\w.-]', '', regex=True\n",
    "    )\n",
    "\n",
    "    # --- Normalize Source ---\n",
    "    # Convert Source to string and strip leading/trailing whitespace\n",
    "    value_coding_processed['Source'] = (\n",
    "        value_coding_processed['Source']\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Group all sources per sanitized name\n",
    "    # ------------------------------------------------------------------\n",
    "    # For each ItemName_Sanitized, collect the set of all non-null sources\n",
    "    sources_per_name = (\n",
    "        value_coding_processed\n",
    "        .groupby('ItemName_Sanitized')['Source']\n",
    "        .apply(lambda s: set(s.dropna()))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Helper to decide category per sanitized name\n",
    "    # ------------------------------------------------------------------\n",
    "    def decide_category(name, sources: set):\n",
    "        \"\"\"\n",
    "        Decide a category string (\"Mixed\", \"Annualized\", \"Special\", or None)\n",
    "        for a given sanitized item name based on its set of sources.\n",
    "        \"\"\"\n",
    "        # Item-specific overrides (these take precedence over generic rules)\n",
    "        if name == 'Depreciation_Depletion__Amortization':\n",
    "            # Prefer 'IS' interpretation -> treat as Mixed\n",
    "            return 'Mixed'\n",
    "        if name == 'Minority_Interest':\n",
    "            # Prefer 'BS' interpretation -> treat as Annualized\n",
    "            return 'Annualized'\n",
    "\n",
    "        # Generic rules:\n",
    "\n",
    "        # If any of the sources is Income Statement or \"Other\", classify as Mixed\n",
    "        if any(src in ['IS', 'Market'] for src in sources):\n",
    "            return 'Mixed'\n",
    "\n",
    "        # If any of the sources is Market or Balance Sheet, classify as Annualized\n",
    "        if any(src in ['BS'] for src in sources):\n",
    "            return 'Annualized'\n",
    "\n",
    "        # If any of the sources is Cash Flow Statement, classify as Special\n",
    "        if any(src in ['CFS'] for src in sources):\n",
    "            return 'Special'\n",
    "\n",
    "        # If none of the rules matched, leave as None (no clear mapping)\n",
    "        return None\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Build category_map for all sanitized names\n",
    "    # ------------------------------------------------------------------\n",
    "    # Map each sanitized item name to a Category by applying decide_category\n",
    "    category_map = {\n",
    "        name: decide_category(name, srcs)\n",
    "        for name, srcs in sources_per_name.items()\n",
    "    }\n",
    "\n",
    "    # Attach final Category back to each row, via ItemName_Sanitized\n",
    "    value_coding_processed['Category'] = (\n",
    "        value_coding_processed['ItemName_Sanitized'].map(category_map)\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Derive annualized_items / mixed_items / special_items\n",
    "    # as unique per sanitized name\n",
    "    # ------------------------------------------------------------------\n",
    "    annualized_items = (\n",
    "        value_coding_processed[value_coding_processed['Category'] == 'Annualized']\n",
    "        .drop_duplicates(subset=['ItemName_Sanitized'])\n",
    "        .copy()\n",
    "    )\n",
    "    mixed_items = (\n",
    "        value_coding_processed[value_coding_processed['Category'] == 'Mixed']\n",
    "        .drop_duplicates(subset=['ItemName_Sanitized'])\n",
    "        .copy()\n",
    "    )\n",
    "    special_items = (\n",
    "        value_coding_processed[value_coding_processed['Category'] == 'Special']\n",
    "        .drop_duplicates(subset=['ItemName_Sanitized'])\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Export to globals for use in later cells\n",
    "    # ------------------------------------------------------------------\n",
    "    globals()['value_coding_processed'] = value_coding_processed\n",
    "    globals()['annualized_items'] = annualized_items\n",
    "    globals()['mixed_items'] = mixed_items\n",
    "    globals()['special_items'] = special_items\n",
    "    globals()['category_map'] = category_map\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Display sample and counts\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\nProcessed ValueCoding DataFrame (sample):\")\n",
    "    display(value_coding_processed.head())\n",
    "\n",
    "    print(f\"\\nNumber of Annualized items: {len(annualized_items)}\")\n",
    "    print(f\"Number of Mixed items: {len(mixed_items)}\")\n",
    "    print(f\"Number of Special items: {len(special_items)}\")\n",
    "\n",
    "else:\n",
    "    # If ValueCoding is not available or has no rows, skip processing\n",
    "    print(\"ValueCoding DataFrame not found or is empty. Skipping processing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d59odhYbz4yc"
   },
   "source": [
    "### Sort into correct bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1765204798378,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "UqDUV12jz4yc",
    "outputId": "72be93ce-166a-449a-f8a9-d90f7db1239a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying work_subset files and creating variables based on categories...\n",
      "\n",
      "Found 49 work_subset files in Temp directory.\n",
      "  'work_subset_Accounts_Payable.txt' -> Annualized (variable 'Annualized_1').\n",
      "  'work_subset_Capital_Expenditures_Addtns_to_Fixed_Assets.txt' -> Special (variable 'Special_1').\n",
      "  'work_subset_Cash_Dividends_Paid___Total.txt' -> Special (variable 'Special_2').\n",
      "  'work_subset_Cash__Short_Term_Investments.txt' -> Annualized (variable 'Annualized_2').\n",
      "  'work_subset_Com_Pfd_Redeemed_Retired_Converted_Etc..txt' -> Special (variable 'Special_3').\n",
      "  'work_subset_Common_Equity.txt' -> Annualized (variable 'Annualized_3').\n",
      "  'work_subset_Cost_of_Goods_Sold_Excl_Depreciation.txt' -> Mixed (variable 'Mixed_1').\n",
      "  'work_subset_Current_Assets___Total.txt' -> Annualized (variable 'Annualized_4').\n",
      "  'work_subset_Current_Liabilities___Total.txt' -> Annualized (variable 'Annualized_5').\n",
      "  'work_subset_Deferred_Taxes.txt' -> Annualized (variable 'Annualized_6').\n",
      "  'work_subset_Depreciation_Depletion__Amortization.txt' -> Mixed (variable 'Mixed_2').\n",
      "  'work_subset_Disposal_of_Fixed_Assets.txt' -> Special (variable 'Special_4').\n",
      "  'work_subset_Earnings_Per_Share_Fiscal_Year_End.txt' -> Mixed (variable 'Mixed_3').\n",
      "  'work_subset_Extraordinary_Items.txt' -> Special (variable 'Special_5').\n",
      "  'work_subset_Funds_From_For_Other_Operating_Activities.txt' -> Special (variable 'Special_6').\n",
      "  'work_subset_Funds_From_Operations.txt' -> Special (variable 'Special_7').\n",
      "  'work_subset_Income_Taxes.txt' -> Mixed (variable 'Mixed_4').\n",
      "  'work_subset_Income_Taxes_Payable.txt' -> Annualized (variable 'Annualized_7').\n",
      "  'work_subset_Interest_Expense___Total.txt' -> Mixed (variable 'Mixed_5').\n",
      "  'work_subset_Inventories___Total.txt' -> Annualized (variable 'Annualized_8').\n",
      "  'work_subset_Investments_in_Associated_Companies.txt' -> Annualized (variable 'Annualized_9').\n",
      "  'work_subset_Investments_in_Sales__Direct_Financing_Leases.txt' -> Annualized (variable 'Annualized_10').\n",
      "  'work_subset_Long_Term_Borrowings.txt' -> Special (variable 'Special_8').\n",
      "  'work_subset_Long_Term_Debt.txt' -> Annualized (variable 'Annualized_11').\n",
      "  'work_subset_Long_Term_Receivables.txt' -> Annualized (variable 'Annualized_12').\n",
      "  'work_subset_Minority_Interest.txt' -> Annualized (variable 'Annualized_13').\n",
      "  'work_subset_Net_Cash_Flow___Financing.txt' -> Special (variable 'Special_9').\n",
      "  'work_subset_Net_Cash_Flow___Investing.txt' -> Special (variable 'Special_10').\n",
      "  'work_subset_Net_Cash_Flow___Operating_Activities.txt' -> Special (variable 'Special_11').\n",
      "  'work_subset_Net_Income_Before_Extra_Items_Preferred_Divs.txt' -> Mixed (variable 'Mixed_6').\n",
      "  'work_subset_Net_Income_Used_to_Calculate_Basic_EPS.txt' -> Mixed (variable 'Mixed_7').\n",
      "  'work_subset_Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt' -> Special (variable 'Special_12').\n",
      "  'work_subset_Net_Sales_or_Revenues.txt' -> Mixed (variable 'Mixed_8').\n",
      "  'work_subset_Operating_Income.txt' -> Mixed (variable 'Mixed_9').\n",
      "  'work_subset_Other_Assets___Total.txt' -> Annualized (variable 'Annualized_14').\n",
      "  'work_subset_Other_Current_Assets.txt' -> Annualized (variable 'Annualized_15').\n",
      "  'work_subset_Other_Current_Liabilities.txt' -> Annualized (variable 'Annualized_16').\n",
      "  'work_subset_Other_Investments.txt' -> Annualized (variable 'Annualized_17').\n",
      "  'work_subset_Other_Liabilities.txt' -> Annualized (variable 'Annualized_18').\n",
      "  'work_subset_Preferred_Stock.txt' -> Annualized (variable 'Annualized_19').\n",
      "  'work_subset_Property_Plant__Equipment___Net.txt' -> Annualized (variable 'Annualized_20').\n",
      "  'work_subset_ReceivablesNet.txt' -> Annualized (variable 'Annualized_21').\n",
      "  'work_subset_Reduction_in_Long_Term_Debt.txt' -> Special (variable 'Special_13').\n",
      "  'work_subset_Sales_Per_Share.txt' -> Mixed (variable 'Mixed_10').\n",
      "  'work_subset_Selling_General__Administrative_Expenses.txt' -> Mixed (variable 'Mixed_11').\n",
      "  'work_subset_Short_Term_Debt__Current_Portion_of_LT_Debt.txt' -> Annualized (variable 'Annualized_22').\n",
      "  'work_subset_Total_Assets.txt' -> Annualized (variable 'Annualized_23').\n",
      "  'work_subset_Total_Liabilities.txt' -> Annualized (variable 'Annualized_24').\n",
      "  'work_subset_Unspecified_Other_Loans.txt' -> Annualized (variable 'Annualized_25').\n",
      "\n",
      "Variable creation complete.\n",
      "Created 25 Annualized variables.\n",
      "Created 11 Mixed variables.\n",
      "Created 13 Special variables.\n",
      "\n",
      "Annualized Variables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Annualized_1': 'Accounts_Payable',\n",
       " 'Annualized_2': 'Cash__Short_Term_Investments',\n",
       " 'Annualized_3': 'Common_Equity',\n",
       " 'Annualized_4': 'Current_Assets___Total',\n",
       " 'Annualized_5': 'Current_Liabilities___Total',\n",
       " 'Annualized_6': 'Deferred_Taxes',\n",
       " 'Annualized_7': 'Income_Taxes_Payable',\n",
       " 'Annualized_8': 'Inventories___Total',\n",
       " 'Annualized_9': 'Investments_in_Associated_Companies',\n",
       " 'Annualized_10': 'Investments_in_Sales__Direct_Financing_Leases',\n",
       " 'Annualized_11': 'Long_Term_Debt',\n",
       " 'Annualized_12': 'Long_Term_Receivables',\n",
       " 'Annualized_13': 'Minority_Interest',\n",
       " 'Annualized_14': 'Other_Assets___Total',\n",
       " 'Annualized_15': 'Other_Current_Assets',\n",
       " 'Annualized_16': 'Other_Current_Liabilities',\n",
       " 'Annualized_17': 'Other_Investments',\n",
       " 'Annualized_18': 'Other_Liabilities',\n",
       " 'Annualized_19': 'Preferred_Stock',\n",
       " 'Annualized_20': 'Property_Plant__Equipment___Net',\n",
       " 'Annualized_21': 'ReceivablesNet',\n",
       " 'Annualized_22': 'Short_Term_Debt__Current_Portion_of_LT_Debt',\n",
       " 'Annualized_23': 'Total_Assets',\n",
       " 'Annualized_24': 'Total_Liabilities',\n",
       " 'Annualized_25': 'Unspecified_Other_Loans'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixed Variables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Mixed_1': 'Cost_of_Goods_Sold_Excl_Depreciation',\n",
       " 'Mixed_2': 'Depreciation_Depletion__Amortization',\n",
       " 'Mixed_3': 'Earnings_Per_Share_Fiscal_Year_End',\n",
       " 'Mixed_4': 'Income_Taxes',\n",
       " 'Mixed_5': 'Interest_Expense___Total',\n",
       " 'Mixed_6': 'Net_Income_Before_Extra_Items_Preferred_Divs',\n",
       " 'Mixed_7': 'Net_Income_Used_to_Calculate_Basic_EPS',\n",
       " 'Mixed_8': 'Net_Sales_or_Revenues',\n",
       " 'Mixed_9': 'Operating_Income',\n",
       " 'Mixed_10': 'Sales_Per_Share',\n",
       " 'Mixed_11': 'Selling_General__Administrative_Expenses'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Special Variables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Special_1': 'Capital_Expenditures_Addtns_to_Fixed_Assets',\n",
       " 'Special_2': 'Cash_Dividends_Paid___Total',\n",
       " 'Special_3': 'Com_Pfd_Redeemed_Retired_Converted_Etc.',\n",
       " 'Special_4': 'Disposal_of_Fixed_Assets',\n",
       " 'Special_5': 'Extraordinary_Items',\n",
       " 'Special_6': 'Funds_From_For_Other_Operating_Activities',\n",
       " 'Special_7': 'Funds_From_Operations',\n",
       " 'Special_8': 'Long_Term_Borrowings',\n",
       " 'Special_9': 'Net_Cash_Flow___Financing',\n",
       " 'Special_10': 'Net_Cash_Flow___Investing',\n",
       " 'Special_11': 'Net_Cash_Flow___Operating_Activities',\n",
       " 'Special_12': 'Net_Proceeds_From_Sale_Issue_of_Com__Pfd',\n",
       " 'Special_13': 'Reduction_in_Long_Term_Debt'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# CELL 2 maps work_subset_*.txt files to categories (\"Annualized\", \"Mixed\", \"Special\")\n",
    "# based on the ItemName_Sanitized that was derived in the previous cell.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Check that the required categorized DataFrames (annualized_items, mixed_items,\n",
    "#      special_items) and the temporary directory path Temp_file_path_DP exist.\n",
    "#   2. Build three sets of sanitized item names (annualized_names, mixed_names,\n",
    "#      special_names) from those DataFrames.\n",
    "#   3. List all files in Temp_file_path_DP and filter for those matching\n",
    "#      \"work_subset_*.txt\".\n",
    "#   4. For each work_subset file:\n",
    "#        - Extract the sanitized item name from the filename.\n",
    "#        - Determine whether it belongs to Mixed, Annualized, or Special based on\n",
    "#          the sets created in step 2.\n",
    "#        - Assign it a variable name (Mixed_n, Annualized_n, Special_n) and store\n",
    "#          that mapping in dicts mixed_vars, annualized_vars, special_vars.\n",
    "#   5. Store these dicts in globals() for use in later cells.\n",
    "#   6. Print summary information and display the created dictionaries.\n",
    "#   7. Perform garbage collection at the end.\n",
    "#\n",
    "# If any of the prerequisites are missing, it prints a message and skips the mapping.\n",
    "\n",
    "\n",
    "# CELL 2 — Map work_subset files to categories using ItemName_Sanitized\n",
    "\n",
    "if ('annualized_items' not in globals() or annualized_items is None or\n",
    "    'mixed_items' not in globals() or mixed_items is None or\n",
    "    'special_items' not in globals() or special_items is None or\n",
    "    'Temp_file_path_DP' not in globals()):\n",
    "    # If required DataFrames or directory path are missing, do not proceed\n",
    "    print(\"Required DataFrames (annualized_items, mixed_items, special_items) or Temp_file_path_DP not found. Please run the categorization cell.\")\n",
    "else:\n",
    "    print(\"Identifying work_subset files and creating variables based on categories...\")\n",
    "\n",
    "    # Sets of sanitized names that are final Annualized/Mixed/Special\n",
    "    annualized_names = set(annualized_items['ItemName_Sanitized'].dropna())\n",
    "    mixed_names      = set(mixed_items['ItemName_Sanitized'].dropna())\n",
    "    special_names    = set(special_items['ItemName_Sanitized'].dropna())\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Get a list of work_subset files from the temp directory\n",
    "    # ------------------------------------------------------------------\n",
    "    temp_files = os.listdir(Temp_file_path_DP)\n",
    "    work_subset_files = [\n",
    "        f for f in temp_files\n",
    "        if f.startswith('work_subset_') and f.endswith('.txt')\n",
    "    ]\n",
    "\n",
    "    # Dictionaries to hold mappings:\n",
    "    #   \"Annualized_1\" -> \"SomeItemName\"\n",
    "    #   \"Mixed_1\"      -> \"AnotherItemName\"\n",
    "    #   \"Special_1\"    -> \"SpecialItemName\"\n",
    "    annualized_vars = {}\n",
    "    mixed_vars = {}\n",
    "    special_vars = {}\n",
    "\n",
    "    print(f\"\\nFound {len(work_subset_files)} work_subset files in Temp directory.\")\n",
    "\n",
    "    # Sort files to have deterministic order when assigning variable names\n",
    "    work_subset_files.sort()\n",
    "\n",
    "    # Counters for how many items fall into each category; used for variable suffixes\n",
    "    annualized_count = 0\n",
    "    mixed_count = 0\n",
    "    special_count = 0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Iterate over each work_subset file and map it to a category\n",
    "    # ------------------------------------------------------------------\n",
    "    for file_name in work_subset_files:\n",
    "        # Extract sanitized item name from filename, expecting \"work_subset_<name>.txt\"\n",
    "        match = re.match(r'work_subset_(.+)\\.txt$', file_name)\n",
    "        if not match:\n",
    "            print(f\"  Filename format not as expected for '{file_name}'. Skipping processing.\")\n",
    "            continue\n",
    "\n",
    "        sanitized_item_name = match.group(1)\n",
    "\n",
    "        # Use the resolved sets. No more ambiguous precedence:\n",
    "        # priority Mixed -> Annualized -> Special, in this order of checks.\n",
    "        if sanitized_item_name in mixed_names:\n",
    "            mixed_count += 1\n",
    "            var_name = f\"Mixed_{mixed_count}\"\n",
    "            mixed_vars[var_name] = sanitized_item_name\n",
    "            print(f\"  '{file_name}' -> Mixed (variable '{var_name}').\")\n",
    "\n",
    "        elif sanitized_item_name in annualized_names:\n",
    "            annualized_count += 1\n",
    "            var_name = f\"Annualized_{annualized_count}\"\n",
    "            annualized_vars[var_name] = sanitized_item_name\n",
    "            print(f\"  '{file_name}' -> Annualized (variable '{var_name}').\")\n",
    "\n",
    "        elif sanitized_item_name in special_names:\n",
    "            special_count += 1\n",
    "            var_name = f\"Special_{special_count}\"\n",
    "            special_vars[var_name] = sanitized_item_name\n",
    "            print(f\"  '{file_name}' -> Special (variable '{var_name}').\")\n",
    "\n",
    "        else:\n",
    "            # No category mapping found for this sanitized name\n",
    "            print(f\"  '{file_name}' -> No matching Category (might be unmapped or ambiguous). Skipping.\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Expose the mapping dictionaries globally for later use\n",
    "    # ------------------------------------------------------------------\n",
    "    globals()['annualized_vars'] = annualized_vars\n",
    "    globals()['mixed_vars'] = mixed_vars\n",
    "    globals()['special_vars'] = special_vars\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Summary output and inspection\n",
    "    # ------------------------------------------------------------------\n",
    "    print(f\"\\nVariable creation complete.\")\n",
    "    print(f\"Created {len(annualized_vars)} Annualized variables.\")\n",
    "    print(f\"Created {len(mixed_vars)} Mixed variables.\")\n",
    "    print(f\"Created {len(special_vars)} Special variables.\")\n",
    "\n",
    "    print(\"\\nAnnualized Variables:\")\n",
    "    display(annualized_vars)\n",
    "\n",
    "    print(\"\\nMixed Variables:\")\n",
    "    display(mixed_vars)\n",
    "\n",
    "    print(\"\\nSpecial Variables:\")\n",
    "    display(special_vars)\n",
    "\n",
    "    # Run garbage collection after building mappings\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh7zifBYNg5C"
   },
   "source": [
    "## 4.4. Balance Sheet (1-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXSsUb80SBPa"
   },
   "source": [
    "### Annualized 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVaGRdVY3Y39"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1765204798459,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "-paeYHwe3Y3-",
    "outputId": "508dd7d6-8a96-4749-d886-7c7f452979a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_1  ->  ItemName: 'Accounts_Payable'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 1  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sgkpW_73Y3_"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1765204799281,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "J5Y_qkPS3Y3_",
    "outputId": "ddf1693b-2a3f-4314-fb11-dc5fd7ab3baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Accounts_Payable' ...\n",
      "Full annualized dataset loaded successfully: 3,058,863 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3040</td>\n",
       "      <td>380.421036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3040</td>\n",
       "      <td>369.24053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3040</td>\n",
       "      <td>401.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3040</td>\n",
       "      <td>228.545754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3040</td>\n",
       "      <td>264.087772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     3040  380.421036  \n",
       "1     3040   369.24053  \n",
       "2     3040  401.581395  \n",
       "3     3040  228.545754  \n",
       "4     3040  264.087772  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7qE3h2C3Y4A"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1765204799604,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "1Voujf2F3Y4A",
    "outputId": "3b8d9334-e400-4cb7-e321-b37146fcab91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Accounts_Payable' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4014211/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y07' 'Q2Y07' 'Q3Y07' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3040</td>\n",
       "      <td>380.421036</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3040</td>\n",
       "      <td>369.24053</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3040</td>\n",
       "      <td>401.581395</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3040</td>\n",
       "      <td>228.545754</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3040</td>\n",
       "      <td>264.087772</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     3040  380.421036           Y92  \n",
       "1     3040   369.24053           Y93  \n",
       "2     3040  401.581395           Y94  \n",
       "3     3040  228.545754           Y95  \n",
       "4     3040  264.087772           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBoK14aS3Y4B"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23283,
     "status": "ok",
     "timestamp": 1765204822889,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "2hbPfTq03Y4B",
    "outputId": "2948da73-ab01-428f-be4a-82b30d596aa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,058,863 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 2953426\n",
      "                mean: 5107362.038584086\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 115.44289714446015\n",
      "                 p10: 79.59068207901444\n",
      "                 p20: 99.07344279761138\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 102.59893249811991\n",
      "                 p90: 137.964828798095\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 249,448\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 2703978\n",
      "                mean: 101.52850711540884\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 101.46940540330344\n",
      "                 p10: 85.04545634887302\n",
      "                 p20: 99.99999980824025\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 116.24761578847256\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Accounts_Payable_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Accounts_Payable_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,058,863\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 249,448\n",
      "Output rows (final):            2,809,415\n",
      "Check: excluded + dropped + output = 3,058,863\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nB2IiDmSCJS"
   },
   "source": [
    "### Annualized 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USi9UxAA3fSB"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1765204823136,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "2PanQrzu3fSD",
    "outputId": "86ff134c-c6e8-407c-c8fb-c7b588dbef3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_2  ->  ItemName: 'Cash__Short_Term_Investments'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 2  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9OQS6dk3fSE"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1765204823272,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "a79_edrj3fSF",
    "outputId": "b0a13399-4739-4f55-cf3a-eac78a24ebca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Cash__Short_Term_Investments' ...\n",
      "Full annualized dataset loaded successfully: 3,624,152 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2001</td>\n",
       "      <td>167.849993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2001</td>\n",
       "      <td>320.816779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2001</td>\n",
       "      <td>167.728265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2001</td>\n",
       "      <td>110.705844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2001</td>\n",
       "      <td>70.212302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     2001  167.849993  \n",
       "1     2001  320.816779  \n",
       "2     2001  167.728265  \n",
       "3     2001  110.705844  \n",
       "4     2001   70.212302  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjWQogb63fSF"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1765204823461,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "36kZk6dL3fSF",
    "outputId": "ac1bb7fe-d1da-4d4e-ce97-ebd85c1c5267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Cash__Short_Term_Investments' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4014211/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y00' 'Q4Y01' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2001</td>\n",
       "      <td>167.849993</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2001</td>\n",
       "      <td>320.816779</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2001</td>\n",
       "      <td>167.728265</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2001</td>\n",
       "      <td>110.705844</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2001</td>\n",
       "      <td>70.212302</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     2001  167.849993           Y92  \n",
       "1     2001  320.816779           Y93  \n",
       "2     2001  167.728265           Y94  \n",
       "3     2001  110.705844           Y95  \n",
       "4     2001   70.212302           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUO9SEcr3fSG"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24576,
     "status": "ok",
     "timestamp": 1765204848069,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "6F50dYby3fSG",
    "outputId": "c3c88f40-e455-4c4d-b5b6-0790b12ec140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,624,152 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 3504849\n",
      "                mean: 23366.10316848247\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 104.68099707984614\n",
      "                 p10: 70.41967537889334\n",
      "                 p20: 93.21114357569256\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0003420353733\n",
      "                 p90: 122.0449509555434\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 306,344\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 3198505\n",
      "                mean: 100.38405038913653\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.29702948332785\n",
      "                 p10: 82.70619748657923\n",
      "                 p20: 98.43183437862383\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 112.9377533731338\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Cash__Short_Term_Investments_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Cash__Short_Term_Investments_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,624,152\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 306,344\n",
      "Output rows (final):            3,317,808\n",
      "Check: excluded + dropped + output = 3,624,152\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1JakGszSDEm"
   },
   "source": [
    "### Annualized 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0j7HjRx3gHC"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1765204848345,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "AGa8fP7Z3gHD",
    "outputId": "3ad566e2-0229-44c8-8c43-b9c92c323735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_3  ->  ItemName: 'Common_Equity'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 3  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jHCUIwZ3gHE"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1765204848629,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "kwQRG0mA3gHE",
    "outputId": "9f4f26b3-c47b-458a-efe2-790473213d3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Common_Equity' ...\n",
      "Full annualized dataset loaded successfully: 3,633,273 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3501</td>\n",
       "      <td>378.411011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3501</td>\n",
       "      <td>434.756779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3501</td>\n",
       "      <td>518.204722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3501</td>\n",
       "      <td>386.14278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3501</td>\n",
       "      <td>387.109014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     3501  378.411011  \n",
       "1     3501  434.756779  \n",
       "2     3501  518.204722  \n",
       "3     3501   386.14278  \n",
       "4     3501  387.109014  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "td7OkRT53gHF"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1765204848834,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "8GgZVLgS3gHF",
    "outputId": "be733bc1-3b56-460a-870d-7a62773e772c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Common_Equity' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4014211/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y00' 'Q4Y01' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3501</td>\n",
       "      <td>378.411011</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3501</td>\n",
       "      <td>434.756779</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3501</td>\n",
       "      <td>518.204722</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3501</td>\n",
       "      <td>386.14278</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3501</td>\n",
       "      <td>387.109014</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     3501  378.411011           Y92  \n",
       "1     3501  434.756779           Y93  \n",
       "2     3501  518.204722           Y94  \n",
       "3     3501   386.14278           Y95  \n",
       "4     3501  387.109014           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjhFGPz53gHG"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26003,
     "status": "ok",
     "timestamp": 1765204874874,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "UHCj7s-f3gHG",
    "outputId": "4a37a4fa-d79f-4ccd-c950-aa8e8e0a6823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,633,273 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 3530172\n",
      "                mean: 28242.62116630743\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.00167277420879\n",
      "                 p10: 90.74869375256846\n",
      "                 p20: 97.06041111110679\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 103.58511892704527\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 119,253\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 3410919\n",
      "                mean: 99.7094299332743\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.57579560408712\n",
      "                 p10: 92.6076821043988\n",
      "                 p20: 97.7111882231887\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 102.87933105981647\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Common_Equity_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Common_Equity_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,633,273\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 119,253\n",
      "Output rows (final):            3,514,020\n",
      "Check: excluded + dropped + output = 3,633,273\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9vM4WYsSEAP"
   },
   "source": [
    "### Annualized 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJ5SSTUy3g39"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1765204875069,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "IEwK6edz3g3-",
    "outputId": "9ce3e6e6-a548-4773-ed26-5dd4c758d6ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_4  ->  ItemName: 'Current_Assets___Total'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 4  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IP9tL-A33g3_"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1765204875237,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "e1QdCH923g4A",
    "outputId": "caed3d78-e439-4221-fb6a-c32abb2759da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Current_Assets___Total' ...\n",
      "Full annualized dataset loaded successfully: 3,551,641 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2201</td>\n",
       "      <td>748.140365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2201</td>\n",
       "      <td>864.906741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2201</td>\n",
       "      <td>975.879591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2201</td>\n",
       "      <td>762.389782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2201</td>\n",
       "      <td>665.184633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     2201  748.140365  \n",
       "1     2201  864.906741  \n",
       "2     2201  975.879591  \n",
       "3     2201  762.389782  \n",
       "4     2201  665.184633  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcXDq9mO3g4B"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1765204875515,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "CbRuHZqJ3g4B",
    "outputId": "a5494f67-80bb-4193-dd7c-310c816e0dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Current_Assets___Total' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4014211/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y00' 'Q4Y01' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2201</td>\n",
       "      <td>748.140365</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2201</td>\n",
       "      <td>864.906741</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2201</td>\n",
       "      <td>975.879591</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2201</td>\n",
       "      <td>762.389782</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2201</td>\n",
       "      <td>665.184633</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     2201  748.140365           Y92  \n",
       "1     2201  864.906741           Y93  \n",
       "2     2201  975.879591           Y94  \n",
       "3     2201  762.389782           Y95  \n",
       "4     2201  665.184633           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfI_YRR43g4C"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25692,
     "status": "ok",
     "timestamp": 1765204901212,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "TJ3Ziol63g4C",
    "outputId": "f94b7838-527e-42cd-80ab-1cdf753300df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,551,641 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 3448829\n",
      "                mean: 23618.976151775518\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.29664618582204\n",
      "                 p10: 87.85928855984785\n",
      "                 p20: 97.47523938800086\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.21294398824752\n",
      "                 p90: 109.39895940062765\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 88,373\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 3360456\n",
      "                mean: 100.1777373893305\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.05616343996435\n",
      "                 p10: 89.55831451473193\n",
      "                 p20: 98.19813980620282\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0715032214191\n",
      "                 p90: 108.2277289562339\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Current_Assets___Total_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Current_Assets___Total_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,551,641\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 88,373\n",
      "Output rows (final):            3,463,268\n",
      "Check: excluded + dropped + output = 3,551,641\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hu8pT-JASFPb"
   },
   "source": [
    "### Annualized 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyEGQfNY3hiZ"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1765204901563,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "PMhOSBG83hia",
    "outputId": "456c2e14-2983-4eec-c71d-04f8ec996c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_5  ->  ItemName: 'Current_Liabilities___Total'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 5  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztoWmv3b3hia"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1765204901742,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ysYvbJlo3hib",
    "outputId": "4b9cc26c-28e6-4681-9ac1-5fae53ec44f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Current_Liabilities___Total' ...\n",
      "Full annualized dataset loaded successfully: 3,568,264 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3101</td>\n",
       "      <td>663.515421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3101</td>\n",
       "      <td>693.189519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3101</td>\n",
       "      <td>773.728435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3101</td>\n",
       "      <td>852.251409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3101</td>\n",
       "      <td>841.844876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     3101  663.515421  \n",
       "1     3101  693.189519  \n",
       "2     3101  773.728435  \n",
       "3     3101  852.251409  \n",
       "4     3101  841.844876  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJBlaooh3hib"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1765204901973,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "5AecOPWM3hic",
    "outputId": "f94d6792-a52b-46f3-82a3-07b1104903e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Current_Liabilities___Total' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4014211/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y00' 'Q4Y01' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3101</td>\n",
       "      <td>663.515421</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3101</td>\n",
       "      <td>693.189519</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3101</td>\n",
       "      <td>773.728435</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3101</td>\n",
       "      <td>852.251409</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3101</td>\n",
       "      <td>841.844876</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     3101  663.515421           Y92  \n",
       "1     3101  693.189519           Y93  \n",
       "2     3101  773.728435           Y94  \n",
       "3     3101  852.251409           Y95  \n",
       "4     3101  841.844876           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa2mMajE3hic"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25095,
     "status": "ok",
     "timestamp": 1765204927072,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "kDS0HW1s3hic",
    "outputId": "3be7210c-2d98-4f49-c071-e0d70a1e2acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,568,264 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 3466846\n",
      "                mean: 1573506.9621457502\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.4730978559391\n",
      "                 p10: 81.03415010611616\n",
      "                 p20: 94.70694669530315\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 110.40676222588017\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 118,269\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 3348577\n",
      "                mean: 99.41491224027521\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.28768395186282\n",
      "                 p10: 84.35450356283611\n",
      "                 p20: 96.22155118450655\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 108.7493962052198\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Current_Liabilities___Total_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Current_Liabilities___Total_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,568,264\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 118,269\n",
      "Output rows (final):            3,449,995\n",
      "Check: excluded + dropped + output = 3,568,264\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5DWUCxkSHOO"
   },
   "source": [
    "### Annualized 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-2vHQEo3ihA"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1765204927425,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "kugNkZks3ihA",
    "outputId": "aa63b327-3fa3-40e0-e4bd-a71aa315400b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_6  ->  ItemName: 'Deferred_Taxes'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 6  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YegAzUaN3ihB"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 659,
     "status": "ok",
     "timestamp": 1765204928089,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "jMeS_Keq3ihC",
    "outputId": "6b7415d2-6f36-4a31-e981-a99f1f6059c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Deferred_Taxes' ...\n",
      "Full annualized dataset loaded successfully: 2,861,796 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1997</td>\n",
       "      <td>December</td>\n",
       "      <td>3263</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1998</td>\n",
       "      <td>December</td>\n",
       "      <td>3263</td>\n",
       "      <td>1.90201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>2000-05-19</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>October</td>\n",
       "      <td>3263</td>\n",
       "      <td>1.252426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1987</td>\n",
       "      <td>June</td>\n",
       "      <td>3263</td>\n",
       "      <td>0.0026782265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1991</td>\n",
       "      <td>June</td>\n",
       "      <td>3263</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName ImplCountry  \\\n",
       "0  C02500770            PEUGEOT CITROEN ARGENTINA S.A.   Argentina   \n",
       "1  C02500770            PEUGEOT CITROEN ARGENTINA S.A.   Argentina   \n",
       "2  C02500770            PEUGEOT CITROEN ARGENTINA S.A.   Argentina   \n",
       "3  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "4  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "\n",
       "  CurrentCurrency HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod  \\\n",
       "0             Ars          Ars  1998-07-03         A          3         1997   \n",
       "1             Ars          Ars  1999-10-01         A          3         1998   \n",
       "2             Ars          Ars  2000-05-19         A          2         1999   \n",
       "3             Ars          Ars  1996-05-03         A          3         1987   \n",
       "4             Ars          Ars  1996-05-03         A          3         1991   \n",
       "\n",
       "  FYE Month ItemCode         Value  \n",
       "0  December     3263           0.0  \n",
       "1  December     3263       1.90201  \n",
       "2   October     3263      1.252426  \n",
       "3      June     3263  0.0026782265  \n",
       "4      June     3263           0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3St4ALy33ihC"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1765204928388,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "lgiD-bqk3ihD",
    "outputId": "b2682136-48fc-4604-ac9c-f4a847918c56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Deferred_Taxes' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4014211/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y00' 'Q4Y01' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1997</td>\n",
       "      <td>December</td>\n",
       "      <td>3263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1998</td>\n",
       "      <td>December</td>\n",
       "      <td>3263</td>\n",
       "      <td>1.90201</td>\n",
       "      <td>Y98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>2000-05-19</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>October</td>\n",
       "      <td>3263</td>\n",
       "      <td>1.252426</td>\n",
       "      <td>Y99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1987</td>\n",
       "      <td>June</td>\n",
       "      <td>3263</td>\n",
       "      <td>0.0026782265</td>\n",
       "      <td>Y87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02520200</td>\n",
       "      <td>ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1991</td>\n",
       "      <td>June</td>\n",
       "      <td>3263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                               CompanyName ImplCountry  \\\n",
       "0  C02500770            PEUGEOT CITROEN ARGENTINA S.A.   Argentina   \n",
       "1  C02500770            PEUGEOT CITROEN ARGENTINA S.A.   Argentina   \n",
       "2  C02500770            PEUGEOT CITROEN ARGENTINA S.A.   Argentina   \n",
       "3  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "4  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA   Argentina   \n",
       "\n",
       "  CurrentCurrency HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod  \\\n",
       "0             Ars          Ars  1998-07-03         A          3          1997   \n",
       "1             Ars          Ars  1999-10-01         A          3          1998   \n",
       "2             Ars          Ars  2000-05-19         A          2          1999   \n",
       "3             Ars          Ars  1996-05-03         A          3          1987   \n",
       "4             Ars          Ars  1996-05-03         A          3          1991   \n",
       "\n",
       "  FYE Month ItemCode         Value Str_FiscalPrd  \n",
       "0  December     3263           0.0           Y97  \n",
       "1  December     3263       1.90201           Y98  \n",
       "2   October     3263      1.252426           Y99  \n",
       "3      June     3263  0.0026782265           Y87  \n",
       "4      June     3263           0.0           Y91  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ybc8ATmN3ihD"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18745,
     "status": "ok",
     "timestamp": 1765204947137,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Fd2u2Kr93ihE",
    "outputId": "67e60d7f-9af6-49ba-9b76-ef72e0a5ae58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 2,861,796 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 2424156\n",
      "                mean: 15695.481367535955\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 95.98993743459458\n",
      "                 p10: 52.828383544285686\n",
      "                 p20: 90.59805665799917\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 116.12178467155692\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 311,755\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 2112401\n",
      "                mean: 100.5711365907513\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.47746782452336\n",
      "                 p10: 87.02224407029617\n",
      "                 p20: 99.98532325530198\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 109.85626283367556\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Deferred_Taxes_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Deferred_Taxes_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     2,861,796\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 311,755\n",
      "Output rows (final):            2,550,041\n",
      "Check: excluded + dropped + output = 2,861,796\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlJBz31ZSIVo"
   },
   "source": [
    "### Annualized 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX36VG3b3jUU"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1765204947337,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "YYFlkKiB3jUU",
    "outputId": "274c07d5-31ae-4f3e-97e8-5e18e7c5dff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_7  ->  ItemName: 'Income_Taxes_Payable'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 7  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cNFK-8Q3jUV"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1765204947499,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ux_Y8MYw3jUW",
    "outputId": "dd240d6a-0744-4da8-bcb7-bc32e7320960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Income_Taxes_Payable' ...\n",
      "Full annualized dataset loaded successfully: 1,979,636 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3063</td>\n",
       "      <td>10.582409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3063</td>\n",
       "      <td>12.508193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3063</td>\n",
       "      <td>8.751102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3063</td>\n",
       "      <td>9.041866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3063</td>\n",
       "      <td>9.454759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode      Value  \n",
       "0     3063  10.582409  \n",
       "1     3063  12.508193  \n",
       "2     3063   8.751102  \n",
       "3     3063   9.041866  \n",
       "4     3063   9.454759  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nE4bBHNL3jUW"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1765204947676,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "TpU9DK643jUW",
    "outputId": "33f1dfa0-ecbe-47b1-f37a-419ab696f544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Income_Taxes_Payable' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4014211/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y07' 'Q2Y07' 'Q3Y07' ... 'Q1Y17' 'Q2Y17' 'Q3Y17']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>3063</td>\n",
       "      <td>10.582409</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>3063</td>\n",
       "      <td>12.508193</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>3063</td>\n",
       "      <td>8.751102</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>3063</td>\n",
       "      <td>9.041866</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>3063</td>\n",
       "      <td>9.454759</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode      Value Str_FiscalPrd  \n",
       "0     3063  10.582409           Y92  \n",
       "1     3063  12.508193           Y93  \n",
       "2     3063   8.751102           Y94  \n",
       "3     3063   9.041866           Y95  \n",
       "4     3063   9.454759           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UC-YbdQq3jUX"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13647,
     "status": "ok",
     "timestamp": 1765204961329,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "yUlb5Cvs3jUX",
    "outputId": "5b01e169-6149-4873-8086-6221b5266f0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,979,636 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 1776396\n",
      "                mean: 247102.09123122873\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 143.2997482565826\n",
      "                 p10: 52.03050494652123\n",
      "                 p20: 90.74235927873305\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 101.39946722695382\n",
      "                 p90: 175.95969101080811\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 321,621\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 1454775\n",
      "                mean: 101.22983209459582\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 101.1723193068005\n",
      "                 p10: 82.04634758484467\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 115.00000000000001\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Income_Taxes_Payable_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Income_Taxes_Payable_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,979,636\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 321,621\n",
      "Output rows (final):            1,658,015\n",
      "Check: excluded + dropped + output = 1,979,636\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dym4HXYfSJgN"
   },
   "source": [
    "### Annualized 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT9RxiKx3XnC"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1765204961529,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "cxEACmek3XnG",
    "outputId": "e2babd95-3279-4321-9e36-cd9fcf52f4b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_8  ->  ItemName: 'Inventories___Total'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 8  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX6tCx5t3XnI"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 942,
     "status": "ok",
     "timestamp": 1765204962478,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "horyPBIr3XnJ",
    "outputId": "23e32826-89cd-404e-85f7-b89bcf362598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Inventories___Total' ...\n",
      "Full annualized dataset loaded successfully: 3,452,774 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2101</td>\n",
       "      <td>237.4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2101</td>\n",
       "      <td>201.182443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2101</td>\n",
       "      <td>289.925405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2101</td>\n",
       "      <td>223.628192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2101</td>\n",
       "      <td>177.096926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     2101    237.4096  \n",
       "1     2101  201.182443  \n",
       "2     2101  289.925405  \n",
       "3     2101  223.628192  \n",
       "4     2101  177.096926  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DReyFe3d3XnK"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1765204962792,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "7_exn72V3XnL",
    "outputId": "34c14f68-e395-41ec-e93e-26bd465ed238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Inventories___Total' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4014211/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y00' 'Q4Y01' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2101</td>\n",
       "      <td>237.4096</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2101</td>\n",
       "      <td>201.182443</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2101</td>\n",
       "      <td>289.925405</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2101</td>\n",
       "      <td>223.628192</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2101</td>\n",
       "      <td>177.096926</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     2101    237.4096           Y92  \n",
       "1     2101  201.182443           Y93  \n",
       "2     2101  289.925405           Y94  \n",
       "3     2101  223.628192           Y95  \n",
       "4     2101  177.096926           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmKVoqSb3XnM"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25368,
     "status": "ok",
     "timestamp": 1765204988213,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "SwN3dczs3XnM",
    "outputId": "74e9171a-a8a2-4e31-8594-4c2c6dba8c97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,452,774 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 2901116\n",
      "                mean: 25288.08700486864\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 102.10634046382053\n",
      "                 p10: 86.05661339096373\n",
      "                 p20: 98.49794910040086\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 101.44238389602155\n",
      "                 p90: 115.48123728513532\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 124,089\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 2777027\n",
      "                mean: 101.16515665026624\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 101.06266115787489\n",
      "                 p10: 89.5432730765468\n",
      "                 p20: 99.72173525492961\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.63546661614026\n",
      "                 p90: 112.62611899992567\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Inventories___Total_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Inventories___Total_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,452,774\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 124,089\n",
      "Output rows (final):            3,328,685\n",
      "Check: excluded + dropped + output = 3,452,774\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7teK3AFSKh5"
   },
   "source": [
    "### Annualized 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htl5urSISKh6"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1765204988415,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "_5zsemDfSKh6",
    "outputId": "c7723b17-8b85-48ac-e7e7-586b59f0a62b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Annualized_9  ->  ItemName: 'Investments_in_Associated_Companies'\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SELECT A SINGLE ANNUALIZED_* ITEM AND PREPARE PATHS\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Selects which Annualized_* item (from annualized_vars) should be processed.\n",
    "#   2. Validates that annualized_vars and Temp_file_path_DP are available.\n",
    "#   3. Builds the input file path for the selected \"work_subset_<item>.txt\".\n",
    "#   4. Defines a base_output_filename used later when saving processed results.\n",
    "#   5. Ensures the data-preparation temp directory exists.\n",
    "#\n",
    "# Usage:\n",
    "#   - Change `annualized_index` to process a different Annualized_* dataset\n",
    "#     (e.g. 1, 2, 10, ...).\n",
    "#   - Assumes `annualized_vars` was created earlier (mapping \"Annualized_n\" to\n",
    "#     sanitized item names) and `Temp_file_path_DP` was set in your environment\n",
    "#     setup cell.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Choose which Annualized_* item to run\n",
    "annualized_index = 9  # change this to re-run a different dataset\n",
    "\n",
    "# annualized_vars should look like: {'Annualized_1': 'SomeItem', ...}\n",
    "assert 'annualized_vars' in globals(), \"annualized_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key and fetch the corresponding sanitized item name\n",
    "item_key = f\"Annualized_{annualized_index}\"\n",
    "target_item_name = annualized_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in annualized_vars.\"\n",
    "\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# 2) Construct file paths based on the selected item\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Input file produced by earlier steps (merging characteristics etc.)\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Base name for all output files created in the annualized pipeline\n",
    "base_output_filename = f\"Annualized_{target_item_name}_complete\"\n",
    "\n",
    "# 3) Ensure the output directory exists\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSEQKgsNSKh7"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1765204988541,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "bT056HIcSKh7",
    "outputId": "d1fc14ac-fed6-4931-a4f0-09c19f2a1d28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full annualized dataset for Item: 'Investments_in_Associated_Companies' ...\n",
      "Full annualized dataset loaded successfully: 2,761,273 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2256</td>\n",
       "      <td>52.290086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2256</td>\n",
       "      <td>12.353737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2256</td>\n",
       "      <td>15.324648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2256</td>\n",
       "      <td>14.287399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2256</td>\n",
       "      <td>74.116172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode      Value  \n",
       "0     2256  52.290086  \n",
       "1     2256  12.353737  \n",
       "2     2256  15.324648  \n",
       "3     2256  14.287399  \n",
       "4     2256  74.116172  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE FULL DATASET FOR THE SELECTED SPECIAL ITEM (ANNUALIZED VERSION)\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Uses `target_item_name` and `file_path` (defined in the previous cell)\n",
    "#      to load the corresponding work_subset file.\n",
    "#   2. Imports the file using `import_file_to_dataframe`.\n",
    "#   3. Performs safety checks for existence and emptiness.\n",
    "#   4. Shows a preview of the loaded dataset.\n",
    "#   5. Falls back to an empty DataFrame if loading fails.\n",
    "#   6. Runs garbage collection afterwards.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nImporting full annualized dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    annualized_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if annualized_raw is not None and not annualized_raw.empty:\n",
    "        print(f\"Full annualized dataset loaded successfully: {len(annualized_raw):,} rows total.\")\n",
    "        try:\n",
    "            display(annualized_raw.head())\n",
    "        except Exception:\n",
    "            print(annualized_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"Annualized dataset appears empty or could not be loaded.\")\n",
    "        annualized_raw = pd.DataFrame()\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    annualized_raw = pd.DataFrame()\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHqjDUatSKh8"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 110,
     "status": "ok",
     "timestamp": 1765204988669,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "-LXiCedGSKh8",
    "outputId": "f3d964b8-6caf-45ea-da2c-a4c50037e34e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fiscal period encoding to Annualized dataset for 'Investments_in_Associated_Companies' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4014211/200153074.py:92: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y07' 'Q2Y07' 'Q3Y07' ... 'Q3Y18' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies between original FiscalPeriod and calculated ImplFiscPer for annual (A, B) rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>2256</td>\n",
       "      <td>52.290086</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>2256</td>\n",
       "      <td>12.353737</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>2256</td>\n",
       "      <td>15.324648</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>2256</td>\n",
       "      <td>14.287399</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>2256</td>\n",
       "      <td>74.116172</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode      Value Str_FiscalPrd  \n",
       "0     2256  52.290086           Y92  \n",
       "1     2256  12.353737           Y93  \n",
       "2     2256  15.324648           Y94  \n",
       "3     2256  14.287399           Y95  \n",
       "4     2256  74.116172           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FISCAL PERIOD ENCODING FOR ANNUALIZED DATASET\n",
    "# =============================================================================\n",
    "# This cell:\n",
    "#   1. Defines helper functions:\n",
    "#        - last2: extract last two digits of a number as a zero-padded string.\n",
    "#        - add_str_fiscalprd: create Str_FiscalPrd from numeric FiscalPeriod\n",
    "#          and Frequency, derive an implied full-year FiscalPeriod, and check\n",
    "#          for inconsistencies on annual rows.\n",
    "#   2. Applies this encoding to `annualized_raw` (if available) and stores\n",
    "#      the result in `annualized_encoded`.\n",
    "#   3. Shows a preview of the encoded DataFrame.\n",
    "#\n",
    "# Assumptions:\n",
    "#   - `annualized_raw` has already been loaded in a previous cell.\n",
    "#   - `target_item_name` is defined and is just used for printing context.\n",
    "#   - DataFrame contains at least the columns: 'Frequency', 'FiscalPeriod'.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"\n",
    "    Return the last two digits of a number as a zero-padded string.\n",
    "\n",
    "    Examples:\n",
    "        n = 2023 -> \"23\"\n",
    "        n = 85   -> \"85\"\n",
    "        n = NaN  -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Format as 4-digit integer (e.g. 23 -> \"0023\") and take the last 2 characters.\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build 'Str_FiscalPrd' and overwrite 'FiscalPeriod' with an implied full year.\n",
    "\n",
    "    Logic:\n",
    "      1) Normalize Frequency to uppercase string.\n",
    "      2) For each row, interpret numeric FiscalPeriod depending on Frequency\n",
    "         and create a string fiscal-period label Str_FiscalPrd:\n",
    "           - C, Q, E, R: quarter-based  -> \"Q{1-4}Y{yy}\"\n",
    "           - A, B:       annual         -> \"Y{yy}\"\n",
    "           - F, S:       semiannual     -> \"S{1-2}Y{yy}\"\n",
    "           - K, T, L, U: trimester-like -> \"T{1-3}Y{yy}\"\n",
    "         The numbers (1..n) come from arithmetic on the numeric FiscalPeriod.\n",
    "      3) Extract the \"yy\" part from Str_FiscalPrd and map to a full year:\n",
    "           yy >= 80 -> 19yy (e.g. \"85\" -> 1985)\n",
    "           yy <  80 -> 20yy (e.g. \"23\" -> 2023)\n",
    "         This becomes ImplFiscPer_Calculated.\n",
    "      4) For rows with annual frequency (A,B), compare ImplFiscPer_Calculated\n",
    "         to the original FiscalPeriod and print a short discrepancy summary.\n",
    "      5) Overwrite 'FiscalPeriod' with ImplFiscPer_Calculated and drop the\n",
    "         helper columns used for the check.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame with:\n",
    "        - 'Str_FiscalPrd'\n",
    "        - updated 'FiscalPeriod' (full-year integer)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes for consistent logic\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod for validation later\n",
    "    df[\"Original_FiscalPeriod\"] = df[\"FiscalPeriod\"]\n",
    "\n",
    "    # Numeric version of FiscalPeriod for modular arithmetic\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Frequency masks\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])\n",
    "\n",
    "    # Initialize column for string fiscal period\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # --- Quarter-based (C, Q, E, R) ---\n",
    "    # Quarter number: 1..4\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year component (integer), then reduced to last 2 digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" + q_part.astype(\"Int64\").astype(str) + \"Y\" + q_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Annual (A, B) ---\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna(\"\")\n",
    "\n",
    "    # --- Semiannual (F, S) ---\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)     # semester index 1 or 2\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" + fs_sem.astype(\"Int64\").astype(str) + \"Y\" + fs_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Trimester-like (K, T, L, U) ---\n",
    "    t_term = ((fp % 3) + 1).where(m_KTLU)    # term index 1..3\n",
    "    t_year = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" + t_term.astype(\"Int64\").astype(str) + \"Y\" + t_year.fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Derive implied full-year FiscalPeriod from Str_FiscalPrd ---\n",
    "    # Extract the \"yy\" part following \"Y\" in labels like \"Q1Y23\", \"Y21\", etc.\n",
    "    year_part = df[\"Str_FiscalPrd\"].str.extract(r\"Y(\\d{2})\", expand=False)\n",
    "    year_numeric = pd.to_numeric(year_part, errors=\"coerce\")\n",
    "\n",
    "    # Map yy to either 19yy or 20yy, depending on cutoff at 80\n",
    "    df[\"ImplFiscPer_Calculated\"] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # --- Discrepancy check for annual rows (A,B only) ---\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Case 1: numeric equality\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"]\n",
    "            == pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "        )\n",
    "        |\n",
    "        # Case 2: both fields are NaN\n",
    "        (\n",
    "            annual_rows_for_check[\"ImplFiscPer_Calculated\"].isna()\n",
    "            & pd.to_numeric(\n",
    "                annual_rows_for_check[\"Original_FiscalPeriod\"],\n",
    "                errors=\"coerce\"\n",
    "            ).isna()\n",
    "        )\n",
    "    )\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\n",
    "            \"\\nDiscrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows:\"\n",
    "        )\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                [\"ID\", \"Frequency\", \"Original_FiscalPeriod\",\n",
    "                 \"Str_FiscalPrd\", \"ImplFiscPer_Calculated\"]\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies for annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\nNo discrepancies between original FiscalPeriod and calculated \"\n",
    "            \"ImplFiscPer for annual (A, B) rows.\"\n",
    "        )\n",
    "\n",
    "    # Overwrite FiscalPeriod with the implied year\n",
    "    df[\"FiscalPeriod\"] = df[\"ImplFiscPer_Calculated\"]\n",
    "\n",
    "    # Remove helper columns that are no longer needed\n",
    "    df.drop(columns=[\"Original_FiscalPeriod\", \"ImplFiscPer_Calculated\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Apply encoding to the Annualized dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "if \"annualized_raw\" in globals() and annualized_raw is not None and not annualized_raw.empty:\n",
    "    print(f\"Applying fiscal period encoding to Annualized dataset for '{target_item_name}' ...\")\n",
    "    annualized_encoded = add_str_fiscalprd(annualized_raw)\n",
    "    display(annualized_encoded.head())\n",
    "else:\n",
    "    print(\"annualized_raw not found or empty. Cannot perform encoding.\")\n",
    "    annualized_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNO2PV90SKh8"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20195,
     "status": "ok",
     "timestamp": 1765205008868,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "MzZjhrcRSKh8",
    "outputId": "4577edd2-d43a-4392-d70c-f816a3b780fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 2,761,273 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary — BEFORE quality drop ===\n",
      "         finite_rows: 1138479\n",
      "                mean: 154913.18387507054\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 97.34392593012758\n",
      "                 p10: 14.194587333883005\n",
      "                 p20: 95.38924156364851\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 104.11422880173936\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 158,833\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 979646\n",
      "                mean: 100.42279177292231\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.29719029879111\n",
      "                 p10: 95.51115296540593\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 102.60408703688363\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Investments_in_Associated_Companies_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Annualized_Investments_in_Associated_Companies_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     2,761,273\n",
      "Excluded by Frequency (E/L/R/U):0\n",
      "Dropped by quality (Pct rules): 158,833\n",
      "Output rows (final):            2,602,440\n",
      "Check: excluded + dropped + output = 2,761,273\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ANNUALIZED PIPELINE: BUILD AnnPITValue FROM A/Q/S/T, QC, AND SAVE OUTPUT\n",
    "# =============================================================================\n",
    "# High-level overview:\n",
    "#\n",
    "#   1. Helper utilities\n",
    "#      - _key, asof_numpy:\n",
    "#          * Implement a fast, vectorized “as-of” join:\n",
    "#              For each row in a left DataFrame, find the latest value in a\n",
    "#              right DataFrame with the same keys and PIT Date <= left PIT Date.\n",
    "#      - pctile, summarize_pct:\n",
    "#          * Compute robust summary statistics for quality control, including\n",
    "#            winsorized mean and decile percentiles.\n",
    "#      - pick_latest_nonzero_within_year:\n",
    "#          * For a given PIT Date and FiscalPeriod, evaluate all available\n",
    "#            period values (A, Q1..Q4, S1..S2, T1..T3) with known origin\n",
    "#            fiscal periods.\n",
    "#          * Choose a single value as AnnPITValue based on:\n",
    "#               - same-year vs prior-year vs other origin\n",
    "#               - period priority (A > Q4 > T3 > S2 > Q3 > ... > Q1)\n",
    "#               - the latest date within a one-year window before PIT.\n",
    "#          * NEW: returns both the value and the period label from which it\n",
    "#                 was chosen (AnnPITValue_Period).\n",
    "#\n",
    "#   2. Main pipeline for annualized_encoded:\n",
    "#      - Filter out unsupported frequencies (E/L/R/U).\n",
    "#      - Normalize types for PIT Date, FiscalPeriod, Value, and key columns.\n",
    "#      - Derive QNUM, SNUM, TNUM indices from Str_FiscalPrd.\n",
    "#      - Ensure all period- and date-columns (A/Q/S/T) exist.\n",
    "#      - Build TrueValue from annual rows (A,B) as the last observed annual\n",
    "#        value per (ID, FiscalPeriod, HistCurrency).\n",
    "#      - Use asof_numpy to populate:\n",
    "#           A, A_Date, A_OriginFP\n",
    "#           Q1..Q4, S1..S2, T1..T3 and their dates + OriginFP (from origin FP).\n",
    "#      - For each row, compute:\n",
    "#           * AnnPITValue        = chosen value\n",
    "#           * AnnPITValue_Period = 'A', 'Q4', 'S1', 'T3', etc.\n",
    "#      - Check for any period dates that are after PIT Date (future-date errors).\n",
    "#      - Compute AnnPITValue_Pct = AnnPITValue / TrueValue * 100 and drop rows\n",
    "#        outside [50, 200] or with infinite ratios.\n",
    "#      - Build a final, lean set of columns and save:\n",
    "#           * full file:   <base_output_filename>.txt\n",
    "#           * subset file: <base_output_filename>_subset.txt\n",
    "#        NEW: AnnPITValue_Period is placed directly before AnnPITValue and\n",
    "#             included in both full and subset outputs.\n",
    "#      - Print a row-accounting overview for reconciliation.\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: build a single key column from multiple columns\n",
    "# -----------------------------------------------------------------------------\n",
    "def _key(fr: pd.DataFrame, cols):\n",
    "    \"\"\"\n",
    "    Build a composite string key by concatenating several columns with '||'.\n",
    "\n",
    "    This is used to group records by (ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "    as a single vectorizable key for the as-of join.\n",
    "\n",
    "    Example:\n",
    "        _key(df, ['ID', 'HistCurrency']) -> \"123||USD\"\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helper: fast as-of join (right.PIT <= left.PIT)\n",
    "# -----------------------------------------------------------------------------\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df such that:\n",
    "\n",
    "        1) by_cols are equal on both sides (e.g. ID, HistCurrency, ItemCode, FiscalPeriod)\n",
    "        2) right_df['PIT Date'] <= left_df['PIT Date']\n",
    "\n",
    "    Implementation notes:\n",
    "      - Both left and right PIT Date columns are converted to datetime and floored to days.\n",
    "      - A composite key '__k' (string) is built from by_cols on both dataframes.\n",
    "      - The right-hand dataframe is sorted by key and PIT Date.\n",
    "      - For each distinct key, we keep a slice of PIT Date and Value arrays.\n",
    "      - Left-hand keys are sorted; for each group of identical keys we:\n",
    "          * binary-search in the right PIT Date array via np.searchsorted\n",
    "            to find the index of the last PIT Date <= each left PIT Date.\n",
    "          * fill out_vals and out_dates at the original left index positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vals : np.ndarray\n",
    "        Array of matched values (float64), default NaN where no match.\n",
    "    out_dates : np.ndarray\n",
    "        Array of matched dates (datetime64[ns]), default NaT where no match.\n",
    "    \"\"\"\n",
    "    # Initialize output arrays with NaNs/NaT\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns: keys plus PIT Date and Value on the right\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Drop rows with missing key or PIT Date on either side\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime, day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Composite keys for grouping\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right by key and PIT Date so we can slice by key and binary-search by date\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Convert right side to NumPy arrays\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and the start index of each key block in rk\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Pre-slice rdt, rval for each key to avoid repeated filtering\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Original indices of the filtered left rows\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left keys so that identical keys form contiguous blocks\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # Process each contiguous block of the same key\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]\n",
    "        j = i + 1\n",
    "        # identify the block [i, j) with the same key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # For each left date, search the insertion position in right dates\n",
    "            # side='right' gives index of first element > date, minus 1 =>\n",
    "            # index of the last element <= date.\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0\n",
    "            if np.any(valid):\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Small helpers for QC statistics\n",
    "# -----------------------------------------------------------------------------\n",
    "def pctile(s: pd.Series, q: float):\n",
    "    \"\"\"\n",
    "    Safe wrapper around Series.quantile that returns NaN if anything fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Compute summary statistics for a numeric series, ignoring inf and NaN.\n",
    "\n",
    "    Returned dictionary keys:\n",
    "      - finite_rows: number of finite (non-inf, non-NaN) observations\n",
    "      - mean, median\n",
    "      - winsorized_mean_1pct: mean after 1% winsorization on both tails\n",
    "      - p10, p20, ..., p90: decile quantiles from 10% to 90%\n",
    "    \"\"\"\n",
    "    # Replace infinities, drop missing values\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: s.to_numpy().copy() ensures we pass a writable array\n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean(),\n",
    "        \"p10\": pctile(s, 0.10),\n",
    "        \"p20\": pctile(s, 0.20),\n",
    "        \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40),\n",
    "        \"p50\": pctile(s, 0.50),\n",
    "        \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70),\n",
    "        \"p80\": pctile(s, 0.80),\n",
    "        \"p90\": pctile(s, 0.90),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Period prioritization and label helper\n",
    "# -----------------------------------------------------------------------------\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,   # annual\n",
    "    'Q4': 90,\n",
    "    'T3': 80,\n",
    "    'S2': 70,\n",
    "    'Q3': 60,\n",
    "    'T2': 50,\n",
    "    'S1': 40,\n",
    "    'Q2': 30,\n",
    "    'T1': 20,\n",
    "    'Q1': 10,\n",
    "}\n",
    "\n",
    "\n",
    "def _label_from_colname(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Map a value column name to a period label.\n",
    "\n",
    "    Currently this is a thin wrapper:\n",
    "      - 'A' stays 'A'\n",
    "      - 'Q1'..'Q4', 'S1'.., 'T1'.. remain unchanged.\n",
    "    \"\"\"\n",
    "    return 'A' if colname == 'A' else colname\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# AnnPITValue selection using OriginFP and priority rules\n",
    "# -----------------------------------------------------------------------------\n",
    "def pick_latest_nonzero_within_year(\n",
    "    row,\n",
    "    value_cols,\n",
    "    date_cols,\n",
    "    pit_col='PIT Date',\n",
    "    fp_col='FiscalPeriod'\n",
    "):\n",
    "    \"\"\"\n",
    "    Select a single annualized value (AnnPITValue) for a row, using information\n",
    "    about:\n",
    "      - available period values (A, Q1..Q4, S1..S2, T1..T3),\n",
    "      - their dates,\n",
    "      - their origin fiscal periods (*_OriginFP),\n",
    "      - the current PIT Date and FiscalPeriod of the row.\n",
    "\n",
    "    The logic:\n",
    "      1) Only consider candidates where:\n",
    "           - date is not missing,\n",
    "           - date <= PIT Date,\n",
    "           - date >= PIT Date - 365 days.\n",
    "      2) Determine year relation between each candidate's origin fiscal period\n",
    "         and the current row's FiscalPeriod (FP):\n",
    "           - same  : OriginFP == FP\n",
    "           - prior : OriginFP == FP - 1\n",
    "           - other : everything else\n",
    "           - unknown: if either FP or OriginFP is missing\n",
    "      3) For all candidates, ignore NaN and 0.0 values when selecting.\n",
    "         If we only find zeros and no positive/negative values, we return 0.0\n",
    "         and still record the period label.\n",
    "         If there are no candidates at all, return NaN.\n",
    "      4) Selection priority:\n",
    "           - same-year Annual ('A', year_rel == 'same'):\n",
    "               pick the latest by date.\n",
    "           - else, same-year partials (Q/S/T, year_rel == 'same'):\n",
    "               pick the highest priority (e.g. Q4 > Q3 > ...), then latest date.\n",
    "           - else, prior-year Annual ('A', year_rel == 'prior'):\n",
    "               pick the latest by date (push-forward).\n",
    "           - else, fallback:\n",
    "               pick candidate with highest priority, then latest date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (value, label)\n",
    "      value : float or NaN\n",
    "      label : str or NaN (e.g. 'A', 'Q4', 'S1', 'T3')\n",
    "    \"\"\"\n",
    "    pit = row[pit_col]\n",
    "    if pd.isna(pit):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    cutoff = pit - timedelta(days=365)\n",
    "\n",
    "    # Current row's fiscal period, used to interpret origin fiscal periods\n",
    "    fp = row.get(fp_col, np.nan)\n",
    "    try:\n",
    "        fp_int = int(fp) if not pd.isna(fp) else None\n",
    "    except Exception:\n",
    "        fp_int = None\n",
    "\n",
    "    candidates = []\n",
    "    for vcol, dcol in zip(value_cols, date_cols):\n",
    "        if vcol not in row or dcol not in row:\n",
    "            continue\n",
    "\n",
    "        val = row[vcol]\n",
    "        dt  = row[dcol]\n",
    "\n",
    "        # Ignore missing or future dates\n",
    "        if pd.isna(dt) or dt > pit:\n",
    "            continue\n",
    "\n",
    "        dt = pd.to_datetime(dt, errors='coerce')\n",
    "        if pd.isna(dt):\n",
    "            continue\n",
    "\n",
    "        dt = dt.floor('D')\n",
    "        if dt < cutoff:\n",
    "            # older than 1 year before PIT\n",
    "            continue\n",
    "\n",
    "        # Map column name to period label (A, Q1..Q4, etc.) and priority\n",
    "        label = _label_from_colname(vcol)\n",
    "        prio  = _PERIOD_PRIORITY.get(label, -1)\n",
    "\n",
    "        # Convert value to float for numeric comparisons\n",
    "        vnum  = float(val) if pd.notna(val) else np.nan\n",
    "\n",
    "        # Determine origin fiscal period\n",
    "        origin_col = f'{label}_OriginFP'\n",
    "        origin_fp = row.get(origin_col, np.nan)\n",
    "        if pd.isna(origin_fp):\n",
    "            # fallback to current FP if origin not explicitly stored\n",
    "            origin_fp = fp_int\n",
    "        try:\n",
    "            if origin_fp is not None and not pd.isna(origin_fp):\n",
    "                origin_fp = int(origin_fp)\n",
    "            else:\n",
    "                origin_fp = None\n",
    "        except Exception:\n",
    "            origin_fp = fp_int\n",
    "\n",
    "        # Compute relationship between origin fiscal period and current FP\n",
    "        if fp_int is not None and origin_fp is not None:\n",
    "            if origin_fp == fp_int:\n",
    "                year_rel = 'same'\n",
    "            elif origin_fp == fp_int - 1:\n",
    "                year_rel = 'prior'\n",
    "            else:\n",
    "                year_rel = 'other'\n",
    "        else:\n",
    "            year_rel = 'unknown'\n",
    "\n",
    "        candidates.append((label, prio, dt, vnum, year_rel))\n",
    "\n",
    "    if not candidates:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    # Only non-NaN, non-zero values are considered as strong candidates\n",
    "    def valid(seq):\n",
    "        return [c for c in seq if not np.isnan(c[3]) and c[3] != 0.0]\n",
    "\n",
    "    # 1) Same-year Annual A: prefer the latest annual that matches the row's FP\n",
    "    same_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'same')\n",
    "    if same_year_annuals:\n",
    "        best = max(same_year_annuals, key=lambda x: x[2])  # latest date\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 2) Same-year partial periods (Q, S, T) if no same-year A is available\n",
    "    same_year_partials = valid(c for c in candidates if c[0] != 'A' and c[4] == 'same')\n",
    "    if same_year_partials:\n",
    "        # choose best by (priority, date)\n",
    "        best = max(same_year_partials, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 3) Prior-year annual push-forward: last annual from previous FP\n",
    "    prior_year_annuals = valid(c for c in candidates if c[0] == 'A' and c[4] == 'prior')\n",
    "    if prior_year_annuals:\n",
    "        best = max(prior_year_annuals, key=lambda x: x[2])\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # 4) Fallback: any candidate by (priority, date)\n",
    "    others = valid(candidates)\n",
    "    if others:\n",
    "        best = max(others, key=lambda x: (x[1], x[2]))\n",
    "        return (best[3], best[0])\n",
    "\n",
    "    # If we get here, only zeros are present. Return 0.0 explicitly and keep label.\n",
    "    zeros = [c for c in candidates if not np.isnan(c[3]) and c[3] == 0.0]\n",
    "    if zeros:\n",
    "        best_zero = max(zeros, key=lambda x: (x[1], x[2]))\n",
    "        return (best_zero[3], best_zero[0])\n",
    "\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: annualized_encoded -> annualized_processed\n",
    "# =============================================================================\n",
    "if 'annualized_encoded' in globals() and annualized_encoded is not None:\n",
    "    input_rows = len(annualized_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original DataFrame\n",
    "    working = annualized_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Exclude frequencies that are not supported by this pipeline (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) Basic type normalization\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PIT Date as datetime (day precision)\n",
    "    working['PIT Date'] = pd.to_datetime(\n",
    "        working['PIT Date'], format='%Y-%m-%d', errors='coerce'\n",
    "    ).dt.floor('D')\n",
    "\n",
    "    # FiscalPeriod and Value as numeric\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Key-like columns as string (consistent joins and as-of keys)\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Parse Q/S/T sequence numbers from Str_FiscalPrd\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter index 1..4 from strings like \"Q1Y23\"\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual index 1..2 from \"S1Y23\"\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester index 1..3 from \"T1Y23\"\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Ensure A/Q/S/T value and date columns exist\n",
    "    # -------------------------------------------------------------------------\n",
    "    period_vals = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                  [f'S{i}' for i in range(1, 3)] + \\\n",
    "                  [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "\n",
    "    period_dates = [f'{p}_Date' for p in [f'Q{i}' for i in range(1, 5)] +\n",
    "                                       [f'S{i}' for i in range(1, 3)] +\n",
    "                                       [f'T{i}' for i in range(1, 4)]] + ['A_Date']\n",
    "\n",
    "    # Create missing value/date columns initialized to NaN / NaT\n",
    "    for c in period_vals:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "    for c in period_dates:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) Build TrueValue from annual (A,B) rows\n",
    "    # -------------------------------------------------------------------------\n",
    "    # TrueValue is the last known annual value per (ID, FiscalPeriod, HistCurrency)\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) As-of mapping for each frequency (same FiscalPeriod only)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6.1 Annual as-of (A, B)\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # 6.2 Quarterly as-of (Q, C)\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.3 Semiannual as-of (S, F)\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # 6.4 Trimester as-of (T, K)\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        mask = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[mask, ocol] = working.loc[mask, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Normalize date columns (no forward-fill; only directly mapped values)\n",
    "    # -------------------------------------------------------------------------\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    value_labels  = period_vals\n",
    "    date_labels   = period_dates\n",
    "    origin_labels = [f'{lbl}_OriginFP' for lbl in value_labels]\n",
    "\n",
    "    for c in date_labels:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Compute AnnPITValue and AnnPITValue_Period\n",
    "    # -------------------------------------------------------------------------\n",
    "    # NEW: we compute both the chosen annualized value and the period label\n",
    "    # it came from (e.g., 'A', 'Q4', 'S1', 'T3') and store the label in\n",
    "    # AnnPITValue_Period.\n",
    "    ann_res = working.apply(\n",
    "        lambda r: pd.Series(\n",
    "            pick_latest_nonzero_within_year(\n",
    "                r,\n",
    "                value_cols=value_labels,\n",
    "                date_cols=date_labels,\n",
    "                pit_col='PIT Date',\n",
    "                fp_col='FiscalPeriod'\n",
    "            ),\n",
    "            index=['AnnPITValue', 'AnnPITValue_Period']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    working = pd.concat([working, ann_res], axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 9) Future-date QC check (period dates > PIT Date)\n",
    "    # -------------------------------------------------------------------------\n",
    "    date_cols_all = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    present = [c for c in date_cols_all if c in working.columns]\n",
    "    viol_counts = {}\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 10) Compute AnnPITValue_Pct vs TrueValue and apply quality filter\n",
    "    # -------------------------------------------------------------------------\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)\n",
    "    is_finite = np.isfinite(pct)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11) Final columns and export\n",
    "    # -------------------------------------------------------------------------\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included and explicitly placed\n",
    "    # directly before AnnPITValue in the final column order.\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue_Period', 'AnnPITValue',\n",
    "         'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Drop helper columns that are only needed for intermediate computations\n",
    "    drop_cols = [\n",
    "        c for c in working.columns\n",
    "        if c.endswith('_OriginFP') or c in ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    ]\n",
    "    working.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    annualized_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 12) Save full and subset outputs\n",
    "    # -------------------------------------------------------------------------\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set earlier).\"\n",
    "\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    annualized_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # NEW: AnnPITValue_Period is included in the subset and appears before\n",
    "    # AnnPITValue.\n",
    "    subset_cols = [\n",
    "        \"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "        \"FiscalPeriod\", \"AnnPITValue_Period\", \"AnnPITValue\"\n",
    "    ]\n",
    "    subset_cols_existing = [col for col in subset_cols if col in annualized_processed.columns]\n",
    "    subset_df = annualized_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP, f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(annualized_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excluded by Frequency (E/L/R/U):{excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. Please investigate.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"annualized_encoded not found or None; skipping.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPlqcbEW/d4SNINsmR/sw8v",
   "collapsed_sections": [
    "ctXZHVs-zurM",
    "uQMCPR6XzurR",
    "FmcsWZB0oDMl",
    "fcll5N-9z4yZ",
    "d59odhYbz4yc",
    "nXSsUb80SBPa",
    "mVaGRdVY3Y39",
    "1sgkpW_73Y3_",
    "o7qE3h2C3Y4A",
    "WBoK14aS3Y4B",
    "7nB2IiDmSCJS",
    "USi9UxAA3fSB",
    "O9OQS6dk3fSE",
    "AjWQogb63fSF",
    "FUO9SEcr3fSG",
    "A1JakGszSDEm",
    "x0j7HjRx3gHC",
    "0jHCUIwZ3gHE",
    "td7OkRT53gHF",
    "YjhFGPz53gHG",
    "z9vM4WYsSEAP",
    "KJ5SSTUy3g39",
    "IP9tL-A33g3_",
    "rcXDq9mO3g4B",
    "NfI_YRR43g4C",
    "Hu8pT-JASFPb",
    "dyEGQfNY3hiZ",
    "ztoWmv3b3hia",
    "eJBlaooh3hib",
    "oa2mMajE3hic",
    "e5DWUCxkSHOO",
    "f-2vHQEo3ihA",
    "YegAzUaN3ihB",
    "3St4ALy33ihC",
    "Ybc8ATmN3ihD",
    "NlJBz31ZSIVo",
    "IX36VG3b3jUU",
    "5cNFK-8Q3jUV",
    "nE4bBHNL3jUW",
    "UC-YbdQq3jUX",
    "dym4HXYfSJgN",
    "CT9RxiKx3XnC",
    "cX6tCx5t3XnI",
    "DReyFe3d3XnK",
    "hmKVoqSb3XnM",
    "Y7teK3AFSKh5",
    "eSEQKgsNSKh7",
    "KHqjDUatSKh8",
    "gNO2PV90SKh8"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
