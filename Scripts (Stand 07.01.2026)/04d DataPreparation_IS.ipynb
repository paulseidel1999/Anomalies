{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b06f6df4"
   },
   "source": [
    "### Mount Google Drive, Import Libraries and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22901,
     "status": "ok",
     "timestamp": 1765651100639,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "DVbbtM4ztajO",
    "outputId": "e3f32f91-ef09-4453-c94a-cb9b6bb8b804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB: False\n",
      "BASE_PATH: /home/jovyan/work/hpool1/pseidel/test\n",
      "Importing numpy ...\n",
      "numpy OK\n",
      "Importing scipy ...\n",
      "scipy OK\n",
      "Importing pandas ...\n",
      "pandas OK\n",
      "Importing linearmodels ...\n",
      "linearmodels OK\n",
      "Importing xlsxwriter ...\n",
      "xlsxwriter OK\n",
      "Paths configured. Temp outputs -> /home/jovyan/work/hpool1/pseidel/test/Temp/TempGeneralOverview\n",
      "Example input path -> /home/jovyan/work/hpool1/pseidel/test/Input/WSFV_f_20250131.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT SETUP + PATH CONFIGURATION (SERVER / COLAB COMPATIBLE)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "from datetime import timedelta\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) HARD SAFETY: cap native thread usage (prevents pthread_create EAGAIN)\n",
    "#    MUST be set before importing numpy / scipy / pandas\n",
    "# -----------------------------------------------------------------------------\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"BLIS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Detect environment\n",
    "# -----------------------------------------------------------------------------\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) (Colab only) Mount Google Drive\n",
    "# -----------------------------------------------------------------------------\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "else:\n",
    "    # Server base path (your target)\n",
    "    BASE_PATH = \"/home/jovyan/work/hpool1/pseidel/test\"\n",
    "\n",
    "print(\"IN_COLAB:\", IN_COLAB)\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Sanity checks: path exists + write permission\n",
    "# -----------------------------------------------------------------------------\n",
    "BASE = Path(BASE_PATH)\n",
    "if not BASE.exists():\n",
    "    raise FileNotFoundError(f\"BASE_PATH does not exist: {BASE}\")\n",
    "\n",
    "# quick write test (fails fast if you don't have permissions)\n",
    "test_file = BASE / \".write_test_tmp\"\n",
    "try:\n",
    "    test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "    test_file.unlink()\n",
    "except Exception as e:\n",
    "    raise PermissionError(f\"No write permission in {BASE}. Error: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Environment check: ensure required packages import cleanly\n",
    "# -----------------------------------------------------------------------------\n",
    "required_packages = [\"numpy\", \"scipy\", \"pandas\", \"linearmodels\", \"xlsxwriter\"]\n",
    "\n",
    "for pkg in required_packages:\n",
    "    print(f\"Importing {pkg} ...\")\n",
    "    importlib.import_module(pkg)\n",
    "    print(f\"{pkg} OK\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Base paths and input/output locations\n",
    "# -----------------------------------------------------------------------------\n",
    "Input_file_path   = str(BASE / \"Input\")\n",
    "Temp_file_path    = str(BASE / \"Temp\")\n",
    "Output_file_path  = str(BASE / \"Output\")\n",
    "\n",
    "Fundamentals_file_path = f\"{Input_file_path}/WSFV_f_20250131.txt\"\n",
    "Current_file_path      = f\"{Input_file_path}/WSCurrent_f_20250131.txt\"\n",
    "Calendar_file_path     = f\"{Input_file_path}/WSCalendarPrd_f_20250131.txt\"\n",
    "Meta_file_path         = f\"{Input_file_path}/WSMetaData_f_20250131.txt\"\n",
    "Excel_file_path        = f\"{Input_file_path}/WS PIT Table Definitions V5 with start dates.xls\"\n",
    "\n",
    "MarketValues_file_path          = f\"{Input_file_path}/Daily MV USD\"\n",
    "MarketValues_file_path_LC       = f\"{Input_file_path}/Daily MV LC\"\n",
    "DailyTotalReturns_file_path     = f\"{Input_file_path}/Daily Returns USD\"\n",
    "DailyIndexReturns_file_path     = f\"{Input_file_path}/Daily Index Returns USD\"\n",
    "Constituents_file_path          = f\"{Input_file_path}/Constituents.01.csv\"\n",
    "UniversalMatching_file_path     = f\"{Input_file_path}/Universal Matching File\"\n",
    "\n",
    "Temp_file_path_GO  = f\"{Temp_file_path}/TempGeneralOverview\"\n",
    "Temp_file_path_EoC = f\"{Temp_file_path}/TempExtractionofCharacteristics\"\n",
    "Temp_file_path_DP  = f\"{Temp_file_path}/TempDataPreparation\"\n",
    "Temp_file_path_A   = f\"{Temp_file_path}/TempAnomalies\"\n",
    "Temp_file_path_R   = f\"{Temp_file_path}/TempRegressionModel\"\n",
    "\n",
    "Relevant_items_path   = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_A = f\"{Input_file_path}/RelevantItems.txt\"\n",
    "Relevant_items_path_B = f\"{Input_file_path}/RelevantItemsB.txt\"\n",
    "Relevant_items_path_C = f\"{Input_file_path}/RelevantItemsC.txt\"\n",
    "Relevant_items_path_D = f\"{Input_file_path}/RelevantItemsD.txt\"\n",
    "\n",
    "Subset_file_path = f\"{Temp_file_path_GO}/Subsets\"\n",
    "Fundamentals_clean_file_path = f\"{Temp_file_path_GO}/Fundamentals_clean.txt\"\n",
    "Current_clean_file_path      = f\"{Temp_file_path_GO}/Current_clean.txt\"\n",
    "Calendar_clean_file_path     = f\"{Temp_file_path_GO}/Input/Calendar_clean.txt\"\n",
    "Meta_clean_file_path         = f\"{Temp_file_path_GO}/Input/Meta_clean.txt\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Ensure required directories exist\n",
    "# -----------------------------------------------------------------------------\n",
    "Path(Output_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_GO).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_EoC).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_A).mkdir(parents=True, exist_ok=True)\n",
    "Path(Temp_file_path_R).mkdir(parents=True, exist_ok=True)\n",
    "Path(Subset_file_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(Path(Calendar_clean_file_path).parent).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Streaming / deduplication settings\n",
    "# -----------------------------------------------------------------------------\n",
    "CHUNK_SIZE = 2_000_000\n",
    "DATE_COL = \"PIT Date\"\n",
    "DEDUP_KEYS = [\"ID\", \"ItemCode\", DATE_COL]\n",
    "\n",
    "print(\"Paths configured. Temp outputs ->\", Temp_file_path_GO)\n",
    "print(\"Example input path ->\", Fundamentals_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQMCPR6XzurR"
   },
   "source": [
    "### Import Data Files to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8027,
     "status": "ok",
     "timestamp": 1765651108681,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "zW-FBOO8zurR",
    "outputId": "19f4146a-9bc2-4a62-cd7a-3dd62660e83a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imported RelevantItems.txt as DataFrame 'RelevantItems'\n",
      "Preview of 'RelevantItems':\n",
      "  ItemCode\n",
      "0    01001\n",
      "1    01051\n",
      "2    01075\n",
      "3    01101\n",
      "4    01151 \n",
      "\n",
      "\n",
      "Imported CountryCodes.txt as DataFrame 'CountryCodes'\n",
      "Preview of 'CountryCodes':\n",
      "  NatCo ImplCountry\n",
      "0   012     Algeria\n",
      "1   440   Lithuania\n",
      "2   025   Argentina\n",
      "3   442  Luxembourg\n",
      "4   036   Australia \n",
      "\n",
      "\n",
      "Imported ADR_clean.txt as DataFrame 'ADR_clean'\n",
      "Preview of 'ADR_clean':\n",
      "          ID ADRIndicator\n",
      "0  C036F63D0            N\n",
      "1  C056879S0            X\n",
      "2  C2461T100            N\n",
      "3  C2504O500            N\n",
      "4  C250C9180            N \n",
      "\n",
      "\n",
      "Imported CompanyName_clean.txt as DataFrame 'CompanyName_clean'\n",
      "Preview of 'CompanyName_clean':\n",
      "          ID                               CompanyName\n",
      "0  C00948205             AGRIFORCE GROWING SYSTEMS LTD\n",
      "1  C02500770            PEUGEOT CITROEN ARGENTINA S.A.\n",
      "2  C02520200  ACINDAR INDUSTRIA ARGENTINA DE ACEROS SA\n",
      "3  C02520220                       ALPARGATAS S.A.I.C.\n",
      "4  C02520230               ALUAR ALUMINIO ARGENTINO SA \n",
      "\n",
      "\n",
      "Imported CurrencyCodes_clean.txt as DataFrame 'CurrencyCodes_clean'\n",
      "Preview of 'CurrencyCodes_clean':\n",
      "          ID CurrencyCode1 SwitchDate1 CurrencyCode2 SwitchDate2  \\\n",
      "0  C00948205           Usd  2021-07-09           NaN         NaN   \n",
      "1  C02500770           Ars  1995-12-29           NaN         NaN   \n",
      "2  C0250077A           Ars  1999-10-01           NaN         NaN   \n",
      "3  C0250077B           Ars  1999-10-01           NaN         NaN   \n",
      "4  C0250077C           Ars  1999-10-01           NaN         NaN   \n",
      "\n",
      "  CurrencyCode3 SwitchDate3 CurrentCurrency  \n",
      "0           NaN         NaN             Usd  \n",
      "1           NaN         NaN             Ars  \n",
      "2           NaN         NaN             Ars  \n",
      "3           NaN         NaN             Ars  \n",
      "4           NaN         NaN             Ars   \n",
      "\n",
      "\n",
      "Imported FYE_clean.txt as DataFrame 'FYE_clean'\n",
      "Preview of 'FYE_clean':\n",
      "          ID    FY FYE Month\n",
      "0  C00948205  2018  December\n",
      "1  C00948205  2019  December\n",
      "2  C00948205  2020     March\n",
      "3  C00948205  2021  December\n",
      "4  C00948205  2022  December \n",
      "\n",
      "\n",
      "Imported ID_clean.txt as DataFrame 'ID_clean'\n",
      "Preview of 'ID_clean':\n",
      "          ID\n",
      "0  C02500770\n",
      "1  C02520200\n",
      "2  C02520220\n",
      "3  C02520230\n",
      "4  C02520240 \n",
      "\n",
      "\n",
      "Imported UpdateCodes_clean.txt as DataFrame 'UpdateCodes_clean'\n",
      "Preview of 'UpdateCodes_clean':\n",
      "          ID    PIT Date Frequency FiscalPeriod UpdateCode\n",
      "0  C02500770  1995-12-29         A         1985          3\n",
      "1  C02500770  1995-12-29         A         1986          3\n",
      "2  C02500770  1995-12-29         A         1987          3\n",
      "3  C02500770  1995-12-29         A         1988          3\n",
      "4  C02500770  1995-12-29         A         1989          3 \n",
      "\n",
      "\n",
      "Imported ValueCoding.txt as DataFrame 'ValueCoding'\n",
      "Preview of 'ValueCoding':\n",
      "  ItemCode                           ItemName  Source\n",
      "0    05006               Market Price Current  Market\n",
      "1    05007      Market Price YTD High Current  Market\n",
      "2    05008       Market Price YTD Low Current  Market\n",
      "3    05009              Date of Current Price  Market\n",
      "4    05091  Market Price 52 Week High Current  Market \n",
      "\n",
      "\n",
      "Identifying subset files to process...\n",
      "  Found subset_01001.txt\n",
      "  Found subset_01051.txt\n",
      "  Found subset_01075.txt\n",
      "  Found subset_01101.txt\n",
      "  Found subset_01151.txt\n",
      "  Found subset_01250.txt\n",
      "  Found subset_01451.txt\n",
      "  Found subset_01551.txt\n",
      "  Found subset_01706.txt\n",
      "  Found subset_02001.txt\n",
      "  Found subset_02051.txt\n",
      "  Found subset_02101.txt\n",
      "  Found subset_02149.txt\n",
      "  Found subset_02201.txt\n",
      "  Found subset_02250.txt\n",
      "  Found subset_02256.txt\n",
      "  Found subset_02257.txt\n",
      "  Found subset_02258.txt\n",
      "  Found subset_02263.txt\n",
      "  Found subset_02501.txt\n",
      "  Found subset_02652.txt\n",
      "  Found subset_02999.txt\n",
      "  Found subset_03040.txt\n",
      "  Found subset_03051.txt\n",
      "  Found subset_03063.txt\n",
      "  Found subset_03066.txt\n",
      "  Found subset_03101.txt\n",
      "  Found subset_03251.txt\n",
      "  Found subset_03263.txt\n",
      "  Found subset_03273.txt\n",
      "  Found subset_03351.txt\n",
      "  Found subset_03426.txt\n",
      "  Found subset_03451.txt\n",
      "  Found subset_03501.txt\n",
      "  Found subset_04251.txt\n",
      "  Found subset_04401.txt\n",
      "  Found subset_04551.txt\n",
      "  Found subset_04601.txt\n",
      "  Found subset_04701.txt\n",
      "  Found subset_04751.txt\n",
      "  Found subset_04860.txt\n",
      "  Found subset_04870.txt\n",
      "  Found subset_04890.txt\n",
      "  Found subset_05202.txt\n",
      "  Found subset_04201.txt\n",
      "  Found subset_04225.txt\n",
      "  Found subset_04831.txt\n",
      "  Found subset_04351.txt\n",
      "  Found subset_05508.txt\n",
      "\n",
      "Identified 49 subset files for processing.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell:\n",
    "#\n",
    "#   1. Defines a helper function `import_file_to_dataframe` that reads a pipe-delimited\n",
    "#      text file into a pandas DataFrame (all columns as string; returns None on error).\n",
    "#   2. Imports a list of \"input\" files from Input_file_path into DataFrames\n",
    "#      (RelevantItems, CountryCodes, ...), storing them in globals() by filename.\n",
    "#   3. Imports a list of \"temp\" files from Temp_file_path_EoC into DataFrames\n",
    "#      (ADR_clean, CompanyName_clean, CurrencyCodes_clean, FYE_clean, ID_clean,\n",
    "#       UpdateCodes_clean, ValueCoding), also stored in globals().\n",
    "#   4. Identifies which subset_*.txt files exist in Subset_file_path based on the IDs\n",
    "#      listed in RelevantItems.txt, and records their names (without .txt) in\n",
    "#      `successful_subset_names`.\n",
    "#\n",
    "# No actual subset data is loaded here; that is deferred to later steps to keep\n",
    "# memory usage under control.\n",
    "\n",
    "\n",
    "# Function to import a file and return a pandas DataFrame\n",
    "def import_file_to_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Import a pipe-separated text file as a pandas DataFrame.\n",
    "\n",
    "    - Uses sep='|' to read pipe-delimited files.\n",
    "    - Reads all columns as strings (dtype=str), which helps preserve things like\n",
    "      leading zeros in codes (e.g., NatCo, ItemCode).\n",
    "    - Returns None on failure and prints a brief error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='|', dtype=str)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error importing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Import files from Input directory\n",
    "# -------------------------------------------------------------------------\n",
    "input_files_to_import = [\"RelevantItems.txt\", \"CountryCodes.txt\"]\n",
    "\n",
    "for file_name in input_files_to_import:\n",
    "    file_path = os.path.join(Input_file_path, file_name)\n",
    "    var_name = file_name.replace(\".txt\", \"\")  # e.g. \"RelevantItems\"\n",
    "    globals()[var_name] = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if globals()[var_name] is not None:\n",
    "        print(f\"\\nImported {file_name} as DataFrame '{var_name}'\")\n",
    "        print(f\"Preview of '{var_name}':\")\n",
    "        print(globals()[var_name].head(), \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Import files from Temp directory (end-of-cleaning stage)\n",
    "# -------------------------------------------------------------------------\n",
    "temp_files_to_import = [\n",
    "    \"ADR_clean.txt\",\n",
    "    \"CompanyName_clean.txt\",\n",
    "    \"CurrencyCodes_clean.txt\",\n",
    "    \"FYE_clean.txt\",\n",
    "    \"ID_clean.txt\",\n",
    "    \"UpdateCodes_clean.txt\",\n",
    "    \"ValueCoding.txt\"\n",
    "]\n",
    "\n",
    "for file_name in temp_files_to_import:\n",
    "    file_path = os.path.join(Temp_file_path_EoC, file_name)\n",
    "    var_name = file_name.replace(\".txt\", \"\")  # e.g. \"ADR_clean\"\n",
    "    globals()[var_name] = import_file_to_dataframe(file_path)\n",
    "\n",
    "    if globals()[var_name] is not None:\n",
    "        print(f\"\\nImported {file_name} as DataFrame '{var_name}'\")\n",
    "        print(f\"Preview of '{var_name}':\")\n",
    "        print(globals()[var_name].head(), \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Identify subset files that exist for the relevant items\n",
    "# -------------------------------------------------------------------------\n",
    "successful_subset_names = []\n",
    "\n",
    "if 'RelevantItems' in globals() and RelevantItems is not None:\n",
    "    # Assume first column of RelevantItems holds the item IDs used in subset filenames\n",
    "    relevant_ids = RelevantItems.iloc[:, 0].astype(str).tolist()\n",
    "\n",
    "    print(\"\\nIdentifying subset files to process...\")\n",
    "    for item_id in relevant_ids:\n",
    "        file_name = f\"subset_{item_id}.txt\"\n",
    "        file_path = os.path.join(Subset_file_path, file_name)\n",
    "\n",
    "        # Check the existence of each candidate subset file\n",
    "        if os.path.exists(file_path):\n",
    "            successful_subset_names.append(f\"subset_{item_id}\")\n",
    "            print(f\"  Found {file_name}\")\n",
    "        else:\n",
    "            print(f\"  File not found: {file_name}. Skipping.\")\n",
    "\n",
    "    print(f\"\\nIdentified {len(successful_subset_names)} subset files for processing.\")\n",
    "else:\n",
    "    print(\"RelevantItems DataFrame not found or is empty. Cannot identify subset files.\")\n",
    "\n",
    "# Note: actual loading and processing of subset files happens later, in\n",
    "# batch-based steps, to manage memory usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmcsWZB0oDMl"
   },
   "source": [
    "# 4.0. Extracting the most recent, annualized values per PIT Date (incl. Plausibility checks for the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WkpqfMTNN5n"
   },
   "source": [
    "## 4.1. Split according to source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1765651108867,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "moZxdHacNpxi",
    "outputId": "00580225-0902-457a-dd89-2c18e17bd8a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ValueCoding DataFrame...\n",
      "\n",
      "Processed ValueCoding DataFrame (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>ItemName</th>\n",
       "      <th>Source</th>\n",
       "      <th>ItemName_Sanitized</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05006</td>\n",
       "      <td>Market Price Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05007</td>\n",
       "      <td>Market Price YTD High Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_YTD_High_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05008</td>\n",
       "      <td>Market Price YTD Low Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_YTD_Low_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05009</td>\n",
       "      <td>Date of Current Price</td>\n",
       "      <td>Market</td>\n",
       "      <td>Date_of_Current_Price</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05091</td>\n",
       "      <td>Market Price 52 Week High Current</td>\n",
       "      <td>Market</td>\n",
       "      <td>Market_Price_52_Week_High_Current</td>\n",
       "      <td>Mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ItemCode                           ItemName  Source  \\\n",
       "0    05006               Market Price Current  Market   \n",
       "1    05007      Market Price YTD High Current  Market   \n",
       "2    05008       Market Price YTD Low Current  Market   \n",
       "3    05009              Date of Current Price  Market   \n",
       "4    05091  Market Price 52 Week High Current  Market   \n",
       "\n",
       "                  ItemName_Sanitized Category  \n",
       "0               Market_Price_Current    Mixed  \n",
       "1      Market_Price_YTD_High_Current    Mixed  \n",
       "2       Market_Price_YTD_Low_Current    Mixed  \n",
       "3              Date_of_Current_Price    Mixed  \n",
       "4  Market_Price_52_Week_High_Current    Mixed  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Annualized items: 148\n",
      "Number of Mixed items: 141\n",
      "Number of Special items: 62\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This cell processes a ValueCoding DataFrame and assigns a Category to each item\n",
    "# (per sanitized item name), based on its data sources:\n",
    "#\n",
    "#   1. Validates that `ValueCoding` exists and is non-empty.\n",
    "#   2. Sanitizes `ItemName` to a filesystem-safe `ItemName_Sanitized` (same rules as\n",
    "#      used for filenames).\n",
    "#   3. Normalizes the `Source` column (string type, trimmed).\n",
    "#   4. Groups all distinct sources per `ItemName_Sanitized`.\n",
    "#   5. Uses `decide_category` to map each sanitized name to a Category:\n",
    "#        - Hardcoded overrides for certain items.\n",
    "#        - Generic rules:\n",
    "#             * presence of IS / Other  -> \"Mixed\"\n",
    "#             * presence of Market / BS -> \"Annualized\"\n",
    "#             * presence of CFS         -> \"Special\"\n",
    "#        - otherwise                    -> None\n",
    "#   6. Attaches the Category back to each row based on `ItemName_Sanitized`.\n",
    "#   7. Creates three unique-item DataFrames:\n",
    "#        - `annualized_items`\n",
    "#        - `mixed_items`\n",
    "#        - `special_items`\n",
    "#   8. Exposes the processed objects in `globals()` for use in later cells.\n",
    "#   9. Shows a sample and prints counts of each category.\n",
    "#\n",
    "# If `ValueCoding` is not present or is empty, processing is skipped.\n",
    "\n",
    "# CELL 1 — Process ValueCoding and assign Category per ItemName_Sanitized\n",
    "\n",
    "if 'ValueCoding' in globals() and ValueCoding is not None and not ValueCoding.empty:\n",
    "    # Inform that processing of ValueCoding is starting\n",
    "    print(\"Processing ValueCoding DataFrame...\")\n",
    "\n",
    "    # Work on a copy to avoid mutating the original ValueCoding\n",
    "    value_coding_processed = ValueCoding.copy()\n",
    "\n",
    "    # --- Sanitize ItemName ---\n",
    "    # Ensure ItemName is treated as string (avoid issues with numbers / NaNs)\n",
    "    value_coding_processed['ItemName'] = value_coding_processed['ItemName'].astype(str)\n",
    "\n",
    "    # First pass: replace spaces and certain filesystem-unsafe characters with underscores\n",
    "    # Same rule set as used for building filenames elsewhere\n",
    "    value_coding_processed['ItemName_Sanitized'] = value_coding_processed['ItemName'].str.replace(\n",
    "        r'[ \\-/\\:\\\\*\\?\"<>|]', '_', regex=True\n",
    "    )\n",
    "    # Second pass: strip any remaining characters not in [word chars, dot, hyphen]\n",
    "    value_coding_processed['ItemName_Sanitized'] = value_coding_processed['ItemName_Sanitized'].str.replace(\n",
    "        r'[^\\w.-]', '', regex=True\n",
    "    )\n",
    "\n",
    "    # --- Normalize Source ---\n",
    "    # Convert Source to string and strip leading/trailing whitespace\n",
    "    value_coding_processed['Source'] = (\n",
    "        value_coding_processed['Source']\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Group all sources per sanitized name\n",
    "    # ------------------------------------------------------------------\n",
    "    # For each ItemName_Sanitized, collect the set of all non-null sources\n",
    "    sources_per_name = (\n",
    "        value_coding_processed\n",
    "        .groupby('ItemName_Sanitized')['Source']\n",
    "        .apply(lambda s: set(s.dropna()))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Helper to decide category per sanitized name\n",
    "    # ------------------------------------------------------------------\n",
    "    def decide_category(name, sources: set):\n",
    "        \"\"\"\n",
    "        Decide a category string (\"Mixed\", \"Annualized\", \"Special\", or None)\n",
    "        for a given sanitized item name based on its set of sources.\n",
    "        \"\"\"\n",
    "        # Item-specific overrides (these take precedence over generic rules)\n",
    "        if name == 'Depreciation_Depletion__Amortization':\n",
    "            # Prefer 'IS' interpretation -> treat as Mixed\n",
    "            return 'Mixed'\n",
    "        if name == 'Minority_Interest':\n",
    "            # Prefer 'BS' interpretation -> treat as Annualized\n",
    "            return 'Annualized'\n",
    "\n",
    "        # Generic rules:\n",
    "\n",
    "        # If any of the sources is Income Statement or \"Other\", classify as Mixed\n",
    "        if any(src in ['IS', 'Market'] for src in sources):\n",
    "            return 'Mixed'\n",
    "\n",
    "        # If any of the sources is Market or Balance Sheet, classify as Annualized\n",
    "        if any(src in ['BS'] for src in sources):\n",
    "            return 'Annualized'\n",
    "\n",
    "        # If any of the sources is Cash Flow Statement, classify as Special\n",
    "        if any(src in ['CFS'] for src in sources):\n",
    "            return 'Special'\n",
    "\n",
    "        # If none of the rules matched, leave as None (no clear mapping)\n",
    "        return None\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Build category_map for all sanitized names\n",
    "    # ------------------------------------------------------------------\n",
    "    # Map each sanitized item name to a Category by applying decide_category\n",
    "    category_map = {\n",
    "        name: decide_category(name, srcs)\n",
    "        for name, srcs in sources_per_name.items()\n",
    "    }\n",
    "\n",
    "    # Attach final Category back to each row, via ItemName_Sanitized\n",
    "    value_coding_processed['Category'] = (\n",
    "        value_coding_processed['ItemName_Sanitized'].map(category_map)\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Derive annualized_items / mixed_items / special_items\n",
    "    # as unique per sanitized name\n",
    "    # ------------------------------------------------------------------\n",
    "    annualized_items = (\n",
    "        value_coding_processed[value_coding_processed['Category'] == 'Annualized']\n",
    "        .drop_duplicates(subset=['ItemName_Sanitized'])\n",
    "        .copy()\n",
    "    )\n",
    "    mixed_items = (\n",
    "        value_coding_processed[value_coding_processed['Category'] == 'Mixed']\n",
    "        .drop_duplicates(subset=['ItemName_Sanitized'])\n",
    "        .copy()\n",
    "    )\n",
    "    special_items = (\n",
    "        value_coding_processed[value_coding_processed['Category'] == 'Special']\n",
    "        .drop_duplicates(subset=['ItemName_Sanitized'])\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Export to globals for use in later cells\n",
    "    # ------------------------------------------------------------------\n",
    "    globals()['value_coding_processed'] = value_coding_processed\n",
    "    globals()['annualized_items'] = annualized_items\n",
    "    globals()['mixed_items'] = mixed_items\n",
    "    globals()['special_items'] = special_items\n",
    "    globals()['category_map'] = category_map\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Display sample and counts\n",
    "    # ------------------------------------------------------------------\n",
    "    print(\"\\nProcessed ValueCoding DataFrame (sample):\")\n",
    "    display(value_coding_processed.head())\n",
    "\n",
    "    print(f\"\\nNumber of Annualized items: {len(annualized_items)}\")\n",
    "    print(f\"Number of Mixed items: {len(mixed_items)}\")\n",
    "    print(f\"Number of Special items: {len(special_items)}\")\n",
    "\n",
    "else:\n",
    "    # If ValueCoding is not available or has no rows, skip processing\n",
    "    print(\"ValueCoding DataFrame not found or is empty. Skipping processing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Nb7Pqn0eFOw"
   },
   "source": [
    "### Sort into correct bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1765651109020,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "735bc0be",
    "outputId": "0b869444-588d-445d-94ad-eefe0fde4c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying work_subset files and creating variables based on categories...\n",
      "\n",
      "Found 49 work_subset files in Temp directory.\n",
      "  'work_subset_Accounts_Payable.txt' -> Annualized (variable 'Annualized_1').\n",
      "  'work_subset_Capital_Expenditures_Addtns_to_Fixed_Assets.txt' -> Special (variable 'Special_1').\n",
      "  'work_subset_Cash_Dividends_Paid___Total.txt' -> Special (variable 'Special_2').\n",
      "  'work_subset_Cash__Short_Term_Investments.txt' -> Annualized (variable 'Annualized_2').\n",
      "  'work_subset_Com_Pfd_Redeemed_Retired_Converted_Etc..txt' -> Special (variable 'Special_3').\n",
      "  'work_subset_Common_Equity.txt' -> Annualized (variable 'Annualized_3').\n",
      "  'work_subset_Cost_of_Goods_Sold_Excl_Depreciation.txt' -> Mixed (variable 'Mixed_1').\n",
      "  'work_subset_Current_Assets___Total.txt' -> Annualized (variable 'Annualized_4').\n",
      "  'work_subset_Current_Liabilities___Total.txt' -> Annualized (variable 'Annualized_5').\n",
      "  'work_subset_Deferred_Taxes.txt' -> Annualized (variable 'Annualized_6').\n",
      "  'work_subset_Depreciation_Depletion__Amortization.txt' -> Mixed (variable 'Mixed_2').\n",
      "  'work_subset_Disposal_of_Fixed_Assets.txt' -> Special (variable 'Special_4').\n",
      "  'work_subset_Earnings_Per_Share_Fiscal_Year_End.txt' -> Mixed (variable 'Mixed_3').\n",
      "  'work_subset_Extraordinary_Items.txt' -> Special (variable 'Special_5').\n",
      "  'work_subset_Funds_From_For_Other_Operating_Activities.txt' -> Special (variable 'Special_6').\n",
      "  'work_subset_Funds_From_Operations.txt' -> Special (variable 'Special_7').\n",
      "  'work_subset_Income_Taxes.txt' -> Mixed (variable 'Mixed_4').\n",
      "  'work_subset_Income_Taxes_Payable.txt' -> Annualized (variable 'Annualized_7').\n",
      "  'work_subset_Interest_Expense___Total.txt' -> Mixed (variable 'Mixed_5').\n",
      "  'work_subset_Inventories___Total.txt' -> Annualized (variable 'Annualized_8').\n",
      "  'work_subset_Investments_in_Associated_Companies.txt' -> Annualized (variable 'Annualized_9').\n",
      "  'work_subset_Investments_in_Sales__Direct_Financing_Leases.txt' -> Annualized (variable 'Annualized_10').\n",
      "  'work_subset_Long_Term_Borrowings.txt' -> Special (variable 'Special_8').\n",
      "  'work_subset_Long_Term_Debt.txt' -> Annualized (variable 'Annualized_11').\n",
      "  'work_subset_Long_Term_Receivables.txt' -> Annualized (variable 'Annualized_12').\n",
      "  'work_subset_Minority_Interest.txt' -> Annualized (variable 'Annualized_13').\n",
      "  'work_subset_Net_Cash_Flow___Financing.txt' -> Special (variable 'Special_9').\n",
      "  'work_subset_Net_Cash_Flow___Investing.txt' -> Special (variable 'Special_10').\n",
      "  'work_subset_Net_Cash_Flow___Operating_Activities.txt' -> Special (variable 'Special_11').\n",
      "  'work_subset_Net_Income_Before_Extra_Items_Preferred_Divs.txt' -> Mixed (variable 'Mixed_6').\n",
      "  'work_subset_Net_Income_Used_to_Calculate_Basic_EPS.txt' -> Mixed (variable 'Mixed_7').\n",
      "  'work_subset_Net_Proceeds_From_Sale_Issue_of_Com__Pfd.txt' -> Special (variable 'Special_12').\n",
      "  'work_subset_Net_Sales_or_Revenues.txt' -> Mixed (variable 'Mixed_8').\n",
      "  'work_subset_Operating_Income.txt' -> Mixed (variable 'Mixed_9').\n",
      "  'work_subset_Other_Assets___Total.txt' -> Annualized (variable 'Annualized_14').\n",
      "  'work_subset_Other_Current_Assets.txt' -> Annualized (variable 'Annualized_15').\n",
      "  'work_subset_Other_Current_Liabilities.txt' -> Annualized (variable 'Annualized_16').\n",
      "  'work_subset_Other_Investments.txt' -> Annualized (variable 'Annualized_17').\n",
      "  'work_subset_Other_Liabilities.txt' -> Annualized (variable 'Annualized_18').\n",
      "  'work_subset_Preferred_Stock.txt' -> Annualized (variable 'Annualized_19').\n",
      "  'work_subset_Property_Plant__Equipment___Net.txt' -> Annualized (variable 'Annualized_20').\n",
      "  'work_subset_ReceivablesNet.txt' -> Annualized (variable 'Annualized_21').\n",
      "  'work_subset_Reduction_in_Long_Term_Debt.txt' -> Special (variable 'Special_13').\n",
      "  'work_subset_Sales_Per_Share.txt' -> Mixed (variable 'Mixed_10').\n",
      "  'work_subset_Selling_General__Administrative_Expenses.txt' -> Mixed (variable 'Mixed_11').\n",
      "  'work_subset_Short_Term_Debt__Current_Portion_of_LT_Debt.txt' -> Annualized (variable 'Annualized_22').\n",
      "  'work_subset_Total_Assets.txt' -> Annualized (variable 'Annualized_23').\n",
      "  'work_subset_Total_Liabilities.txt' -> Annualized (variable 'Annualized_24').\n",
      "  'work_subset_Unspecified_Other_Loans.txt' -> Annualized (variable 'Annualized_25').\n",
      "\n",
      "Variable creation complete.\n",
      "Created 25 Annualized variables.\n",
      "Created 11 Mixed variables.\n",
      "Created 13 Special variables.\n",
      "\n",
      "Annualized Variables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Annualized_1': 'Accounts_Payable',\n",
       " 'Annualized_2': 'Cash__Short_Term_Investments',\n",
       " 'Annualized_3': 'Common_Equity',\n",
       " 'Annualized_4': 'Current_Assets___Total',\n",
       " 'Annualized_5': 'Current_Liabilities___Total',\n",
       " 'Annualized_6': 'Deferred_Taxes',\n",
       " 'Annualized_7': 'Income_Taxes_Payable',\n",
       " 'Annualized_8': 'Inventories___Total',\n",
       " 'Annualized_9': 'Investments_in_Associated_Companies',\n",
       " 'Annualized_10': 'Investments_in_Sales__Direct_Financing_Leases',\n",
       " 'Annualized_11': 'Long_Term_Debt',\n",
       " 'Annualized_12': 'Long_Term_Receivables',\n",
       " 'Annualized_13': 'Minority_Interest',\n",
       " 'Annualized_14': 'Other_Assets___Total',\n",
       " 'Annualized_15': 'Other_Current_Assets',\n",
       " 'Annualized_16': 'Other_Current_Liabilities',\n",
       " 'Annualized_17': 'Other_Investments',\n",
       " 'Annualized_18': 'Other_Liabilities',\n",
       " 'Annualized_19': 'Preferred_Stock',\n",
       " 'Annualized_20': 'Property_Plant__Equipment___Net',\n",
       " 'Annualized_21': 'ReceivablesNet',\n",
       " 'Annualized_22': 'Short_Term_Debt__Current_Portion_of_LT_Debt',\n",
       " 'Annualized_23': 'Total_Assets',\n",
       " 'Annualized_24': 'Total_Liabilities',\n",
       " 'Annualized_25': 'Unspecified_Other_Loans'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixed Variables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Mixed_1': 'Cost_of_Goods_Sold_Excl_Depreciation',\n",
       " 'Mixed_2': 'Depreciation_Depletion__Amortization',\n",
       " 'Mixed_3': 'Earnings_Per_Share_Fiscal_Year_End',\n",
       " 'Mixed_4': 'Income_Taxes',\n",
       " 'Mixed_5': 'Interest_Expense___Total',\n",
       " 'Mixed_6': 'Net_Income_Before_Extra_Items_Preferred_Divs',\n",
       " 'Mixed_7': 'Net_Income_Used_to_Calculate_Basic_EPS',\n",
       " 'Mixed_8': 'Net_Sales_or_Revenues',\n",
       " 'Mixed_9': 'Operating_Income',\n",
       " 'Mixed_10': 'Sales_Per_Share',\n",
       " 'Mixed_11': 'Selling_General__Administrative_Expenses'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Special Variables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Special_1': 'Capital_Expenditures_Addtns_to_Fixed_Assets',\n",
       " 'Special_2': 'Cash_Dividends_Paid___Total',\n",
       " 'Special_3': 'Com_Pfd_Redeemed_Retired_Converted_Etc.',\n",
       " 'Special_4': 'Disposal_of_Fixed_Assets',\n",
       " 'Special_5': 'Extraordinary_Items',\n",
       " 'Special_6': 'Funds_From_For_Other_Operating_Activities',\n",
       " 'Special_7': 'Funds_From_Operations',\n",
       " 'Special_8': 'Long_Term_Borrowings',\n",
       " 'Special_9': 'Net_Cash_Flow___Financing',\n",
       " 'Special_10': 'Net_Cash_Flow___Investing',\n",
       " 'Special_11': 'Net_Cash_Flow___Operating_Activities',\n",
       " 'Special_12': 'Net_Proceeds_From_Sale_Issue_of_Com__Pfd',\n",
       " 'Special_13': 'Reduction_in_Long_Term_Debt'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# CELL 2 maps work_subset_*.txt files to categories (\"Annualized\", \"Mixed\", \"Special\")\n",
    "# based on the ItemName_Sanitized that was derived in the previous cell.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Check that the required categorized DataFrames (annualized_items, mixed_items,\n",
    "#      special_items) and the temporary directory path Temp_file_path_DP exist.\n",
    "#   2. Build three sets of sanitized item names (annualized_names, mixed_names,\n",
    "#      special_names) from those DataFrames.\n",
    "#   3. List all files in Temp_file_path_DP and filter for those matching\n",
    "#      \"work_subset_*.txt\".\n",
    "#   4. For each work_subset file:\n",
    "#        - Extract the sanitized item name from the filename.\n",
    "#        - Determine whether it belongs to Mixed, Annualized, or Special based on\n",
    "#          the sets created in step 2.\n",
    "#        - Assign it a variable name (Mixed_n, Annualized_n, Special_n) and store\n",
    "#          that mapping in dicts mixed_vars, annualized_vars, special_vars.\n",
    "#   5. Store these dicts in globals() for use in later cells.\n",
    "#   6. Print summary information and display the created dictionaries.\n",
    "#   7. Perform garbage collection at the end.\n",
    "#\n",
    "# If any of the prerequisites are missing, it prints a message and skips the mapping.\n",
    "\n",
    "\n",
    "# CELL 2 — Map work_subset files to categories using ItemName_Sanitized\n",
    "\n",
    "if ('annualized_items' not in globals() or annualized_items is None or\n",
    "    'mixed_items' not in globals() or mixed_items is None or\n",
    "    'special_items' not in globals() or special_items is None or\n",
    "    'Temp_file_path_DP' not in globals()):\n",
    "    # If required DataFrames or directory path are missing, do not proceed\n",
    "    print(\"Required DataFrames (annualized_items, mixed_items, special_items) or Temp_file_path_DP not found. Please run the categorization cell.\")\n",
    "else:\n",
    "    print(\"Identifying work_subset files and creating variables based on categories...\")\n",
    "\n",
    "    # Sets of sanitized names that are final Annualized/Mixed/Special\n",
    "    annualized_names = set(annualized_items['ItemName_Sanitized'].dropna())\n",
    "    mixed_names      = set(mixed_items['ItemName_Sanitized'].dropna())\n",
    "    special_names    = set(special_items['ItemName_Sanitized'].dropna())\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Get a list of work_subset files from the temp directory\n",
    "    # ------------------------------------------------------------------\n",
    "    temp_files = os.listdir(Temp_file_path_DP)\n",
    "    work_subset_files = [\n",
    "        f for f in temp_files\n",
    "        if f.startswith('work_subset_') and f.endswith('.txt')\n",
    "    ]\n",
    "\n",
    "    # Dictionaries to hold mappings:\n",
    "    #   \"Annualized_1\" -> \"SomeItemName\"\n",
    "    #   \"Mixed_1\"      -> \"AnotherItemName\"\n",
    "    #   \"Special_1\"    -> \"SpecialItemName\"\n",
    "    annualized_vars = {}\n",
    "    mixed_vars = {}\n",
    "    special_vars = {}\n",
    "\n",
    "    print(f\"\\nFound {len(work_subset_files)} work_subset files in Temp directory.\")\n",
    "\n",
    "    # Sort files to have deterministic order when assigning variable names\n",
    "    work_subset_files.sort()\n",
    "\n",
    "    # Counters for how many items fall into each category; used for variable suffixes\n",
    "    annualized_count = 0\n",
    "    mixed_count = 0\n",
    "    special_count = 0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Iterate over each work_subset file and map it to a category\n",
    "    # ------------------------------------------------------------------\n",
    "    for file_name in work_subset_files:\n",
    "        # Extract sanitized item name from filename, expecting \"work_subset_<name>.txt\"\n",
    "        match = re.match(r'work_subset_(.+)\\.txt$', file_name)\n",
    "        if not match:\n",
    "            print(f\"  Filename format not as expected for '{file_name}'. Skipping processing.\")\n",
    "            continue\n",
    "\n",
    "        sanitized_item_name = match.group(1)\n",
    "\n",
    "        # Use the resolved sets. No more ambiguous precedence:\n",
    "        # priority Mixed -> Annualized -> Special, in this order of checks.\n",
    "        if sanitized_item_name in mixed_names:\n",
    "            mixed_count += 1\n",
    "            var_name = f\"Mixed_{mixed_count}\"\n",
    "            mixed_vars[var_name] = sanitized_item_name\n",
    "            print(f\"  '{file_name}' -> Mixed (variable '{var_name}').\")\n",
    "\n",
    "        elif sanitized_item_name in annualized_names:\n",
    "            annualized_count += 1\n",
    "            var_name = f\"Annualized_{annualized_count}\"\n",
    "            annualized_vars[var_name] = sanitized_item_name\n",
    "            print(f\"  '{file_name}' -> Annualized (variable '{var_name}').\")\n",
    "\n",
    "        elif sanitized_item_name in special_names:\n",
    "            special_count += 1\n",
    "            var_name = f\"Special_{special_count}\"\n",
    "            special_vars[var_name] = sanitized_item_name\n",
    "            print(f\"  '{file_name}' -> Special (variable '{var_name}').\")\n",
    "\n",
    "        else:\n",
    "            # No category mapping found for this sanitized name\n",
    "            print(f\"  '{file_name}' -> No matching Category (might be unmapped or ambiguous). Skipping.\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Expose the mapping dictionaries globally for later use\n",
    "    # ------------------------------------------------------------------\n",
    "    globals()['annualized_vars'] = annualized_vars\n",
    "    globals()['mixed_vars'] = mixed_vars\n",
    "    globals()['special_vars'] = special_vars\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Summary output and inspection\n",
    "    # ------------------------------------------------------------------\n",
    "    print(f\"\\nVariable creation complete.\")\n",
    "    print(f\"Created {len(annualized_vars)} Annualized variables.\")\n",
    "    print(f\"Created {len(mixed_vars)} Mixed variables.\")\n",
    "    print(f\"Created {len(special_vars)} Special variables.\")\n",
    "\n",
    "    print(\"\\nAnnualized Variables:\")\n",
    "    display(annualized_vars)\n",
    "\n",
    "    print(\"\\nMixed Variables:\")\n",
    "    display(mixed_vars)\n",
    "\n",
    "    print(\"\\nSpecial Variables:\")\n",
    "    display(special_vars)\n",
    "\n",
    "    # Run garbage collection after building mappings\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrzIZ4o5be9O"
   },
   "source": [
    "## 4.2. Income Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jiewmt0-enQr"
   },
   "source": [
    "### Mixed 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOoCUcFp0NXg"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1765651109169,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "8oNJXeG40QPv",
    "outputId": "17b3e858-6e9c-4100-d387-61c291834fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Mixed_1  ->  ItemName: 'Cost_of_Goods_Sold_Excl_Depreciation'\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This code selects which Mixed_* dataset should be processed by choosing an index\n",
    "# (e.g., Mixed_1, Mixed_2, ...). It then:\n",
    "#\n",
    "#   1. Ensures that a dictionary `mixed_vars` exists, mapping keys like \"Mixed_1\"\n",
    "#      to item names.\n",
    "#   2. Builds the key corresponding to the selected index and retrieves the\n",
    "#      associated item name (`target_item_name`).\n",
    "#   3. Prints which Mixed_* item was selected.\n",
    "#   4. Constructs paths and filenames based on global variables and the selected item.\n",
    "#   5. Ensures that the output directory exists by creating it if necessary.\n",
    "#\n",
    "# The goal is to centralize selection of a single Mixed_* dataset and prepare paths\n",
    "# for downstream processing.\n",
    "\n",
    "\n",
    "# === Select which Mixed_* item to run ===\n",
    "mixed_index = 1  # Change this to process another dataset (e.g., 10)\n",
    "\n",
    "# Validate that the dictionary of mixed item names exists\n",
    "assert 'mixed_vars' in globals(), \"mixed_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key (e.g., \"Mixed_1\") and retrieve the associated item name\n",
    "item_key = f\"Mixed_{mixed_index}\"\n",
    "target_item_name = mixed_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in mixed_vars.\"\n",
    "\n",
    "# Inform which item was selected\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Construct the name of the input file for the selected item\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Construct the base name for output files (will later be suffixed)\n",
    "base_output_filename = f\"Mixed_{target_item_name}_complete\"\n",
    "\n",
    "# Ensure the output directory exists; create it (including parent dirs) if needed\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc7c6fde"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 906,
     "status": "ok",
     "timestamp": 1765651110080,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "254e9cb7",
    "outputId": "e81298b3-5255-447d-a259-931b0f8f597a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Cost_of_Goods_Sold_Excl_Depreciation' ...\n",
      "Full dataset loaded successfully: 4,227,003 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1051</td>\n",
       "      <td>1415.675687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1051</td>\n",
       "      <td>1621.318491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1051</td>\n",
       "      <td>1968.910031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1051</td>\n",
       "      <td>1270.982588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1051</td>\n",
       "      <td>1007.403728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode        Value  \n",
       "0     1051  1415.675687  \n",
       "1     1051  1621.318491  \n",
       "2     1051  1968.910031  \n",
       "3     1051  1270.982588  \n",
       "4     1051  1007.403728  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block:\n",
    "#   1. Announces the import of a full dataset for the given `target_item_name`.\n",
    "#   2. Checks whether the file at `file_path` exists.\n",
    "#   3. If it exists, calls `import_file_to_dataframe(file_path)` to load the data\n",
    "#      into `mixed_raw`.\n",
    "#   4. If the loaded DataFrame is non-empty, prints a success message including\n",
    "#      the number of rows and shows the first few rows (via display or fallback\n",
    "#      to text printing).\n",
    "#   5. If the load fails or returns an empty DataFrame, prints a warning and\n",
    "#      creates an empty DataFrame.\n",
    "#   6. If the file does not exist, prints an error message and sets `mixed_raw`\n",
    "#      to an empty DataFrame.\n",
    "#   7. Finally, it runs `gc.collect()` to trigger garbage collection and free\n",
    "#      memory.\n",
    "#\n",
    "# Note: All previous emoji symbols in the print statements have been removed.\n",
    "\n",
    "# Inform the user that we are starting the import for this item\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "# Check if the file path exists on disk\n",
    "if os.path.exists(file_path):\n",
    "    # If the file exists, attempt to import it into a DataFrame\n",
    "    mixed_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    # If the DataFrame is successfully loaded and not empty\n",
    "    if mixed_raw is not None and not mixed_raw.empty:\n",
    "        # Print a success message including row count with thousands separator\n",
    "        print(f\"Full dataset loaded successfully: {len(mixed_raw):,} rows total.\")\n",
    "\n",
    "        try:\n",
    "            # Try to display the first few rows (Jupyter / IPython display)\n",
    "            display(mixed_raw.head())\n",
    "        except Exception:\n",
    "            # If display is not available, fall back to printing as plain text\n",
    "            print(mixed_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        # If DataFrame is None or empty, warn and create an empty DataFrame\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        mixed_raw = pd.DataFrame()\n",
    "else:\n",
    "    # If the file does not exist, report an error and set mixed_raw to empty\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    mixed_raw = pd.DataFrame()\n",
    "\n",
    "# Run garbage collection to free up memory after the load attempt\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mt2zuCrWpqu"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1765651110427,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "129081f6",
    "outputId": "1d42a772-ccd5-4895-9be9-9220aac156b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Mixed dataset for 'Cost_of_Goods_Sold_Excl_Depreciation' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_581638/2687430345.py:85: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y02' 'Q1Y02' 'Q1Y03' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1051</td>\n",
       "      <td>1415.675687</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1051</td>\n",
       "      <td>1621.318491</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1051</td>\n",
       "      <td>1968.910031</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1051</td>\n",
       "      <td>1270.982588</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1051</td>\n",
       "      <td>1007.403728</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode        Value Str_FiscalPrd  \n",
       "0     1051  1415.675687           Y92  \n",
       "1     1051  1621.318491           Y93  \n",
       "2     1051  1968.910031           Y94  \n",
       "3     1051  1270.982588           Y95  \n",
       "4     1051  1007.403728           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This snippet provides:\n",
    "#\n",
    "# 1. A helper function `last2` that returns the last two digits of a number as a\n",
    "#    zero-padded string (for building YY strings).\n",
    "#\n",
    "# 2. A function `add_str_fiscalprd(df)` which:\n",
    "#    - Works on a copy of an input DataFrame containing financial periods.\n",
    "#    - Normalizes the 'Frequency' (upper-case, no missing).\n",
    "#    - Stores the original 'FiscalPeriod' and converts it to numeric.\n",
    "#    - Creates a string representation 'Str_FiscalPrd' depending on the frequency:\n",
    "#         - Q/C/E/R: quarter-based (\"QnYyy\")\n",
    "#         - A/B: annual (\"Yyy\")\n",
    "#         - F/S: semiannual (\"SnYyy\")\n",
    "#         - K/T/L/U: trimester-like (\"TnYyy\")\n",
    "#    - Derives an implied full-year integer 'ImplFiscPer_Calculated' from the\n",
    "#      two-digit year (80–99 => 19xx, else 20xx).\n",
    "#    - For annual rows (A/B), checks discrepancies between original\n",
    "#      'FiscalPeriod' and implied full-year; prints a small preview & total count.\n",
    "#    - Overwrites 'FiscalPeriod' with 'ImplFiscPer_Calculated' and drops helper\n",
    "#      columns.\n",
    "#\n",
    "# 3. A small driver block that:\n",
    "#    - Checks that `mixed_raw` exists and is non-empty.\n",
    "#    - Applies `add_str_fiscalprd` to produce `mixed_encoded`.\n",
    "#    - Displays a head preview or prints a message and sets `mixed_encoded = None`\n",
    "#      if input is missing/empty.\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    # If input is NaN (or pandas-style missing), return None (no digits)\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Convert to int, format as 4-digit zero-padded string, take last 2 chars\n",
    "    # Example: n=2023 -> \"2023\"[-2:] -> \"23\"\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    \"\"\"\n",
    "    Creates 'Str_FiscalPrd' and overwrites 'FiscalPeriod' with an implied full year.\n",
    "    Also prints discrepancies for Annual (A,B) rows between original and calculated year.\n",
    "\n",
    "    The mapping logic:\n",
    "      - Quarterly (C, Q, E, R):   fp encodes a quarter index -> \"Q{1-4}Y{yy}\"\n",
    "      - Annual   (A, B):         fp is the year itself      -> \"Y{yy}\"\n",
    "      - Semiannual (F, S):       fp encodes half-year       -> \"S{1-2}Y{yy}\"\n",
    "      - Trimester-like (K, T,\n",
    "         L, U):                  fp encodes trimester       -> \"T{1-3}Y{yy}\"\n",
    "    Then we re-infer the full year from the YY part with 19xx/20xx rule.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the original input DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes:\n",
    "    # - uppercase for consistency\n",
    "    # - fill missing values with empty string\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod in a separate column for later comparison\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    # Convert FiscalPeriod to numeric, coerce errors to NaN\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Masks for different frequency groups\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])  # quarter-based\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])            # annual\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])            # semiannual\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])  # trimester-like\n",
    "\n",
    "    # Initialize Str_FiscalPrd as NaN; we will fill per frequency group\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Quarter-based encoding\n",
    "    # -------------------------\n",
    "    # Quarter number: (fp % 4) + 1 => yields 1..4 (if fp is integer-based encoding)\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year part for quarter: integer division by 4, then take last two digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    # Assign \"Q{quarter}Y{yy}\" for quarter frequencies\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" +\n",
    "        q_part.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Annual encoding (A, B)\n",
    "    # -------------------------\n",
    "    # Year part is fp itself for A/B; take last two digits via last2\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    # Assign \"Y{yy}\" for annual frequencies\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    # -------------------------\n",
    "    # Semiannual encoding (F, S)\n",
    "    # -------------------------\n",
    "    # Semester number: (fp % 2) + 1 => 1 or 2\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    # Year part: fp // 2, then last two digits\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    # Assign \"S{sem}Y{yy}\" for F/S\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" +\n",
    "        fs_sem.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Trimester-like encoding (K, T, L, U)\n",
    "    # -------------------------\n",
    "    # Term number: (fp % 3) + 1 => 1..3\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    # Year part: fp // 3, then last two digits\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    # Assign \"T{term}Y{yy}\" for K/T/L/U frequencies\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" +\n",
    "        t_term.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Implied full year from Str_FiscalPrd (19xx / 20xx reconstruction)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Extract two-digit year from \"Yyy\" pattern, e.g., \"Q1Y23\" -> \"23\"\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    # Convert that to numeric (NaN if not parseable)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    # Map two-digit year to full year:\n",
    "    #  - 80–99 => 19xx\n",
    "    #  - else  => 20xx\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Discrepancies check for Annual frequencies (A, B)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Work on a subset of annual frequencies only\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "\n",
    "    # Build boolean mask where implied full year does NOT match the original\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Either they match numerically...\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce'))\n",
    "        |\n",
    "        # ...or both are NaN\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    # Subset rows with discrepancies\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    # If we have any, show a short sample and the total count\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Overwrite FiscalPeriod and drop temporary helper columns\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Replace 'FiscalPeriod' with the implied full-year value we just calculated\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    # Drop helper columns not needed downstream\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to mixed_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'mixed_raw' in globals() and mixed_raw is not None and not mixed_raw.empty:\n",
    "    # Inform which item we are encoding for\n",
    "    print(f\"Applying encoding to Mixed dataset for '{target_item_name}' ...\")\n",
    "    # Apply fiscal period encoding\n",
    "    mixed_encoded = add_str_fiscalprd(mixed_raw)\n",
    "    # Show a preview of the encoded data\n",
    "    display(mixed_encoded.head())\n",
    "else:\n",
    "    # If mixed_raw is missing or empty, we skip and mark mixed_encoded as None\n",
    "    print(\"mixed_raw not found or empty. Cannot perform encoding.\")\n",
    "    mixed_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9d056d5"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9608,
     "status": "ok",
     "timestamp": 1765651120036,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "nvSkzDhQ1C6y",
    "outputId": "48360405-3454-4fbf-b2dc-1a046f22d25d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 4,227,003 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\n",
      "         finite_rows: 2488370\n",
      "                mean: 22087.72593423967\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 102.20628914622627\n",
      "                 p10: 99.14078087541672\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 103.0348917912802\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 70,143\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 2418227\n",
      "                mean: 101.12530173470029\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 101.03618058377953\n",
      "                 p10: 99.43157862352895\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 101.99831950492842\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Cost_of_Goods_Sold_Excl_Depreciation_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Cost_of_Goods_Sold_Excl_Depreciation_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     4,227,003\n",
      "Excludedby Frequency (E/L/R/U): 0\n",
      "Dropped by quality (Pct rules): 70,143\n",
      "Output rows (final):            4,156,860\n",
      "Check: excluded + dropped + output = 4,227,003\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This script takes an input DataFrame `mixed_encoded` (if present in the global scope)\n",
    "# that contains financial time-series data (per company, item, currency, fiscal period,\n",
    "# and PIT Date). It then:\n",
    "#\n",
    "# 1. Cleans and standardizes key columns (dates, numeric types, string IDs).\n",
    "# 2. Excludes rows with certain frequencies (E/L/R/U).\n",
    "# 3. Parses fiscal period strings into quarter/semester/trimester indicators (QNUM/SNUM/TNUM).\n",
    "# 4. Uses a custom, vectorized \"as-of\" join (`asof_numpy`) to attach the most recent\n",
    "#    annual, quarterly, semiannual, and trimester values for each (ID, HistCurrency,\n",
    "#    ItemCode, FiscalPeriod) up to each row’s PIT Date.\n",
    "# 5. Builds \"full-year\" candidate values from:\n",
    "#       - actual annuals (A),\n",
    "#       - sum of Q1..Q4 (Q4 proxy),\n",
    "#       - sum of S1..S2 (S2 proxy),\n",
    "#       - sum of T1..T3 (T3 proxy),\n",
    "#    and selects the best candidate based on priorities and relationship to the row’s\n",
    "#    fiscal period (same-year vs prior-year).\n",
    "# 6. Computes an annual PIT-based metric `AnnPITValue` and compares it to the “true”\n",
    "#    annual value (`TrueValue`) to derive a percentage `AnnPITValue_Pct` for QC.\n",
    "# 7. Performs quality checks:\n",
    "#       - Ensures no period-date is after the PIT Date.\n",
    "#       - Drops rows whose `AnnPITValue_Pct` is outside the range [50, 200] or infinite.\n",
    "# 8. Keeps a curated set of columns, drops helper columns, and saves:\n",
    "#       - a full output file\n",
    "#       - a subset file with key columns for quick inspection.\n",
    "# 9. Prints row-accounting stats and frees some memory.\n",
    "#\n",
    "# If `mixed_encoded` is not defined or is None, it simply prints a message and exits.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta            # <--- Added to fix NameError\n",
    "from scipy.stats.mstats import winsorize  # <--- Added to fix NameError\n",
    "\n",
    "# Enable pandas \"copy-on-write\" behavior to reduce unintended chained assignment effects\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Helper function to build a string key from multiple columns.\n",
    "    For each row, join the values of 'cols' with '||'.\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    This is a manual, NumPy-based implementation of an \"as-of merge\" grouped by `by_cols`.\n",
    "    It:\n",
    "    - filters out rows with missing required fields,\n",
    "    - normalizes PIT Date to daily precision,\n",
    "    - groups right_df by composite key of `by_cols`,\n",
    "    - for each left row, binary-searches into the matching right group by PIT Date,\n",
    "      picking the last date <= the left PIT Date,\n",
    "    - returns two aligned arrays: (values, dates) for the left_df rows.\n",
    "    \"\"\"\n",
    "    # Initialize outputs with NaNs and NaT for all left_df rows\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns on each side\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask: rows that have all required fields non-null\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If either side has no valid rows, return the default empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on filtered copies only (avoid side effects)\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime at day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build composite keys for group-level match (based on by_cols)\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and date to enable binary search per key\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract NumPy arrays for right side\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and first index for each key in the sorted right side\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Build a dict: key -> (dates array, values array) slice\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]  # start of this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end of this key\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Left side indices and arrays\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left side by key (stable mergesort to preserve original row order within key)\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # i iterates over the sorted left rows\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current composite key\n",
    "        j = i + 1\n",
    "        # Find contiguous block [i:j) for this key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Dates and original positions of this key’s left rows\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # If we have matching right-hand slices for this key, do the as-of search\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # searchsorted(..., 'right') - 1 gives index of last r_date <= block_date\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # positions where such a date exists\n",
    "            if np.any(valid):\n",
    "                # Fill outputs for left rows where we found a valid match\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        # Move to next key block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile calculation: returns quantile q of Series `s`,\n",
    "    or NaN if quantile fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Computes a summary of finite values of a series:\n",
    "      - count\n",
    "      - mean, median\n",
    "      - 1% winsorized mean\n",
    "      - selected deciles (p10, p20, ..., p90)\n",
    "    Infinite values are treated as NaN and removed.\n",
    "    If no finite values remain, returns an empty dict.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop all NaNs\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures the array is writable for winsorize\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Priority for full-year candidates ----------\n",
    "\n",
    "# Fixed priority mapping for full-year candidates:\n",
    "#   'A'  : actual annual value\n",
    "#   'Q4' : annual proxy from four quarters\n",
    "#   'T3' : annual proxy from three trimesters\n",
    "#   'S2' : annual proxy from two semesters\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # highest priority: actual annual\n",
    "    'Q4': 90,  # then Q1+Q2+Q3+Q4\n",
    "    'T3': 80,  # then T1+T2+T3\n",
    "    'S2': 70,  # then S1+S2\n",
    "}\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "# Only run the main logic if a global DataFrame `mixed_encoded` exists and is not None\n",
    "if 'mixed_encoded' in globals() and mixed_encoded is not None:\n",
    "    # Count initial input rows\n",
    "    input_rows = len(mixed_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy of the input dataset\n",
    "    working = mixed_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Exclude certain frequencies (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create mask of rows whose Frequency is one of E, L, R, U (case-insensitive)\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    # Count how many rows will be excluded\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    # Keep only rows that are NOT in the exclusion set\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Set dtypes and normalize important columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert PIT Date to datetime (coerce errors -> NaT), floor to day\n",
    "    working['PIT Date']     = pd.to_datetime(working['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    # FiscalPeriod: numeric (e.g., 2021, 2022, ...)\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    # Value: numeric (float)\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Convert key ID / code columns to string to ensure consistency\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse Q/S/T markers from Str_FiscalPrd (like 'Q1Y2023')\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter number Q1..Q4 from e.g. \"Q1Y2023\" into QNUM\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual number S1..S2 into SNUM\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester number T1..T3 into TNUM\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ensure period columns exist (Q1..Q4, S1..S2, T1..T3, A + their date cols)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create value columns for Q1..Q4, S1..S2, T1..T3, A if they are missing\n",
    "    for c in [*(f'Q{i}' for i in range(1, 5)),\n",
    "              *(f'S{i}' for i in range(1, 3)),\n",
    "              *(f'T{i}' for i in range(1, 4)),\n",
    "              'A']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Create corresponding *_Date columns if missing\n",
    "    for c in [*(f'Q{i}_Date' for i in range(1, 5)),\n",
    "              *(f'S{i}_Date' for i in range(1, 3)),\n",
    "              *(f'T{i}_Date' for i in range(1, 4)),\n",
    "              'A_Date']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Base key for many of the as-of mappings\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Derive TrueValue from annuals (A/B frequencies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Mask annual-like rows where Value is present\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    # annual_src: one row per (ID, FiscalPeriod, HistCurrency) with last PIT Date\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Left-join true annual value back onto working\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping (same FiscalPeriod) for A/Q/S/T\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----- Annual -----\n",
    "    # Source rows for annual frequencies A/B\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    # As-of join: for each working row, get most recent annual value by PIT Date\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period of annual value (same as row's FiscalPeriod when present)\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # ----- Quarterly -----\n",
    "    # Source rows for quarterly frequencies (Q/C) with valid QNUM\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Restrict to a specific quarter q\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        # As-of join for that quarter\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        # Origin FP column for that quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        # Fill origin FP only where quarter value is non-null and origin not yet set\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Semiannual -----\n",
    "    # Source rows for semiannual frequencies (S/F) with valid SNUM\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Trimester -----\n",
    "    # Source rows for trimester frequencies (T/K) with valid TNUM\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels & normalize dates (NO prev-year fill, NO forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort working data consistently for downstream calculations\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    # List of all period value columns\n",
    "    value_cols_all  = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                      [f'S{i}' for i in range(1, 3)] + \\\n",
    "                      [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    # Corresponding date columns\n",
    "    date_cols_all   = [f'{c}_Date' for c in value_cols_all]\n",
    "    # Corresponding origin FP columns\n",
    "    origin_cols_all = [f'{c}_OriginFP' for c in value_cols_all]\n",
    "\n",
    "    # Ensure that all date columns are proper datetimes (floor to day)\n",
    "    # Note: explicitly no groupby-forward-fill here – only asof-filled values remain\n",
    "    for c in date_cols_all:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Build full-year candidates from fixed sets (Q1–Q4, S1–S2, T1–T3)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def full_year_from_fixed(row, labels, pit, cutoff):\n",
    "        \"\"\"\n",
    "        Fixed full-year from given labels (e.g. Q1..Q4, S1..S2, T1..T3):\n",
    "\n",
    "          - Requires ALL labels to have:\n",
    "              value, date, origin_fp.\n",
    "          - All dates must be within [cutoff, pit].\n",
    "          - origin_fp = max(origin_fp_i)  (the 'newest year' across components).\n",
    "\n",
    "        Returns (dt, val, origin_fp) or (NaT, NaN, NaN) if any requirement fails.\n",
    "        \"\"\"\n",
    "        vals, dts, fps = [], [], []\n",
    "        for lbl in labels:\n",
    "            v = row.get(lbl, np.nan)\n",
    "            d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "            o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "            # Require non-null for value, date, origin FP\n",
    "            if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            # Normalize date and check it lies within [cutoff, pit]\n",
    "            d = pd.to_datetime(d, errors='coerce')\n",
    "            if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            vals.append(float(v))\n",
    "            dts.append(d)\n",
    "            fps.append(int(o))\n",
    "\n",
    "        # Sum all component values, pick latest date and max origin FP\n",
    "        total_val = float(np.nansum(vals))\n",
    "        latest_dt = max(dts)\n",
    "        origin_fp = max(fps)\n",
    "        return latest_dt, total_val, origin_fp\n",
    "\n",
    "    def pick_annpit_sum_with_origin(row):\n",
    "        \"\"\"\n",
    "        For each row, pick the best annual PIT-based value (AnnPITValue) by:\n",
    "\n",
    "        1) Building a time window [PIT - 365 days, PIT].\n",
    "        2) Generating candidate annual values:\n",
    "           - actual annual A (if within window and with origin FP),\n",
    "           - Q4 proxy from Q1..Q4,\n",
    "           - S2 proxy from S1..S2,\n",
    "           - T3 proxy from T1..T3,\n",
    "           each with associated priority and origin FP.\n",
    "        3) Ranking candidates according to:\n",
    "           - Same-year A (highest),\n",
    "           - Same-year proxies (Q4/T3/S2),\n",
    "           - Prior-year A,\n",
    "           - Prior-year proxies,\n",
    "           - Otherwise: any remaining candidate by (priority, latest date).\n",
    "        4) Returning the chosen candidate's value (keeping zeros, but filtering NaNs).\n",
    "        \"\"\"\n",
    "        pit = row['PIT Date']\n",
    "        if pd.isna(pit):\n",
    "            return np.nan\n",
    "        cutoff = pit - timedelta(days=365)  # Now works because timedelta is imported\n",
    "\n",
    "        # Current row's fiscal period, cast to int if possible\n",
    "        fp = row.get('FiscalPeriod', np.nan)\n",
    "        try:\n",
    "            fp_int = int(fp) if not pd.isna(fp) else None\n",
    "        except Exception:\n",
    "            fp_int = None\n",
    "\n",
    "        # Collect candidate tuples: (label, priority, date, value, origin_fp)\n",
    "        candidates = []\n",
    "\n",
    "        # --- Candidate A: actual annual (0 is allowed)\n",
    "        A_val = row.get('A', np.nan)\n",
    "        A_dt  = row.get('A_Date', pd.NaT)\n",
    "        A_ofp = row.get('A_OriginFP', np.nan)\n",
    "        if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "            A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "            if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "                candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "        # --- Candidate Q4: sum of Q1..Q4 (0 allowed)\n",
    "        q4_dt, q4_val, q4_fp = full_year_from_fixed(\n",
    "            row, [f'Q{i}' for i in range(1, 5)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "            candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "        # --- Candidate S2: sum of S1..S2 (0 allowed)\n",
    "        s2_dt, s2_val, s2_fp = full_year_from_fixed(\n",
    "            row, [f'S{i}' for i in range(1, 3)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(s2_val) and pd.notna(s2_dt) and not pd.isna(s2_fp):\n",
    "            candidates.append(('S2', _PERIOD_PRIORITY['S2'], s2_dt, float(s2_val), int(s2_fp)))\n",
    "\n",
    "        # --- Candidate T3: sum of T1..T3 (0 allowed)\n",
    "        t3_dt, t3_val, t3_fp = full_year_from_fixed(\n",
    "            row, [f'T{i}' for i in range(1, 4)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(t3_val) and pd.notna(t3_dt) and not pd.isna(t3_fp):\n",
    "            candidates.append(('T3', _PERIOD_PRIORITY['T3'], t3_dt, float(t3_val), int(t3_fp)))\n",
    "\n",
    "        # If no candidates, return NaN\n",
    "        if not candidates:\n",
    "            return np.nan\n",
    "\n",
    "        # Filter out NaN-valued candidates (keep 0-valued ones)\n",
    "        def valid(seq):\n",
    "            return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "        # Relationship between row's fiscal period and candidate's origin FP:\n",
    "        # same / prior / other / unknown\n",
    "        def rel(c):\n",
    "            _, _, _, _, ofp = c\n",
    "            if fp_int is None or ofp is None:\n",
    "                return 'unknown'\n",
    "            if ofp == fp_int:\n",
    "                return 'same'\n",
    "            if ofp == fp_int - 1:\n",
    "                return 'prior'\n",
    "            return 'other'\n",
    "\n",
    "        # 1) Same-year actual annual A (max by date)\n",
    "        same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "        if same_A:\n",
    "            best = max(same_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 2) Same-year proxies (Q4/T3/S2), max by (priority, date)\n",
    "        same_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'same'\n",
    "        )\n",
    "        if same_proxies:\n",
    "            best = max(same_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 3) Prior-year actual annual A\n",
    "        prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "        if prior_A:\n",
    "            best = max(prior_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 4) Prior-year proxies (Q4/S2/T3) if no prior A\n",
    "        prior_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'prior'\n",
    "        )\n",
    "        if prior_proxies:\n",
    "            best = max(prior_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "        others = valid(candidates)\n",
    "        if others:\n",
    "            best = max(others, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # Final fallback: 0.0 (should rarely be reached)\n",
    "        return 0.0\n",
    "\n",
    "    # Apply the selection function row-wise to produce AnnPITValue\n",
    "    working['AnnPITValue'] = working.apply(pick_annpit_sum_with_origin, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check + PRE-DROP stats\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Columns whose dates should not exceed PIT Date\n",
    "    date_cols = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Restrict to ones actually present\n",
    "    present = [c for c in date_cols if c in working.columns]\n",
    "\n",
    "    viol_counts = {}  # per-label violation counts\n",
    "    # Mask for rows with any future-dated period\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        # A violation is when period date > PIT Date (both need to be non-null)\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m  # accumulate violations across columns\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flagrows with at least one future-date error\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct + quality drop\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute AnnPITValue as % of TrueValue (only when TrueValue != 0)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Stats before dropping low-quality rows\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    # Build masks for dropping:\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)  # infinite percentages\n",
    "    is_finite = np.isfinite(pct)\n",
    "    # Out-of-range if % > 200 or % < 50 (but finite)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    # Rows to drop: infinite or out-of-range values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that passed the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Stats after dropping\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns & save\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Base descriptive columns to keep (if present)\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Build ordered list of period date/value columns\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Final set of columns to keep in output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Helper columns to drop before export\n",
    "    drop_cols = ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    # Also drop all *_OriginFP columns\n",
    "    drop_cols += [c for c in working.columns if c.endswith('_OriginFP')]\n",
    "    working.drop(columns=[c for c in drop_cols if c in working.columns],\n",
    "                 inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder and restrict columns to the final layout\n",
    "    mixed_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # Sanity checks: necessary globals must exist\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Build full output path and save pipe-delimited file\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    mixed_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Create a subset for lighter inspection\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "                   \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    # Only keep subset columns that actually exist\n",
    "    subset_cols_existing = [col for col in subset_cols if col in mixed_processed.columns]\n",
    "    subset_df = mixed_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP,\n",
    "                              f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    # Explicitly delete subset_df to free memory\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(mixed_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excludedby Frequency (E/L/R/U): {excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    # Sum up excluded + dropped + remaining and check against original count\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. \"\n",
    "              \"Investigate upstream filtering or unexpected drops.\")\n",
    "\n",
    "    # Trigger garbage collection as a final cleanup step\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # If the main input dataset is not available, skip all processing\n",
    "    print(\"mixed_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCYcPbuN7daJ"
   },
   "source": [
    "### Mixed 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfymFmHKlaxp"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1765651120328,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ncEItBRSlaxw",
    "outputId": "04c01529-fec1-4087-de0b-67b3f04955d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Mixed_2  ->  ItemName: 'Depreciation_Depletion__Amortization'\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This code selects which Mixed_* dataset should be processed by choosing an index\n",
    "# (e.g., Mixed_1, Mixed_2, ...). It then:\n",
    "#\n",
    "#   1. Ensures that a dictionary `mixed_vars` exists, mapping keys like \"Mixed_1\"\n",
    "#      to item names.\n",
    "#   2. Builds the key corresponding to the selected index and retrieves the\n",
    "#      associated item name (`target_item_name`).\n",
    "#   3. Prints which Mixed_* item was selected.\n",
    "#   4. Constructs paths and filenames based on global variables and the selected item.\n",
    "#   5. Ensures that the output directory exists by creating it if necessary.\n",
    "#\n",
    "# The goal is to centralize selection of a single Mixed_* dataset and prepare paths\n",
    "# for downstream processing.\n",
    "\n",
    "\n",
    "# === Select which Mixed_* item to run ===\n",
    "mixed_index = 2  # Change this to process another dataset (e.g., 10)\n",
    "\n",
    "# Validate that the dictionary of mixed item names exists\n",
    "assert 'mixed_vars' in globals(), \"mixed_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key (e.g., \"Mixed_1\") and retrieve the associated item name\n",
    "item_key = f\"Mixed_{mixed_index}\"\n",
    "target_item_name = mixed_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in mixed_vars.\"\n",
    "\n",
    "# Inform which item was selected\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Construct the name of the input file for the selected item\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Construct the base name for output files (will later be suffixed)\n",
    "base_output_filename = f\"Mixed_{target_item_name}_complete\"\n",
    "\n",
    "# Ensure the output directory exists; create it (including parent dirs) if needed\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvvZArvSlaxy"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 1551,
     "status": "ok",
     "timestamp": 1765651121913,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "rNCyNCetlaxz",
    "outputId": "6512c65a-cb6a-4089-b35b-db3c0cb56479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Depreciation_Depletion__Amortization' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset loaded successfully: 3,390,222 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1151</td>\n",
       "      <td>38.094417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1151</td>\n",
       "      <td>68.245694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1151</td>\n",
       "      <td>89.54678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1151</td>\n",
       "      <td>98.504938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1151</td>\n",
       "      <td>84.007512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode      Value  \n",
       "0     1151  38.094417  \n",
       "1     1151  68.245694  \n",
       "2     1151   89.54678  \n",
       "3     1151  98.504938  \n",
       "4     1151  84.007512  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block:\n",
    "#   1. Announces the import of a full dataset for the given `target_item_name`.\n",
    "#   2. Checks whether the file at `file_path` exists.\n",
    "#   3. If it exists, calls `import_file_to_dataframe(file_path)` to load the data\n",
    "#      into `mixed_raw`.\n",
    "#   4. If the loaded DataFrame is non-empty, prints a success message including\n",
    "#      the number of rows and shows the first few rows (via display or fallback\n",
    "#      to text printing).\n",
    "#   5. If the load fails or returns an empty DataFrame, prints a warning and\n",
    "#      creates an empty DataFrame.\n",
    "#   6. If the file does not exist, prints an error message and sets `mixed_raw`\n",
    "#      to an empty DataFrame.\n",
    "#   7. Finally, it runs `gc.collect()` to trigger garbage collection and free\n",
    "#      memory.\n",
    "#\n",
    "# Note: All previous emoji symbols in the print statements have been removed.\n",
    "\n",
    "# Inform the user that we are starting the import for this item\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "# Check if the file path exists on disk\n",
    "if os.path.exists(file_path):\n",
    "    # If the file exists, attempt to import it into a DataFrame\n",
    "    mixed_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    # If the DataFrame is successfully loaded and not empty\n",
    "    if mixed_raw is not None and not mixed_raw.empty:\n",
    "        # Print a success message including row count with thousands separator\n",
    "        print(f\"Full dataset loaded successfully: {len(mixed_raw):,} rows total.\")\n",
    "\n",
    "        try:\n",
    "            # Try to display the first few rows (Jupyter / IPython display)\n",
    "            display(mixed_raw.head())\n",
    "        except Exception:\n",
    "            # If display is not available, fall back to printing as plain text\n",
    "            print(mixed_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        # If DataFrame is None or empty, warn and create an empty DataFrame\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        mixed_raw = pd.DataFrame()\n",
    "else:\n",
    "    # If the file does not exist, report an error and set mixed_raw to empty\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    mixed_raw = pd.DataFrame()\n",
    "\n",
    "# Run garbage collection to free up memory after the load attempt\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTgQiaZRlax0"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1765651122228,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "-xGiqjDilax1",
    "outputId": "fb991520-e3d6-4fc2-8506-227bd6120337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Mixed dataset for 'Depreciation_Depletion__Amortization' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_581638/2687430345.py:85: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y00' 'Q2Y00' ... 'Q1Y10' 'Q2Y10' 'Q4Y10']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1151</td>\n",
       "      <td>38.094417</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1151</td>\n",
       "      <td>68.245694</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1151</td>\n",
       "      <td>89.54678</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1151</td>\n",
       "      <td>98.504938</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1151</td>\n",
       "      <td>84.007512</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode      Value Str_FiscalPrd  \n",
       "0     1151  38.094417           Y92  \n",
       "1     1151  68.245694           Y93  \n",
       "2     1151   89.54678           Y94  \n",
       "3     1151  98.504938           Y95  \n",
       "4     1151  84.007512           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This snippet provides:\n",
    "#\n",
    "# 1. A helper function `last2` that returns the last two digits of a number as a\n",
    "#    zero-padded string (for building YY strings).\n",
    "#\n",
    "# 2. A function `add_str_fiscalprd(df)` which:\n",
    "#    - Works on a copy of an input DataFrame containing financial periods.\n",
    "#    - Normalizes the 'Frequency' (upper-case, no missing).\n",
    "#    - Stores the original 'FiscalPeriod' and converts it to numeric.\n",
    "#    - Creates a string representation 'Str_FiscalPrd' depending on the frequency:\n",
    "#         - Q/C/E/R: quarter-based (\"QnYyy\")\n",
    "#         - A/B: annual (\"Yyy\")\n",
    "#         - F/S: semiannual (\"SnYyy\")\n",
    "#         - K/T/L/U: trimester-like (\"TnYyy\")\n",
    "#    - Derives an implied full-year integer 'ImplFiscPer_Calculated' from the\n",
    "#      two-digit year (80–99 => 19xx, else 20xx).\n",
    "#    - For annual rows (A/B), checks discrepancies between original\n",
    "#      'FiscalPeriod' and implied full-year; prints a small preview & total count.\n",
    "#    - Overwrites 'FiscalPeriod' with 'ImplFiscPer_Calculated' and drops helper\n",
    "#      columns.\n",
    "#\n",
    "# 3. A small driver block that:\n",
    "#    - Checks that `mixed_raw` exists and is non-empty.\n",
    "#    - Applies `add_str_fiscalprd` to produce `mixed_encoded`.\n",
    "#    - Displays a head preview or prints a message and sets `mixed_encoded = None`\n",
    "#      if input is missing/empty.\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    # If input is NaN (or pandas-style missing), return None (no digits)\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Convert to int, format as 4-digit zero-padded string, take last 2 chars\n",
    "    # Example: n=2023 -> \"2023\"[-2:] -> \"23\"\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    \"\"\"\n",
    "    Creates 'Str_FiscalPrd' and overwrites 'FiscalPeriod' with an implied full year.\n",
    "    Also prints discrepancies for Annual (A,B) rows between original and calculated year.\n",
    "\n",
    "    The mapping logic:\n",
    "      - Quarterly (C, Q, E, R):   fp encodes a quarter index -> \"Q{1-4}Y{yy}\"\n",
    "      - Annual   (A, B):         fp is the year itself      -> \"Y{yy}\"\n",
    "      - Semiannual (F, S):       fp encodes half-year       -> \"S{1-2}Y{yy}\"\n",
    "      - Trimester-like (K, T,\n",
    "         L, U):                  fp encodes trimester       -> \"T{1-3}Y{yy}\"\n",
    "    Then we re-infer the full year from the YY part with 19xx/20xx rule.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the original input DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes:\n",
    "    # - uppercase for consistency\n",
    "    # - fill missing values with empty string\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod in a separate column for later comparison\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    # Convert FiscalPeriod to numeric, coerce errors to NaN\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Masks for different frequency groups\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])  # quarter-based\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])            # annual\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])            # semiannual\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])  # trimester-like\n",
    "\n",
    "    # Initialize Str_FiscalPrd as NaN; we will fill per frequency group\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Quarter-based encoding\n",
    "    # -------------------------\n",
    "    # Quarter number: (fp % 4) + 1 => yields 1..4 (if fp is integer-based encoding)\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year part for quarter: integer division by 4, then take last two digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    # Assign \"Q{quarter}Y{yy}\" for quarter frequencies\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" +\n",
    "        q_part.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Annual encoding (A, B)\n",
    "    # -------------------------\n",
    "    # Year part is fp itself for A/B; take last two digits via last2\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    # Assign \"Y{yy}\" for annual frequencies\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    # -------------------------\n",
    "    # Semiannual encoding (F, S)\n",
    "    # -------------------------\n",
    "    # Semester number: (fp % 2) + 1 => 1 or 2\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    # Year part: fp // 2, then last two digits\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    # Assign \"S{sem}Y{yy}\" for F/S\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" +\n",
    "        fs_sem.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Trimester-like encoding (K, T, L, U)\n",
    "    # -------------------------\n",
    "    # Term number: (fp % 3) + 1 => 1..3\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    # Year part: fp // 3, then last two digits\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    # Assign \"T{term}Y{yy}\" for K/T/L/U frequencies\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" +\n",
    "        t_term.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Implied full year from Str_FiscalPrd (19xx / 20xx reconstruction)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Extract two-digit year from \"Yyy\" pattern, e.g., \"Q1Y23\" -> \"23\"\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    # Convert that to numeric (NaN if not parseable)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    # Map two-digit year to full year:\n",
    "    #  - 80–99 => 19xx\n",
    "    #  - else  => 20xx\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Discrepancies check for Annual frequencies (A, B)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Work on a subset of annual frequencies only\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "\n",
    "    # Build boolean mask where implied full year does NOT match the original\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Either they match numerically...\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce'))\n",
    "        |\n",
    "        # ...or both are NaN\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    # Subset rows with discrepancies\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    # If we have any, show a short sample and the total count\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Overwrite FiscalPeriod and drop temporary helper columns\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Replace 'FiscalPeriod' with the implied full-year value we just calculated\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    # Drop helper columns not needed downstream\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to mixed_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'mixed_raw' in globals() and mixed_raw is not None and not mixed_raw.empty:\n",
    "    # Inform which item we are encoding for\n",
    "    print(f\"Applying encoding to Mixed dataset for '{target_item_name}' ...\")\n",
    "    # Apply fiscal period encoding\n",
    "    mixed_encoded = add_str_fiscalprd(mixed_raw)\n",
    "    # Show a preview of the encoded data\n",
    "    display(mixed_encoded.head())\n",
    "else:\n",
    "    # If mixed_raw is missing or empty, we skip and mark mixed_encoded as None\n",
    "    print(\"mixed_raw not found or empty. Cannot perform encoding.\")\n",
    "    mixed_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxDnp4oTlax2"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7235,
     "status": "ok",
     "timestamp": 1765651129464,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "JEpuKKItnefC",
    "outputId": "df42657f-7935-4b3c-b558-c1d59a4d7bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,390,222 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\n",
      "         finite_rows: 2062861\n",
      "                mean: 22266.512812640318\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.66014210935954\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 37,553\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 2025308\n",
      "                mean: 100.40068385378902\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.23677476037444\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Depreciation_Depletion__Amortization_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Depreciation_Depletion__Amortization_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,390,222\n",
      "Excludedby Frequency (E/L/R/U): 0\n",
      "Dropped by quality (Pct rules): 37,553\n",
      "Output rows (final):            3,352,669\n",
      "Check: excluded + dropped + output = 3,390,222\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This script takes an input DataFrame `mixed_encoded` (if present in the global scope)\n",
    "# that contains financial time-series data (per company, item, currency, fiscal period,\n",
    "# and PIT Date). It then:\n",
    "#\n",
    "# 1. Cleans and standardizes key columns (dates, numeric types, string IDs).\n",
    "# 2. Excludes rows with certain frequencies (E/L/R/U).\n",
    "# 3. Parses fiscal period strings into quarter/semester/trimester indicators (QNUM/SNUM/TNUM).\n",
    "# 4. Uses a custom, vectorized \"as-of\" join (`asof_numpy`) to attach the most recent\n",
    "#    annual, quarterly, semiannual, and trimester values for each (ID, HistCurrency,\n",
    "#    ItemCode, FiscalPeriod) up to each row’s PIT Date.\n",
    "# 5. Builds \"full-year\" candidate values from:\n",
    "#       - actual annuals (A),\n",
    "#       - sum of Q1..Q4 (Q4 proxy),\n",
    "#       - sum of S1..S2 (S2 proxy),\n",
    "#       - sum of T1..T3 (T3 proxy),\n",
    "#    and selects the best candidate based on priorities and relationship to the row’s\n",
    "#    fiscal period (same-year vs prior-year).\n",
    "# 6. Computes an annual PIT-based metric `AnnPITValue` and compares it to the “true”\n",
    "#    annual value (`TrueValue`) to derive a percentage `AnnPITValue_Pct` for QC.\n",
    "# 7. Performs quality checks:\n",
    "#       - Ensures no period-date is after the PIT Date.\n",
    "#       - Drops rows whose `AnnPITValue_Pct` is outside the range [50, 200] or infinite.\n",
    "# 8. Keeps a curated set of columns, drops helper columns, and saves:\n",
    "#       - a full output file\n",
    "#       - a subset file with key columns for quick inspection.\n",
    "# 9. Prints row-accounting stats and frees some memory.\n",
    "#\n",
    "# If `mixed_encoded` is not defined or is None, it simply prints a message and exits.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta            # <--- Added to fix NameError\n",
    "from scipy.stats.mstats import winsorize  # <--- Added to fix NameError\n",
    "\n",
    "# Enable pandas \"copy-on-write\" behavior to reduce unintended chained assignment effects\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Helper function to build a string key from multiple columns.\n",
    "    For each row, join the values of 'cols' with '||'.\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    This is a manual, NumPy-based implementation of an \"as-of merge\" grouped by `by_cols`.\n",
    "    It:\n",
    "    - filters out rows with missing required fields,\n",
    "    - normalizes PIT Date to daily precision,\n",
    "    - groups right_df by composite key of `by_cols`,\n",
    "    - for each left row, binary-searches into the matching right group by PIT Date,\n",
    "      picking the last date <= the left PIT Date,\n",
    "    - returns two aligned arrays: (values, dates) for the left_df rows.\n",
    "    \"\"\"\n",
    "    # Initialize outputs with NaNs and NaT for all left_df rows\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns on each side\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask: rows that have all required fields non-null\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If either side has no valid rows, return the default empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on filtered copies only (avoid side effects)\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime at day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build composite keys for group-level match (based on by_cols)\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and date to enable binary search per key\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract NumPy arrays for right side\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and first index for each key in the sorted right side\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Build a dict: key -> (dates array, values array) slice\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]  # start of this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end of this key\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Left side indices and arrays\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left side by key (stable mergesort to preserve original row order within key)\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # i iterates over the sorted left rows\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current composite key\n",
    "        j = i + 1\n",
    "        # Find contiguous block [i:j) for this key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Dates and original positions of this key’s left rows\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # If we have matching right-hand slices for this key, do the as-of search\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # searchsorted(..., 'right') - 1 gives index of last r_date <= block_date\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # positions where such a date exists\n",
    "            if np.any(valid):\n",
    "                # Fill outputs for left rows where we found a valid match\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        # Move to next key block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile calculation: returns quantile q of Series `s`,\n",
    "    or NaN if quantile fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Computes a summary of finite values of a series:\n",
    "      - count\n",
    "      - mean, median\n",
    "      - 1% winsorized mean\n",
    "      - selected deciles (p10, p20, ..., p90)\n",
    "    Infinite values are treated as NaN and removed.\n",
    "    If no finite values remain, returns an empty dict.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop all NaNs\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures the array is writable for winsorize\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Priority for full-year candidates ----------\n",
    "\n",
    "# Fixed priority mapping for full-year candidates:\n",
    "#   'A'  : actual annual value\n",
    "#   'Q4' : annual proxy from four quarters\n",
    "#   'T3' : annual proxy from three trimesters\n",
    "#   'S2' : annual proxy from two semesters\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # highest priority: actual annual\n",
    "    'Q4': 90,  # then Q1+Q2+Q3+Q4\n",
    "    'T3': 80,  # then T1+T2+T3\n",
    "    'S2': 70,  # then S1+S2\n",
    "}\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "# Only run the main logic if a global DataFrame `mixed_encoded` exists and is not None\n",
    "if 'mixed_encoded' in globals() and mixed_encoded is not None:\n",
    "    # Count initial input rows\n",
    "    input_rows = len(mixed_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy of the input dataset\n",
    "    working = mixed_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Exclude certain frequencies (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create mask of rows whose Frequency is one of E, L, R, U (case-insensitive)\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    # Count how many rows will be excluded\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    # Keep only rows that are NOT in the exclusion set\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Set dtypes and normalize important columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert PIT Date to datetime (coerce errors -> NaT), floor to day\n",
    "    working['PIT Date']     = pd.to_datetime(working['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    # FiscalPeriod: numeric (e.g., 2021, 2022, ...)\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    # Value: numeric (float)\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Convert key ID / code columns to string to ensure consistency\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse Q/S/T markers from Str_FiscalPrd (like 'Q1Y2023')\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter number Q1..Q4 from e.g. \"Q1Y2023\" into QNUM\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual number S1..S2 into SNUM\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester number T1..T3 into TNUM\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ensure period columns exist (Q1..Q4, S1..S2, T1..T3, A + their date cols)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create value columns for Q1..Q4, S1..S2, T1..T3, A if they are missing\n",
    "    for c in [*(f'Q{i}' for i in range(1, 5)),\n",
    "              *(f'S{i}' for i in range(1, 3)),\n",
    "              *(f'T{i}' for i in range(1, 4)),\n",
    "              'A']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Create corresponding *_Date columns if missing\n",
    "    for c in [*(f'Q{i}_Date' for i in range(1, 5)),\n",
    "              *(f'S{i}_Date' for i in range(1, 3)),\n",
    "              *(f'T{i}_Date' for i in range(1, 4)),\n",
    "              'A_Date']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Base key for many of the as-of mappings\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Derive TrueValue from annuals (A/B frequencies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Mask annual-like rows where Value is present\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    # annual_src: one row per (ID, FiscalPeriod, HistCurrency) with last PIT Date\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Left-join true annual value back onto working\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping (same FiscalPeriod) for A/Q/S/T\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----- Annual -----\n",
    "    # Source rows for annual frequencies A/B\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    # As-of join: for each working row, get most recent annual value by PIT Date\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period of annual value (same as row's FiscalPeriod when present)\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # ----- Quarterly -----\n",
    "    # Source rows for quarterly frequencies (Q/C) with valid QNUM\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Restrict to a specific quarter q\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        # As-of join for that quarter\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        # Origin FP column for that quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        # Fill origin FP only where quarter value is non-null and origin not yet set\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Semiannual -----\n",
    "    # Source rows for semiannual frequencies (S/F) with valid SNUM\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Trimester -----\n",
    "    # Source rows for trimester frequencies (T/K) with valid TNUM\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels & normalize dates (NO prev-year fill, NO forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort working data consistently for downstream calculations\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    # List of all period value columns\n",
    "    value_cols_all  = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                      [f'S{i}' for i in range(1, 3)] + \\\n",
    "                      [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    # Corresponding date columns\n",
    "    date_cols_all   = [f'{c}_Date' for c in value_cols_all]\n",
    "    # Corresponding origin FP columns\n",
    "    origin_cols_all = [f'{c}_OriginFP' for c in value_cols_all]\n",
    "\n",
    "    # Ensure that all date columns are proper datetimes (floor to day)\n",
    "    # Note: explicitly no groupby-forward-fill here – only asof-filled values remain\n",
    "    for c in date_cols_all:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Build full-year candidates from fixed sets (Q1–Q4, S1–S2, T1–T3)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def full_year_from_fixed(row, labels, pit, cutoff):\n",
    "        \"\"\"\n",
    "        Fixed full-year from given labels (e.g. Q1..Q4, S1..S2, T1..T3):\n",
    "\n",
    "          - Requires ALL labels to have:\n",
    "              value, date, origin_fp.\n",
    "          - All dates must be within [cutoff, pit].\n",
    "          - origin_fp = max(origin_fp_i)  (the 'newest year' across components).\n",
    "\n",
    "        Returns (dt, val, origin_fp) or (NaT, NaN, NaN) if any requirement fails.\n",
    "        \"\"\"\n",
    "        vals, dts, fps = [], [], []\n",
    "        for lbl in labels:\n",
    "            v = row.get(lbl, np.nan)\n",
    "            d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "            o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "            # Require non-null for value, date, origin FP\n",
    "            if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            # Normalize date and check it lies within [cutoff, pit]\n",
    "            d = pd.to_datetime(d, errors='coerce')\n",
    "            if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            vals.append(float(v))\n",
    "            dts.append(d)\n",
    "            fps.append(int(o))\n",
    "\n",
    "        # Sum all component values, pick latest date and max origin FP\n",
    "        total_val = float(np.nansum(vals))\n",
    "        latest_dt = max(dts)\n",
    "        origin_fp = max(fps)\n",
    "        return latest_dt, total_val, origin_fp\n",
    "\n",
    "    def pick_annpit_sum_with_origin(row):\n",
    "        \"\"\"\n",
    "        For each row, pick the best annual PIT-based value (AnnPITValue) by:\n",
    "\n",
    "        1) Building a time window [PIT - 365 days, PIT].\n",
    "        2) Generating candidate annual values:\n",
    "           - actual annual A (if within window and with origin FP),\n",
    "           - Q4 proxy from Q1..Q4,\n",
    "           - S2 proxy from S1..S2,\n",
    "           - T3 proxy from T1..T3,\n",
    "           each with associated priority and origin FP.\n",
    "        3) Ranking candidates according to:\n",
    "           - Same-year A (highest),\n",
    "           - Same-year proxies (Q4/T3/S2),\n",
    "           - Prior-year A,\n",
    "           - Prior-year proxies,\n",
    "           - Otherwise: any remaining candidate by (priority, latest date).\n",
    "        4) Returning the chosen candidate's value (keeping zeros, but filtering NaNs).\n",
    "        \"\"\"\n",
    "        pit = row['PIT Date']\n",
    "        if pd.isna(pit):\n",
    "            return np.nan\n",
    "        cutoff = pit - timedelta(days=365)  # Now works because timedelta is imported\n",
    "\n",
    "        # Current row's fiscal period, cast to int if possible\n",
    "        fp = row.get('FiscalPeriod', np.nan)\n",
    "        try:\n",
    "            fp_int = int(fp) if not pd.isna(fp) else None\n",
    "        except Exception:\n",
    "            fp_int = None\n",
    "\n",
    "        # Collect candidate tuples: (label, priority, date, value, origin_fp)\n",
    "        candidates = []\n",
    "\n",
    "        # --- Candidate A: actual annual (0 is allowed)\n",
    "        A_val = row.get('A', np.nan)\n",
    "        A_dt  = row.get('A_Date', pd.NaT)\n",
    "        A_ofp = row.get('A_OriginFP', np.nan)\n",
    "        if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "            A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "            if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "                candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "        # --- Candidate Q4: sum of Q1..Q4 (0 allowed)\n",
    "        q4_dt, q4_val, q4_fp = full_year_from_fixed(\n",
    "            row, [f'Q{i}' for i in range(1, 5)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "            candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "        # --- Candidate S2: sum of S1..S2 (0 allowed)\n",
    "        s2_dt, s2_val, s2_fp = full_year_from_fixed(\n",
    "            row, [f'S{i}' for i in range(1, 3)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(s2_val) and pd.notna(s2_dt) and not pd.isna(s2_fp):\n",
    "            candidates.append(('S2', _PERIOD_PRIORITY['S2'], s2_dt, float(s2_val), int(s2_fp)))\n",
    "\n",
    "        # --- Candidate T3: sum of T1..T3 (0 allowed)\n",
    "        t3_dt, t3_val, t3_fp = full_year_from_fixed(\n",
    "            row, [f'T{i}' for i in range(1, 4)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(t3_val) and pd.notna(t3_dt) and not pd.isna(t3_fp):\n",
    "            candidates.append(('T3', _PERIOD_PRIORITY['T3'], t3_dt, float(t3_val), int(t3_fp)))\n",
    "\n",
    "        # If no candidates, return NaN\n",
    "        if not candidates:\n",
    "            return np.nan\n",
    "\n",
    "        # Filter out NaN-valued candidates (keep 0-valued ones)\n",
    "        def valid(seq):\n",
    "            return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "        # Relationship between row's fiscal period and candidate's origin FP:\n",
    "        # same / prior / other / unknown\n",
    "        def rel(c):\n",
    "            _, _, _, _, ofp = c\n",
    "            if fp_int is None or ofp is None:\n",
    "                return 'unknown'\n",
    "            if ofp == fp_int:\n",
    "                return 'same'\n",
    "            if ofp == fp_int - 1:\n",
    "                return 'prior'\n",
    "            return 'other'\n",
    "\n",
    "        # 1) Same-year actual annual A (max by date)\n",
    "        same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "        if same_A:\n",
    "            best = max(same_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 2) Same-year proxies (Q4/T3/S2), max by (priority, date)\n",
    "        same_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'same'\n",
    "        )\n",
    "        if same_proxies:\n",
    "            best = max(same_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 3) Prior-year actual annual A\n",
    "        prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "        if prior_A:\n",
    "            best = max(prior_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 4) Prior-year proxies (Q4/S2/T3) if no prior A\n",
    "        prior_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'prior'\n",
    "        )\n",
    "        if prior_proxies:\n",
    "            best = max(prior_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "        others = valid(candidates)\n",
    "        if others:\n",
    "            best = max(others, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # Final fallback: 0.0 (should rarely be reached)\n",
    "        return 0.0\n",
    "\n",
    "    # Apply the selection function row-wise to produce AnnPITValue\n",
    "    working['AnnPITValue'] = working.apply(pick_annpit_sum_with_origin, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check + PRE-DROP stats\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Columns whose dates should not exceed PIT Date\n",
    "    date_cols = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Restrict to ones actually present\n",
    "    present = [c for c in date_cols if c in working.columns]\n",
    "\n",
    "    viol_counts = {}  # per-label violation counts\n",
    "    # Mask for rows with any future-dated period\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        # A violation is when period date > PIT Date (both need to be non-null)\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m  # accumulate violations across columns\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flagrows with at least one future-date error\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct + quality drop\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute AnnPITValue as % of TrueValue (only when TrueValue != 0)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Stats before dropping low-quality rows\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    # Build masks for dropping:\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)  # infinite percentages\n",
    "    is_finite = np.isfinite(pct)\n",
    "    # Out-of-range if % > 200 or % < 50 (but finite)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    # Rows to drop: infinite or out-of-range values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that passed the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Stats after dropping\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns & save\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Base descriptive columns to keep (if present)\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Build ordered list of period date/value columns\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Final set of columns to keep in output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Helper columns to drop before export\n",
    "    drop_cols = ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    # Also drop all *_OriginFP columns\n",
    "    drop_cols += [c for c in working.columns if c.endswith('_OriginFP')]\n",
    "    working.drop(columns=[c for c in drop_cols if c in working.columns],\n",
    "                 inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder and restrict columns to the final layout\n",
    "    mixed_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # Sanity checks: necessary globals must exist\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Build full output path and save pipe-delimited file\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    mixed_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Create a subset for lighter inspection\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "                   \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    # Only keep subset columns that actually exist\n",
    "    subset_cols_existing = [col for col in subset_cols if col in mixed_processed.columns]\n",
    "    subset_df = mixed_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP,\n",
    "                              f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    # Explicitly delete subset_df to free memory\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(mixed_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excludedby Frequency (E/L/R/U): {excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    # Sum up excluded + dropped + remaining and check against original count\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. \"\n",
    "              \"Investigate upstream filtering or unexpected drops.\")\n",
    "\n",
    "    # Trigger garbage collection as a final cleanup step\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # If the main input dataset is not available, skip all processing\n",
    "    print(\"mixed_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNeI0C596evi"
   },
   "source": [
    "### Mixed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdqMzznT6evk"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97,
     "status": "ok",
     "timestamp": 1765651129568,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "WyCTC85a6evk",
    "outputId": "cf6e3b8a-cb32-427f-a131-09eb7144c4e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Mixed_3  ->  ItemName: 'Earnings_Per_Share_Fiscal_Year_End'\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This code selects which Mixed_* dataset should be processed by choosing an index\n",
    "# (e.g., Mixed_1, Mixed_2, ...). It then:\n",
    "#\n",
    "#   1. Ensures that a dictionary `mixed_vars` exists, mapping keys like \"Mixed_1\"\n",
    "#      to item names.\n",
    "#   2. Builds the key corresponding to the selected index and retrieves the\n",
    "#      associated item name (`target_item_name`).\n",
    "#   3. Prints which Mixed_* item was selected.\n",
    "#   4. Constructs paths and filenames based on global variables and the selected item.\n",
    "#   5. Ensures that the output directory exists by creating it if necessary.\n",
    "#\n",
    "# The goal is to centralize selection of a single Mixed_* dataset and prepare paths\n",
    "# for downstream processing.\n",
    "\n",
    "\n",
    "# === Select which Mixed_* item to run ===\n",
    "mixed_index = 3  # Change this to process another dataset (e.g., 10)\n",
    "\n",
    "# Validate that the dictionary of mixed item names exists\n",
    "assert 'mixed_vars' in globals(), \"mixed_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key (e.g., \"Mixed_1\") and retrieve the associated item name\n",
    "item_key = f\"Mixed_{mixed_index}\"\n",
    "target_item_name = mixed_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in mixed_vars.\"\n",
    "\n",
    "# Inform which item was selected\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Construct the name of the input file for the selected item\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Construct the base name for output files (will later be suffixed)\n",
    "base_output_filename = f\"Mixed_{target_item_name}_complete\"\n",
    "\n",
    "# Ensure the output directory exists; create it (including parent dirs) if needed\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qik10gqk6evl"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1765651130191,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "nDEMwi3Y6evm",
    "outputId": "76a35c66-ae61-4687-b156-d88a3c36bab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Earnings_Per_Share_Fiscal_Year_End' ...\n",
      "Full dataset loaded successfully: 8,799,912 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>5202</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>5202</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>5202</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>5202</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>5202</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode  Value  \n",
       "0     5202   0.09  \n",
       "1     5202   0.08  \n",
       "2     5202    0.1  \n",
       "3     5202  -0.04  \n",
       "4     5202   0.03  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block:\n",
    "#   1. Announces the import of a full dataset for the given `target_item_name`.\n",
    "#   2. Checks whether the file at `file_path` exists.\n",
    "#   3. If it exists, calls `import_file_to_dataframe(file_path)` to load the data\n",
    "#      into `mixed_raw`.\n",
    "#   4. If the loaded DataFrame is non-empty, prints a success message including\n",
    "#      the number of rows and shows the first few rows (via display or fallback\n",
    "#      to text printing).\n",
    "#   5. If the load fails or returns an empty DataFrame, prints a warning and\n",
    "#      creates an empty DataFrame.\n",
    "#   6. If the file does not exist, prints an error message and sets `mixed_raw`\n",
    "#      to an empty DataFrame.\n",
    "#   7. Finally, it runs `gc.collect()` to trigger garbage collection and free\n",
    "#      memory.\n",
    "#\n",
    "# Note: All previous emoji symbols in the print statements have been removed.\n",
    "\n",
    "# Inform the user that we are starting the import for this item\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "# Check if the file path exists on disk\n",
    "if os.path.exists(file_path):\n",
    "    # If the file exists, attempt to import it into a DataFrame\n",
    "    mixed_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    # If the DataFrame is successfully loaded and not empty\n",
    "    if mixed_raw is not None and not mixed_raw.empty:\n",
    "        # Print a success message including row count with thousands separator\n",
    "        print(f\"Full dataset loaded successfully: {len(mixed_raw):,} rows total.\")\n",
    "\n",
    "        try:\n",
    "            # Try to display the first few rows (Jupyter / IPython display)\n",
    "            display(mixed_raw.head())\n",
    "        except Exception:\n",
    "            # If display is not available, fall back to printing as plain text\n",
    "            print(mixed_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        # If DataFrame is None or empty, warn and create an empty DataFrame\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        mixed_raw = pd.DataFrame()\n",
    "else:\n",
    "    # If the file does not exist, report an error and set mixed_raw to empty\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    mixed_raw = pd.DataFrame()\n",
    "\n",
    "# Run garbage collection to free up memory after the load attempt\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUVVfe4m7irY"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1765651130758,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "p5LQscDU7ira",
    "outputId": "a31a3585-4758-4888-c3d0-99d1b5d28bb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Mixed dataset for 'Earnings_Per_Share_Fiscal_Year_End' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_581638/2687430345.py:85: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y94' 'Q2Y94' 'Q3Y94' ... 'Q4Y07' 'Q4Y05' 'Q4Y01']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>5202</td>\n",
       "      <td>0.09</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>5202</td>\n",
       "      <td>0.08</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>5202</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>5202</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>5202</td>\n",
       "      <td>0.03</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode  Value Str_FiscalPrd  \n",
       "0     5202   0.09           Y92  \n",
       "1     5202   0.08           Y93  \n",
       "2     5202    0.1           Y94  \n",
       "3     5202  -0.04           Y95  \n",
       "4     5202   0.03           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This snippet provides:\n",
    "#\n",
    "# 1. A helper function `last2` that returns the last two digits of a number as a\n",
    "#    zero-padded string (for building YY strings).\n",
    "#\n",
    "# 2. A function `add_str_fiscalprd(df)` which:\n",
    "#    - Works on a copy of an input DataFrame containing financial periods.\n",
    "#    - Normalizes the 'Frequency' (upper-case, no missing).\n",
    "#    - Stores the original 'FiscalPeriod' and converts it to numeric.\n",
    "#    - Creates a string representation 'Str_FiscalPrd' depending on the frequency:\n",
    "#         - Q/C/E/R: quarter-based (\"QnYyy\")\n",
    "#         - A/B: annual (\"Yyy\")\n",
    "#         - F/S: semiannual (\"SnYyy\")\n",
    "#         - K/T/L/U: trimester-like (\"TnYyy\")\n",
    "#    - Derives an implied full-year integer 'ImplFiscPer_Calculated' from the\n",
    "#      two-digit year (80–99 => 19xx, else 20xx).\n",
    "#    - For annual rows (A/B), checks discrepancies between original\n",
    "#      'FiscalPeriod' and implied full-year; prints a small preview & total count.\n",
    "#    - Overwrites 'FiscalPeriod' with 'ImplFiscPer_Calculated' and drops helper\n",
    "#      columns.\n",
    "#\n",
    "# 3. A small driver block that:\n",
    "#    - Checks that `mixed_raw` exists and is non-empty.\n",
    "#    - Applies `add_str_fiscalprd` to produce `mixed_encoded`.\n",
    "#    - Displays a head preview or prints a message and sets `mixed_encoded = None`\n",
    "#      if input is missing/empty.\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    # If input is NaN (or pandas-style missing), return None (no digits)\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Convert to int, format as 4-digit zero-padded string, take last 2 chars\n",
    "    # Example: n=2023 -> \"2023\"[-2:] -> \"23\"\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    \"\"\"\n",
    "    Creates 'Str_FiscalPrd' and overwrites 'FiscalPeriod' with an implied full year.\n",
    "    Also prints discrepancies for Annual (A,B) rows between original and calculated year.\n",
    "\n",
    "    The mapping logic:\n",
    "      - Quarterly (C, Q, E, R):   fp encodes a quarter index -> \"Q{1-4}Y{yy}\"\n",
    "      - Annual   (A, B):         fp is the year itself      -> \"Y{yy}\"\n",
    "      - Semiannual (F, S):       fp encodes half-year       -> \"S{1-2}Y{yy}\"\n",
    "      - Trimester-like (K, T,\n",
    "         L, U):                  fp encodes trimester       -> \"T{1-3}Y{yy}\"\n",
    "    Then we re-infer the full year from the YY part with 19xx/20xx rule.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the original input DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes:\n",
    "    # - uppercase for consistency\n",
    "    # - fill missing values with empty string\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod in a separate column for later comparison\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    # Convert FiscalPeriod to numeric, coerce errors to NaN\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Masks for different frequency groups\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])  # quarter-based\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])            # annual\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])            # semiannual\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])  # trimester-like\n",
    "\n",
    "    # Initialize Str_FiscalPrd as NaN; we will fill per frequency group\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Quarter-based encoding\n",
    "    # -------------------------\n",
    "    # Quarter number: (fp % 4) + 1 => yields 1..4 (if fp is integer-based encoding)\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year part for quarter: integer division by 4, then take last two digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    # Assign \"Q{quarter}Y{yy}\" for quarter frequencies\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" +\n",
    "        q_part.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Annual encoding (A, B)\n",
    "    # -------------------------\n",
    "    # Year part is fp itself for A/B; take last two digits via last2\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    # Assign \"Y{yy}\" for annual frequencies\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    # -------------------------\n",
    "    # Semiannual encoding (F, S)\n",
    "    # -------------------------\n",
    "    # Semester number: (fp % 2) + 1 => 1 or 2\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    # Year part: fp // 2, then last two digits\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    # Assign \"S{sem}Y{yy}\" for F/S\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" +\n",
    "        fs_sem.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Trimester-like encoding (K, T, L, U)\n",
    "    # -------------------------\n",
    "    # Term number: (fp % 3) + 1 => 1..3\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    # Year part: fp // 3, then last two digits\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    # Assign \"T{term}Y{yy}\" for K/T/L/U frequencies\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" +\n",
    "        t_term.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Implied full year from Str_FiscalPrd (19xx / 20xx reconstruction)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Extract two-digit year from \"Yyy\" pattern, e.g., \"Q1Y23\" -> \"23\"\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    # Convert that to numeric (NaN if not parseable)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    # Map two-digit year to full year:\n",
    "    #  - 80–99 => 19xx\n",
    "    #  - else  => 20xx\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Discrepancies check for Annual frequencies (A, B)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Work on a subset of annual frequencies only\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "\n",
    "    # Build boolean mask where implied full year does NOT match the original\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Either they match numerically...\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce'))\n",
    "        |\n",
    "        # ...or both are NaN\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    # Subset rows with discrepancies\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    # If we have any, show a short sample and the total count\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Overwrite FiscalPeriod and drop temporary helper columns\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Replace 'FiscalPeriod' with the implied full-year value we just calculated\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    # Drop helper columns not needed downstream\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to mixed_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'mixed_raw' in globals() and mixed_raw is not None and not mixed_raw.empty:\n",
    "    # Inform which item we are encoding for\n",
    "    print(f\"Applying encoding to Mixed dataset for '{target_item_name}' ...\")\n",
    "    # Apply fiscal period encoding\n",
    "    mixed_encoded = add_str_fiscalprd(mixed_raw)\n",
    "    # Show a preview of the encoded data\n",
    "    display(mixed_encoded.head())\n",
    "else:\n",
    "    # If mixed_raw is missing or empty, we skip and mark mixed_encoded as None\n",
    "    print(\"mixed_raw not found or empty. Cannot perform encoding.\")\n",
    "    mixed_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl2F2O9Z7irb"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15498,
     "status": "ok",
     "timestamp": 1765651146257,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "RP1byFMG7irc",
    "outputId": "3a67b4ce-371e-4dff-ff4b-c3e715123dbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 8,799,912 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\n",
      "         finite_rows: 6394790\n",
      "                mean: 7176573831.922597\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 149.79110460094466\n",
      "                 p10: 33.774834437086085\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.75566750629723\n",
      "                 p80: 121.15601310083875\n",
      "                 p90: 200.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 1,320,622\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 5074168\n",
      "                mean: 107.66009907590976\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 107.7308091857585\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 105.3650810551814\n",
      "                 p90: 132.40732576813164\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Earnings_Per_Share_Fiscal_Year_End_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Earnings_Per_Share_Fiscal_Year_End_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     8,799,912\n",
      "Excludedby Frequency (E/L/R/U): 0\n",
      "Dropped by quality (Pct rules): 1,320,622\n",
      "Output rows (final):            7,479,290\n",
      "Check: excluded + dropped + output = 8,799,912\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This script takes an input DataFrame `mixed_encoded` (if present in the global scope)\n",
    "# that contains financial time-series data (per company, item, currency, fiscal period,\n",
    "# and PIT Date). It then:\n",
    "#\n",
    "# 1. Cleans and standardizes key columns (dates, numeric types, string IDs).\n",
    "# 2. Excludes rows with certain frequencies (E/L/R/U).\n",
    "# 3. Parses fiscal period strings into quarter/semester/trimester indicators (QNUM/SNUM/TNUM).\n",
    "# 4. Uses a custom, vectorized \"as-of\" join (`asof_numpy`) to attach the most recent\n",
    "#    annual, quarterly, semiannual, and trimester values for each (ID, HistCurrency,\n",
    "#    ItemCode, FiscalPeriod) up to each row’s PIT Date.\n",
    "# 5. Builds \"full-year\" candidate values from:\n",
    "#       - actual annuals (A),\n",
    "#       - sum of Q1..Q4 (Q4 proxy),\n",
    "#       - sum of S1..S2 (S2 proxy),\n",
    "#       - sum of T1..T3 (T3 proxy),\n",
    "#    and selects the best candidate based on priorities and relationship to the row’s\n",
    "#    fiscal period (same-year vs prior-year).\n",
    "# 6. Computes an annual PIT-based metric `AnnPITValue` and compares it to the “true”\n",
    "#    annual value (`TrueValue`) to derive a percentage `AnnPITValue_Pct` for QC.\n",
    "# 7. Performs quality checks:\n",
    "#       - Ensures no period-date is after the PIT Date.\n",
    "#       - Drops rows whose `AnnPITValue_Pct` is outside the range [50, 200] or infinite.\n",
    "# 8. Keeps a curated set of columns, drops helper columns, and saves:\n",
    "#       - a full output file\n",
    "#       - a subset file with key columns for quick inspection.\n",
    "# 9. Prints row-accounting stats and frees some memory.\n",
    "#\n",
    "# If `mixed_encoded` is not defined or is None, it simply prints a message and exits.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta            # <--- Added to fix NameError\n",
    "from scipy.stats.mstats import winsorize  # <--- Added to fix NameError\n",
    "\n",
    "# Enable pandas \"copy-on-write\" behavior to reduce unintended chained assignment effects\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Helper function to build a string key from multiple columns.\n",
    "    For each row, join the values of 'cols' with '||'.\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    This is a manual, NumPy-based implementation of an \"as-of merge\" grouped by `by_cols`.\n",
    "    It:\n",
    "    - filters out rows with missing required fields,\n",
    "    - normalizes PIT Date to daily precision,\n",
    "    - groups right_df by composite key of `by_cols`,\n",
    "    - for each left row, binary-searches into the matching right group by PIT Date,\n",
    "      picking the last date <= the left PIT Date,\n",
    "    - returns two aligned arrays: (values, dates) for the left_df rows.\n",
    "    \"\"\"\n",
    "    # Initialize outputs with NaNs and NaT for all left_df rows\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns on each side\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask: rows that have all required fields non-null\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If either side has no valid rows, return the default empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on filtered copies only (avoid side effects)\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime at day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build composite keys for group-level match (based on by_cols)\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and date to enable binary search per key\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract NumPy arrays for right side\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and first index for each key in the sorted right side\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Build a dict: key -> (dates array, values array) slice\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]  # start of this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end of this key\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Left side indices and arrays\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left side by key (stable mergesort to preserve original row order within key)\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # i iterates over the sorted left rows\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current composite key\n",
    "        j = i + 1\n",
    "        # Find contiguous block [i:j) for this key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Dates and original positions of this key’s left rows\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # If we have matching right-hand slices for this key, do the as-of search\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # searchsorted(..., 'right') - 1 gives index of last r_date <= block_date\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # positions where such a date exists\n",
    "            if np.any(valid):\n",
    "                # Fill outputs for left rows where we found a valid match\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        # Move to next key block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile calculation: returns quantile q of Series `s`,\n",
    "    or NaN if quantile fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Computes a summary of finite values of a series:\n",
    "      - count\n",
    "      - mean, median\n",
    "      - 1% winsorized mean\n",
    "      - selected deciles (p10, p20, ..., p90)\n",
    "    Infinite values are treated as NaN and removed.\n",
    "    If no finite values remain, returns an empty dict.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop all NaNs\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures the array is writable for winsorize\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Priority for full-year candidates ----------\n",
    "\n",
    "# Fixed priority mapping for full-year candidates:\n",
    "#   'A'  : actual annual value\n",
    "#   'Q4' : annual proxy from four quarters\n",
    "#   'T3' : annual proxy from three trimesters\n",
    "#   'S2' : annual proxy from two semesters\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # highest priority: actual annual\n",
    "    'Q4': 90,  # then Q1+Q2+Q3+Q4\n",
    "    'T3': 80,  # then T1+T2+T3\n",
    "    'S2': 70,  # then S1+S2\n",
    "}\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "# Only run the main logic if a global DataFrame `mixed_encoded` exists and is not None\n",
    "if 'mixed_encoded' in globals() and mixed_encoded is not None:\n",
    "    # Count initial input rows\n",
    "    input_rows = len(mixed_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy of the input dataset\n",
    "    working = mixed_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Exclude certain frequencies (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create mask of rows whose Frequency is one of E, L, R, U (case-insensitive)\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    # Count how many rows will be excluded\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    # Keep only rows that are NOT in the exclusion set\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Set dtypes and normalize important columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert PIT Date to datetime (coerce errors -> NaT), floor to day\n",
    "    working['PIT Date']     = pd.to_datetime(working['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    # FiscalPeriod: numeric (e.g., 2021, 2022, ...)\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    # Value: numeric (float)\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Convert key ID / code columns to string to ensure consistency\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse Q/S/T markers from Str_FiscalPrd (like 'Q1Y2023')\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter number Q1..Q4 from e.g. \"Q1Y2023\" into QNUM\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual number S1..S2 into SNUM\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester number T1..T3 into TNUM\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ensure period columns exist (Q1..Q4, S1..S2, T1..T3, A + their date cols)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create value columns for Q1..Q4, S1..S2, T1..T3, A if they are missing\n",
    "    for c in [*(f'Q{i}' for i in range(1, 5)),\n",
    "              *(f'S{i}' for i in range(1, 3)),\n",
    "              *(f'T{i}' for i in range(1, 4)),\n",
    "              'A']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Create corresponding *_Date columns if missing\n",
    "    for c in [*(f'Q{i}_Date' for i in range(1, 5)),\n",
    "              *(f'S{i}_Date' for i in range(1, 3)),\n",
    "              *(f'T{i}_Date' for i in range(1, 4)),\n",
    "              'A_Date']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Base key for many of the as-of mappings\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Derive TrueValue from annuals (A/B frequencies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Mask annual-like rows where Value is present\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    # annual_src: one row per (ID, FiscalPeriod, HistCurrency) with last PIT Date\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Left-join true annual value back onto working\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping (same FiscalPeriod) for A/Q/S/T\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----- Annual -----\n",
    "    # Source rows for annual frequencies A/B\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    # As-of join: for each working row, get most recent annual value by PIT Date\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period of annual value (same as row's FiscalPeriod when present)\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # ----- Quarterly -----\n",
    "    # Source rows for quarterly frequencies (Q/C) with valid QNUM\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Restrict to a specific quarter q\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        # As-of join for that quarter\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        # Origin FP column for that quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        # Fill origin FP only where quarter value is non-null and origin not yet set\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Semiannual -----\n",
    "    # Source rows for semiannual frequencies (S/F) with valid SNUM\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Trimester -----\n",
    "    # Source rows for trimester frequencies (T/K) with valid TNUM\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels & normalize dates (NO prev-year fill, NO forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort working data consistently for downstream calculations\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    # List of all period value columns\n",
    "    value_cols_all  = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                      [f'S{i}' for i in range(1, 3)] + \\\n",
    "                      [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    # Corresponding date columns\n",
    "    date_cols_all   = [f'{c}_Date' for c in value_cols_all]\n",
    "    # Corresponding origin FP columns\n",
    "    origin_cols_all = [f'{c}_OriginFP' for c in value_cols_all]\n",
    "\n",
    "    # Ensure that all date columns are proper datetimes (floor to day)\n",
    "    # Note: explicitly no groupby-forward-fill here – only asof-filled values remain\n",
    "    for c in date_cols_all:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Build full-year candidates from fixed sets (Q1–Q4, S1–S2, T1–T3)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def full_year_from_fixed(row, labels, pit, cutoff):\n",
    "        \"\"\"\n",
    "        Fixed full-year from given labels (e.g. Q1..Q4, S1..S2, T1..T3):\n",
    "\n",
    "          - Requires ALL labels to have:\n",
    "              value, date, origin_fp.\n",
    "          - All dates must be within [cutoff, pit].\n",
    "          - origin_fp = max(origin_fp_i)  (the 'newest year' across components).\n",
    "\n",
    "        Returns (dt, val, origin_fp) or (NaT, NaN, NaN) if any requirement fails.\n",
    "        \"\"\"\n",
    "        vals, dts, fps = [], [], []\n",
    "        for lbl in labels:\n",
    "            v = row.get(lbl, np.nan)\n",
    "            d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "            o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "            # Require non-null for value, date, origin FP\n",
    "            if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            # Normalize date and check it lies within [cutoff, pit]\n",
    "            d = pd.to_datetime(d, errors='coerce')\n",
    "            if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            vals.append(float(v))\n",
    "            dts.append(d)\n",
    "            fps.append(int(o))\n",
    "\n",
    "        # Sum all component values, pick latest date and max origin FP\n",
    "        total_val = float(np.nansum(vals))\n",
    "        latest_dt = max(dts)\n",
    "        origin_fp = max(fps)\n",
    "        return latest_dt, total_val, origin_fp\n",
    "\n",
    "    def pick_annpit_sum_with_origin(row):\n",
    "        \"\"\"\n",
    "        For each row, pick the best annual PIT-based value (AnnPITValue) by:\n",
    "\n",
    "        1) Building a time window [PIT - 365 days, PIT].\n",
    "        2) Generating candidate annual values:\n",
    "           - actual annual A (if within window and with origin FP),\n",
    "           - Q4 proxy from Q1..Q4,\n",
    "           - S2 proxy from S1..S2,\n",
    "           - T3 proxy from T1..T3,\n",
    "           each with associated priority and origin FP.\n",
    "        3) Ranking candidates according to:\n",
    "           - Same-year A (highest),\n",
    "           - Same-year proxies (Q4/T3/S2),\n",
    "           - Prior-year A,\n",
    "           - Prior-year proxies,\n",
    "           - Otherwise: any remaining candidate by (priority, latest date).\n",
    "        4) Returning the chosen candidate's value (keeping zeros, but filtering NaNs).\n",
    "        \"\"\"\n",
    "        pit = row['PIT Date']\n",
    "        if pd.isna(pit):\n",
    "            return np.nan\n",
    "        cutoff = pit - timedelta(days=365)  # Now works because timedelta is imported\n",
    "\n",
    "        # Current row's fiscal period, cast to int if possible\n",
    "        fp = row.get('FiscalPeriod', np.nan)\n",
    "        try:\n",
    "            fp_int = int(fp) if not pd.isna(fp) else None\n",
    "        except Exception:\n",
    "            fp_int = None\n",
    "\n",
    "        # Collect candidate tuples: (label, priority, date, value, origin_fp)\n",
    "        candidates = []\n",
    "\n",
    "        # --- Candidate A: actual annual (0 is allowed)\n",
    "        A_val = row.get('A', np.nan)\n",
    "        A_dt  = row.get('A_Date', pd.NaT)\n",
    "        A_ofp = row.get('A_OriginFP', np.nan)\n",
    "        if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "            A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "            if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "                candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "        # --- Candidate Q4: sum of Q1..Q4 (0 allowed)\n",
    "        q4_dt, q4_val, q4_fp = full_year_from_fixed(\n",
    "            row, [f'Q{i}' for i in range(1, 5)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "            candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "        # --- Candidate S2: sum of S1..S2 (0 allowed)\n",
    "        s2_dt, s2_val, s2_fp = full_year_from_fixed(\n",
    "            row, [f'S{i}' for i in range(1, 3)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(s2_val) and pd.notna(s2_dt) and not pd.isna(s2_fp):\n",
    "            candidates.append(('S2', _PERIOD_PRIORITY['S2'], s2_dt, float(s2_val), int(s2_fp)))\n",
    "\n",
    "        # --- Candidate T3: sum of T1..T3 (0 allowed)\n",
    "        t3_dt, t3_val, t3_fp = full_year_from_fixed(\n",
    "            row, [f'T{i}' for i in range(1, 4)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(t3_val) and pd.notna(t3_dt) and not pd.isna(t3_fp):\n",
    "            candidates.append(('T3', _PERIOD_PRIORITY['T3'], t3_dt, float(t3_val), int(t3_fp)))\n",
    "\n",
    "        # If no candidates, return NaN\n",
    "        if not candidates:\n",
    "            return np.nan\n",
    "\n",
    "        # Filter out NaN-valued candidates (keep 0-valued ones)\n",
    "        def valid(seq):\n",
    "            return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "        # Relationship between row's fiscal period and candidate's origin FP:\n",
    "        # same / prior / other / unknown\n",
    "        def rel(c):\n",
    "            _, _, _, _, ofp = c\n",
    "            if fp_int is None or ofp is None:\n",
    "                return 'unknown'\n",
    "            if ofp == fp_int:\n",
    "                return 'same'\n",
    "            if ofp == fp_int - 1:\n",
    "                return 'prior'\n",
    "            return 'other'\n",
    "\n",
    "        # 1) Same-year actual annual A (max by date)\n",
    "        same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "        if same_A:\n",
    "            best = max(same_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 2) Same-year proxies (Q4/T3/S2), max by (priority, date)\n",
    "        same_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'same'\n",
    "        )\n",
    "        if same_proxies:\n",
    "            best = max(same_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 3) Prior-year actual annual A\n",
    "        prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "        if prior_A:\n",
    "            best = max(prior_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 4) Prior-year proxies (Q4/S2/T3) if no prior A\n",
    "        prior_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'prior'\n",
    "        )\n",
    "        if prior_proxies:\n",
    "            best = max(prior_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "        others = valid(candidates)\n",
    "        if others:\n",
    "            best = max(others, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # Final fallback: 0.0 (should rarely be reached)\n",
    "        return 0.0\n",
    "\n",
    "    # Apply the selection function row-wise to produce AnnPITValue\n",
    "    working['AnnPITValue'] = working.apply(pick_annpit_sum_with_origin, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check + PRE-DROP stats\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Columns whose dates should not exceed PIT Date\n",
    "    date_cols = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Restrict to ones actually present\n",
    "    present = [c for c in date_cols if c in working.columns]\n",
    "\n",
    "    viol_counts = {}  # per-label violation counts\n",
    "    # Mask for rows with any future-dated period\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        # A violation is when period date > PIT Date (both need to be non-null)\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m  # accumulate violations across columns\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flagrows with at least one future-date error\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct + quality drop\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute AnnPITValue as % of TrueValue (only when TrueValue != 0)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Stats before dropping low-quality rows\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    # Build masks for dropping:\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)  # infinite percentages\n",
    "    is_finite = np.isfinite(pct)\n",
    "    # Out-of-range if % > 200 or % < 50 (but finite)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    # Rows to drop: infinite or out-of-range values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that passed the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Stats after dropping\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns & save\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Base descriptive columns to keep (if present)\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Build ordered list of period date/value columns\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Final set of columns to keep in output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Helper columns to drop before export\n",
    "    drop_cols = ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    # Also drop all *_OriginFP columns\n",
    "    drop_cols += [c for c in working.columns if c.endswith('_OriginFP')]\n",
    "    working.drop(columns=[c for c in drop_cols if c in working.columns],\n",
    "                 inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder and restrict columns to the final layout\n",
    "    mixed_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # Sanity checks: necessary globals must exist\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Build full output path and save pipe-delimited file\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    mixed_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Create a subset for lighter inspection\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "                   \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    # Only keep subset columns that actually exist\n",
    "    subset_cols_existing = [col for col in subset_cols if col in mixed_processed.columns]\n",
    "    subset_df = mixed_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP,\n",
    "                              f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    # Explicitly delete subset_df to free memory\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(mixed_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excludedby Frequency (E/L/R/U): {excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    # Sum up excluded + dropped + remaining and check against original count\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. \"\n",
    "              \"Investigate upstream filtering or unexpected drops.\")\n",
    "\n",
    "    # Trigger garbage collection as a final cleanup step\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # If the main input dataset is not available, skip all processing\n",
    "    print(\"mixed_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy618EX27f-z"
   },
   "source": [
    "### Mixed 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1P0EPkJld6L"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1765651146469,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "ofDhB9pZld6M",
    "outputId": "a49234be-897a-41c1-e271-0c4d88343707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Mixed_4  ->  ItemName: 'Income_Taxes'\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This code selects which Mixed_* dataset should be processed by choosing an index\n",
    "# (e.g., Mixed_1, Mixed_2, ...). It then:\n",
    "#\n",
    "#   1. Ensures that a dictionary `mixed_vars` exists, mapping keys like \"Mixed_1\"\n",
    "#      to item names.\n",
    "#   2. Builds the key corresponding to the selected index and retrieves the\n",
    "#      associated item name (`target_item_name`).\n",
    "#   3. Prints which Mixed_* item was selected.\n",
    "#   4. Constructs paths and filenames based on global variables and the selected item.\n",
    "#   5. Ensures that the output directory exists by creating it if necessary.\n",
    "#\n",
    "# The goal is to centralize selection of a single Mixed_* dataset and prepare paths\n",
    "# for downstream processing.\n",
    "\n",
    "\n",
    "# === Select which Mixed_* item to run ===\n",
    "mixed_index = 4  # Change this to process another dataset (e.g., 10)\n",
    "\n",
    "# Validate that the dictionary of mixed item names exists\n",
    "assert 'mixed_vars' in globals(), \"mixed_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key (e.g., \"Mixed_1\") and retrieve the associated item name\n",
    "item_key = f\"Mixed_{mixed_index}\"\n",
    "target_item_name = mixed_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in mixed_vars.\"\n",
    "\n",
    "# Inform which item was selected\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Construct the name of the input file for the selected item\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Construct the base name for output files (will later be suffixed)\n",
    "base_output_filename = f\"Mixed_{target_item_name}_complete\"\n",
    "\n",
    "# Ensure the output directory exists; create it (including parent dirs) if needed\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVPkkyBPld6M"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 948,
     "status": "ok",
     "timestamp": 1765651147424,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "g9aK5y4zld6N",
    "outputId": "ac1302f6-a1e3-4042-9c91-a4ea3b5802c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Income_Taxes' ...\n",
      "Full dataset loaded successfully: 3,556,775 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1451</td>\n",
       "      <td>24.615657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1451</td>\n",
       "      <td>20.007064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1451</td>\n",
       "      <td>26.888541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1451</td>\n",
       "      <td>4.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1451</td>\n",
       "      <td>2.526497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode      Value  \n",
       "0     1451  24.615657  \n",
       "1     1451  20.007064  \n",
       "2     1451  26.888541  \n",
       "3     1451      4.741  \n",
       "4     1451   2.526497  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block:\n",
    "#   1. Announces the import of a full dataset for the given `target_item_name`.\n",
    "#   2. Checks whether the file at `file_path` exists.\n",
    "#   3. If it exists, calls `import_file_to_dataframe(file_path)` to load the data\n",
    "#      into `mixed_raw`.\n",
    "#   4. If the loaded DataFrame is non-empty, prints a success message including\n",
    "#      the number of rows and shows the first few rows (via display or fallback\n",
    "#      to text printing).\n",
    "#   5. If the load fails or returns an empty DataFrame, prints a warning and\n",
    "#      creates an empty DataFrame.\n",
    "#   6. If the file does not exist, prints an error message and sets `mixed_raw`\n",
    "#      to an empty DataFrame.\n",
    "#   7. Finally, it runs `gc.collect()` to trigger garbage collection and free\n",
    "#      memory.\n",
    "#\n",
    "# Note: All previous emoji symbols in the print statements have been removed.\n",
    "\n",
    "# Inform the user that we are starting the import for this item\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "# Check if the file path exists on disk\n",
    "if os.path.exists(file_path):\n",
    "    # If the file exists, attempt to import it into a DataFrame\n",
    "    mixed_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    # If the DataFrame is successfully loaded and not empty\n",
    "    if mixed_raw is not None and not mixed_raw.empty:\n",
    "        # Print a success message including row count with thousands separator\n",
    "        print(f\"Full dataset loaded successfully: {len(mixed_raw):,} rows total.\")\n",
    "\n",
    "        try:\n",
    "            # Try to display the first few rows (Jupyter / IPython display)\n",
    "            display(mixed_raw.head())\n",
    "        except Exception:\n",
    "            # If display is not available, fall back to printing as plain text\n",
    "            print(mixed_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        # If DataFrame is None or empty, warn and create an empty DataFrame\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        mixed_raw = pd.DataFrame()\n",
    "else:\n",
    "    # If the file does not exist, report an error and set mixed_raw to empty\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    mixed_raw = pd.DataFrame()\n",
    "\n",
    "# Run garbage collection to free up memory after the load attempt\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23tAphkild6N"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1765651147665,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "vR0XNOjfld6O",
    "outputId": "3e3b4319-2d1f-4110-8add-af9e998c91ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Mixed dataset for 'Income_Taxes' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_581638/2687430345.py:85: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q3Y01' 'Q1Y02' 'Q1Y03' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1451</td>\n",
       "      <td>24.615657</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1451</td>\n",
       "      <td>20.007064</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1451</td>\n",
       "      <td>26.888541</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1451</td>\n",
       "      <td>4.741</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1451</td>\n",
       "      <td>2.526497</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode      Value Str_FiscalPrd  \n",
       "0     1451  24.615657           Y92  \n",
       "1     1451  20.007064           Y93  \n",
       "2     1451  26.888541           Y94  \n",
       "3     1451      4.741           Y95  \n",
       "4     1451   2.526497           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This snippet provides:\n",
    "#\n",
    "# 1. A helper function `last2` that returns the last two digits of a number as a\n",
    "#    zero-padded string (for building YY strings).\n",
    "#\n",
    "# 2. A function `add_str_fiscalprd(df)` which:\n",
    "#    - Works on a copy of an input DataFrame containing financial periods.\n",
    "#    - Normalizes the 'Frequency' (upper-case, no missing).\n",
    "#    - Stores the original 'FiscalPeriod' and converts it to numeric.\n",
    "#    - Creates a string representation 'Str_FiscalPrd' depending on the frequency:\n",
    "#         - Q/C/E/R: quarter-based (\"QnYyy\")\n",
    "#         - A/B: annual (\"Yyy\")\n",
    "#         - F/S: semiannual (\"SnYyy\")\n",
    "#         - K/T/L/U: trimester-like (\"TnYyy\")\n",
    "#    - Derives an implied full-year integer 'ImplFiscPer_Calculated' from the\n",
    "#      two-digit year (80–99 => 19xx, else 20xx).\n",
    "#    - For annual rows (A/B), checks discrepancies between original\n",
    "#      'FiscalPeriod' and implied full-year; prints a small preview & total count.\n",
    "#    - Overwrites 'FiscalPeriod' with 'ImplFiscPer_Calculated' and drops helper\n",
    "#      columns.\n",
    "#\n",
    "# 3. A small driver block that:\n",
    "#    - Checks that `mixed_raw` exists and is non-empty.\n",
    "#    - Applies `add_str_fiscalprd` to produce `mixed_encoded`.\n",
    "#    - Displays a head preview or prints a message and sets `mixed_encoded = None`\n",
    "#      if input is missing/empty.\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    # If input is NaN (or pandas-style missing), return None (no digits)\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Convert to int, format as 4-digit zero-padded string, take last 2 chars\n",
    "    # Example: n=2023 -> \"2023\"[-2:] -> \"23\"\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    \"\"\"\n",
    "    Creates 'Str_FiscalPrd' and overwrites 'FiscalPeriod' with an implied full year.\n",
    "    Also prints discrepancies for Annual (A,B) rows between original and calculated year.\n",
    "\n",
    "    The mapping logic:\n",
    "      - Quarterly (C, Q, E, R):   fp encodes a quarter index -> \"Q{1-4}Y{yy}\"\n",
    "      - Annual   (A, B):         fp is the year itself      -> \"Y{yy}\"\n",
    "      - Semiannual (F, S):       fp encodes half-year       -> \"S{1-2}Y{yy}\"\n",
    "      - Trimester-like (K, T,\n",
    "         L, U):                  fp encodes trimester       -> \"T{1-3}Y{yy}\"\n",
    "    Then we re-infer the full year from the YY part with 19xx/20xx rule.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the original input DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes:\n",
    "    # - uppercase for consistency\n",
    "    # - fill missing values with empty string\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod in a separate column for later comparison\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    # Convert FiscalPeriod to numeric, coerce errors to NaN\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Masks for different frequency groups\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])  # quarter-based\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])            # annual\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])            # semiannual\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])  # trimester-like\n",
    "\n",
    "    # Initialize Str_FiscalPrd as NaN; we will fill per frequency group\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Quarter-based encoding\n",
    "    # -------------------------\n",
    "    # Quarter number: (fp % 4) + 1 => yields 1..4 (if fp is integer-based encoding)\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year part for quarter: integer division by 4, then take last two digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    # Assign \"Q{quarter}Y{yy}\" for quarter frequencies\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" +\n",
    "        q_part.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Annual encoding (A, B)\n",
    "    # -------------------------\n",
    "    # Year part is fp itself for A/B; take last two digits via last2\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    # Assign \"Y{yy}\" for annual frequencies\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    # -------------------------\n",
    "    # Semiannual encoding (F, S)\n",
    "    # -------------------------\n",
    "    # Semester number: (fp % 2) + 1 => 1 or 2\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    # Year part: fp // 2, then last two digits\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    # Assign \"S{sem}Y{yy}\" for F/S\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" +\n",
    "        fs_sem.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Trimester-like encoding (K, T, L, U)\n",
    "    # -------------------------\n",
    "    # Term number: (fp % 3) + 1 => 1..3\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    # Year part: fp // 3, then last two digits\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    # Assign \"T{term}Y{yy}\" for K/T/L/U frequencies\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" +\n",
    "        t_term.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Implied full year from Str_FiscalPrd (19xx / 20xx reconstruction)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Extract two-digit year from \"Yyy\" pattern, e.g., \"Q1Y23\" -> \"23\"\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    # Convert that to numeric (NaN if not parseable)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    # Map two-digit year to full year:\n",
    "    #  - 80–99 => 19xx\n",
    "    #  - else  => 20xx\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Discrepancies check for Annual frequencies (A, B)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Work on a subset of annual frequencies only\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "\n",
    "    # Build boolean mask where implied full year does NOT match the original\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Either they match numerically...\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce'))\n",
    "        |\n",
    "        # ...or both are NaN\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    # Subset rows with discrepancies\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    # If we have any, show a short sample and the total count\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Overwrite FiscalPeriod and drop temporary helper columns\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Replace 'FiscalPeriod' with the implied full-year value we just calculated\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    # Drop helper columns not needed downstream\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to mixed_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'mixed_raw' in globals() and mixed_raw is not None and not mixed_raw.empty:\n",
    "    # Inform which item we are encoding for\n",
    "    print(f\"Applying encoding to Mixed dataset for '{target_item_name}' ...\")\n",
    "    # Apply fiscal period encoding\n",
    "    mixed_encoded = add_str_fiscalprd(mixed_raw)\n",
    "    # Show a preview of the encoded data\n",
    "    display(mixed_encoded.head())\n",
    "else:\n",
    "    # If mixed_raw is missing or empty, we skip and mark mixed_encoded as None\n",
    "    print(\"mixed_raw not found or empty. Cannot perform encoding.\")\n",
    "    mixed_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt2R_38_ld6O"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7790,
     "status": "ok",
     "timestamp": 1765651155458,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Uh-FgHMpnkru",
    "outputId": "4a7365c0-29d9-4c31-c078-491de34da36a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,556,775 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\n",
      "         finite_rows: 1834011\n",
      "                mean: 24484.88774335648\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 98.53536041208515\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 51,482\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 1782529\n",
      "                mean: 100.26270636271235\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.12720799405506\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Income_Taxes_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Income_Taxes_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,556,775\n",
      "Excludedby Frequency (E/L/R/U): 0\n",
      "Dropped by quality (Pct rules): 51,482\n",
      "Output rows (final):            3,505,293\n",
      "Check: excluded + dropped + output = 3,556,775\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This script takes an input DataFrame `mixed_encoded` (if present in the global scope)\n",
    "# that contains financial time-series data (per company, item, currency, fiscal period,\n",
    "# and PIT Date). It then:\n",
    "#\n",
    "# 1. Cleans and standardizes key columns (dates, numeric types, string IDs).\n",
    "# 2. Excludes rows with certain frequencies (E/L/R/U).\n",
    "# 3. Parses fiscal period strings into quarter/semester/trimester indicators (QNUM/SNUM/TNUM).\n",
    "# 4. Uses a custom, vectorized \"as-of\" join (`asof_numpy`) to attach the most recent\n",
    "#    annual, quarterly, semiannual, and trimester values for each (ID, HistCurrency,\n",
    "#    ItemCode, FiscalPeriod) up to each row’s PIT Date.\n",
    "# 5. Builds \"full-year\" candidate values from:\n",
    "#       - actual annuals (A),\n",
    "#       - sum of Q1..Q4 (Q4 proxy),\n",
    "#       - sum of S1..S2 (S2 proxy),\n",
    "#       - sum of T1..T3 (T3 proxy),\n",
    "#    and selects the best candidate based on priorities and relationship to the row’s\n",
    "#    fiscal period (same-year vs prior-year).\n",
    "# 6. Computes an annual PIT-based metric `AnnPITValue` and compares it to the “true”\n",
    "#    annual value (`TrueValue`) to derive a percentage `AnnPITValue_Pct` for QC.\n",
    "# 7. Performs quality checks:\n",
    "#       - Ensures no period-date is after the PIT Date.\n",
    "#       - Drops rows whose `AnnPITValue_Pct` is outside the range [50, 200] or infinite.\n",
    "# 8. Keeps a curated set of columns, drops helper columns, and saves:\n",
    "#       - a full output file\n",
    "#       - a subset file with key columns for quick inspection.\n",
    "# 9. Prints row-accounting stats and frees some memory.\n",
    "#\n",
    "# If `mixed_encoded` is not defined or is None, it simply prints a message and exits.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta            # <--- Added to fix NameError\n",
    "from scipy.stats.mstats import winsorize  # <--- Added to fix NameError\n",
    "\n",
    "# Enable pandas \"copy-on-write\" behavior to reduce unintended chained assignment effects\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Helper function to build a string key from multiple columns.\n",
    "    For each row, join the values of 'cols' with '||'.\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    This is a manual, NumPy-based implementation of an \"as-of merge\" grouped by `by_cols`.\n",
    "    It:\n",
    "    - filters out rows with missing required fields,\n",
    "    - normalizes PIT Date to daily precision,\n",
    "    - groups right_df by composite key of `by_cols`,\n",
    "    - for each left row, binary-searches into the matching right group by PIT Date,\n",
    "      picking the last date <= the left PIT Date,\n",
    "    - returns two aligned arrays: (values, dates) for the left_df rows.\n",
    "    \"\"\"\n",
    "    # Initialize outputs with NaNs and NaT for all left_df rows\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns on each side\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask: rows that have all required fields non-null\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If either side has no valid rows, return the default empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on filtered copies only (avoid side effects)\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime at day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build composite keys for group-level match (based on by_cols)\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and date to enable binary search per key\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract NumPy arrays for right side\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and first index for each key in the sorted right side\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Build a dict: key -> (dates array, values array) slice\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]  # start of this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end of this key\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Left side indices and arrays\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left side by key (stable mergesort to preserve original row order within key)\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # i iterates over the sorted left rows\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current composite key\n",
    "        j = i + 1\n",
    "        # Find contiguous block [i:j) for this key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Dates and original positions of this key’s left rows\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # If we have matching right-hand slices for this key, do the as-of search\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # searchsorted(..., 'right') - 1 gives index of last r_date <= block_date\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # positions where such a date exists\n",
    "            if np.any(valid):\n",
    "                # Fill outputs for left rows where we found a valid match\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        # Move to next key block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile calculation: returns quantile q of Series `s`,\n",
    "    or NaN if quantile fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Computes a summary of finite values of a series:\n",
    "      - count\n",
    "      - mean, median\n",
    "      - 1% winsorized mean\n",
    "      - selected deciles (p10, p20, ..., p90)\n",
    "    Infinite values are treated as NaN and removed.\n",
    "    If no finite values remain, returns an empty dict.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop all NaNs\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures the array is writable for winsorize\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Priority for full-year candidates ----------\n",
    "\n",
    "# Fixed priority mapping for full-year candidates:\n",
    "#   'A'  : actual annual value\n",
    "#   'Q4' : annual proxy from four quarters\n",
    "#   'T3' : annual proxy from three trimesters\n",
    "#   'S2' : annual proxy from two semesters\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # highest priority: actual annual\n",
    "    'Q4': 90,  # then Q1+Q2+Q3+Q4\n",
    "    'T3': 80,  # then T1+T2+T3\n",
    "    'S2': 70,  # then S1+S2\n",
    "}\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "# Only run the main logic if a global DataFrame `mixed_encoded` exists and is not None\n",
    "if 'mixed_encoded' in globals() and mixed_encoded is not None:\n",
    "    # Count initial input rows\n",
    "    input_rows = len(mixed_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy of the input dataset\n",
    "    working = mixed_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Exclude certain frequencies (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create mask of rows whose Frequency is one of E, L, R, U (case-insensitive)\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    # Count how many rows will be excluded\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    # Keep only rows that are NOT in the exclusion set\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Set dtypes and normalize important columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert PIT Date to datetime (coerce errors -> NaT), floor to day\n",
    "    working['PIT Date']     = pd.to_datetime(working['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    # FiscalPeriod: numeric (e.g., 2021, 2022, ...)\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    # Value: numeric (float)\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Convert key ID / code columns to string to ensure consistency\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse Q/S/T markers from Str_FiscalPrd (like 'Q1Y2023')\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter number Q1..Q4 from e.g. \"Q1Y2023\" into QNUM\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual number S1..S2 into SNUM\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester number T1..T3 into TNUM\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ensure period columns exist (Q1..Q4, S1..S2, T1..T3, A + their date cols)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create value columns for Q1..Q4, S1..S2, T1..T3, A if they are missing\n",
    "    for c in [*(f'Q{i}' for i in range(1, 5)),\n",
    "              *(f'S{i}' for i in range(1, 3)),\n",
    "              *(f'T{i}' for i in range(1, 4)),\n",
    "              'A']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Create corresponding *_Date columns if missing\n",
    "    for c in [*(f'Q{i}_Date' for i in range(1, 5)),\n",
    "              *(f'S{i}_Date' for i in range(1, 3)),\n",
    "              *(f'T{i}_Date' for i in range(1, 4)),\n",
    "              'A_Date']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Base key for many of the as-of mappings\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Derive TrueValue from annuals (A/B frequencies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Mask annual-like rows where Value is present\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    # annual_src: one row per (ID, FiscalPeriod, HistCurrency) with last PIT Date\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Left-join true annual value back onto working\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping (same FiscalPeriod) for A/Q/S/T\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----- Annual -----\n",
    "    # Source rows for annual frequencies A/B\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    # As-of join: for each working row, get most recent annual value by PIT Date\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period of annual value (same as row's FiscalPeriod when present)\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # ----- Quarterly -----\n",
    "    # Source rows for quarterly frequencies (Q/C) with valid QNUM\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Restrict to a specific quarter q\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        # As-of join for that quarter\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        # Origin FP column for that quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        # Fill origin FP only where quarter value is non-null and origin not yet set\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Semiannual -----\n",
    "    # Source rows for semiannual frequencies (S/F) with valid SNUM\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Trimester -----\n",
    "    # Source rows for trimester frequencies (T/K) with valid TNUM\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels & normalize dates (NO prev-year fill, NO forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort working data consistently for downstream calculations\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    # List of all period value columns\n",
    "    value_cols_all  = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                      [f'S{i}' for i in range(1, 3)] + \\\n",
    "                      [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    # Corresponding date columns\n",
    "    date_cols_all   = [f'{c}_Date' for c in value_cols_all]\n",
    "    # Corresponding origin FP columns\n",
    "    origin_cols_all = [f'{c}_OriginFP' for c in value_cols_all]\n",
    "\n",
    "    # Ensure that all date columns are proper datetimes (floor to day)\n",
    "    # Note: explicitly no groupby-forward-fill here – only asof-filled values remain\n",
    "    for c in date_cols_all:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Build full-year candidates from fixed sets (Q1–Q4, S1–S2, T1–T3)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def full_year_from_fixed(row, labels, pit, cutoff):\n",
    "        \"\"\"\n",
    "        Fixed full-year from given labels (e.g. Q1..Q4, S1..S2, T1..T3):\n",
    "\n",
    "          - Requires ALL labels to have:\n",
    "              value, date, origin_fp.\n",
    "          - All dates must be within [cutoff, pit].\n",
    "          - origin_fp = max(origin_fp_i)  (the 'newest year' across components).\n",
    "\n",
    "        Returns (dt, val, origin_fp) or (NaT, NaN, NaN) if any requirement fails.\n",
    "        \"\"\"\n",
    "        vals, dts, fps = [], [], []\n",
    "        for lbl in labels:\n",
    "            v = row.get(lbl, np.nan)\n",
    "            d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "            o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "            # Require non-null for value, date, origin FP\n",
    "            if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            # Normalize date and check it lies within [cutoff, pit]\n",
    "            d = pd.to_datetime(d, errors='coerce')\n",
    "            if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            vals.append(float(v))\n",
    "            dts.append(d)\n",
    "            fps.append(int(o))\n",
    "\n",
    "        # Sum all component values, pick latest date and max origin FP\n",
    "        total_val = float(np.nansum(vals))\n",
    "        latest_dt = max(dts)\n",
    "        origin_fp = max(fps)\n",
    "        return latest_dt, total_val, origin_fp\n",
    "\n",
    "    def pick_annpit_sum_with_origin(row):\n",
    "        \"\"\"\n",
    "        For each row, pick the best annual PIT-based value (AnnPITValue) by:\n",
    "\n",
    "        1) Building a time window [PIT - 365 days, PIT].\n",
    "        2) Generating candidate annual values:\n",
    "           - actual annual A (if within window and with origin FP),\n",
    "           - Q4 proxy from Q1..Q4,\n",
    "           - S2 proxy from S1..S2,\n",
    "           - T3 proxy from T1..T3,\n",
    "           each with associated priority and origin FP.\n",
    "        3) Ranking candidates according to:\n",
    "           - Same-year A (highest),\n",
    "           - Same-year proxies (Q4/T3/S2),\n",
    "           - Prior-year A,\n",
    "           - Prior-year proxies,\n",
    "           - Otherwise: any remaining candidate by (priority, latest date).\n",
    "        4) Returning the chosen candidate's value (keeping zeros, but filtering NaNs).\n",
    "        \"\"\"\n",
    "        pit = row['PIT Date']\n",
    "        if pd.isna(pit):\n",
    "            return np.nan\n",
    "        cutoff = pit - timedelta(days=365)  # Now works because timedelta is imported\n",
    "\n",
    "        # Current row's fiscal period, cast to int if possible\n",
    "        fp = row.get('FiscalPeriod', np.nan)\n",
    "        try:\n",
    "            fp_int = int(fp) if not pd.isna(fp) else None\n",
    "        except Exception:\n",
    "            fp_int = None\n",
    "\n",
    "        # Collect candidate tuples: (label, priority, date, value, origin_fp)\n",
    "        candidates = []\n",
    "\n",
    "        # --- Candidate A: actual annual (0 is allowed)\n",
    "        A_val = row.get('A', np.nan)\n",
    "        A_dt  = row.get('A_Date', pd.NaT)\n",
    "        A_ofp = row.get('A_OriginFP', np.nan)\n",
    "        if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "            A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "            if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "                candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "        # --- Candidate Q4: sum of Q1..Q4 (0 allowed)\n",
    "        q4_dt, q4_val, q4_fp = full_year_from_fixed(\n",
    "            row, [f'Q{i}' for i in range(1, 5)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "            candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "        # --- Candidate S2: sum of S1..S2 (0 allowed)\n",
    "        s2_dt, s2_val, s2_fp = full_year_from_fixed(\n",
    "            row, [f'S{i}' for i in range(1, 3)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(s2_val) and pd.notna(s2_dt) and not pd.isna(s2_fp):\n",
    "            candidates.append(('S2', _PERIOD_PRIORITY['S2'], s2_dt, float(s2_val), int(s2_fp)))\n",
    "\n",
    "        # --- Candidate T3: sum of T1..T3 (0 allowed)\n",
    "        t3_dt, t3_val, t3_fp = full_year_from_fixed(\n",
    "            row, [f'T{i}' for i in range(1, 4)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(t3_val) and pd.notna(t3_dt) and not pd.isna(t3_fp):\n",
    "            candidates.append(('T3', _PERIOD_PRIORITY['T3'], t3_dt, float(t3_val), int(t3_fp)))\n",
    "\n",
    "        # If no candidates, return NaN\n",
    "        if not candidates:\n",
    "            return np.nan\n",
    "\n",
    "        # Filter out NaN-valued candidates (keep 0-valued ones)\n",
    "        def valid(seq):\n",
    "            return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "        # Relationship between row's fiscal period and candidate's origin FP:\n",
    "        # same / prior / other / unknown\n",
    "        def rel(c):\n",
    "            _, _, _, _, ofp = c\n",
    "            if fp_int is None or ofp is None:\n",
    "                return 'unknown'\n",
    "            if ofp == fp_int:\n",
    "                return 'same'\n",
    "            if ofp == fp_int - 1:\n",
    "                return 'prior'\n",
    "            return 'other'\n",
    "\n",
    "        # 1) Same-year actual annual A (max by date)\n",
    "        same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "        if same_A:\n",
    "            best = max(same_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 2) Same-year proxies (Q4/T3/S2), max by (priority, date)\n",
    "        same_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'same'\n",
    "        )\n",
    "        if same_proxies:\n",
    "            best = max(same_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 3) Prior-year actual annual A\n",
    "        prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "        if prior_A:\n",
    "            best = max(prior_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 4) Prior-year proxies (Q4/S2/T3) if no prior A\n",
    "        prior_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'prior'\n",
    "        )\n",
    "        if prior_proxies:\n",
    "            best = max(prior_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "        others = valid(candidates)\n",
    "        if others:\n",
    "            best = max(others, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # Final fallback: 0.0 (should rarely be reached)\n",
    "        return 0.0\n",
    "\n",
    "    # Apply the selection function row-wise to produce AnnPITValue\n",
    "    working['AnnPITValue'] = working.apply(pick_annpit_sum_with_origin, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check + PRE-DROP stats\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Columns whose dates should not exceed PIT Date\n",
    "    date_cols = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Restrict to ones actually present\n",
    "    present = [c for c in date_cols if c in working.columns]\n",
    "\n",
    "    viol_counts = {}  # per-label violation counts\n",
    "    # Mask for rows with any future-dated period\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        # A violation is when period date > PIT Date (both need to be non-null)\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m  # accumulate violations across columns\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flagrows with at least one future-date error\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct + quality drop\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute AnnPITValue as % of TrueValue (only when TrueValue != 0)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Stats before dropping low-quality rows\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    # Build masks for dropping:\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)  # infinite percentages\n",
    "    is_finite = np.isfinite(pct)\n",
    "    # Out-of-range if % > 200 or % < 50 (but finite)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    # Rows to drop: infinite or out-of-range values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that passed the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Stats after dropping\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns & save\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Base descriptive columns to keep (if present)\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Build ordered list of period date/value columns\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Final set of columns to keep in output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Helper columns to drop before export\n",
    "    drop_cols = ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    # Also drop all *_OriginFP columns\n",
    "    drop_cols += [c for c in working.columns if c.endswith('_OriginFP')]\n",
    "    working.drop(columns=[c for c in drop_cols if c in working.columns],\n",
    "                 inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder and restrict columns to the final layout\n",
    "    mixed_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # Sanity checks: necessary globals must exist\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Build full output path and save pipe-delimited file\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    mixed_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Create a subset for lighter inspection\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "                   \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    # Only keep subset columns that actually exist\n",
    "    subset_cols_existing = [col for col in subset_cols if col in mixed_processed.columns]\n",
    "    subset_df = mixed_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP,\n",
    "                              f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    # Explicitly delete subset_df to free memory\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(mixed_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excludedby Frequency (E/L/R/U): {excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    # Sum up excluded + dropped + remaining and check against original count\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. \"\n",
    "              \"Investigate upstream filtering or unexpected drops.\")\n",
    "\n",
    "    # Trigger garbage collection as a final cleanup step\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # If the main input dataset is not available, skip all processing\n",
    "    print(\"mixed_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YezOgcNDemA3"
   },
   "source": [
    "### Mixed 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StJE6rLplfUL"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1765651155625,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "vW00YOAPlfUN",
    "outputId": "16d9a86c-514f-4139-f9f3-2938b1850841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Mixed_5  ->  ItemName: 'Interest_Expense___Total'\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This code selects which Mixed_* dataset should be processed by choosing an index\n",
    "# (e.g., Mixed_1, Mixed_2, ...). It then:\n",
    "#\n",
    "#   1. Ensures that a dictionary `mixed_vars` exists, mapping keys like \"Mixed_1\"\n",
    "#      to item names.\n",
    "#   2. Builds the key corresponding to the selected index and retrieves the\n",
    "#      associated item name (`target_item_name`).\n",
    "#   3. Prints which Mixed_* item was selected.\n",
    "#   4. Constructs paths and filenames based on global variables and the selected item.\n",
    "#   5. Ensures that the output directory exists by creating it if necessary.\n",
    "#\n",
    "# The goal is to centralize selection of a single Mixed_* dataset and prepare paths\n",
    "# for downstream processing.\n",
    "\n",
    "\n",
    "# === Select which Mixed_* item to run ===\n",
    "mixed_index = 5  # Change this to process another dataset (e.g., 10)\n",
    "\n",
    "# Validate that the dictionary of mixed item names exists\n",
    "assert 'mixed_vars' in globals(), \"mixed_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key (e.g., \"Mixed_1\") and retrieve the associated item name\n",
    "item_key = f\"Mixed_{mixed_index}\"\n",
    "target_item_name = mixed_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in mixed_vars.\"\n",
    "\n",
    "# Inform which item was selected\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Construct the name of the input file for the selected item\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Construct the base name for output files (will later be suffixed)\n",
    "base_output_filename = f\"Mixed_{target_item_name}_complete\"\n",
    "\n",
    "# Ensure the output directory exists; create it (including parent dirs) if needed\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhE_hKKNlfUO"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1765651155760,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "bkdwMKwHlfUO",
    "outputId": "32a7289a-dd70-4292-bef1-5beabfd728f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Interest_Expense___Total' ...\n",
      "Full dataset loaded successfully: 121,029 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1075</td>\n",
       "      <td>11.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1075</td>\n",
       "      <td>15.753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1075</td>\n",
       "      <td>13.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1075</td>\n",
       "      <td>30.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1075</td>\n",
       "      <td>32.078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode   Value  \n",
       "0     1075  11.207  \n",
       "1     1075  15.753  \n",
       "2     1075  13.255  \n",
       "3     1075  30.678  \n",
       "4     1075  32.078  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block:\n",
    "#   1. Announces the import of a full dataset for the given `target_item_name`.\n",
    "#   2. Checks whether the file at `file_path` exists.\n",
    "#   3. If it exists, calls `import_file_to_dataframe(file_path)` to load the data\n",
    "#      into `mixed_raw`.\n",
    "#   4. If the loaded DataFrame is non-empty, prints a success message including\n",
    "#      the number of rows and shows the first few rows (via display or fallback\n",
    "#      to text printing).\n",
    "#   5. If the load fails or returns an empty DataFrame, prints a warning and\n",
    "#      creates an empty DataFrame.\n",
    "#   6. If the file does not exist, prints an error message and sets `mixed_raw`\n",
    "#      to an empty DataFrame.\n",
    "#   7. Finally, it runs `gc.collect()` to trigger garbage collection and free\n",
    "#      memory.\n",
    "#\n",
    "# Note: All previous emoji symbols in the print statements have been removed.\n",
    "\n",
    "# Inform the user that we are starting the import for this item\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "# Check if the file path exists on disk\n",
    "if os.path.exists(file_path):\n",
    "    # If the file exists, attempt to import it into a DataFrame\n",
    "    mixed_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    # If the DataFrame is successfully loaded and not empty\n",
    "    if mixed_raw is not None and not mixed_raw.empty:\n",
    "        # Print a success message including row count with thousands separator\n",
    "        print(f\"Full dataset loaded successfully: {len(mixed_raw):,} rows total.\")\n",
    "\n",
    "        try:\n",
    "            # Try to display the first few rows (Jupyter / IPython display)\n",
    "            display(mixed_raw.head())\n",
    "        except Exception:\n",
    "            # If display is not available, fall back to printing as plain text\n",
    "            print(mixed_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        # If DataFrame is None or empty, warn and create an empty DataFrame\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        mixed_raw = pd.DataFrame()\n",
    "else:\n",
    "    # If the file does not exist, report an error and set mixed_raw to empty\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    mixed_raw = pd.DataFrame()\n",
    "\n",
    "# Run garbage collection to free up memory after the load attempt\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP2kdbv8lfUP"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1765651155920,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "TtxbwwUqlfUQ",
    "outputId": "58d4df24-ce21-4e39-93a2-9592359c6ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Mixed dataset for 'Interest_Expense___Total' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_581638/2687430345.py:85: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y18' 'Q1Y19' 'Q3Y19' ... 'Q1Y24' 'Q2Y24' 'Q3Y24']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1075</td>\n",
       "      <td>11.207</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1075</td>\n",
       "      <td>15.753</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1075</td>\n",
       "      <td>13.255</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1075</td>\n",
       "      <td>30.678</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1075</td>\n",
       "      <td>32.078</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode   Value Str_FiscalPrd  \n",
       "0     1075  11.207           Y92  \n",
       "1     1075  15.753           Y93  \n",
       "2     1075  13.255           Y94  \n",
       "3     1075  30.678           Y95  \n",
       "4     1075  32.078           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This snippet provides:\n",
    "#\n",
    "# 1. A helper function `last2` that returns the last two digits of a number as a\n",
    "#    zero-padded string (for building YY strings).\n",
    "#\n",
    "# 2. A function `add_str_fiscalprd(df)` which:\n",
    "#    - Works on a copy of an input DataFrame containing financial periods.\n",
    "#    - Normalizes the 'Frequency' (upper-case, no missing).\n",
    "#    - Stores the original 'FiscalPeriod' and converts it to numeric.\n",
    "#    - Creates a string representation 'Str_FiscalPrd' depending on the frequency:\n",
    "#         - Q/C/E/R: quarter-based (\"QnYyy\")\n",
    "#         - A/B: annual (\"Yyy\")\n",
    "#         - F/S: semiannual (\"SnYyy\")\n",
    "#         - K/T/L/U: trimester-like (\"TnYyy\")\n",
    "#    - Derives an implied full-year integer 'ImplFiscPer_Calculated' from the\n",
    "#      two-digit year (80–99 => 19xx, else 20xx).\n",
    "#    - For annual rows (A/B), checks discrepancies between original\n",
    "#      'FiscalPeriod' and implied full-year; prints a small preview & total count.\n",
    "#    - Overwrites 'FiscalPeriod' with 'ImplFiscPer_Calculated' and drops helper\n",
    "#      columns.\n",
    "#\n",
    "# 3. A small driver block that:\n",
    "#    - Checks that `mixed_raw` exists and is non-empty.\n",
    "#    - Applies `add_str_fiscalprd` to produce `mixed_encoded`.\n",
    "#    - Displays a head preview or prints a message and sets `mixed_encoded = None`\n",
    "#      if input is missing/empty.\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    # If input is NaN (or pandas-style missing), return None (no digits)\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Convert to int, format as 4-digit zero-padded string, take last 2 chars\n",
    "    # Example: n=2023 -> \"2023\"[-2:] -> \"23\"\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    \"\"\"\n",
    "    Creates 'Str_FiscalPrd' and overwrites 'FiscalPeriod' with an implied full year.\n",
    "    Also prints discrepancies for Annual (A,B) rows between original and calculated year.\n",
    "\n",
    "    The mapping logic:\n",
    "      - Quarterly (C, Q, E, R):   fp encodes a quarter index -> \"Q{1-4}Y{yy}\"\n",
    "      - Annual   (A, B):         fp is the year itself      -> \"Y{yy}\"\n",
    "      - Semiannual (F, S):       fp encodes half-year       -> \"S{1-2}Y{yy}\"\n",
    "      - Trimester-like (K, T,\n",
    "         L, U):                  fp encodes trimester       -> \"T{1-3}Y{yy}\"\n",
    "    Then we re-infer the full year from the YY part with 19xx/20xx rule.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the original input DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes:\n",
    "    # - uppercase for consistency\n",
    "    # - fill missing values with empty string\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod in a separate column for later comparison\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    # Convert FiscalPeriod to numeric, coerce errors to NaN\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Masks for different frequency groups\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])  # quarter-based\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])            # annual\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])            # semiannual\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])  # trimester-like\n",
    "\n",
    "    # Initialize Str_FiscalPrd as NaN; we will fill per frequency group\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Quarter-based encoding\n",
    "    # -------------------------\n",
    "    # Quarter number: (fp % 4) + 1 => yields 1..4 (if fp is integer-based encoding)\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year part for quarter: integer division by 4, then take last two digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    # Assign \"Q{quarter}Y{yy}\" for quarter frequencies\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" +\n",
    "        q_part.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Annual encoding (A, B)\n",
    "    # -------------------------\n",
    "    # Year part is fp itself for A/B; take last two digits via last2\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    # Assign \"Y{yy}\" for annual frequencies\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    # -------------------------\n",
    "    # Semiannual encoding (F, S)\n",
    "    # -------------------------\n",
    "    # Semester number: (fp % 2) + 1 => 1 or 2\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    # Year part: fp // 2, then last two digits\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    # Assign \"S{sem}Y{yy}\" for F/S\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" +\n",
    "        fs_sem.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Trimester-like encoding (K, T, L, U)\n",
    "    # -------------------------\n",
    "    # Term number: (fp % 3) + 1 => 1..3\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    # Year part: fp // 3, then last two digits\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    # Assign \"T{term}Y{yy}\" for K/T/L/U frequencies\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" +\n",
    "        t_term.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Implied full year from Str_FiscalPrd (19xx / 20xx reconstruction)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Extract two-digit year from \"Yyy\" pattern, e.g., \"Q1Y23\" -> \"23\"\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    # Convert that to numeric (NaN if not parseable)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    # Map two-digit year to full year:\n",
    "    #  - 80–99 => 19xx\n",
    "    #  - else  => 20xx\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Discrepancies check for Annual frequencies (A, B)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Work on a subset of annual frequencies only\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "\n",
    "    # Build boolean mask where implied full year does NOT match the original\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Either they match numerically...\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce'))\n",
    "        |\n",
    "        # ...or both are NaN\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    # Subset rows with discrepancies\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    # If we have any, show a short sample and the total count\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Overwrite FiscalPeriod and drop temporary helper columns\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Replace 'FiscalPeriod' with the implied full-year value we just calculated\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    # Drop helper columns not needed downstream\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to mixed_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'mixed_raw' in globals() and mixed_raw is not None and not mixed_raw.empty:\n",
    "    # Inform which item we are encoding for\n",
    "    print(f\"Applying encoding to Mixed dataset for '{target_item_name}' ...\")\n",
    "    # Apply fiscal period encoding\n",
    "    mixed_encoded = add_str_fiscalprd(mixed_raw)\n",
    "    # Show a preview of the encoded data\n",
    "    display(mixed_encoded.head())\n",
    "else:\n",
    "    # If mixed_raw is missing or empty, we skip and mark mixed_encoded as None\n",
    "    print(\"mixed_raw not found or empty. Cannot perform encoding.\")\n",
    "    mixed_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6nPFyvKlfUQ"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1765651156344,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "bGLQpgnlnngF",
    "outputId": "59b0e2bf-9112-480d-d074-d437164e3487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 121,029 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\n",
      "         finite_rows: 113141\n",
      "                mean: 104.68247919935635\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.96134819807163\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 518\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 112623\n",
      "                mean: 100.01269112887455\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.99219204283362\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Interest_Expense___Total_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Interest_Expense___Total_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     121,029\n",
      "Excludedby Frequency (E/L/R/U): 0\n",
      "Dropped by quality (Pct rules): 518\n",
      "Output rows (final):            120,511\n",
      "Check: excluded + dropped + output = 121,029\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This script takes an input DataFrame `mixed_encoded` (if present in the global scope)\n",
    "# that contains financial time-series data (per company, item, currency, fiscal period,\n",
    "# and PIT Date). It then:\n",
    "#\n",
    "# 1. Cleans and standardizes key columns (dates, numeric types, string IDs).\n",
    "# 2. Excludes rows with certain frequencies (E/L/R/U).\n",
    "# 3. Parses fiscal period strings into quarter/semester/trimester indicators (QNUM/SNUM/TNUM).\n",
    "# 4. Uses a custom, vectorized \"as-of\" join (`asof_numpy`) to attach the most recent\n",
    "#    annual, quarterly, semiannual, and trimester values for each (ID, HistCurrency,\n",
    "#    ItemCode, FiscalPeriod) up to each row’s PIT Date.\n",
    "# 5. Builds \"full-year\" candidate values from:\n",
    "#       - actual annuals (A),\n",
    "#       - sum of Q1..Q4 (Q4 proxy),\n",
    "#       - sum of S1..S2 (S2 proxy),\n",
    "#       - sum of T1..T3 (T3 proxy),\n",
    "#    and selects the best candidate based on priorities and relationship to the row’s\n",
    "#    fiscal period (same-year vs prior-year).\n",
    "# 6. Computes an annual PIT-based metric `AnnPITValue` and compares it to the “true”\n",
    "#    annual value (`TrueValue`) to derive a percentage `AnnPITValue_Pct` for QC.\n",
    "# 7. Performs quality checks:\n",
    "#       - Ensures no period-date is after the PIT Date.\n",
    "#       - Drops rows whose `AnnPITValue_Pct` is outside the range [50, 200] or infinite.\n",
    "# 8. Keeps a curated set of columns, drops helper columns, and saves:\n",
    "#       - a full output file\n",
    "#       - a subset file with key columns for quick inspection.\n",
    "# 9. Prints row-accounting stats and frees some memory.\n",
    "#\n",
    "# If `mixed_encoded` is not defined or is None, it simply prints a message and exits.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta            # <--- Added to fix NameError\n",
    "from scipy.stats.mstats import winsorize  # <--- Added to fix NameError\n",
    "\n",
    "# Enable pandas \"copy-on-write\" behavior to reduce unintended chained assignment effects\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Helper function to build a string key from multiple columns.\n",
    "    For each row, join the values of 'cols' with '||'.\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    This is a manual, NumPy-based implementation of an \"as-of merge\" grouped by `by_cols`.\n",
    "    It:\n",
    "    - filters out rows with missing required fields,\n",
    "    - normalizes PIT Date to daily precision,\n",
    "    - groups right_df by composite key of `by_cols`,\n",
    "    - for each left row, binary-searches into the matching right group by PIT Date,\n",
    "      picking the last date <= the left PIT Date,\n",
    "    - returns two aligned arrays: (values, dates) for the left_df rows.\n",
    "    \"\"\"\n",
    "    # Initialize outputs with NaNs and NaT for all left_df rows\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns on each side\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask: rows that have all required fields non-null\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If either side has no valid rows, return the default empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on filtered copies only (avoid side effects)\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime at day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build composite keys for group-level match (based on by_cols)\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and date to enable binary search per key\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract NumPy arrays for right side\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and first index for each key in the sorted right side\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Build a dict: key -> (dates array, values array) slice\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]  # start of this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end of this key\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Left side indices and arrays\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left side by key (stable mergesort to preserve original row order within key)\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # i iterates over the sorted left rows\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current composite key\n",
    "        j = i + 1\n",
    "        # Find contiguous block [i:j) for this key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Dates and original positions of this key’s left rows\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # If we have matching right-hand slices for this key, do the as-of search\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # searchsorted(..., 'right') - 1 gives index of last r_date <= block_date\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # positions where such a date exists\n",
    "            if np.any(valid):\n",
    "                # Fill outputs for left rows where we found a valid match\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        # Move to next key block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile calculation: returns quantile q of Series `s`,\n",
    "    or NaN if quantile fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Computes a summary of finite values of a series:\n",
    "      - count\n",
    "      - mean, median\n",
    "      - 1% winsorized mean\n",
    "      - selected deciles (p10, p20, ..., p90)\n",
    "    Infinite values are treated as NaN and removed.\n",
    "    If no finite values remain, returns an empty dict.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop all NaNs\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures the array is writable for winsorize\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Priority for full-year candidates ----------\n",
    "\n",
    "# Fixed priority mapping for full-year candidates:\n",
    "#   'A'  : actual annual value\n",
    "#   'Q4' : annual proxy from four quarters\n",
    "#   'T3' : annual proxy from three trimesters\n",
    "#   'S2' : annual proxy from two semesters\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # highest priority: actual annual\n",
    "    'Q4': 90,  # then Q1+Q2+Q3+Q4\n",
    "    'T3': 80,  # then T1+T2+T3\n",
    "    'S2': 70,  # then S1+S2\n",
    "}\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "# Only run the main logic if a global DataFrame `mixed_encoded` exists and is not None\n",
    "if 'mixed_encoded' in globals() and mixed_encoded is not None:\n",
    "    # Count initial input rows\n",
    "    input_rows = len(mixed_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy of the input dataset\n",
    "    working = mixed_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Exclude certain frequencies (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create mask of rows whose Frequency is one of E, L, R, U (case-insensitive)\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    # Count how many rows will be excluded\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    # Keep only rows that are NOT in the exclusion set\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Set dtypes and normalize important columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert PIT Date to datetime (coerce errors -> NaT), floor to day\n",
    "    working['PIT Date']     = pd.to_datetime(working['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    # FiscalPeriod: numeric (e.g., 2021, 2022, ...)\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    # Value: numeric (float)\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Convert key ID / code columns to string to ensure consistency\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse Q/S/T markers from Str_FiscalPrd (like 'Q1Y2023')\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter number Q1..Q4 from e.g. \"Q1Y2023\" into QNUM\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual number S1..S2 into SNUM\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester number T1..T3 into TNUM\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ensure period columns exist (Q1..Q4, S1..S2, T1..T3, A + their date cols)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create value columns for Q1..Q4, S1..S2, T1..T3, A if they are missing\n",
    "    for c in [*(f'Q{i}' for i in range(1, 5)),\n",
    "              *(f'S{i}' for i in range(1, 3)),\n",
    "              *(f'T{i}' for i in range(1, 4)),\n",
    "              'A']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Create corresponding *_Date columns if missing\n",
    "    for c in [*(f'Q{i}_Date' for i in range(1, 5)),\n",
    "              *(f'S{i}_Date' for i in range(1, 3)),\n",
    "              *(f'T{i}_Date' for i in range(1, 4)),\n",
    "              'A_Date']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Base key for many of the as-of mappings\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Derive TrueValue from annuals (A/B frequencies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Mask annual-like rows where Value is present\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    # annual_src: one row per (ID, FiscalPeriod, HistCurrency) with last PIT Date\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Left-join true annual value back onto working\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping (same FiscalPeriod) for A/Q/S/T\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----- Annual -----\n",
    "    # Source rows for annual frequencies A/B\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    # As-of join: for each working row, get most recent annual value by PIT Date\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period of annual value (same as row's FiscalPeriod when present)\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # ----- Quarterly -----\n",
    "    # Source rows for quarterly frequencies (Q/C) with valid QNUM\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Restrict to a specific quarter q\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        # As-of join for that quarter\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        # Origin FP column for that quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        # Fill origin FP only where quarter value is non-null and origin not yet set\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Semiannual -----\n",
    "    # Source rows for semiannual frequencies (S/F) with valid SNUM\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Trimester -----\n",
    "    # Source rows for trimester frequencies (T/K) with valid TNUM\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels & normalize dates (NO prev-year fill, NO forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort working data consistently for downstream calculations\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    # List of all period value columns\n",
    "    value_cols_all  = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                      [f'S{i}' for i in range(1, 3)] + \\\n",
    "                      [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    # Corresponding date columns\n",
    "    date_cols_all   = [f'{c}_Date' for c in value_cols_all]\n",
    "    # Corresponding origin FP columns\n",
    "    origin_cols_all = [f'{c}_OriginFP' for c in value_cols_all]\n",
    "\n",
    "    # Ensure that all date columns are proper datetimes (floor to day)\n",
    "    # Note: explicitly no groupby-forward-fill here – only asof-filled values remain\n",
    "    for c in date_cols_all:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Build full-year candidates from fixed sets (Q1–Q4, S1–S2, T1–T3)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def full_year_from_fixed(row, labels, pit, cutoff):\n",
    "        \"\"\"\n",
    "        Fixed full-year from given labels (e.g. Q1..Q4, S1..S2, T1..T3):\n",
    "\n",
    "          - Requires ALL labels to have:\n",
    "              value, date, origin_fp.\n",
    "          - All dates must be within [cutoff, pit].\n",
    "          - origin_fp = max(origin_fp_i)  (the 'newest year' across components).\n",
    "\n",
    "        Returns (dt, val, origin_fp) or (NaT, NaN, NaN) if any requirement fails.\n",
    "        \"\"\"\n",
    "        vals, dts, fps = [], [], []\n",
    "        for lbl in labels:\n",
    "            v = row.get(lbl, np.nan)\n",
    "            d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "            o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "            # Require non-null for value, date, origin FP\n",
    "            if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            # Normalize date and check it lies within [cutoff, pit]\n",
    "            d = pd.to_datetime(d, errors='coerce')\n",
    "            if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            vals.append(float(v))\n",
    "            dts.append(d)\n",
    "            fps.append(int(o))\n",
    "\n",
    "        # Sum all component values, pick latest date and max origin FP\n",
    "        total_val = float(np.nansum(vals))\n",
    "        latest_dt = max(dts)\n",
    "        origin_fp = max(fps)\n",
    "        return latest_dt, total_val, origin_fp\n",
    "\n",
    "    def pick_annpit_sum_with_origin(row):\n",
    "        \"\"\"\n",
    "        For each row, pick the best annual PIT-based value (AnnPITValue) by:\n",
    "\n",
    "        1) Building a time window [PIT - 365 days, PIT].\n",
    "        2) Generating candidate annual values:\n",
    "           - actual annual A (if within window and with origin FP),\n",
    "           - Q4 proxy from Q1..Q4,\n",
    "           - S2 proxy from S1..S2,\n",
    "           - T3 proxy from T1..T3,\n",
    "           each with associated priority and origin FP.\n",
    "        3) Ranking candidates according to:\n",
    "           - Same-year A (highest),\n",
    "           - Same-year proxies (Q4/T3/S2),\n",
    "           - Prior-year A,\n",
    "           - Prior-year proxies,\n",
    "           - Otherwise: any remaining candidate by (priority, latest date).\n",
    "        4) Returning the chosen candidate's value (keeping zeros, but filtering NaNs).\n",
    "        \"\"\"\n",
    "        pit = row['PIT Date']\n",
    "        if pd.isna(pit):\n",
    "            return np.nan\n",
    "        cutoff = pit - timedelta(days=365)  # Now works because timedelta is imported\n",
    "\n",
    "        # Current row's fiscal period, cast to int if possible\n",
    "        fp = row.get('FiscalPeriod', np.nan)\n",
    "        try:\n",
    "            fp_int = int(fp) if not pd.isna(fp) else None\n",
    "        except Exception:\n",
    "            fp_int = None\n",
    "\n",
    "        # Collect candidate tuples: (label, priority, date, value, origin_fp)\n",
    "        candidates = []\n",
    "\n",
    "        # --- Candidate A: actual annual (0 is allowed)\n",
    "        A_val = row.get('A', np.nan)\n",
    "        A_dt  = row.get('A_Date', pd.NaT)\n",
    "        A_ofp = row.get('A_OriginFP', np.nan)\n",
    "        if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "            A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "            if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "                candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "        # --- Candidate Q4: sum of Q1..Q4 (0 allowed)\n",
    "        q4_dt, q4_val, q4_fp = full_year_from_fixed(\n",
    "            row, [f'Q{i}' for i in range(1, 5)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "            candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "        # --- Candidate S2: sum of S1..S2 (0 allowed)\n",
    "        s2_dt, s2_val, s2_fp = full_year_from_fixed(\n",
    "            row, [f'S{i}' for i in range(1, 3)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(s2_val) and pd.notna(s2_dt) and not pd.isna(s2_fp):\n",
    "            candidates.append(('S2', _PERIOD_PRIORITY['S2'], s2_dt, float(s2_val), int(s2_fp)))\n",
    "\n",
    "        # --- Candidate T3: sum of T1..T3 (0 allowed)\n",
    "        t3_dt, t3_val, t3_fp = full_year_from_fixed(\n",
    "            row, [f'T{i}' for i in range(1, 4)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(t3_val) and pd.notna(t3_dt) and not pd.isna(t3_fp):\n",
    "            candidates.append(('T3', _PERIOD_PRIORITY['T3'], t3_dt, float(t3_val), int(t3_fp)))\n",
    "\n",
    "        # If no candidates, return NaN\n",
    "        if not candidates:\n",
    "            return np.nan\n",
    "\n",
    "        # Filter out NaN-valued candidates (keep 0-valued ones)\n",
    "        def valid(seq):\n",
    "            return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "        # Relationship between row's fiscal period and candidate's origin FP:\n",
    "        # same / prior / other / unknown\n",
    "        def rel(c):\n",
    "            _, _, _, _, ofp = c\n",
    "            if fp_int is None or ofp is None:\n",
    "                return 'unknown'\n",
    "            if ofp == fp_int:\n",
    "                return 'same'\n",
    "            if ofp == fp_int - 1:\n",
    "                return 'prior'\n",
    "            return 'other'\n",
    "\n",
    "        # 1) Same-year actual annual A (max by date)\n",
    "        same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "        if same_A:\n",
    "            best = max(same_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 2) Same-year proxies (Q4/T3/S2), max by (priority, date)\n",
    "        same_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'same'\n",
    "        )\n",
    "        if same_proxies:\n",
    "            best = max(same_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 3) Prior-year actual annual A\n",
    "        prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "        if prior_A:\n",
    "            best = max(prior_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 4) Prior-year proxies (Q4/S2/T3) if no prior A\n",
    "        prior_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'prior'\n",
    "        )\n",
    "        if prior_proxies:\n",
    "            best = max(prior_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "        others = valid(candidates)\n",
    "        if others:\n",
    "            best = max(others, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # Final fallback: 0.0 (should rarely be reached)\n",
    "        return 0.0\n",
    "\n",
    "    # Apply the selection function row-wise to produce AnnPITValue\n",
    "    working['AnnPITValue'] = working.apply(pick_annpit_sum_with_origin, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check + PRE-DROP stats\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Columns whose dates should not exceed PIT Date\n",
    "    date_cols = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Restrict to ones actually present\n",
    "    present = [c for c in date_cols if c in working.columns]\n",
    "\n",
    "    viol_counts = {}  # per-label violation counts\n",
    "    # Mask for rows with any future-dated period\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        # A violation is when period date > PIT Date (both need to be non-null)\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m  # accumulate violations across columns\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flagrows with at least one future-date error\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct + quality drop\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute AnnPITValue as % of TrueValue (only when TrueValue != 0)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Stats before dropping low-quality rows\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    # Build masks for dropping:\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)  # infinite percentages\n",
    "    is_finite = np.isfinite(pct)\n",
    "    # Out-of-range if % > 200 or % < 50 (but finite)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    # Rows to drop: infinite or out-of-range values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that passed the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Stats after dropping\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns & save\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Base descriptive columns to keep (if present)\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Build ordered list of period date/value columns\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Final set of columns to keep in output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Helper columns to drop before export\n",
    "    drop_cols = ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    # Also drop all *_OriginFP columns\n",
    "    drop_cols += [c for c in working.columns if c.endswith('_OriginFP')]\n",
    "    working.drop(columns=[c for c in drop_cols if c in working.columns],\n",
    "                 inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder and restrict columns to the final layout\n",
    "    mixed_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # Sanity checks: necessary globals must exist\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Build full output path and save pipe-delimited file\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    mixed_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Create a subset for lighter inspection\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "                   \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    # Only keep subset columns that actually exist\n",
    "    subset_cols_existing = [col for col in subset_cols if col in mixed_processed.columns]\n",
    "    subset_df = mixed_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP,\n",
    "                              f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    # Explicitly delete subset_df to free memory\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(mixed_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excludedby Frequency (E/L/R/U): {excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    # Sum up excluded + dropped + remaining and check against original count\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. \"\n",
    "              \"Investigate upstream filtering or unexpected drops.\")\n",
    "\n",
    "    # Trigger garbage collection as a final cleanup step\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # If the main input dataset is not available, skip all processing\n",
    "    print(\"mixed_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqwfFO1JenlH"
   },
   "source": [
    "### Mixed 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYXZ1x5hlhAL"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1765651156598,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "TzmKi3sOlhAM",
    "outputId": "32debd10-6615-463b-9319-99663463099c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Mixed_6  ->  ItemName: 'Net_Income_Before_Extra_Items_Preferred_Divs'\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This code selects which Mixed_* dataset should be processed by choosing an index\n",
    "# (e.g., Mixed_1, Mixed_2, ...). It then:\n",
    "#\n",
    "#   1. Ensures that a dictionary `mixed_vars` exists, mapping keys like \"Mixed_1\"\n",
    "#      to item names.\n",
    "#   2. Builds the key corresponding to the selected index and retrieves the\n",
    "#      associated item name (`target_item_name`).\n",
    "#   3. Prints which Mixed_* item was selected.\n",
    "#   4. Constructs paths and filenames based on global variables and the selected item.\n",
    "#   5. Ensures that the output directory exists by creating it if necessary.\n",
    "#\n",
    "# The goal is to centralize selection of a single Mixed_* dataset and prepare paths\n",
    "# for downstream processing.\n",
    "\n",
    "\n",
    "# === Select which Mixed_* item to run ===\n",
    "mixed_index = 6  # Change this to process another dataset (e.g., 10)\n",
    "\n",
    "# Validate that the dictionary of mixed item names exists\n",
    "assert 'mixed_vars' in globals(), \"mixed_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key (e.g., \"Mixed_1\") and retrieve the associated item name\n",
    "item_key = f\"Mixed_{mixed_index}\"\n",
    "target_item_name = mixed_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in mixed_vars.\"\n",
    "\n",
    "# Inform which item was selected\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Construct the name of the input file for the selected item\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Construct the base name for output files (will later be suffixed)\n",
    "base_output_filename = f\"Mixed_{target_item_name}_complete\"\n",
    "\n",
    "# Ensure the output directory exists; create it (including parent dirs) if needed\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkFG1rgPlhAN"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 1177,
     "status": "ok",
     "timestamp": 1765651157786,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "RULumsEylhAN",
    "outputId": "4e65880a-75c3-462e-f4e9-df0b70a964e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Net_Income_Before_Extra_Items_Preferred_Divs' ...\n",
      "Full dataset loaded successfully: 3,906,799 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1551</td>\n",
       "      <td>97.90298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1551</td>\n",
       "      <td>92.230294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1551</td>\n",
       "      <td>110.555887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1551</td>\n",
       "      <td>-43.448501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1551</td>\n",
       "      <td>39.188143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     1551    97.90298  \n",
       "1     1551   92.230294  \n",
       "2     1551  110.555887  \n",
       "3     1551  -43.448501  \n",
       "4     1551   39.188143  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block:\n",
    "#   1. Announces the import of a full dataset for the given `target_item_name`.\n",
    "#   2. Checks whether the file at `file_path` exists.\n",
    "#   3. If it exists, calls `import_file_to_dataframe(file_path)` to load the data\n",
    "#      into `mixed_raw`.\n",
    "#   4. If the loaded DataFrame is non-empty, prints a success message including\n",
    "#      the number of rows and shows the first few rows (via display or fallback\n",
    "#      to text printing).\n",
    "#   5. If the load fails or returns an empty DataFrame, prints a warning and\n",
    "#      creates an empty DataFrame.\n",
    "#   6. If the file does not exist, prints an error message and sets `mixed_raw`\n",
    "#      to an empty DataFrame.\n",
    "#   7. Finally, it runs `gc.collect()` to trigger garbage collection and free\n",
    "#      memory.\n",
    "#\n",
    "# Note: All previous emoji symbols in the print statements have been removed.\n",
    "\n",
    "# Inform the user that we are starting the import for this item\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "# Check if the file path exists on disk\n",
    "if os.path.exists(file_path):\n",
    "    # If the file exists, attempt to import it into a DataFrame\n",
    "    mixed_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    # If the DataFrame is successfully loaded and not empty\n",
    "    if mixed_raw is not None and not mixed_raw.empty:\n",
    "        # Print a success message including row count with thousands separator\n",
    "        print(f\"Full dataset loaded successfully: {len(mixed_raw):,} rows total.\")\n",
    "\n",
    "        try:\n",
    "            # Try to display the first few rows (Jupyter / IPython display)\n",
    "            display(mixed_raw.head())\n",
    "        except Exception:\n",
    "            # If display is not available, fall back to printing as plain text\n",
    "            print(mixed_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        # If DataFrame is None or empty, warn and create an empty DataFrame\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        mixed_raw = pd.DataFrame()\n",
    "else:\n",
    "    # If the file does not exist, report an error and set mixed_raw to empty\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    mixed_raw = pd.DataFrame()\n",
    "\n",
    "# Run garbage collection to free up memory after the load attempt\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb3kMzOtlhAO"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1765651158081,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "UF5jOBSVlhAO",
    "outputId": "12790744-d577-4dce-86a2-b3d334ecc4da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Mixed dataset for 'Net_Income_Before_Extra_Items_Preferred_Divs' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_581638/2687430345.py:85: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y94' 'Q2Y94' 'Q3Y94' ... 'Q3Y07' 'Q4Y07' 'Q4Y05']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1551</td>\n",
       "      <td>97.90298</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1551</td>\n",
       "      <td>92.230294</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1551</td>\n",
       "      <td>110.555887</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1551</td>\n",
       "      <td>-43.448501</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1551</td>\n",
       "      <td>39.188143</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     1551    97.90298           Y92  \n",
       "1     1551   92.230294           Y93  \n",
       "2     1551  110.555887           Y94  \n",
       "3     1551  -43.448501           Y95  \n",
       "4     1551   39.188143           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This snippet provides:\n",
    "#\n",
    "# 1. A helper function `last2` that returns the last two digits of a number as a\n",
    "#    zero-padded string (for building YY strings).\n",
    "#\n",
    "# 2. A function `add_str_fiscalprd(df)` which:\n",
    "#    - Works on a copy of an input DataFrame containing financial periods.\n",
    "#    - Normalizes the 'Frequency' (upper-case, no missing).\n",
    "#    - Stores the original 'FiscalPeriod' and converts it to numeric.\n",
    "#    - Creates a string representation 'Str_FiscalPrd' depending on the frequency:\n",
    "#         - Q/C/E/R: quarter-based (\"QnYyy\")\n",
    "#         - A/B: annual (\"Yyy\")\n",
    "#         - F/S: semiannual (\"SnYyy\")\n",
    "#         - K/T/L/U: trimester-like (\"TnYyy\")\n",
    "#    - Derives an implied full-year integer 'ImplFiscPer_Calculated' from the\n",
    "#      two-digit year (80–99 => 19xx, else 20xx).\n",
    "#    - For annual rows (A/B), checks discrepancies between original\n",
    "#      'FiscalPeriod' and implied full-year; prints a small preview & total count.\n",
    "#    - Overwrites 'FiscalPeriod' with 'ImplFiscPer_Calculated' and drops helper\n",
    "#      columns.\n",
    "#\n",
    "# 3. A small driver block that:\n",
    "#    - Checks that `mixed_raw` exists and is non-empty.\n",
    "#    - Applies `add_str_fiscalprd` to produce `mixed_encoded`.\n",
    "#    - Displays a head preview or prints a message and sets `mixed_encoded = None`\n",
    "#      if input is missing/empty.\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    # If input is NaN (or pandas-style missing), return None (no digits)\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Convert to int, format as 4-digit zero-padded string, take last 2 chars\n",
    "    # Example: n=2023 -> \"2023\"[-2:] -> \"23\"\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    \"\"\"\n",
    "    Creates 'Str_FiscalPrd' and overwrites 'FiscalPeriod' with an implied full year.\n",
    "    Also prints discrepancies for Annual (A,B) rows between original and calculated year.\n",
    "\n",
    "    The mapping logic:\n",
    "      - Quarterly (C, Q, E, R):   fp encodes a quarter index -> \"Q{1-4}Y{yy}\"\n",
    "      - Annual   (A, B):         fp is the year itself      -> \"Y{yy}\"\n",
    "      - Semiannual (F, S):       fp encodes half-year       -> \"S{1-2}Y{yy}\"\n",
    "      - Trimester-like (K, T,\n",
    "         L, U):                  fp encodes trimester       -> \"T{1-3}Y{yy}\"\n",
    "    Then we re-infer the full year from the YY part with 19xx/20xx rule.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the original input DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes:\n",
    "    # - uppercase for consistency\n",
    "    # - fill missing values with empty string\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod in a separate column for later comparison\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    # Convert FiscalPeriod to numeric, coerce errors to NaN\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Masks for different frequency groups\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])  # quarter-based\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])            # annual\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])            # semiannual\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])  # trimester-like\n",
    "\n",
    "    # Initialize Str_FiscalPrd as NaN; we will fill per frequency group\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Quarter-based encoding\n",
    "    # -------------------------\n",
    "    # Quarter number: (fp % 4) + 1 => yields 1..4 (if fp is integer-based encoding)\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year part for quarter: integer division by 4, then take last two digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    # Assign \"Q{quarter}Y{yy}\" for quarter frequencies\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" +\n",
    "        q_part.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Annual encoding (A, B)\n",
    "    # -------------------------\n",
    "    # Year part is fp itself for A/B; take last two digits via last2\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    # Assign \"Y{yy}\" for annual frequencies\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    # -------------------------\n",
    "    # Semiannual encoding (F, S)\n",
    "    # -------------------------\n",
    "    # Semester number: (fp % 2) + 1 => 1 or 2\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    # Year part: fp // 2, then last two digits\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    # Assign \"S{sem}Y{yy}\" for F/S\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" +\n",
    "        fs_sem.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Trimester-like encoding (K, T, L, U)\n",
    "    # -------------------------\n",
    "    # Term number: (fp % 3) + 1 => 1..3\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    # Year part: fp // 3, then last two digits\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    # Assign \"T{term}Y{yy}\" for K/T/L/U frequencies\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" +\n",
    "        t_term.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Implied full year from Str_FiscalPrd (19xx / 20xx reconstruction)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Extract two-digit year from \"Yyy\" pattern, e.g., \"Q1Y23\" -> \"23\"\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    # Convert that to numeric (NaN if not parseable)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    # Map two-digit year to full year:\n",
    "    #  - 80–99 => 19xx\n",
    "    #  - else  => 20xx\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Discrepancies check for Annual frequencies (A, B)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Work on a subset of annual frequencies only\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "\n",
    "    # Build boolean mask where implied full year does NOT match the original\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Either they match numerically...\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce'))\n",
    "        |\n",
    "        # ...or both are NaN\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    # Subset rows with discrepancies\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    # If we have any, show a short sample and the total count\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Overwrite FiscalPeriod and drop temporary helper columns\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Replace 'FiscalPeriod' with the implied full-year value we just calculated\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    # Drop helper columns not needed downstream\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to mixed_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'mixed_raw' in globals() and mixed_raw is not None and not mixed_raw.empty:\n",
    "    # Inform which item we are encoding for\n",
    "    print(f\"Applying encoding to Mixed dataset for '{target_item_name}' ...\")\n",
    "    # Apply fiscal period encoding\n",
    "    mixed_encoded = add_str_fiscalprd(mixed_raw)\n",
    "    # Show a preview of the encoded data\n",
    "    display(mixed_encoded.head())\n",
    "else:\n",
    "    # If mixed_raw is missing or empty, we skip and mark mixed_encoded as None\n",
    "    print(\"mixed_raw not found or empty. Cannot perform encoding.\")\n",
    "    mixed_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fk8C60tDlhAP"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7494,
     "status": "ok",
     "timestamp": 1765651165578,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "CV7Gii2snqJ0",
    "outputId": "45a3c6df-3efc-4ae8-b194-503cf3f868f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,906,799 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\n",
      "         finite_rows: 2307432\n",
      "                mean: 28322.45151943236\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.55537956717451\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 40,188\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 2267244\n",
      "                mean: 100.18154532626606\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.06481793229382\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Net_Income_Before_Extra_Items_Preferred_Divs_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Net_Income_Before_Extra_Items_Preferred_Divs_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,906,799\n",
      "Excludedby Frequency (E/L/R/U): 0\n",
      "Dropped by quality (Pct rules): 40,188\n",
      "Output rows (final):            3,866,611\n",
      "Check: excluded + dropped + output = 3,906,799\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This script takes an input DataFrame `mixed_encoded` (if present in the global scope)\n",
    "# that contains financial time-series data (per company, item, currency, fiscal period,\n",
    "# and PIT Date). It then:\n",
    "#\n",
    "# 1. Cleans and standardizes key columns (dates, numeric types, string IDs).\n",
    "# 2. Excludes rows with certain frequencies (E/L/R/U).\n",
    "# 3. Parses fiscal period strings into quarter/semester/trimester indicators (QNUM/SNUM/TNUM).\n",
    "# 4. Uses a custom, vectorized \"as-of\" join (`asof_numpy`) to attach the most recent\n",
    "#    annual, quarterly, semiannual, and trimester values for each (ID, HistCurrency,\n",
    "#    ItemCode, FiscalPeriod) up to each row’s PIT Date.\n",
    "# 5. Builds \"full-year\" candidate values from:\n",
    "#       - actual annuals (A),\n",
    "#       - sum of Q1..Q4 (Q4 proxy),\n",
    "#       - sum of S1..S2 (S2 proxy),\n",
    "#       - sum of T1..T3 (T3 proxy),\n",
    "#    and selects the best candidate based on priorities and relationship to the row’s\n",
    "#    fiscal period (same-year vs prior-year).\n",
    "# 6. Computes an annual PIT-based metric `AnnPITValue` and compares it to the “true”\n",
    "#    annual value (`TrueValue`) to derive a percentage `AnnPITValue_Pct` for QC.\n",
    "# 7. Performs quality checks:\n",
    "#       - Ensures no period-date is after the PIT Date.\n",
    "#       - Drops rows whose `AnnPITValue_Pct` is outside the range [50, 200] or infinite.\n",
    "# 8. Keeps a curated set of columns, drops helper columns, and saves:\n",
    "#       - a full output file\n",
    "#       - a subset file with key columns for quick inspection.\n",
    "# 9. Prints row-accounting stats and frees some memory.\n",
    "#\n",
    "# If `mixed_encoded` is not defined or is None, it simply prints a message and exits.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta            # <--- Added to fix NameError\n",
    "from scipy.stats.mstats import winsorize  # <--- Added to fix NameError\n",
    "\n",
    "# Enable pandas \"copy-on-write\" behavior to reduce unintended chained assignment effects\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Helper function to build a string key from multiple columns.\n",
    "    For each row, join the values of 'cols' with '||'.\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    This is a manual, NumPy-based implementation of an \"as-of merge\" grouped by `by_cols`.\n",
    "    It:\n",
    "    - filters out rows with missing required fields,\n",
    "    - normalizes PIT Date to daily precision,\n",
    "    - groups right_df by composite key of `by_cols`,\n",
    "    - for each left row, binary-searches into the matching right group by PIT Date,\n",
    "      picking the last date <= the left PIT Date,\n",
    "    - returns two aligned arrays: (values, dates) for the left_df rows.\n",
    "    \"\"\"\n",
    "    # Initialize outputs with NaNs and NaT for all left_df rows\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns on each side\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask: rows that have all required fields non-null\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If either side has no valid rows, return the default empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on filtered copies only (avoid side effects)\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime at day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build composite keys for group-level match (based on by_cols)\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and date to enable binary search per key\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract NumPy arrays for right side\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and first index for each key in the sorted right side\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Build a dict: key -> (dates array, values array) slice\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]  # start of this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end of this key\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Left side indices and arrays\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left side by key (stable mergesort to preserve original row order within key)\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # i iterates over the sorted left rows\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current composite key\n",
    "        j = i + 1\n",
    "        # Find contiguous block [i:j) for this key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Dates and original positions of this key’s left rows\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # If we have matching right-hand slices for this key, do the as-of search\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # searchsorted(..., 'right') - 1 gives index of last r_date <= block_date\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # positions where such a date exists\n",
    "            if np.any(valid):\n",
    "                # Fill outputs for left rows where we found a valid match\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        # Move to next key block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile calculation: returns quantile q of Series `s`,\n",
    "    or NaN if quantile fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Computes a summary of finite values of a series:\n",
    "      - count\n",
    "      - mean, median\n",
    "      - 1% winsorized mean\n",
    "      - selected deciles (p10, p20, ..., p90)\n",
    "    Infinite values are treated as NaN and removed.\n",
    "    If no finite values remain, returns an empty dict.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop all NaNs\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures the array is writable for winsorize\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Priority for full-year candidates ----------\n",
    "\n",
    "# Fixed priority mapping for full-year candidates:\n",
    "#   'A'  : actual annual value\n",
    "#   'Q4' : annual proxy from four quarters\n",
    "#   'T3' : annual proxy from three trimesters\n",
    "#   'S2' : annual proxy from two semesters\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # highest priority: actual annual\n",
    "    'Q4': 90,  # then Q1+Q2+Q3+Q4\n",
    "    'T3': 80,  # then T1+T2+T3\n",
    "    'S2': 70,  # then S1+S2\n",
    "}\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "# Only run the main logic if a global DataFrame `mixed_encoded` exists and is not None\n",
    "if 'mixed_encoded' in globals() and mixed_encoded is not None:\n",
    "    # Count initial input rows\n",
    "    input_rows = len(mixed_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy of the input dataset\n",
    "    working = mixed_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Exclude certain frequencies (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create mask of rows whose Frequency is one of E, L, R, U (case-insensitive)\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    # Count how many rows will be excluded\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    # Keep only rows that are NOT in the exclusion set\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Set dtypes and normalize important columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert PIT Date to datetime (coerce errors -> NaT), floor to day\n",
    "    working['PIT Date']     = pd.to_datetime(working['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    # FiscalPeriod: numeric (e.g., 2021, 2022, ...)\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    # Value: numeric (float)\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Convert key ID / code columns to string to ensure consistency\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse Q/S/T markers from Str_FiscalPrd (like 'Q1Y2023')\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter number Q1..Q4 from e.g. \"Q1Y2023\" into QNUM\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual number S1..S2 into SNUM\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester number T1..T3 into TNUM\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ensure period columns exist (Q1..Q4, S1..S2, T1..T3, A + their date cols)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create value columns for Q1..Q4, S1..S2, T1..T3, A if they are missing\n",
    "    for c in [*(f'Q{i}' for i in range(1, 5)),\n",
    "              *(f'S{i}' for i in range(1, 3)),\n",
    "              *(f'T{i}' for i in range(1, 4)),\n",
    "              'A']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Create corresponding *_Date columns if missing\n",
    "    for c in [*(f'Q{i}_Date' for i in range(1, 5)),\n",
    "              *(f'S{i}_Date' for i in range(1, 3)),\n",
    "              *(f'T{i}_Date' for i in range(1, 4)),\n",
    "              'A_Date']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Base key for many of the as-of mappings\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Derive TrueValue from annuals (A/B frequencies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Mask annual-like rows where Value is present\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    # annual_src: one row per (ID, FiscalPeriod, HistCurrency) with last PIT Date\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Left-join true annual value back onto working\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping (same FiscalPeriod) for A/Q/S/T\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----- Annual -----\n",
    "    # Source rows for annual frequencies A/B\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    # As-of join: for each working row, get most recent annual value by PIT Date\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period of annual value (same as row's FiscalPeriod when present)\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # ----- Quarterly -----\n",
    "    # Source rows for quarterly frequencies (Q/C) with valid QNUM\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Restrict to a specific quarter q\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        # As-of join for that quarter\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        # Origin FP column for that quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        # Fill origin FP only where quarter value is non-null and origin not yet set\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Semiannual -----\n",
    "    # Source rows for semiannual frequencies (S/F) with valid SNUM\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Trimester -----\n",
    "    # Source rows for trimester frequencies (T/K) with valid TNUM\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels & normalize dates (NO prev-year fill, NO forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort working data consistently for downstream calculations\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    # List of all period value columns\n",
    "    value_cols_all  = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                      [f'S{i}' for i in range(1, 3)] + \\\n",
    "                      [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    # Corresponding date columns\n",
    "    date_cols_all   = [f'{c}_Date' for c in value_cols_all]\n",
    "    # Corresponding origin FP columns\n",
    "    origin_cols_all = [f'{c}_OriginFP' for c in value_cols_all]\n",
    "\n",
    "    # Ensure that all date columns are proper datetimes (floor to day)\n",
    "    # Note: explicitly no groupby-forward-fill here – only asof-filled values remain\n",
    "    for c in date_cols_all:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Build full-year candidates from fixed sets (Q1–Q4, S1–S2, T1–T3)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def full_year_from_fixed(row, labels, pit, cutoff):\n",
    "        \"\"\"\n",
    "        Fixed full-year from given labels (e.g. Q1..Q4, S1..S2, T1..T3):\n",
    "\n",
    "          - Requires ALL labels to have:\n",
    "              value, date, origin_fp.\n",
    "          - All dates must be within [cutoff, pit].\n",
    "          - origin_fp = max(origin_fp_i)  (the 'newest year' across components).\n",
    "\n",
    "        Returns (dt, val, origin_fp) or (NaT, NaN, NaN) if any requirement fails.\n",
    "        \"\"\"\n",
    "        vals, dts, fps = [], [], []\n",
    "        for lbl in labels:\n",
    "            v = row.get(lbl, np.nan)\n",
    "            d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "            o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "            # Require non-null for value, date, origin FP\n",
    "            if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            # Normalize date and check it lies within [cutoff, pit]\n",
    "            d = pd.to_datetime(d, errors='coerce')\n",
    "            if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            vals.append(float(v))\n",
    "            dts.append(d)\n",
    "            fps.append(int(o))\n",
    "\n",
    "        # Sum all component values, pick latest date and max origin FP\n",
    "        total_val = float(np.nansum(vals))\n",
    "        latest_dt = max(dts)\n",
    "        origin_fp = max(fps)\n",
    "        return latest_dt, total_val, origin_fp\n",
    "\n",
    "    def pick_annpit_sum_with_origin(row):\n",
    "        \"\"\"\n",
    "        For each row, pick the best annual PIT-based value (AnnPITValue) by:\n",
    "\n",
    "        1) Building a time window [PIT - 365 days, PIT].\n",
    "        2) Generating candidate annual values:\n",
    "           - actual annual A (if within window and with origin FP),\n",
    "           - Q4 proxy from Q1..Q4,\n",
    "           - S2 proxy from S1..S2,\n",
    "           - T3 proxy from T1..T3,\n",
    "           each with associated priority and origin FP.\n",
    "        3) Ranking candidates according to:\n",
    "           - Same-year A (highest),\n",
    "           - Same-year proxies (Q4/T3/S2),\n",
    "           - Prior-year A,\n",
    "           - Prior-year proxies,\n",
    "           - Otherwise: any remaining candidate by (priority, latest date).\n",
    "        4) Returning the chosen candidate's value (keeping zeros, but filtering NaNs).\n",
    "        \"\"\"\n",
    "        pit = row['PIT Date']\n",
    "        if pd.isna(pit):\n",
    "            return np.nan\n",
    "        cutoff = pit - timedelta(days=365)  # Now works because timedelta is imported\n",
    "\n",
    "        # Current row's fiscal period, cast to int if possible\n",
    "        fp = row.get('FiscalPeriod', np.nan)\n",
    "        try:\n",
    "            fp_int = int(fp) if not pd.isna(fp) else None\n",
    "        except Exception:\n",
    "            fp_int = None\n",
    "\n",
    "        # Collect candidate tuples: (label, priority, date, value, origin_fp)\n",
    "        candidates = []\n",
    "\n",
    "        # --- Candidate A: actual annual (0 is allowed)\n",
    "        A_val = row.get('A', np.nan)\n",
    "        A_dt  = row.get('A_Date', pd.NaT)\n",
    "        A_ofp = row.get('A_OriginFP', np.nan)\n",
    "        if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "            A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "            if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "                candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "        # --- Candidate Q4: sum of Q1..Q4 (0 allowed)\n",
    "        q4_dt, q4_val, q4_fp = full_year_from_fixed(\n",
    "            row, [f'Q{i}' for i in range(1, 5)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "            candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "        # --- Candidate S2: sum of S1..S2 (0 allowed)\n",
    "        s2_dt, s2_val, s2_fp = full_year_from_fixed(\n",
    "            row, [f'S{i}' for i in range(1, 3)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(s2_val) and pd.notna(s2_dt) and not pd.isna(s2_fp):\n",
    "            candidates.append(('S2', _PERIOD_PRIORITY['S2'], s2_dt, float(s2_val), int(s2_fp)))\n",
    "\n",
    "        # --- Candidate T3: sum of T1..T3 (0 allowed)\n",
    "        t3_dt, t3_val, t3_fp = full_year_from_fixed(\n",
    "            row, [f'T{i}' for i in range(1, 4)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(t3_val) and pd.notna(t3_dt) and not pd.isna(t3_fp):\n",
    "            candidates.append(('T3', _PERIOD_PRIORITY['T3'], t3_dt, float(t3_val), int(t3_fp)))\n",
    "\n",
    "        # If no candidates, return NaN\n",
    "        if not candidates:\n",
    "            return np.nan\n",
    "\n",
    "        # Filter out NaN-valued candidates (keep 0-valued ones)\n",
    "        def valid(seq):\n",
    "            return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "        # Relationship between row's fiscal period and candidate's origin FP:\n",
    "        # same / prior / other / unknown\n",
    "        def rel(c):\n",
    "            _, _, _, _, ofp = c\n",
    "            if fp_int is None or ofp is None:\n",
    "                return 'unknown'\n",
    "            if ofp == fp_int:\n",
    "                return 'same'\n",
    "            if ofp == fp_int - 1:\n",
    "                return 'prior'\n",
    "            return 'other'\n",
    "\n",
    "        # 1) Same-year actual annual A (max by date)\n",
    "        same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "        if same_A:\n",
    "            best = max(same_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 2) Same-year proxies (Q4/T3/S2), max by (priority, date)\n",
    "        same_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'same'\n",
    "        )\n",
    "        if same_proxies:\n",
    "            best = max(same_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 3) Prior-year actual annual A\n",
    "        prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "        if prior_A:\n",
    "            best = max(prior_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 4) Prior-year proxies (Q4/S2/T3) if no prior A\n",
    "        prior_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'prior'\n",
    "        )\n",
    "        if prior_proxies:\n",
    "            best = max(prior_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "        others = valid(candidates)\n",
    "        if others:\n",
    "            best = max(others, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # Final fallback: 0.0 (should rarely be reached)\n",
    "        return 0.0\n",
    "\n",
    "    # Apply the selection function row-wise to produce AnnPITValue\n",
    "    working['AnnPITValue'] = working.apply(pick_annpit_sum_with_origin, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check + PRE-DROP stats\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Columns whose dates should not exceed PIT Date\n",
    "    date_cols = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Restrict to ones actually present\n",
    "    present = [c for c in date_cols if c in working.columns]\n",
    "\n",
    "    viol_counts = {}  # per-label violation counts\n",
    "    # Mask for rows with any future-dated period\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        # A violation is when period date > PIT Date (both need to be non-null)\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m  # accumulate violations across columns\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flagrows with at least one future-date error\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct + quality drop\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute AnnPITValue as % of TrueValue (only when TrueValue != 0)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Stats before dropping low-quality rows\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    # Build masks for dropping:\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)  # infinite percentages\n",
    "    is_finite = np.isfinite(pct)\n",
    "    # Out-of-range if % > 200 or % < 50 (but finite)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    # Rows to drop: infinite or out-of-range values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that passed the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Stats after dropping\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns & save\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Base descriptive columns to keep (if present)\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Build ordered list of period date/value columns\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Final set of columns to keep in output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Helper columns to drop before export\n",
    "    drop_cols = ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    # Also drop all *_OriginFP columns\n",
    "    drop_cols += [c for c in working.columns if c.endswith('_OriginFP')]\n",
    "    working.drop(columns=[c for c in drop_cols if c in working.columns],\n",
    "                 inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder and restrict columns to the final layout\n",
    "    mixed_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # Sanity checks: necessary globals must exist\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Build full output path and save pipe-delimited file\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    mixed_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Create a subset for lighter inspection\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "                   \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    # Only keep subset columns that actually exist\n",
    "    subset_cols_existing = [col for col in subset_cols if col in mixed_processed.columns]\n",
    "    subset_df = mixed_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP,\n",
    "                              f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    # Explicitly delete subset_df to free memory\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(mixed_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excludedby Frequency (E/L/R/U): {excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    # Sum up excluded + dropped + remaining and check against original count\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. \"\n",
    "              \"Investigate upstream filtering or unexpected drops.\")\n",
    "\n",
    "    # Trigger garbage collection as a final cleanup step\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # If the main input dataset is not available, skip all processing\n",
    "    print(\"mixed_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6c9PzcC6OOp"
   },
   "source": [
    "### Mixed 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyjmiRpL6OO0"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1765651165718,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "O0Z4Nia16OO1",
    "outputId": "52ce1db3-e842-402d-d86a-f2391c7f1a56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Mixed_7  ->  ItemName: 'Net_Income_Used_to_Calculate_Basic_EPS'\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This code selects which Mixed_* dataset should be processed by choosing an index\n",
    "# (e.g., Mixed_1, Mixed_2, ...). It then:\n",
    "#\n",
    "#   1. Ensures that a dictionary `mixed_vars` exists, mapping keys like \"Mixed_1\"\n",
    "#      to item names.\n",
    "#   2. Builds the key corresponding to the selected index and retrieves the\n",
    "#      associated item name (`target_item_name`).\n",
    "#   3. Prints which Mixed_* item was selected.\n",
    "#   4. Constructs paths and filenames based on global variables and the selected item.\n",
    "#   5. Ensures that the output directory exists by creating it if necessary.\n",
    "#\n",
    "# The goal is to centralize selection of a single Mixed_* dataset and prepare paths\n",
    "# for downstream processing.\n",
    "\n",
    "\n",
    "# === Select which Mixed_* item to run ===\n",
    "mixed_index = 7  # Change this to process another dataset (e.g., 10)\n",
    "\n",
    "# Validate that the dictionary of mixed item names exists\n",
    "assert 'mixed_vars' in globals(), \"mixed_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key (e.g., \"Mixed_1\") and retrieve the associated item name\n",
    "item_key = f\"Mixed_{mixed_index}\"\n",
    "target_item_name = mixed_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in mixed_vars.\"\n",
    "\n",
    "# Inform which item was selected\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Construct the name of the input file for the selected item\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Construct the base name for output files (will later be suffixed)\n",
    "base_output_filename = f\"Mixed_{target_item_name}_complete\"\n",
    "\n",
    "# Ensure the output directory exists; create it (including parent dirs) if needed\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACFGXfeo6OO3"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 1182,
     "status": "ok",
     "timestamp": 1765651166908,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "fDWZ6Pm66OO4",
    "outputId": "9a5ac275-0619-4a7c-b036-6d39dc8e172f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Net_Income_Used_to_Calculate_Basic_EPS' ...\n",
      "Full dataset loaded successfully: 3,750,698 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1706</td>\n",
       "      <td>97.90298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1706</td>\n",
       "      <td>92.230294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1706</td>\n",
       "      <td>110.555887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1706</td>\n",
       "      <td>-43.448501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1706</td>\n",
       "      <td>39.188143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     1706    97.90298  \n",
       "1     1706   92.230294  \n",
       "2     1706  110.555887  \n",
       "3     1706  -43.448501  \n",
       "4     1706   39.188143  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block:\n",
    "#   1. Announces the import of a full dataset for the given `target_item_name`.\n",
    "#   2. Checks whether the file at `file_path` exists.\n",
    "#   3. If it exists, calls `import_file_to_dataframe(file_path)` to load the data\n",
    "#      into `mixed_raw`.\n",
    "#   4. If the loaded DataFrame is non-empty, prints a success message including\n",
    "#      the number of rows and shows the first few rows (via display or fallback\n",
    "#      to text printing).\n",
    "#   5. If the load fails or returns an empty DataFrame, prints a warning and\n",
    "#      creates an empty DataFrame.\n",
    "#   6. If the file does not exist, prints an error message and sets `mixed_raw`\n",
    "#      to an empty DataFrame.\n",
    "#   7. Finally, it runs `gc.collect()` to trigger garbage collection and free\n",
    "#      memory.\n",
    "#\n",
    "# Note: All previous emoji symbols in the print statements have been removed.\n",
    "\n",
    "# Inform the user that we are starting the import for this item\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "# Check if the file path exists on disk\n",
    "if os.path.exists(file_path):\n",
    "    # If the file exists, attempt to import it into a DataFrame\n",
    "    mixed_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    # If the DataFrame is successfully loaded and not empty\n",
    "    if mixed_raw is not None and not mixed_raw.empty:\n",
    "        # Print a success message including row count with thousands separator\n",
    "        print(f\"Full dataset loaded successfully: {len(mixed_raw):,} rows total.\")\n",
    "\n",
    "        try:\n",
    "            # Try to display the first few rows (Jupyter / IPython display)\n",
    "            display(mixed_raw.head())\n",
    "        except Exception:\n",
    "            # If display is not available, fall back to printing as plain text\n",
    "            print(mixed_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        # If DataFrame is None or empty, warn and create an empty DataFrame\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        mixed_raw = pd.DataFrame()\n",
    "else:\n",
    "    # If the file does not exist, report an error and set mixed_raw to empty\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    mixed_raw = pd.DataFrame()\n",
    "\n",
    "# Run garbage collection to free up memory after the load attempt\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POJ_JyKz6OO4"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1765651167230,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "p1Fs-WvH6OO5",
    "outputId": "979525c0-ef4d-451b-944b-f2ad5d1d4d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Mixed dataset for 'Net_Income_Used_to_Calculate_Basic_EPS' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_581638/2687430345.py:85: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y00' 'Q2Y00' ... 'Q3Y07' 'Q4Y07' 'Q4Y05']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1706</td>\n",
       "      <td>97.90298</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1706</td>\n",
       "      <td>92.230294</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1706</td>\n",
       "      <td>110.555887</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1706</td>\n",
       "      <td>-43.448501</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1706</td>\n",
       "      <td>39.188143</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     1706    97.90298           Y92  \n",
       "1     1706   92.230294           Y93  \n",
       "2     1706  110.555887           Y94  \n",
       "3     1706  -43.448501           Y95  \n",
       "4     1706   39.188143           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This snippet provides:\n",
    "#\n",
    "# 1. A helper function `last2` that returns the last two digits of a number as a\n",
    "#    zero-padded string (for building YY strings).\n",
    "#\n",
    "# 2. A function `add_str_fiscalprd(df)` which:\n",
    "#    - Works on a copy of an input DataFrame containing financial periods.\n",
    "#    - Normalizes the 'Frequency' (upper-case, no missing).\n",
    "#    - Stores the original 'FiscalPeriod' and converts it to numeric.\n",
    "#    - Creates a string representation 'Str_FiscalPrd' depending on the frequency:\n",
    "#         - Q/C/E/R: quarter-based (\"QnYyy\")\n",
    "#         - A/B: annual (\"Yyy\")\n",
    "#         - F/S: semiannual (\"SnYyy\")\n",
    "#         - K/T/L/U: trimester-like (\"TnYyy\")\n",
    "#    - Derives an implied full-year integer 'ImplFiscPer_Calculated' from the\n",
    "#      two-digit year (80–99 => 19xx, else 20xx).\n",
    "#    - For annual rows (A/B), checks discrepancies between original\n",
    "#      'FiscalPeriod' and implied full-year; prints a small preview & total count.\n",
    "#    - Overwrites 'FiscalPeriod' with 'ImplFiscPer_Calculated' and drops helper\n",
    "#      columns.\n",
    "#\n",
    "# 3. A small driver block that:\n",
    "#    - Checks that `mixed_raw` exists and is non-empty.\n",
    "#    - Applies `add_str_fiscalprd` to produce `mixed_encoded`.\n",
    "#    - Displays a head preview or prints a message and sets `mixed_encoded = None`\n",
    "#      if input is missing/empty.\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    # If input is NaN (or pandas-style missing), return None (no digits)\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Convert to int, format as 4-digit zero-padded string, take last 2 chars\n",
    "    # Example: n=2023 -> \"2023\"[-2:] -> \"23\"\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    \"\"\"\n",
    "    Creates 'Str_FiscalPrd' and overwrites 'FiscalPeriod' with an implied full year.\n",
    "    Also prints discrepancies for Annual (A,B) rows between original and calculated year.\n",
    "\n",
    "    The mapping logic:\n",
    "      - Quarterly (C, Q, E, R):   fp encodes a quarter index -> \"Q{1-4}Y{yy}\"\n",
    "      - Annual   (A, B):         fp is the year itself      -> \"Y{yy}\"\n",
    "      - Semiannual (F, S):       fp encodes half-year       -> \"S{1-2}Y{yy}\"\n",
    "      - Trimester-like (K, T,\n",
    "         L, U):                  fp encodes trimester       -> \"T{1-3}Y{yy}\"\n",
    "    Then we re-infer the full year from the YY part with 19xx/20xx rule.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the original input DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes:\n",
    "    # - uppercase for consistency\n",
    "    # - fill missing values with empty string\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod in a separate column for later comparison\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    # Convert FiscalPeriod to numeric, coerce errors to NaN\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Masks for different frequency groups\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])  # quarter-based\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])            # annual\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])            # semiannual\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])  # trimester-like\n",
    "\n",
    "    # Initialize Str_FiscalPrd as NaN; we will fill per frequency group\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Quarter-based encoding\n",
    "    # -------------------------\n",
    "    # Quarter number: (fp % 4) + 1 => yields 1..4 (if fp is integer-based encoding)\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year part for quarter: integer division by 4, then take last two digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    # Assign \"Q{quarter}Y{yy}\" for quarter frequencies\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" +\n",
    "        q_part.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Annual encoding (A, B)\n",
    "    # -------------------------\n",
    "    # Year part is fp itself for A/B; take last two digits via last2\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    # Assign \"Y{yy}\" for annual frequencies\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    # -------------------------\n",
    "    # Semiannual encoding (F, S)\n",
    "    # -------------------------\n",
    "    # Semester number: (fp % 2) + 1 => 1 or 2\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    # Year part: fp // 2, then last two digits\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    # Assign \"S{sem}Y{yy}\" for F/S\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" +\n",
    "        fs_sem.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Trimester-like encoding (K, T, L, U)\n",
    "    # -------------------------\n",
    "    # Term number: (fp % 3) + 1 => 1..3\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    # Year part: fp // 3, then last two digits\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    # Assign \"T{term}Y{yy}\" for K/T/L/U frequencies\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" +\n",
    "        t_term.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Implied full year from Str_FiscalPrd (19xx / 20xx reconstruction)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Extract two-digit year from \"Yyy\" pattern, e.g., \"Q1Y23\" -> \"23\"\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    # Convert that to numeric (NaN if not parseable)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    # Map two-digit year to full year:\n",
    "    #  - 80–99 => 19xx\n",
    "    #  - else  => 20xx\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Discrepancies check for Annual frequencies (A, B)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Work on a subset of annual frequencies only\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "\n",
    "    # Build boolean mask where implied full year does NOT match the original\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Either they match numerically...\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce'))\n",
    "        |\n",
    "        # ...or both are NaN\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    # Subset rows with discrepancies\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    # If we have any, show a short sample and the total count\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Overwrite FiscalPeriod and drop temporary helper columns\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Replace 'FiscalPeriod' with the implied full-year value we just calculated\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    # Drop helper columns not needed downstream\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to mixed_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'mixed_raw' in globals() and mixed_raw is not None and not mixed_raw.empty:\n",
    "    # Inform which item we are encoding for\n",
    "    print(f\"Applying encoding to Mixed dataset for '{target_item_name}' ...\")\n",
    "    # Apply fiscal period encoding\n",
    "    mixed_encoded = add_str_fiscalprd(mixed_raw)\n",
    "    # Show a preview of the encoded data\n",
    "    display(mixed_encoded.head())\n",
    "else:\n",
    "    # If mixed_raw is missing or empty, we skip and mark mixed_encoded as None\n",
    "    print(\"mixed_raw not found or empty. Cannot perform encoding.\")\n",
    "    mixed_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpv0bCD16OO6"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8027,
     "status": "ok",
     "timestamp": 1765651175259,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "Cs9oYN6W6OO6",
    "outputId": "7f9de6b4-459e-42b5-a428-0045a0ca52e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,750,698 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\n",
      "         finite_rows: 2187307\n",
      "                mean: 27694.772770478314\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.52074516226975\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 38,759\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 2148548\n",
      "                mean: 100.18606378381752\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.07000015956615\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Net_Income_Used_to_Calculate_Basic_EPS_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Net_Income_Used_to_Calculate_Basic_EPS_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,750,698\n",
      "Excludedby Frequency (E/L/R/U): 0\n",
      "Dropped by quality (Pct rules): 38,759\n",
      "Output rows (final):            3,711,939\n",
      "Check: excluded + dropped + output = 3,750,698\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This script takes an input DataFrame `mixed_encoded` (if present in the global scope)\n",
    "# that contains financial time-series data (per company, item, currency, fiscal period,\n",
    "# and PIT Date). It then:\n",
    "#\n",
    "# 1. Cleans and standardizes key columns (dates, numeric types, string IDs).\n",
    "# 2. Excludes rows with certain frequencies (E/L/R/U).\n",
    "# 3. Parses fiscal period strings into quarter/semester/trimester indicators (QNUM/SNUM/TNUM).\n",
    "# 4. Uses a custom, vectorized \"as-of\" join (`asof_numpy`) to attach the most recent\n",
    "#    annual, quarterly, semiannual, and trimester values for each (ID, HistCurrency,\n",
    "#    ItemCode, FiscalPeriod) up to each row’s PIT Date.\n",
    "# 5. Builds \"full-year\" candidate values from:\n",
    "#       - actual annuals (A),\n",
    "#       - sum of Q1..Q4 (Q4 proxy),\n",
    "#       - sum of S1..S2 (S2 proxy),\n",
    "#       - sum of T1..T3 (T3 proxy),\n",
    "#    and selects the best candidate based on priorities and relationship to the row’s\n",
    "#    fiscal period (same-year vs prior-year).\n",
    "# 6. Computes an annual PIT-based metric `AnnPITValue` and compares it to the “true”\n",
    "#    annual value (`TrueValue`) to derive a percentage `AnnPITValue_Pct` for QC.\n",
    "# 7. Performs quality checks:\n",
    "#       - Ensures no period-date is after the PIT Date.\n",
    "#       - Drops rows whose `AnnPITValue_Pct` is outside the range [50, 200] or infinite.\n",
    "# 8. Keeps a curated set of columns, drops helper columns, and saves:\n",
    "#       - a full output file\n",
    "#       - a subset file with key columns for quick inspection.\n",
    "# 9. Prints row-accounting stats and frees some memory.\n",
    "#\n",
    "# If `mixed_encoded` is not defined or is None, it simply prints a message and exits.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta            # <--- Added to fix NameError\n",
    "from scipy.stats.mstats import winsorize  # <--- Added to fix NameError\n",
    "\n",
    "# Enable pandas \"copy-on-write\" behavior to reduce unintended chained assignment effects\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Helper function to build a string key from multiple columns.\n",
    "    For each row, join the values of 'cols' with '||'.\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    This is a manual, NumPy-based implementation of an \"as-of merge\" grouped by `by_cols`.\n",
    "    It:\n",
    "    - filters out rows with missing required fields,\n",
    "    - normalizes PIT Date to daily precision,\n",
    "    - groups right_df by composite key of `by_cols`,\n",
    "    - for each left row, binary-searches into the matching right group by PIT Date,\n",
    "      picking the last date <= the left PIT Date,\n",
    "    - returns two aligned arrays: (values, dates) for the left_df rows.\n",
    "    \"\"\"\n",
    "    # Initialize outputs with NaNs and NaT for all left_df rows\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns on each side\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask: rows that have all required fields non-null\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If either side has no valid rows, return the default empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on filtered copies only (avoid side effects)\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime at day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build composite keys for group-level match (based on by_cols)\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and date to enable binary search per key\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract NumPy arrays for right side\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and first index for each key in the sorted right side\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Build a dict: key -> (dates array, values array) slice\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]  # start of this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end of this key\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Left side indices and arrays\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left side by key (stable mergesort to preserve original row order within key)\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # i iterates over the sorted left rows\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current composite key\n",
    "        j = i + 1\n",
    "        # Find contiguous block [i:j) for this key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Dates and original positions of this key’s left rows\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # If we have matching right-hand slices for this key, do the as-of search\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # searchsorted(..., 'right') - 1 gives index of last r_date <= block_date\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # positions where such a date exists\n",
    "            if np.any(valid):\n",
    "                # Fill outputs for left rows where we found a valid match\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        # Move to next key block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile calculation: returns quantile q of Series `s`,\n",
    "    or NaN if quantile fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Computes a summary of finite values of a series:\n",
    "      - count\n",
    "      - mean, median\n",
    "      - 1% winsorized mean\n",
    "      - selected deciles (p10, p20, ..., p90)\n",
    "    Infinite values are treated as NaN and removed.\n",
    "    If no finite values remain, returns an empty dict.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop all NaNs\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures the array is writable for winsorize\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Priority for full-year candidates ----------\n",
    "\n",
    "# Fixed priority mapping for full-year candidates:\n",
    "#   'A'  : actual annual value\n",
    "#   'Q4' : annual proxy from four quarters\n",
    "#   'T3' : annual proxy from three trimesters\n",
    "#   'S2' : annual proxy from two semesters\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # highest priority: actual annual\n",
    "    'Q4': 90,  # then Q1+Q2+Q3+Q4\n",
    "    'T3': 80,  # then T1+T2+T3\n",
    "    'S2': 70,  # then S1+S2\n",
    "}\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "# Only run the main logic if a global DataFrame `mixed_encoded` exists and is not None\n",
    "if 'mixed_encoded' in globals() and mixed_encoded is not None:\n",
    "    # Count initial input rows\n",
    "    input_rows = len(mixed_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy of the input dataset\n",
    "    working = mixed_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Exclude certain frequencies (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create mask of rows whose Frequency is one of E, L, R, U (case-insensitive)\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    # Count how many rows will be excluded\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    # Keep only rows that are NOT in the exclusion set\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Set dtypes and normalize important columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert PIT Date to datetime (coerce errors -> NaT), floor to day\n",
    "    working['PIT Date']     = pd.to_datetime(working['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    # FiscalPeriod: numeric (e.g., 2021, 2022, ...)\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    # Value: numeric (float)\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Convert key ID / code columns to string to ensure consistency\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse Q/S/T markers from Str_FiscalPrd (like 'Q1Y2023')\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter number Q1..Q4 from e.g. \"Q1Y2023\" into QNUM\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual number S1..S2 into SNUM\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester number T1..T3 into TNUM\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ensure period columns exist (Q1..Q4, S1..S2, T1..T3, A + their date cols)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create value columns for Q1..Q4, S1..S2, T1..T3, A if they are missing\n",
    "    for c in [*(f'Q{i}' for i in range(1, 5)),\n",
    "              *(f'S{i}' for i in range(1, 3)),\n",
    "              *(f'T{i}' for i in range(1, 4)),\n",
    "              'A']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Create corresponding *_Date columns if missing\n",
    "    for c in [*(f'Q{i}_Date' for i in range(1, 5)),\n",
    "              *(f'S{i}_Date' for i in range(1, 3)),\n",
    "              *(f'T{i}_Date' for i in range(1, 4)),\n",
    "              'A_Date']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Base key for many of the as-of mappings\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Derive TrueValue from annuals (A/B frequencies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Mask annual-like rows where Value is present\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    # annual_src: one row per (ID, FiscalPeriod, HistCurrency) with last PIT Date\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Left-join true annual value back onto working\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping (same FiscalPeriod) for A/Q/S/T\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----- Annual -----\n",
    "    # Source rows for annual frequencies A/B\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    # As-of join: for each working row, get most recent annual value by PIT Date\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period of annual value (same as row's FiscalPeriod when present)\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # ----- Quarterly -----\n",
    "    # Source rows for quarterly frequencies (Q/C) with valid QNUM\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Restrict to a specific quarter q\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        # As-of join for that quarter\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        # Origin FP column for that quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        # Fill origin FP only where quarter value is non-null and origin not yet set\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Semiannual -----\n",
    "    # Source rows for semiannual frequencies (S/F) with valid SNUM\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Trimester -----\n",
    "    # Source rows for trimester frequencies (T/K) with valid TNUM\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels & normalize dates (NO prev-year fill, NO forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort working data consistently for downstream calculations\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    # List of all period value columns\n",
    "    value_cols_all  = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                      [f'S{i}' for i in range(1, 3)] + \\\n",
    "                      [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    # Corresponding date columns\n",
    "    date_cols_all   = [f'{c}_Date' for c in value_cols_all]\n",
    "    # Corresponding origin FP columns\n",
    "    origin_cols_all = [f'{c}_OriginFP' for c in value_cols_all]\n",
    "\n",
    "    # Ensure that all date columns are proper datetimes (floor to day)\n",
    "    # Note: explicitly no groupby-forward-fill here – only asof-filled values remain\n",
    "    for c in date_cols_all:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Build full-year candidates from fixed sets (Q1–Q4, S1–S2, T1–T3)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def full_year_from_fixed(row, labels, pit, cutoff):\n",
    "        \"\"\"\n",
    "        Fixed full-year from given labels (e.g. Q1..Q4, S1..S2, T1..T3):\n",
    "\n",
    "          - Requires ALL labels to have:\n",
    "              value, date, origin_fp.\n",
    "          - All dates must be within [cutoff, pit].\n",
    "          - origin_fp = max(origin_fp_i)  (the 'newest year' across components).\n",
    "\n",
    "        Returns (dt, val, origin_fp) or (NaT, NaN, NaN) if any requirement fails.\n",
    "        \"\"\"\n",
    "        vals, dts, fps = [], [], []\n",
    "        for lbl in labels:\n",
    "            v = row.get(lbl, np.nan)\n",
    "            d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "            o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "            # Require non-null for value, date, origin FP\n",
    "            if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            # Normalize date and check it lies within [cutoff, pit]\n",
    "            d = pd.to_datetime(d, errors='coerce')\n",
    "            if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            vals.append(float(v))\n",
    "            dts.append(d)\n",
    "            fps.append(int(o))\n",
    "\n",
    "        # Sum all component values, pick latest date and max origin FP\n",
    "        total_val = float(np.nansum(vals))\n",
    "        latest_dt = max(dts)\n",
    "        origin_fp = max(fps)\n",
    "        return latest_dt, total_val, origin_fp\n",
    "\n",
    "    def pick_annpit_sum_with_origin(row):\n",
    "        \"\"\"\n",
    "        For each row, pick the best annual PIT-based value (AnnPITValue) by:\n",
    "\n",
    "        1) Building a time window [PIT - 365 days, PIT].\n",
    "        2) Generating candidate annual values:\n",
    "           - actual annual A (if within window and with origin FP),\n",
    "           - Q4 proxy from Q1..Q4,\n",
    "           - S2 proxy from S1..S2,\n",
    "           - T3 proxy from T1..T3,\n",
    "           each with associated priority and origin FP.\n",
    "        3) Ranking candidates according to:\n",
    "           - Same-year A (highest),\n",
    "           - Same-year proxies (Q4/T3/S2),\n",
    "           - Prior-year A,\n",
    "           - Prior-year proxies,\n",
    "           - Otherwise: any remaining candidate by (priority, latest date).\n",
    "        4) Returning the chosen candidate's value (keeping zeros, but filtering NaNs).\n",
    "        \"\"\"\n",
    "        pit = row['PIT Date']\n",
    "        if pd.isna(pit):\n",
    "            return np.nan\n",
    "        cutoff = pit - timedelta(days=365)  # Now works because timedelta is imported\n",
    "\n",
    "        # Current row's fiscal period, cast to int if possible\n",
    "        fp = row.get('FiscalPeriod', np.nan)\n",
    "        try:\n",
    "            fp_int = int(fp) if not pd.isna(fp) else None\n",
    "        except Exception:\n",
    "            fp_int = None\n",
    "\n",
    "        # Collect candidate tuples: (label, priority, date, value, origin_fp)\n",
    "        candidates = []\n",
    "\n",
    "        # --- Candidate A: actual annual (0 is allowed)\n",
    "        A_val = row.get('A', np.nan)\n",
    "        A_dt  = row.get('A_Date', pd.NaT)\n",
    "        A_ofp = row.get('A_OriginFP', np.nan)\n",
    "        if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "            A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "            if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "                candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "        # --- Candidate Q4: sum of Q1..Q4 (0 allowed)\n",
    "        q4_dt, q4_val, q4_fp = full_year_from_fixed(\n",
    "            row, [f'Q{i}' for i in range(1, 5)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "            candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "        # --- Candidate S2: sum of S1..S2 (0 allowed)\n",
    "        s2_dt, s2_val, s2_fp = full_year_from_fixed(\n",
    "            row, [f'S{i}' for i in range(1, 3)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(s2_val) and pd.notna(s2_dt) and not pd.isna(s2_fp):\n",
    "            candidates.append(('S2', _PERIOD_PRIORITY['S2'], s2_dt, float(s2_val), int(s2_fp)))\n",
    "\n",
    "        # --- Candidate T3: sum of T1..T3 (0 allowed)\n",
    "        t3_dt, t3_val, t3_fp = full_year_from_fixed(\n",
    "            row, [f'T{i}' for i in range(1, 4)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(t3_val) and pd.notna(t3_dt) and not pd.isna(t3_fp):\n",
    "            candidates.append(('T3', _PERIOD_PRIORITY['T3'], t3_dt, float(t3_val), int(t3_fp)))\n",
    "\n",
    "        # If no candidates, return NaN\n",
    "        if not candidates:\n",
    "            return np.nan\n",
    "\n",
    "        # Filter out NaN-valued candidates (keep 0-valued ones)\n",
    "        def valid(seq):\n",
    "            return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "        # Relationship between row's fiscal period and candidate's origin FP:\n",
    "        # same / prior / other / unknown\n",
    "        def rel(c):\n",
    "            _, _, _, _, ofp = c\n",
    "            if fp_int is None or ofp is None:\n",
    "                return 'unknown'\n",
    "            if ofp == fp_int:\n",
    "                return 'same'\n",
    "            if ofp == fp_int - 1:\n",
    "                return 'prior'\n",
    "            return 'other'\n",
    "\n",
    "        # 1) Same-year actual annual A (max by date)\n",
    "        same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "        if same_A:\n",
    "            best = max(same_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 2) Same-year proxies (Q4/T3/S2), max by (priority, date)\n",
    "        same_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'same'\n",
    "        )\n",
    "        if same_proxies:\n",
    "            best = max(same_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 3) Prior-year actual annual A\n",
    "        prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "        if prior_A:\n",
    "            best = max(prior_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 4) Prior-year proxies (Q4/S2/T3) if no prior A\n",
    "        prior_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'prior'\n",
    "        )\n",
    "        if prior_proxies:\n",
    "            best = max(prior_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "        others = valid(candidates)\n",
    "        if others:\n",
    "            best = max(others, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # Final fallback: 0.0 (should rarely be reached)\n",
    "        return 0.0\n",
    "\n",
    "    # Apply the selection function row-wise to produce AnnPITValue\n",
    "    working['AnnPITValue'] = working.apply(pick_annpit_sum_with_origin, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check + PRE-DROP stats\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Columns whose dates should not exceed PIT Date\n",
    "    date_cols = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Restrict to ones actually present\n",
    "    present = [c for c in date_cols if c in working.columns]\n",
    "\n",
    "    viol_counts = {}  # per-label violation counts\n",
    "    # Mask for rows with any future-dated period\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        # A violation is when period date > PIT Date (both need to be non-null)\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m  # accumulate violations across columns\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flagrows with at least one future-date error\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct + quality drop\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute AnnPITValue as % of TrueValue (only when TrueValue != 0)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Stats before dropping low-quality rows\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    # Build masks for dropping:\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)  # infinite percentages\n",
    "    is_finite = np.isfinite(pct)\n",
    "    # Out-of-range if % > 200 or % < 50 (but finite)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    # Rows to drop: infinite or out-of-range values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that passed the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Stats after dropping\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns & save\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Base descriptive columns to keep (if present)\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Build ordered list of period date/value columns\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Final set of columns to keep in output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Helper columns to drop before export\n",
    "    drop_cols = ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    # Also drop all *_OriginFP columns\n",
    "    drop_cols += [c for c in working.columns if c.endswith('_OriginFP')]\n",
    "    working.drop(columns=[c for c in drop_cols if c in working.columns],\n",
    "                 inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder and restrict columns to the final layout\n",
    "    mixed_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # Sanity checks: necessary globals must exist\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Build full output path and save pipe-delimited file\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    mixed_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Create a subset for lighter inspection\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "                   \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    # Only keep subset columns that actually exist\n",
    "    subset_cols_existing = [col for col in subset_cols if col in mixed_processed.columns]\n",
    "    subset_df = mixed_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP,\n",
    "                              f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    # Explicitly delete subset_df to free memory\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(mixed_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excludedby Frequency (E/L/R/U): {excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    # Sum up excluded + dropped + remaining and check against original count\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. \"\n",
    "              \"Investigate upstream filtering or unexpected drops.\")\n",
    "\n",
    "    # Trigger garbage collection as a final cleanup step\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # If the main input dataset is not available, skip all processing\n",
    "    print(\"mixed_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJTHauUwmlXA"
   },
   "source": [
    "### Mixed 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy8emrH0mlXO"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1765651175398,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "fbe7HcVQmlXQ",
    "outputId": "cda35bc4-b675-4ebf-f2fe-0b083aa7fd73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Mixed_8  ->  ItemName: 'Net_Sales_or_Revenues'\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This code selects which Mixed_* dataset should be processed by choosing an index\n",
    "# (e.g., Mixed_1, Mixed_2, ...). It then:\n",
    "#\n",
    "#   1. Ensures that a dictionary `mixed_vars` exists, mapping keys like \"Mixed_1\"\n",
    "#      to item names.\n",
    "#   2. Builds the key corresponding to the selected index and retrieves the\n",
    "#      associated item name (`target_item_name`).\n",
    "#   3. Prints which Mixed_* item was selected.\n",
    "#   4. Constructs paths and filenames based on global variables and the selected item.\n",
    "#   5. Ensures that the output directory exists by creating it if necessary.\n",
    "#\n",
    "# The goal is to centralize selection of a single Mixed_* dataset and prepare paths\n",
    "# for downstream processing.\n",
    "\n",
    "\n",
    "# === Select which Mixed_* item to run ===\n",
    "mixed_index = 8  # Change this to process another dataset (e.g., 10)\n",
    "\n",
    "# Validate that the dictionary of mixed item names exists\n",
    "assert 'mixed_vars' in globals(), \"mixed_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key (e.g., \"Mixed_1\") and retrieve the associated item name\n",
    "item_key = f\"Mixed_{mixed_index}\"\n",
    "target_item_name = mixed_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in mixed_vars.\"\n",
    "\n",
    "# Inform which item was selected\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Construct the name of the input file for the selected item\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Construct the base name for output files (will later be suffixed)\n",
    "base_output_filename = f\"Mixed_{target_item_name}_complete\"\n",
    "\n",
    "# Ensure the output directory exists; create it (including parent dirs) if needed\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkO_AqPWmlXU"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1765651175525,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "IjSKYAklmlXY",
    "outputId": "9b4f6403-f14d-4f83-b263-d0d75180a235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Net_Sales_or_Revenues' ...\n",
      "Full dataset loaded successfully: 4,003,129 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1001</td>\n",
       "      <td>1707.334371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1001</td>\n",
       "      <td>1921.517802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1001</td>\n",
       "      <td>2263.705199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1001</td>\n",
       "      <td>1447.33116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1001</td>\n",
       "      <td>1217.952697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode        Value  \n",
       "0     1001  1707.334371  \n",
       "1     1001  1921.517802  \n",
       "2     1001  2263.705199  \n",
       "3     1001   1447.33116  \n",
       "4     1001  1217.952697  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block:\n",
    "#   1. Announces the import of a full dataset for the given `target_item_name`.\n",
    "#   2. Checks whether the file at `file_path` exists.\n",
    "#   3. If it exists, calls `import_file_to_dataframe(file_path)` to load the data\n",
    "#      into `mixed_raw`.\n",
    "#   4. If the loaded DataFrame is non-empty, prints a success message including\n",
    "#      the number of rows and shows the first few rows (via display or fallback\n",
    "#      to text printing).\n",
    "#   5. If the load fails or returns an empty DataFrame, prints a warning and\n",
    "#      creates an empty DataFrame.\n",
    "#   6. If the file does not exist, prints an error message and sets `mixed_raw`\n",
    "#      to an empty DataFrame.\n",
    "#   7. Finally, it runs `gc.collect()` to trigger garbage collection and free\n",
    "#      memory.\n",
    "#\n",
    "# Note: All previous emoji symbols in the print statements have been removed.\n",
    "\n",
    "# Inform the user that we are starting the import for this item\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "# Check if the file path exists on disk\n",
    "if os.path.exists(file_path):\n",
    "    # If the file exists, attempt to import it into a DataFrame\n",
    "    mixed_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    # If the DataFrame is successfully loaded and not empty\n",
    "    if mixed_raw is not None and not mixed_raw.empty:\n",
    "        # Print a success message including row count with thousands separator\n",
    "        print(f\"Full dataset loaded successfully: {len(mixed_raw):,} rows total.\")\n",
    "\n",
    "        try:\n",
    "            # Try to display the first few rows (Jupyter / IPython display)\n",
    "            display(mixed_raw.head())\n",
    "        except Exception:\n",
    "            # If display is not available, fall back to printing as plain text\n",
    "            print(mixed_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        # If DataFrame is None or empty, warn and create an empty DataFrame\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        mixed_raw = pd.DataFrame()\n",
    "else:\n",
    "    # If the file does not exist, report an error and set mixed_raw to empty\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    mixed_raw = pd.DataFrame()\n",
    "\n",
    "# Run garbage collection to free up memory after the load attempt\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-tZqZmZ74Kh"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1765651175738,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "By7a4qSm74Ki",
    "outputId": "a5b68dc8-c2a4-4271-deee-2b1739366562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Mixed dataset for 'Net_Sales_or_Revenues' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_581638/2687430345.py:85: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y94' 'Q2Y94' 'Q3Y94' ... 'Q4Y07' 'Q3Y07' 'Q4Y07']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1001</td>\n",
       "      <td>1707.334371</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1001</td>\n",
       "      <td>1921.517802</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1001</td>\n",
       "      <td>2263.705199</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1001</td>\n",
       "      <td>1447.33116</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1001</td>\n",
       "      <td>1217.952697</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode        Value Str_FiscalPrd  \n",
       "0     1001  1707.334371           Y92  \n",
       "1     1001  1921.517802           Y93  \n",
       "2     1001  2263.705199           Y94  \n",
       "3     1001   1447.33116           Y95  \n",
       "4     1001  1217.952697           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This snippet provides:\n",
    "#\n",
    "# 1. A helper function `last2` that returns the last two digits of a number as a\n",
    "#    zero-padded string (for building YY strings).\n",
    "#\n",
    "# 2. A function `add_str_fiscalprd(df)` which:\n",
    "#    - Works on a copy of an input DataFrame containing financial periods.\n",
    "#    - Normalizes the 'Frequency' (upper-case, no missing).\n",
    "#    - Stores the original 'FiscalPeriod' and converts it to numeric.\n",
    "#    - Creates a string representation 'Str_FiscalPrd' depending on the frequency:\n",
    "#         - Q/C/E/R: quarter-based (\"QnYyy\")\n",
    "#         - A/B: annual (\"Yyy\")\n",
    "#         - F/S: semiannual (\"SnYyy\")\n",
    "#         - K/T/L/U: trimester-like (\"TnYyy\")\n",
    "#    - Derives an implied full-year integer 'ImplFiscPer_Calculated' from the\n",
    "#      two-digit year (80–99 => 19xx, else 20xx).\n",
    "#    - For annual rows (A/B), checks discrepancies between original\n",
    "#      'FiscalPeriod' and implied full-year; prints a small preview & total count.\n",
    "#    - Overwrites 'FiscalPeriod' with 'ImplFiscPer_Calculated' and drops helper\n",
    "#      columns.\n",
    "#\n",
    "# 3. A small driver block that:\n",
    "#    - Checks that `mixed_raw` exists and is non-empty.\n",
    "#    - Applies `add_str_fiscalprd` to produce `mixed_encoded`.\n",
    "#    - Displays a head preview or prints a message and sets `mixed_encoded = None`\n",
    "#      if input is missing/empty.\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    # If input is NaN (or pandas-style missing), return None (no digits)\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Convert to int, format as 4-digit zero-padded string, take last 2 chars\n",
    "    # Example: n=2023 -> \"2023\"[-2:] -> \"23\"\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    \"\"\"\n",
    "    Creates 'Str_FiscalPrd' and overwrites 'FiscalPeriod' with an implied full year.\n",
    "    Also prints discrepancies for Annual (A,B) rows between original and calculated year.\n",
    "\n",
    "    The mapping logic:\n",
    "      - Quarterly (C, Q, E, R):   fp encodes a quarter index -> \"Q{1-4}Y{yy}\"\n",
    "      - Annual   (A, B):         fp is the year itself      -> \"Y{yy}\"\n",
    "      - Semiannual (F, S):       fp encodes half-year       -> \"S{1-2}Y{yy}\"\n",
    "      - Trimester-like (K, T,\n",
    "         L, U):                  fp encodes trimester       -> \"T{1-3}Y{yy}\"\n",
    "    Then we re-infer the full year from the YY part with 19xx/20xx rule.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the original input DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes:\n",
    "    # - uppercase for consistency\n",
    "    # - fill missing values with empty string\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod in a separate column for later comparison\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    # Convert FiscalPeriod to numeric, coerce errors to NaN\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Masks for different frequency groups\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])  # quarter-based\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])            # annual\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])            # semiannual\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])  # trimester-like\n",
    "\n",
    "    # Initialize Str_FiscalPrd as NaN; we will fill per frequency group\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Quarter-based encoding\n",
    "    # -------------------------\n",
    "    # Quarter number: (fp % 4) + 1 => yields 1..4 (if fp is integer-based encoding)\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year part for quarter: integer division by 4, then take last two digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    # Assign \"Q{quarter}Y{yy}\" for quarter frequencies\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" +\n",
    "        q_part.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Annual encoding (A, B)\n",
    "    # -------------------------\n",
    "    # Year part is fp itself for A/B; take last two digits via last2\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    # Assign \"Y{yy}\" for annual frequencies\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    # -------------------------\n",
    "    # Semiannual encoding (F, S)\n",
    "    # -------------------------\n",
    "    # Semester number: (fp % 2) + 1 => 1 or 2\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    # Year part: fp // 2, then last two digits\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    # Assign \"S{sem}Y{yy}\" for F/S\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" +\n",
    "        fs_sem.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Trimester-like encoding (K, T, L, U)\n",
    "    # -------------------------\n",
    "    # Term number: (fp % 3) + 1 => 1..3\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    # Year part: fp // 3, then last two digits\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    # Assign \"T{term}Y{yy}\" for K/T/L/U frequencies\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" +\n",
    "        t_term.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Implied full year from Str_FiscalPrd (19xx / 20xx reconstruction)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Extract two-digit year from \"Yyy\" pattern, e.g., \"Q1Y23\" -> \"23\"\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    # Convert that to numeric (NaN if not parseable)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    # Map two-digit year to full year:\n",
    "    #  - 80–99 => 19xx\n",
    "    #  - else  => 20xx\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Discrepancies check for Annual frequencies (A, B)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Work on a subset of annual frequencies only\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "\n",
    "    # Build boolean mask where implied full year does NOT match the original\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Either they match numerically...\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce'))\n",
    "        |\n",
    "        # ...or both are NaN\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    # Subset rows with discrepancies\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    # If we have any, show a short sample and the total count\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Overwrite FiscalPeriod and drop temporary helper columns\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Replace 'FiscalPeriod' with the implied full-year value we just calculated\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    # Drop helper columns not needed downstream\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to mixed_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'mixed_raw' in globals() and mixed_raw is not None and not mixed_raw.empty:\n",
    "    # Inform which item we are encoding for\n",
    "    print(f\"Applying encoding to Mixed dataset for '{target_item_name}' ...\")\n",
    "    # Apply fiscal period encoding\n",
    "    mixed_encoded = add_str_fiscalprd(mixed_raw)\n",
    "    # Show a preview of the encoded data\n",
    "    display(mixed_encoded.head())\n",
    "else:\n",
    "    # If mixed_raw is missing or empty, we skip and mark mixed_encoded as None\n",
    "    print(\"mixed_raw not found or empty. Cannot perform encoding.\")\n",
    "    mixed_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMZLiokn74Kk"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8396,
     "status": "ok",
     "timestamp": 1765651184137,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "jc5g1CcV74Kk",
    "outputId": "59e94aba-ad5c-40df-f1ae-aa05c988980a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 4,003,129 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\n",
      "         finite_rows: 2275935\n",
      "                mean: 28875.64133107392\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.10577968502807\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 29,212\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 2246723\n",
      "                mean: 100.32657752247285\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.22525054021247\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 100.0\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Net_Sales_or_Revenues_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Net_Sales_or_Revenues_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     4,003,129\n",
      "Excludedby Frequency (E/L/R/U): 0\n",
      "Dropped by quality (Pct rules): 29,212\n",
      "Output rows (final):            3,973,917\n",
      "Check: excluded + dropped + output = 4,003,129\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This script takes an input DataFrame `mixed_encoded` (if present in the global scope)\n",
    "# that contains financial time-series data (per company, item, currency, fiscal period,\n",
    "# and PIT Date). It then:\n",
    "#\n",
    "# 1. Cleans and standardizes key columns (dates, numeric types, string IDs).\n",
    "# 2. Excludes rows with certain frequencies (E/L/R/U).\n",
    "# 3. Parses fiscal period strings into quarter/semester/trimester indicators (QNUM/SNUM/TNUM).\n",
    "# 4. Uses a custom, vectorized \"as-of\" join (`asof_numpy`) to attach the most recent\n",
    "#    annual, quarterly, semiannual, and trimester values for each (ID, HistCurrency,\n",
    "#    ItemCode, FiscalPeriod) up to each row’s PIT Date.\n",
    "# 5. Builds \"full-year\" candidate values from:\n",
    "#       - actual annuals (A),\n",
    "#       - sum of Q1..Q4 (Q4 proxy),\n",
    "#       - sum of S1..S2 (S2 proxy),\n",
    "#       - sum of T1..T3 (T3 proxy),\n",
    "#    and selects the best candidate based on priorities and relationship to the row’s\n",
    "#    fiscal period (same-year vs prior-year).\n",
    "# 6. Computes an annual PIT-based metric `AnnPITValue` and compares it to the “true”\n",
    "#    annual value (`TrueValue`) to derive a percentage `AnnPITValue_Pct` for QC.\n",
    "# 7. Performs quality checks:\n",
    "#       - Ensures no period-date is after the PIT Date.\n",
    "#       - Drops rows whose `AnnPITValue_Pct` is outside the range [50, 200] or infinite.\n",
    "# 8. Keeps a curated set of columns, drops helper columns, and saves:\n",
    "#       - a full output file\n",
    "#       - a subset file with key columns for quick inspection.\n",
    "# 9. Prints row-accounting stats and frees some memory.\n",
    "#\n",
    "# If `mixed_encoded` is not defined or is None, it simply prints a message and exits.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta            # <--- Added to fix NameError\n",
    "from scipy.stats.mstats import winsorize  # <--- Added to fix NameError\n",
    "\n",
    "# Enable pandas \"copy-on-write\" behavior to reduce unintended chained assignment effects\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Helper function to build a string key from multiple columns.\n",
    "    For each row, join the values of 'cols' with '||'.\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    This is a manual, NumPy-based implementation of an \"as-of merge\" grouped by `by_cols`.\n",
    "    It:\n",
    "    - filters out rows with missing required fields,\n",
    "    - normalizes PIT Date to daily precision,\n",
    "    - groups right_df by composite key of `by_cols`,\n",
    "    - for each left row, binary-searches into the matching right group by PIT Date,\n",
    "      picking the last date <= the left PIT Date,\n",
    "    - returns two aligned arrays: (values, dates) for the left_df rows.\n",
    "    \"\"\"\n",
    "    # Initialize outputs with NaNs and NaT for all left_df rows\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns on each side\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask: rows that have all required fields non-null\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If either side has no valid rows, return the default empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on filtered copies only (avoid side effects)\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime at day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build composite keys for group-level match (based on by_cols)\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and date to enable binary search per key\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract NumPy arrays for right side\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and first index for each key in the sorted right side\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Build a dict: key -> (dates array, values array) slice\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]  # start of this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end of this key\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Left side indices and arrays\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left side by key (stable mergesort to preserve original row order within key)\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # i iterates over the sorted left rows\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current composite key\n",
    "        j = i + 1\n",
    "        # Find contiguous block [i:j) for this key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Dates and original positions of this key’s left rows\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # If we have matching right-hand slices for this key, do the as-of search\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # searchsorted(..., 'right') - 1 gives index of last r_date <= block_date\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # positions where such a date exists\n",
    "            if np.any(valid):\n",
    "                # Fill outputs for left rows where we found a valid match\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        # Move to next key block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile calculation: returns quantile q of Series `s`,\n",
    "    or NaN if quantile fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Computes a summary of finite values of a series:\n",
    "      - count\n",
    "      - mean, median\n",
    "      - 1% winsorized mean\n",
    "      - selected deciles (p10, p20, ..., p90)\n",
    "    Infinite values are treated as NaN and removed.\n",
    "    If no finite values remain, returns an empty dict.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop all NaNs\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures the array is writable for winsorize\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Priority for full-year candidates ----------\n",
    "\n",
    "# Fixed priority mapping for full-year candidates:\n",
    "#   'A'  : actual annual value\n",
    "#   'Q4' : annual proxy from four quarters\n",
    "#   'T3' : annual proxy from three trimesters\n",
    "#   'S2' : annual proxy from two semesters\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # highest priority: actual annual\n",
    "    'Q4': 90,  # then Q1+Q2+Q3+Q4\n",
    "    'T3': 80,  # then T1+T2+T3\n",
    "    'S2': 70,  # then S1+S2\n",
    "}\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "# Only run the main logic if a global DataFrame `mixed_encoded` exists and is not None\n",
    "if 'mixed_encoded' in globals() and mixed_encoded is not None:\n",
    "    # Count initial input rows\n",
    "    input_rows = len(mixed_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy of the input dataset\n",
    "    working = mixed_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Exclude certain frequencies (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create mask of rows whose Frequency is one of E, L, R, U (case-insensitive)\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    # Count how many rows will be excluded\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    # Keep only rows that are NOT in the exclusion set\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Set dtypes and normalize important columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert PIT Date to datetime (coerce errors -> NaT), floor to day\n",
    "    working['PIT Date']     = pd.to_datetime(working['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    # FiscalPeriod: numeric (e.g., 2021, 2022, ...)\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    # Value: numeric (float)\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Convert key ID / code columns to string to ensure consistency\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse Q/S/T markers from Str_FiscalPrd (like 'Q1Y2023')\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter number Q1..Q4 from e.g. \"Q1Y2023\" into QNUM\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual number S1..S2 into SNUM\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester number T1..T3 into TNUM\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ensure period columns exist (Q1..Q4, S1..S2, T1..T3, A + their date cols)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create value columns for Q1..Q4, S1..S2, T1..T3, A if they are missing\n",
    "    for c in [*(f'Q{i}' for i in range(1, 5)),\n",
    "              *(f'S{i}' for i in range(1, 3)),\n",
    "              *(f'T{i}' for i in range(1, 4)),\n",
    "              'A']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Create corresponding *_Date columns if missing\n",
    "    for c in [*(f'Q{i}_Date' for i in range(1, 5)),\n",
    "              *(f'S{i}_Date' for i in range(1, 3)),\n",
    "              *(f'T{i}_Date' for i in range(1, 4)),\n",
    "              'A_Date']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Base key for many of the as-of mappings\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Derive TrueValue from annuals (A/B frequencies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Mask annual-like rows where Value is present\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    # annual_src: one row per (ID, FiscalPeriod, HistCurrency) with last PIT Date\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Left-join true annual value back onto working\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping (same FiscalPeriod) for A/Q/S/T\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----- Annual -----\n",
    "    # Source rows for annual frequencies A/B\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    # As-of join: for each working row, get most recent annual value by PIT Date\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period of annual value (same as row's FiscalPeriod when present)\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # ----- Quarterly -----\n",
    "    # Source rows for quarterly frequencies (Q/C) with valid QNUM\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Restrict to a specific quarter q\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        # As-of join for that quarter\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        # Origin FP column for that quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        # Fill origin FP only where quarter value is non-null and origin not yet set\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Semiannual -----\n",
    "    # Source rows for semiannual frequencies (S/F) with valid SNUM\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Trimester -----\n",
    "    # Source rows for trimester frequencies (T/K) with valid TNUM\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels & normalize dates (NO prev-year fill, NO forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort working data consistently for downstream calculations\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    # List of all period value columns\n",
    "    value_cols_all  = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                      [f'S{i}' for i in range(1, 3)] + \\\n",
    "                      [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    # Corresponding date columns\n",
    "    date_cols_all   = [f'{c}_Date' for c in value_cols_all]\n",
    "    # Corresponding origin FP columns\n",
    "    origin_cols_all = [f'{c}_OriginFP' for c in value_cols_all]\n",
    "\n",
    "    # Ensure that all date columns are proper datetimes (floor to day)\n",
    "    # Note: explicitly no groupby-forward-fill here – only asof-filled values remain\n",
    "    for c in date_cols_all:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Build full-year candidates from fixed sets (Q1–Q4, S1–S2, T1–T3)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def full_year_from_fixed(row, labels, pit, cutoff):\n",
    "        \"\"\"\n",
    "        Fixed full-year from given labels (e.g. Q1..Q4, S1..S2, T1..T3):\n",
    "\n",
    "          - Requires ALL labels to have:\n",
    "              value, date, origin_fp.\n",
    "          - All dates must be within [cutoff, pit].\n",
    "          - origin_fp = max(origin_fp_i)  (the 'newest year' across components).\n",
    "\n",
    "        Returns (dt, val, origin_fp) or (NaT, NaN, NaN) if any requirement fails.\n",
    "        \"\"\"\n",
    "        vals, dts, fps = [], [], []\n",
    "        for lbl in labels:\n",
    "            v = row.get(lbl, np.nan)\n",
    "            d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "            o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "            # Require non-null for value, date, origin FP\n",
    "            if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            # Normalize date and check it lies within [cutoff, pit]\n",
    "            d = pd.to_datetime(d, errors='coerce')\n",
    "            if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            vals.append(float(v))\n",
    "            dts.append(d)\n",
    "            fps.append(int(o))\n",
    "\n",
    "        # Sum all component values, pick latest date and max origin FP\n",
    "        total_val = float(np.nansum(vals))\n",
    "        latest_dt = max(dts)\n",
    "        origin_fp = max(fps)\n",
    "        return latest_dt, total_val, origin_fp\n",
    "\n",
    "    def pick_annpit_sum_with_origin(row):\n",
    "        \"\"\"\n",
    "        For each row, pick the best annual PIT-based value (AnnPITValue) by:\n",
    "\n",
    "        1) Building a time window [PIT - 365 days, PIT].\n",
    "        2) Generating candidate annual values:\n",
    "           - actual annual A (if within window and with origin FP),\n",
    "           - Q4 proxy from Q1..Q4,\n",
    "           - S2 proxy from S1..S2,\n",
    "           - T3 proxy from T1..T3,\n",
    "           each with associated priority and origin FP.\n",
    "        3) Ranking candidates according to:\n",
    "           - Same-year A (highest),\n",
    "           - Same-year proxies (Q4/T3/S2),\n",
    "           - Prior-year A,\n",
    "           - Prior-year proxies,\n",
    "           - Otherwise: any remaining candidate by (priority, latest date).\n",
    "        4) Returning the chosen candidate's value (keeping zeros, but filtering NaNs).\n",
    "        \"\"\"\n",
    "        pit = row['PIT Date']\n",
    "        if pd.isna(pit):\n",
    "            return np.nan\n",
    "        cutoff = pit - timedelta(days=365)  # Now works because timedelta is imported\n",
    "\n",
    "        # Current row's fiscal period, cast to int if possible\n",
    "        fp = row.get('FiscalPeriod', np.nan)\n",
    "        try:\n",
    "            fp_int = int(fp) if not pd.isna(fp) else None\n",
    "        except Exception:\n",
    "            fp_int = None\n",
    "\n",
    "        # Collect candidate tuples: (label, priority, date, value, origin_fp)\n",
    "        candidates = []\n",
    "\n",
    "        # --- Candidate A: actual annual (0 is allowed)\n",
    "        A_val = row.get('A', np.nan)\n",
    "        A_dt  = row.get('A_Date', pd.NaT)\n",
    "        A_ofp = row.get('A_OriginFP', np.nan)\n",
    "        if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "            A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "            if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "                candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "        # --- Candidate Q4: sum of Q1..Q4 (0 allowed)\n",
    "        q4_dt, q4_val, q4_fp = full_year_from_fixed(\n",
    "            row, [f'Q{i}' for i in range(1, 5)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "            candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "        # --- Candidate S2: sum of S1..S2 (0 allowed)\n",
    "        s2_dt, s2_val, s2_fp = full_year_from_fixed(\n",
    "            row, [f'S{i}' for i in range(1, 3)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(s2_val) and pd.notna(s2_dt) and not pd.isna(s2_fp):\n",
    "            candidates.append(('S2', _PERIOD_PRIORITY['S2'], s2_dt, float(s2_val), int(s2_fp)))\n",
    "\n",
    "        # --- Candidate T3: sum of T1..T3 (0 allowed)\n",
    "        t3_dt, t3_val, t3_fp = full_year_from_fixed(\n",
    "            row, [f'T{i}' for i in range(1, 4)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(t3_val) and pd.notna(t3_dt) and not pd.isna(t3_fp):\n",
    "            candidates.append(('T3', _PERIOD_PRIORITY['T3'], t3_dt, float(t3_val), int(t3_fp)))\n",
    "\n",
    "        # If no candidates, return NaN\n",
    "        if not candidates:\n",
    "            return np.nan\n",
    "\n",
    "        # Filter out NaN-valued candidates (keep 0-valued ones)\n",
    "        def valid(seq):\n",
    "            return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "        # Relationship between row's fiscal period and candidate's origin FP:\n",
    "        # same / prior / other / unknown\n",
    "        def rel(c):\n",
    "            _, _, _, _, ofp = c\n",
    "            if fp_int is None or ofp is None:\n",
    "                return 'unknown'\n",
    "            if ofp == fp_int:\n",
    "                return 'same'\n",
    "            if ofp == fp_int - 1:\n",
    "                return 'prior'\n",
    "            return 'other'\n",
    "\n",
    "        # 1) Same-year actual annual A (max by date)\n",
    "        same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "        if same_A:\n",
    "            best = max(same_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 2) Same-year proxies (Q4/T3/S2), max by (priority, date)\n",
    "        same_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'same'\n",
    "        )\n",
    "        if same_proxies:\n",
    "            best = max(same_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 3) Prior-year actual annual A\n",
    "        prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "        if prior_A:\n",
    "            best = max(prior_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 4) Prior-year proxies (Q4/S2/T3) if no prior A\n",
    "        prior_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'prior'\n",
    "        )\n",
    "        if prior_proxies:\n",
    "            best = max(prior_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "        others = valid(candidates)\n",
    "        if others:\n",
    "            best = max(others, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # Final fallback: 0.0 (should rarely be reached)\n",
    "        return 0.0\n",
    "\n",
    "    # Apply the selection function row-wise to produce AnnPITValue\n",
    "    working['AnnPITValue'] = working.apply(pick_annpit_sum_with_origin, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check + PRE-DROP stats\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Columns whose dates should not exceed PIT Date\n",
    "    date_cols = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Restrict to ones actually present\n",
    "    present = [c for c in date_cols if c in working.columns]\n",
    "\n",
    "    viol_counts = {}  # per-label violation counts\n",
    "    # Mask for rows with any future-dated period\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        # A violation is when period date > PIT Date (both need to be non-null)\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m  # accumulate violations across columns\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flagrows with at least one future-date error\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct + quality drop\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute AnnPITValue as % of TrueValue (only when TrueValue != 0)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Stats before dropping low-quality rows\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    # Build masks for dropping:\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)  # infinite percentages\n",
    "    is_finite = np.isfinite(pct)\n",
    "    # Out-of-range if % > 200 or % < 50 (but finite)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    # Rows to drop: infinite or out-of-range values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that passed the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Stats after dropping\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns & save\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Base descriptive columns to keep (if present)\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Build ordered list of period date/value columns\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Final set of columns to keep in output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Helper columns to drop before export\n",
    "    drop_cols = ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    # Also drop all *_OriginFP columns\n",
    "    drop_cols += [c for c in working.columns if c.endswith('_OriginFP')]\n",
    "    working.drop(columns=[c for c in drop_cols if c in working.columns],\n",
    "                 inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder and restrict columns to the final layout\n",
    "    mixed_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # Sanity checks: necessary globals must exist\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Build full output path and save pipe-delimited file\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    mixed_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Create a subset for lighter inspection\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "                   \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    # Only keep subset columns that actually exist\n",
    "    subset_cols_existing = [col for col in subset_cols if col in mixed_processed.columns]\n",
    "    subset_df = mixed_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP,\n",
    "                              f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    # Explicitly delete subset_df to free memory\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(mixed_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excludedby Frequency (E/L/R/U): {excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    # Sum up excluded + dropped + remaining and check against original count\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. \"\n",
    "              \"Investigate upstream filtering or unexpected drops.\")\n",
    "\n",
    "    # Trigger garbage collection as a final cleanup step\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # If the main input dataset is not available, skip all processing\n",
    "    print(\"mixed_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCAiquVXeraA"
   },
   "source": [
    "### Mixed 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "768TBlTPll2d"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1765651184284,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "mwdZMAQjll2d",
    "outputId": "64e1fef4-6d02-47f5-d7c5-4a0771610e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Mixed_9  ->  ItemName: 'Operating_Income'\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This code selects which Mixed_* dataset should be processed by choosing an index\n",
    "# (e.g., Mixed_1, Mixed_2, ...). It then:\n",
    "#\n",
    "#   1. Ensures that a dictionary `mixed_vars` exists, mapping keys like \"Mixed_1\"\n",
    "#      to item names.\n",
    "#   2. Builds the key corresponding to the selected index and retrieves the\n",
    "#      associated item name (`target_item_name`).\n",
    "#   3. Prints which Mixed_* item was selected.\n",
    "#   4. Constructs paths and filenames based on global variables and the selected item.\n",
    "#   5. Ensures that the output directory exists by creating it if necessary.\n",
    "#\n",
    "# The goal is to centralize selection of a single Mixed_* dataset and prepare paths\n",
    "# for downstream processing.\n",
    "\n",
    "\n",
    "# === Select which Mixed_* item to run ===\n",
    "mixed_index = 9  # Change this to process another dataset (e.g., 10)\n",
    "\n",
    "# Validate that the dictionary of mixed item names exists\n",
    "assert 'mixed_vars' in globals(), \"mixed_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key (e.g., \"Mixed_1\") and retrieve the associated item name\n",
    "item_key = f\"Mixed_{mixed_index}\"\n",
    "target_item_name = mixed_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in mixed_vars.\"\n",
    "\n",
    "# Inform which item was selected\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Construct the name of the input file for the selected item\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Construct the base name for output files (will later be suffixed)\n",
    "base_output_filename = f\"Mixed_{target_item_name}_complete\"\n",
    "\n",
    "# Ensure the output directory exists; create it (including parent dirs) if needed\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9impe7mYll2e"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 1102,
     "status": "ok",
     "timestamp": 1765651185395,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "7gMQ_Vkell2e",
    "outputId": "6f95f6ad-dd3a-4127-8608-26098c0fd9a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Operating_Income' ...\n",
      "Full dataset loaded successfully: 4,253,913 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1250</td>\n",
       "      <td>81.946988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1250</td>\n",
       "      <td>42.404318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1250</td>\n",
       "      <td>75.373553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1250</td>\n",
       "      <td>-59.739189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1250</td>\n",
       "      <td>27.980425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     1250   81.946988  \n",
       "1     1250   42.404318  \n",
       "2     1250   75.373553  \n",
       "3     1250  -59.739189  \n",
       "4     1250   27.980425  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block:\n",
    "#   1. Announces the import of a full dataset for the given `target_item_name`.\n",
    "#   2. Checks whether the file at `file_path` exists.\n",
    "#   3. If it exists, calls `import_file_to_dataframe(file_path)` to load the data\n",
    "#      into `mixed_raw`.\n",
    "#   4. If the loaded DataFrame is non-empty, prints a success message including\n",
    "#      the number of rows and shows the first few rows (via display or fallback\n",
    "#      to text printing).\n",
    "#   5. If the load fails or returns an empty DataFrame, prints a warning and\n",
    "#      creates an empty DataFrame.\n",
    "#   6. If the file does not exist, prints an error message and sets `mixed_raw`\n",
    "#      to an empty DataFrame.\n",
    "#   7. Finally, it runs `gc.collect()` to trigger garbage collection and free\n",
    "#      memory.\n",
    "#\n",
    "# Note: All previous emoji symbols in the print statements have been removed.\n",
    "\n",
    "# Inform the user that we are starting the import for this item\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "# Check if the file path exists on disk\n",
    "if os.path.exists(file_path):\n",
    "    # If the file exists, attempt to import it into a DataFrame\n",
    "    mixed_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    # If the DataFrame is successfully loaded and not empty\n",
    "    if mixed_raw is not None and not mixed_raw.empty:\n",
    "        # Print a success message including row count with thousands separator\n",
    "        print(f\"Full dataset loaded successfully: {len(mixed_raw):,} rows total.\")\n",
    "\n",
    "        try:\n",
    "            # Try to display the first few rows (Jupyter / IPython display)\n",
    "            display(mixed_raw.head())\n",
    "        except Exception:\n",
    "            # If display is not available, fall back to printing as plain text\n",
    "            print(mixed_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        # If DataFrame is None or empty, warn and create an empty DataFrame\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        mixed_raw = pd.DataFrame()\n",
    "else:\n",
    "    # If the file does not exist, report an error and set mixed_raw to empty\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    mixed_raw = pd.DataFrame()\n",
    "\n",
    "# Run garbage collection to free up memory after the load attempt\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxLTJ3rwll2f"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1765651185747,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "FfJVC2A6ll2f",
    "outputId": "d36a8e1e-024f-494c-b2ba-c6b37de00140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Mixed dataset for 'Operating_Income' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_581638/2687430345.py:85: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y99' 'Q1Y00' 'Q2Y00' ... 'Q3Y07' 'Q4Y07' 'Q4Y05']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1250</td>\n",
       "      <td>81.946988</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1250</td>\n",
       "      <td>42.404318</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1250</td>\n",
       "      <td>75.373553</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1250</td>\n",
       "      <td>-59.739189</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1250</td>\n",
       "      <td>27.980425</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     1250   81.946988           Y92  \n",
       "1     1250   42.404318           Y93  \n",
       "2     1250   75.373553           Y94  \n",
       "3     1250  -59.739189           Y95  \n",
       "4     1250   27.980425           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This snippet provides:\n",
    "#\n",
    "# 1. A helper function `last2` that returns the last two digits of a number as a\n",
    "#    zero-padded string (for building YY strings).\n",
    "#\n",
    "# 2. A function `add_str_fiscalprd(df)` which:\n",
    "#    - Works on a copy of an input DataFrame containing financial periods.\n",
    "#    - Normalizes the 'Frequency' (upper-case, no missing).\n",
    "#    - Stores the original 'FiscalPeriod' and converts it to numeric.\n",
    "#    - Creates a string representation 'Str_FiscalPrd' depending on the frequency:\n",
    "#         - Q/C/E/R: quarter-based (\"QnYyy\")\n",
    "#         - A/B: annual (\"Yyy\")\n",
    "#         - F/S: semiannual (\"SnYyy\")\n",
    "#         - K/T/L/U: trimester-like (\"TnYyy\")\n",
    "#    - Derives an implied full-year integer 'ImplFiscPer_Calculated' from the\n",
    "#      two-digit year (80–99 => 19xx, else 20xx).\n",
    "#    - For annual rows (A/B), checks discrepancies between original\n",
    "#      'FiscalPeriod' and implied full-year; prints a small preview & total count.\n",
    "#    - Overwrites 'FiscalPeriod' with 'ImplFiscPer_Calculated' and drops helper\n",
    "#      columns.\n",
    "#\n",
    "# 3. A small driver block that:\n",
    "#    - Checks that `mixed_raw` exists and is non-empty.\n",
    "#    - Applies `add_str_fiscalprd` to produce `mixed_encoded`.\n",
    "#    - Displays a head preview or prints a message and sets `mixed_encoded = None`\n",
    "#      if input is missing/empty.\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    # If input is NaN (or pandas-style missing), return None (no digits)\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Convert to int, format as 4-digit zero-padded string, take last 2 chars\n",
    "    # Example: n=2023 -> \"2023\"[-2:] -> \"23\"\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    \"\"\"\n",
    "    Creates 'Str_FiscalPrd' and overwrites 'FiscalPeriod' with an implied full year.\n",
    "    Also prints discrepancies for Annual (A,B) rows between original and calculated year.\n",
    "\n",
    "    The mapping logic:\n",
    "      - Quarterly (C, Q, E, R):   fp encodes a quarter index -> \"Q{1-4}Y{yy}\"\n",
    "      - Annual   (A, B):         fp is the year itself      -> \"Y{yy}\"\n",
    "      - Semiannual (F, S):       fp encodes half-year       -> \"S{1-2}Y{yy}\"\n",
    "      - Trimester-like (K, T,\n",
    "         L, U):                  fp encodes trimester       -> \"T{1-3}Y{yy}\"\n",
    "    Then we re-infer the full year from the YY part with 19xx/20xx rule.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the original input DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes:\n",
    "    # - uppercase for consistency\n",
    "    # - fill missing values with empty string\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod in a separate column for later comparison\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    # Convert FiscalPeriod to numeric, coerce errors to NaN\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Masks for different frequency groups\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])  # quarter-based\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])            # annual\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])            # semiannual\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])  # trimester-like\n",
    "\n",
    "    # Initialize Str_FiscalPrd as NaN; we will fill per frequency group\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Quarter-based encoding\n",
    "    # -------------------------\n",
    "    # Quarter number: (fp % 4) + 1 => yields 1..4 (if fp is integer-based encoding)\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year part for quarter: integer division by 4, then take last two digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    # Assign \"Q{quarter}Y{yy}\" for quarter frequencies\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" +\n",
    "        q_part.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Annual encoding (A, B)\n",
    "    # -------------------------\n",
    "    # Year part is fp itself for A/B; take last two digits via last2\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    # Assign \"Y{yy}\" for annual frequencies\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    # -------------------------\n",
    "    # Semiannual encoding (F, S)\n",
    "    # -------------------------\n",
    "    # Semester number: (fp % 2) + 1 => 1 or 2\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    # Year part: fp // 2, then last two digits\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    # Assign \"S{sem}Y{yy}\" for F/S\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" +\n",
    "        fs_sem.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Trimester-like encoding (K, T, L, U)\n",
    "    # -------------------------\n",
    "    # Term number: (fp % 3) + 1 => 1..3\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    # Year part: fp // 3, then last two digits\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    # Assign \"T{term}Y{yy}\" for K/T/L/U frequencies\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" +\n",
    "        t_term.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Implied full year from Str_FiscalPrd (19xx / 20xx reconstruction)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Extract two-digit year from \"Yyy\" pattern, e.g., \"Q1Y23\" -> \"23\"\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    # Convert that to numeric (NaN if not parseable)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    # Map two-digit year to full year:\n",
    "    #  - 80–99 => 19xx\n",
    "    #  - else  => 20xx\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Discrepancies check for Annual frequencies (A, B)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Work on a subset of annual frequencies only\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "\n",
    "    # Build boolean mask where implied full year does NOT match the original\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Either they match numerically...\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce'))\n",
    "        |\n",
    "        # ...or both are NaN\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    # Subset rows with discrepancies\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    # If we have any, show a short sample and the total count\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Overwrite FiscalPeriod and drop temporary helper columns\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Replace 'FiscalPeriod' with the implied full-year value we just calculated\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    # Drop helper columns not needed downstream\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to mixed_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'mixed_raw' in globals() and mixed_raw is not None and not mixed_raw.empty:\n",
    "    # Inform which item we are encoding for\n",
    "    print(f\"Applying encoding to Mixed dataset for '{target_item_name}' ...\")\n",
    "    # Apply fiscal period encoding\n",
    "    mixed_encoded = add_str_fiscalprd(mixed_raw)\n",
    "    # Show a preview of the encoded data\n",
    "    display(mixed_encoded.head())\n",
    "else:\n",
    "    # If mixed_raw is missing or empty, we skip and mark mixed_encoded as None\n",
    "    print(\"mixed_raw not found or empty. Cannot perform encoding.\")\n",
    "    mixed_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJHoFnUsll2g"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8624,
     "status": "ok",
     "timestamp": 1765651194373,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "xLLFoAhYnyVh",
    "outputId": "f6aead66-0eca-41f4-af82-5d7a03d73be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 4,253,913 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\n",
      "         finite_rows: 2589547\n",
      "                mean: 23775.823858645716\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 99.75021033963792\n",
      "                 p10: 97.67441860465115\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 101.70941367530939\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 103,696\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 2485851\n",
      "                mean: 100.59589215405154\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.47105984800059\n",
      "                 p10: 99.19574861806363\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 101.02207237649696\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Operating_Income_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Operating_Income_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     4,253,913\n",
      "Excludedby Frequency (E/L/R/U): 0\n",
      "Dropped by quality (Pct rules): 103,696\n",
      "Output rows (final):            4,150,217\n",
      "Check: excluded + dropped + output = 4,253,913\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This script takes an input DataFrame `mixed_encoded` (if present in the global scope)\n",
    "# that contains financial time-series data (per company, item, currency, fiscal period,\n",
    "# and PIT Date). It then:\n",
    "#\n",
    "# 1. Cleans and standardizes key columns (dates, numeric types, string IDs).\n",
    "# 2. Excludes rows with certain frequencies (E/L/R/U).\n",
    "# 3. Parses fiscal period strings into quarter/semester/trimester indicators (QNUM/SNUM/TNUM).\n",
    "# 4. Uses a custom, vectorized \"as-of\" join (`asof_numpy`) to attach the most recent\n",
    "#    annual, quarterly, semiannual, and trimester values for each (ID, HistCurrency,\n",
    "#    ItemCode, FiscalPeriod) up to each row’s PIT Date.\n",
    "# 5. Builds \"full-year\" candidate values from:\n",
    "#       - actual annuals (A),\n",
    "#       - sum of Q1..Q4 (Q4 proxy),\n",
    "#       - sum of S1..S2 (S2 proxy),\n",
    "#       - sum of T1..T3 (T3 proxy),\n",
    "#    and selects the best candidate based on priorities and relationship to the row’s\n",
    "#    fiscal period (same-year vs prior-year).\n",
    "# 6. Computes an annual PIT-based metric `AnnPITValue` and compares it to the “true”\n",
    "#    annual value (`TrueValue`) to derive a percentage `AnnPITValue_Pct` for QC.\n",
    "# 7. Performs quality checks:\n",
    "#       - Ensures no period-date is after the PIT Date.\n",
    "#       - Drops rows whose `AnnPITValue_Pct` is outside the range [50, 200] or infinite.\n",
    "# 8. Keeps a curated set of columns, drops helper columns, and saves:\n",
    "#       - a full output file\n",
    "#       - a subset file with key columns for quick inspection.\n",
    "# 9. Prints row-accounting stats and frees some memory.\n",
    "#\n",
    "# If `mixed_encoded` is not defined or is None, it simply prints a message and exits.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta            # <--- Added to fix NameError\n",
    "from scipy.stats.mstats import winsorize  # <--- Added to fix NameError\n",
    "\n",
    "# Enable pandas \"copy-on-write\" behavior to reduce unintended chained assignment effects\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Helper function to build a string key from multiple columns.\n",
    "    For each row, join the values of 'cols' with '||'.\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    This is a manual, NumPy-based implementation of an \"as-of merge\" grouped by `by_cols`.\n",
    "    It:\n",
    "    - filters out rows with missing required fields,\n",
    "    - normalizes PIT Date to daily precision,\n",
    "    - groups right_df by composite key of `by_cols`,\n",
    "    - for each left row, binary-searches into the matching right group by PIT Date,\n",
    "      picking the last date <= the left PIT Date,\n",
    "    - returns two aligned arrays: (values, dates) for the left_df rows.\n",
    "    \"\"\"\n",
    "    # Initialize outputs with NaNs and NaT for all left_df rows\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns on each side\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask: rows that have all required fields non-null\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If either side has no valid rows, return the default empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on filtered copies only (avoid side effects)\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime at day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build composite keys for group-level match (based on by_cols)\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and date to enable binary search per key\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract NumPy arrays for right side\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and first index for each key in the sorted right side\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Build a dict: key -> (dates array, values array) slice\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]  # start of this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end of this key\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Left side indices and arrays\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left side by key (stable mergesort to preserve original row order within key)\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # i iterates over the sorted left rows\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current composite key\n",
    "        j = i + 1\n",
    "        # Find contiguous block [i:j) for this key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Dates and original positions of this key’s left rows\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # If we have matching right-hand slices for this key, do the as-of search\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # searchsorted(..., 'right') - 1 gives index of last r_date <= block_date\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # positions where such a date exists\n",
    "            if np.any(valid):\n",
    "                # Fill outputs for left rows where we found a valid match\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        # Move to next key block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile calculation: returns quantile q of Series `s`,\n",
    "    or NaN if quantile fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Computes a summary of finite values of a series:\n",
    "      - count\n",
    "      - mean, median\n",
    "      - 1% winsorized mean\n",
    "      - selected deciles (p10, p20, ..., p90)\n",
    "    Infinite values are treated as NaN and removed.\n",
    "    If no finite values remain, returns an empty dict.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop all NaNs\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures the array is writable for winsorize\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Priority for full-year candidates ----------\n",
    "\n",
    "# Fixed priority mapping for full-year candidates:\n",
    "#   'A'  : actual annual value\n",
    "#   'Q4' : annual proxy from four quarters\n",
    "#   'T3' : annual proxy from three trimesters\n",
    "#   'S2' : annual proxy from two semesters\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # highest priority: actual annual\n",
    "    'Q4': 90,  # then Q1+Q2+Q3+Q4\n",
    "    'T3': 80,  # then T1+T2+T3\n",
    "    'S2': 70,  # then S1+S2\n",
    "}\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "# Only run the main logic if a global DataFrame `mixed_encoded` exists and is not None\n",
    "if 'mixed_encoded' in globals() and mixed_encoded is not None:\n",
    "    # Count initial input rows\n",
    "    input_rows = len(mixed_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy of the input dataset\n",
    "    working = mixed_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Exclude certain frequencies (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create mask of rows whose Frequency is one of E, L, R, U (case-insensitive)\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    # Count how many rows will be excluded\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    # Keep only rows that are NOT in the exclusion set\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Set dtypes and normalize important columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert PIT Date to datetime (coerce errors -> NaT), floor to day\n",
    "    working['PIT Date']     = pd.to_datetime(working['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    # FiscalPeriod: numeric (e.g., 2021, 2022, ...)\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    # Value: numeric (float)\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Convert key ID / code columns to string to ensure consistency\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse Q/S/T markers from Str_FiscalPrd (like 'Q1Y2023')\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter number Q1..Q4 from e.g. \"Q1Y2023\" into QNUM\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual number S1..S2 into SNUM\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester number T1..T3 into TNUM\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ensure period columns exist (Q1..Q4, S1..S2, T1..T3, A + their date cols)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create value columns for Q1..Q4, S1..S2, T1..T3, A if they are missing\n",
    "    for c in [*(f'Q{i}' for i in range(1, 5)),\n",
    "              *(f'S{i}' for i in range(1, 3)),\n",
    "              *(f'T{i}' for i in range(1, 4)),\n",
    "              'A']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Create corresponding *_Date columns if missing\n",
    "    for c in [*(f'Q{i}_Date' for i in range(1, 5)),\n",
    "              *(f'S{i}_Date' for i in range(1, 3)),\n",
    "              *(f'T{i}_Date' for i in range(1, 4)),\n",
    "              'A_Date']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Base key for many of the as-of mappings\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Derive TrueValue from annuals (A/B frequencies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Mask annual-like rows where Value is present\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    # annual_src: one row per (ID, FiscalPeriod, HistCurrency) with last PIT Date\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Left-join true annual value back onto working\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping (same FiscalPeriod) for A/Q/S/T\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----- Annual -----\n",
    "    # Source rows for annual frequencies A/B\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    # As-of join: for each working row, get most recent annual value by PIT Date\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period of annual value (same as row's FiscalPeriod when present)\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # ----- Quarterly -----\n",
    "    # Source rows for quarterly frequencies (Q/C) with valid QNUM\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Restrict to a specific quarter q\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        # As-of join for that quarter\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        # Origin FP column for that quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        # Fill origin FP only where quarter value is non-null and origin not yet set\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Semiannual -----\n",
    "    # Source rows for semiannual frequencies (S/F) with valid SNUM\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Trimester -----\n",
    "    # Source rows for trimester frequencies (T/K) with valid TNUM\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels & normalize dates (NO prev-year fill, NO forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort working data consistently for downstream calculations\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    # List of all period value columns\n",
    "    value_cols_all  = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                      [f'S{i}' for i in range(1, 3)] + \\\n",
    "                      [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    # Corresponding date columns\n",
    "    date_cols_all   = [f'{c}_Date' for c in value_cols_all]\n",
    "    # Corresponding origin FP columns\n",
    "    origin_cols_all = [f'{c}_OriginFP' for c in value_cols_all]\n",
    "\n",
    "    # Ensure that all date columns are proper datetimes (floor to day)\n",
    "    # Note: explicitly no groupby-forward-fill here – only asof-filled values remain\n",
    "    for c in date_cols_all:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Build full-year candidates from fixed sets (Q1–Q4, S1–S2, T1–T3)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def full_year_from_fixed(row, labels, pit, cutoff):\n",
    "        \"\"\"\n",
    "        Fixed full-year from given labels (e.g. Q1..Q4, S1..S2, T1..T3):\n",
    "\n",
    "          - Requires ALL labels to have:\n",
    "              value, date, origin_fp.\n",
    "          - All dates must be within [cutoff, pit].\n",
    "          - origin_fp = max(origin_fp_i)  (the 'newest year' across components).\n",
    "\n",
    "        Returns (dt, val, origin_fp) or (NaT, NaN, NaN) if any requirement fails.\n",
    "        \"\"\"\n",
    "        vals, dts, fps = [], [], []\n",
    "        for lbl in labels:\n",
    "            v = row.get(lbl, np.nan)\n",
    "            d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "            o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "            # Require non-null for value, date, origin FP\n",
    "            if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            # Normalize date and check it lies within [cutoff, pit]\n",
    "            d = pd.to_datetime(d, errors='coerce')\n",
    "            if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            vals.append(float(v))\n",
    "            dts.append(d)\n",
    "            fps.append(int(o))\n",
    "\n",
    "        # Sum all component values, pick latest date and max origin FP\n",
    "        total_val = float(np.nansum(vals))\n",
    "        latest_dt = max(dts)\n",
    "        origin_fp = max(fps)\n",
    "        return latest_dt, total_val, origin_fp\n",
    "\n",
    "    def pick_annpit_sum_with_origin(row):\n",
    "        \"\"\"\n",
    "        For each row, pick the best annual PIT-based value (AnnPITValue) by:\n",
    "\n",
    "        1) Building a time window [PIT - 365 days, PIT].\n",
    "        2) Generating candidate annual values:\n",
    "           - actual annual A (if within window and with origin FP),\n",
    "           - Q4 proxy from Q1..Q4,\n",
    "           - S2 proxy from S1..S2,\n",
    "           - T3 proxy from T1..T3,\n",
    "           each with associated priority and origin FP.\n",
    "        3) Ranking candidates according to:\n",
    "           - Same-year A (highest),\n",
    "           - Same-year proxies (Q4/T3/S2),\n",
    "           - Prior-year A,\n",
    "           - Prior-year proxies,\n",
    "           - Otherwise: any remaining candidate by (priority, latest date).\n",
    "        4) Returning the chosen candidate's value (keeping zeros, but filtering NaNs).\n",
    "        \"\"\"\n",
    "        pit = row['PIT Date']\n",
    "        if pd.isna(pit):\n",
    "            return np.nan\n",
    "        cutoff = pit - timedelta(days=365)  # Now works because timedelta is imported\n",
    "\n",
    "        # Current row's fiscal period, cast to int if possible\n",
    "        fp = row.get('FiscalPeriod', np.nan)\n",
    "        try:\n",
    "            fp_int = int(fp) if not pd.isna(fp) else None\n",
    "        except Exception:\n",
    "            fp_int = None\n",
    "\n",
    "        # Collect candidate tuples: (label, priority, date, value, origin_fp)\n",
    "        candidates = []\n",
    "\n",
    "        # --- Candidate A: actual annual (0 is allowed)\n",
    "        A_val = row.get('A', np.nan)\n",
    "        A_dt  = row.get('A_Date', pd.NaT)\n",
    "        A_ofp = row.get('A_OriginFP', np.nan)\n",
    "        if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "            A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "            if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "                candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "        # --- Candidate Q4: sum of Q1..Q4 (0 allowed)\n",
    "        q4_dt, q4_val, q4_fp = full_year_from_fixed(\n",
    "            row, [f'Q{i}' for i in range(1, 5)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "            candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "        # --- Candidate S2: sum of S1..S2 (0 allowed)\n",
    "        s2_dt, s2_val, s2_fp = full_year_from_fixed(\n",
    "            row, [f'S{i}' for i in range(1, 3)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(s2_val) and pd.notna(s2_dt) and not pd.isna(s2_fp):\n",
    "            candidates.append(('S2', _PERIOD_PRIORITY['S2'], s2_dt, float(s2_val), int(s2_fp)))\n",
    "\n",
    "        # --- Candidate T3: sum of T1..T3 (0 allowed)\n",
    "        t3_dt, t3_val, t3_fp = full_year_from_fixed(\n",
    "            row, [f'T{i}' for i in range(1, 4)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(t3_val) and pd.notna(t3_dt) and not pd.isna(t3_fp):\n",
    "            candidates.append(('T3', _PERIOD_PRIORITY['T3'], t3_dt, float(t3_val), int(t3_fp)))\n",
    "\n",
    "        # If no candidates, return NaN\n",
    "        if not candidates:\n",
    "            return np.nan\n",
    "\n",
    "        # Filter out NaN-valued candidates (keep 0-valued ones)\n",
    "        def valid(seq):\n",
    "            return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "        # Relationship between row's fiscal period and candidate's origin FP:\n",
    "        # same / prior / other / unknown\n",
    "        def rel(c):\n",
    "            _, _, _, _, ofp = c\n",
    "            if fp_int is None or ofp is None:\n",
    "                return 'unknown'\n",
    "            if ofp == fp_int:\n",
    "                return 'same'\n",
    "            if ofp == fp_int - 1:\n",
    "                return 'prior'\n",
    "            return 'other'\n",
    "\n",
    "        # 1) Same-year actual annual A (max by date)\n",
    "        same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "        if same_A:\n",
    "            best = max(same_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 2) Same-year proxies (Q4/T3/S2), max by (priority, date)\n",
    "        same_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'same'\n",
    "        )\n",
    "        if same_proxies:\n",
    "            best = max(same_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 3) Prior-year actual annual A\n",
    "        prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "        if prior_A:\n",
    "            best = max(prior_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 4) Prior-year proxies (Q4/S2/T3) if no prior A\n",
    "        prior_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'prior'\n",
    "        )\n",
    "        if prior_proxies:\n",
    "            best = max(prior_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "        others = valid(candidates)\n",
    "        if others:\n",
    "            best = max(others, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # Final fallback: 0.0 (should rarely be reached)\n",
    "        return 0.0\n",
    "\n",
    "    # Apply the selection function row-wise to produce AnnPITValue\n",
    "    working['AnnPITValue'] = working.apply(pick_annpit_sum_with_origin, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check + PRE-DROP stats\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Columns whose dates should not exceed PIT Date\n",
    "    date_cols = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Restrict to ones actually present\n",
    "    present = [c for c in date_cols if c in working.columns]\n",
    "\n",
    "    viol_counts = {}  # per-label violation counts\n",
    "    # Mask for rows with any future-dated period\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        # A violation is when period date > PIT Date (both need to be non-null)\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m  # accumulate violations across columns\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flagrows with at least one future-date error\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct + quality drop\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute AnnPITValue as % of TrueValue (only when TrueValue != 0)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Stats before dropping low-quality rows\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    # Build masks for dropping:\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)  # infinite percentages\n",
    "    is_finite = np.isfinite(pct)\n",
    "    # Out-of-range if % > 200 or % < 50 (but finite)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    # Rows to drop: infinite or out-of-range values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that passed the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Stats after dropping\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns & save\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Base descriptive columns to keep (if present)\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Build ordered list of period date/value columns\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Final set of columns to keep in output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Helper columns to drop before export\n",
    "    drop_cols = ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    # Also drop all *_OriginFP columns\n",
    "    drop_cols += [c for c in working.columns if c.endswith('_OriginFP')]\n",
    "    working.drop(columns=[c for c in drop_cols if c in working.columns],\n",
    "                 inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder and restrict columns to the final layout\n",
    "    mixed_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # Sanity checks: necessary globals must exist\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Build full output path and save pipe-delimited file\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    mixed_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Create a subset for lighter inspection\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "                   \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    # Only keep subset columns that actually exist\n",
    "    subset_cols_existing = [col for col in subset_cols if col in mixed_processed.columns]\n",
    "    subset_df = mixed_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP,\n",
    "                              f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    # Explicitly delete subset_df to free memory\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(mixed_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excludedby Frequency (E/L/R/U): {excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    # Sum up excluded + dropped + remaining and check against original count\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. \"\n",
    "              \"Investigate upstream filtering or unexpected drops.\")\n",
    "\n",
    "    # Trigger garbage collection as a final cleanup step\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # If the main input dataset is not available, skip all processing\n",
    "    print(\"mixed_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmv5sQGjkadZ"
   },
   "source": [
    "### Mixed 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwTT6Ghqkadb"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1765651194740,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "b6ZDwA-Skadb",
    "outputId": "a1893ce3-b74b-4309-dd3a-58a2e622d5ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Mixed_10  ->  ItemName: 'Sales_Per_Share'\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This code selects which Mixed_* dataset should be processed by choosing an index\n",
    "# (e.g., Mixed_1, Mixed_2, ...). It then:\n",
    "#\n",
    "#   1. Ensures that a dictionary `mixed_vars` exists, mapping keys like \"Mixed_1\"\n",
    "#      to item names.\n",
    "#   2. Builds the key corresponding to the selected index and retrieves the\n",
    "#      associated item name (`target_item_name`).\n",
    "#   3. Prints which Mixed_* item was selected.\n",
    "#   4. Constructs paths and filenames based on global variables and the selected item.\n",
    "#   5. Ensures that the output directory exists by creating it if necessary.\n",
    "#\n",
    "# The goal is to centralize selection of a single Mixed_* dataset and prepare paths\n",
    "# for downstream processing.\n",
    "\n",
    "\n",
    "# === Select which Mixed_* item to run ===\n",
    "mixed_index = 10  # Change this to process another dataset (e.g., 10)\n",
    "\n",
    "# Validate that the dictionary of mixed item names exists\n",
    "assert 'mixed_vars' in globals(), \"mixed_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key (e.g., \"Mixed_1\") and retrieve the associated item name\n",
    "item_key = f\"Mixed_{mixed_index}\"\n",
    "target_item_name = mixed_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in mixed_vars.\"\n",
    "\n",
    "# Inform which item was selected\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Construct the name of the input file for the selected item\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Construct the base name for output files (will later be suffixed)\n",
    "base_output_filename = f\"Mixed_{target_item_name}_complete\"\n",
    "\n",
    "# Ensure the output directory exists; create it (including parent dirs) if needed\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYYmkDA0kadc"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 965,
     "status": "ok",
     "timestamp": 1765651195715,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "SaLbLR5okadd",
    "outputId": "08b4f8a1-a575-4d84-936d-75d2def9dc31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Sales_Per_Share' ...\n",
      "Full dataset loaded successfully: 1,929,643 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>5508</td>\n",
       "      <td>15.29247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>5508</td>\n",
       "      <td>17.2111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>5508</td>\n",
       "      <td>20.27582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>5508</td>\n",
       "      <td>12.96369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>5508</td>\n",
       "      <td>10.90916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1999-10-01         A        NaN         1992  December   \n",
       "1          Ars  1999-10-01         A        NaN         1993  December   \n",
       "2          Ars  1999-10-01         A        NaN         1994  December   \n",
       "3          Ars  1999-10-01         A        NaN         1995  December   \n",
       "4          Ars  1999-10-01         A        NaN         1996  December   \n",
       "\n",
       "  ItemCode     Value  \n",
       "0     5508  15.29247  \n",
       "1     5508   17.2111  \n",
       "2     5508  20.27582  \n",
       "3     5508  12.96369  \n",
       "4     5508  10.90916  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block:\n",
    "#   1. Announces the import of a full dataset for the given `target_item_name`.\n",
    "#   2. Checks whether the file at `file_path` exists.\n",
    "#   3. If it exists, calls `import_file_to_dataframe(file_path)` to load the data\n",
    "#      into `mixed_raw`.\n",
    "#   4. If the loaded DataFrame is non-empty, prints a success message including\n",
    "#      the number of rows and shows the first few rows (via display or fallback\n",
    "#      to text printing).\n",
    "#   5. If the load fails or returns an empty DataFrame, prints a warning and\n",
    "#      creates an empty DataFrame.\n",
    "#   6. If the file does not exist, prints an error message and sets `mixed_raw`\n",
    "#      to an empty DataFrame.\n",
    "#   7. Finally, it runs `gc.collect()` to trigger garbage collection and free\n",
    "#      memory.\n",
    "#\n",
    "# Note: All previous emoji symbols in the print statements have been removed.\n",
    "\n",
    "# Inform the user that we are starting the import for this item\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "# Check if the file path exists on disk\n",
    "if os.path.exists(file_path):\n",
    "    # If the file exists, attempt to import it into a DataFrame\n",
    "    mixed_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    # If the DataFrame is successfully loaded and not empty\n",
    "    if mixed_raw is not None and not mixed_raw.empty:\n",
    "        # Print a success message including row count with thousands separator\n",
    "        print(f\"Full dataset loaded successfully: {len(mixed_raw):,} rows total.\")\n",
    "\n",
    "        try:\n",
    "            # Try to display the first few rows (Jupyter / IPython display)\n",
    "            display(mixed_raw.head())\n",
    "        except Exception:\n",
    "            # If display is not available, fall back to printing as plain text\n",
    "            print(mixed_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        # If DataFrame is None or empty, warn and create an empty DataFrame\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        mixed_raw = pd.DataFrame()\n",
    "else:\n",
    "    # If the file does not exist, report an error and set mixed_raw to empty\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    mixed_raw = pd.DataFrame()\n",
    "\n",
    "# Run garbage collection to free up memory after the load attempt\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_PVN-U07-6h"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1765651195853,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "eh5o7ab97-6i",
    "outputId": "0a50f274-afc0-487e-b0af-6d893d08e009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Mixed dataset for 'Sales_Per_Share' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_581638/2687430345.py:98: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Y92' 'Y93' 'Y94' ... 'Y08' 'Y07' 'Y09']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>5508</td>\n",
       "      <td>15.29247</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>5508</td>\n",
       "      <td>17.2111</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>5508</td>\n",
       "      <td>20.27582</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>5508</td>\n",
       "      <td>12.96369</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1999-10-01</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>5508</td>\n",
       "      <td>10.90916</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1999-10-01         A        NaN          1992  December   \n",
       "1          Ars  1999-10-01         A        NaN          1993  December   \n",
       "2          Ars  1999-10-01         A        NaN          1994  December   \n",
       "3          Ars  1999-10-01         A        NaN          1995  December   \n",
       "4          Ars  1999-10-01         A        NaN          1996  December   \n",
       "\n",
       "  ItemCode     Value Str_FiscalPrd  \n",
       "0     5508  15.29247           Y92  \n",
       "1     5508   17.2111           Y93  \n",
       "2     5508  20.27582           Y94  \n",
       "3     5508  12.96369           Y95  \n",
       "4     5508  10.90916           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This snippet provides:\n",
    "#\n",
    "# 1. A helper function `last2` that returns the last two digits of a number as a\n",
    "#    zero-padded string (for building YY strings).\n",
    "#\n",
    "# 2. A function `add_str_fiscalprd(df)` which:\n",
    "#    - Works on a copy of an input DataFrame containing financial periods.\n",
    "#    - Normalizes the 'Frequency' (upper-case, no missing).\n",
    "#    - Stores the original 'FiscalPeriod' and converts it to numeric.\n",
    "#    - Creates a string representation 'Str_FiscalPrd' depending on the frequency:\n",
    "#         - Q/C/E/R: quarter-based (\"QnYyy\")\n",
    "#         - A/B: annual (\"Yyy\")\n",
    "#         - F/S: semiannual (\"SnYyy\")\n",
    "#         - K/T/L/U: trimester-like (\"TnYyy\")\n",
    "#    - Derives an implied full-year integer 'ImplFiscPer_Calculated' from the\n",
    "#      two-digit year (80–99 => 19xx, else 20xx).\n",
    "#    - For annual rows (A/B), checks discrepancies between original\n",
    "#      'FiscalPeriod' and implied full-year; prints a small preview & total count.\n",
    "#    - Overwrites 'FiscalPeriod' with 'ImplFiscPer_Calculated' and drops helper\n",
    "#      columns.\n",
    "#\n",
    "# 3. A small driver block that:\n",
    "#    - Checks that `mixed_raw` exists and is non-empty.\n",
    "#    - Applies `add_str_fiscalprd` to produce `mixed_encoded`.\n",
    "#    - Displays a head preview or prints a message and sets `mixed_encoded = None`\n",
    "#      if input is missing/empty.\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    # If input is NaN (or pandas-style missing), return None (no digits)\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Convert to int, format as 4-digit zero-padded string, take last 2 chars\n",
    "    # Example: n=2023 -> \"2023\"[-2:] -> \"23\"\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    \"\"\"\n",
    "    Creates 'Str_FiscalPrd' and overwrites 'FiscalPeriod' with an implied full year.\n",
    "    Also prints discrepancies for Annual (A,B) rows between original and calculated year.\n",
    "\n",
    "    The mapping logic:\n",
    "      - Quarterly (C, Q, E, R):   fp encodes a quarter index -> \"Q{1-4}Y{yy}\"\n",
    "      - Annual   (A, B):         fp is the year itself      -> \"Y{yy}\"\n",
    "      - Semiannual (F, S):       fp encodes half-year       -> \"S{1-2}Y{yy}\"\n",
    "      - Trimester-like (K, T,\n",
    "         L, U):                  fp encodes trimester       -> \"T{1-3}Y{yy}\"\n",
    "    Then we re-infer the full year from the YY part with 19xx/20xx rule.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the original input DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes:\n",
    "    # - uppercase for consistency\n",
    "    # - fill missing values with empty string\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod in a separate column for later comparison\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    # Convert FiscalPeriod to numeric, coerce errors to NaN\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Masks for different frequency groups\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])  # quarter-based\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])            # annual\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])            # semiannual\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])  # trimester-like\n",
    "\n",
    "    # Initialize Str_FiscalPrd as NaN; we will fill per frequency group\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Quarter-based encoding\n",
    "    # -------------------------\n",
    "    # Quarter number: (fp % 4) + 1 => yields 1..4 (if fp is integer-based encoding)\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year part for quarter: integer division by 4, then take last two digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    # Assign \"Q{quarter}Y{yy}\" for quarter frequencies\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" +\n",
    "        q_part.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Annual encoding (A, B)\n",
    "    # -------------------------\n",
    "    # Year part is fp itself for A/B; take last two digits via last2\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    # Assign \"Y{yy}\" for annual frequencies\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    # -------------------------\n",
    "    # Semiannual encoding (F, S)\n",
    "    # -------------------------\n",
    "    # Semester number: (fp % 2) + 1 => 1 or 2\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    # Year part: fp // 2, then last two digits\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    # Assign \"S{sem}Y{yy}\" for F/S\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" +\n",
    "        fs_sem.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Trimester-like encoding (K, T, L, U)\n",
    "    # -------------------------\n",
    "    # Term number: (fp % 3) + 1 => 1..3\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    # Year part: fp // 3, then last two digits\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    # Assign \"T{term}Y{yy}\" for K/T/L/U frequencies\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" +\n",
    "        t_term.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Implied full year from Str_FiscalPrd (19xx / 20xx reconstruction)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Extract two-digit year from \"Yyy\" pattern, e.g., \"Q1Y23\" -> \"23\"\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    # Convert that to numeric (NaN if not parseable)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    # Map two-digit year to full year:\n",
    "    #  - 80–99 => 19xx\n",
    "    #  - else  => 20xx\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Discrepancies check for Annual frequencies (A, B)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Work on a subset of annual frequencies only\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "\n",
    "    # Build boolean mask where implied full year does NOT match the original\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Either they match numerically...\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce'))\n",
    "        |\n",
    "        # ...or both are NaN\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    # Subset rows with discrepancies\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    # If we have any, show a short sample and the total count\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Overwrite FiscalPeriod and drop temporary helper columns\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Replace 'FiscalPeriod' with the implied full-year value we just calculated\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    # Drop helper columns not needed downstream\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to mixed_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'mixed_raw' in globals() and mixed_raw is not None and not mixed_raw.empty:\n",
    "    # Inform which item we are encoding for\n",
    "    print(f\"Applying encoding to Mixed dataset for '{target_item_name}' ...\")\n",
    "    # Apply fiscal period encoding\n",
    "    mixed_encoded = add_str_fiscalprd(mixed_raw)\n",
    "    # Show a preview of the encoded data\n",
    "    display(mixed_encoded.head())\n",
    "else:\n",
    "    # If mixed_raw is missing or empty, we skip and mark mixed_encoded as None\n",
    "    print(\"mixed_raw not found or empty. Cannot perform encoding.\")\n",
    "    mixed_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GfGZvJL7-6k"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1919,
     "status": "ok",
     "timestamp": 1765651197774,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "R9ub7I_y7-6k",
    "outputId": "88b18b98-a77d-48b0-f93b-d2b91a07de40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 1,929,643 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\n",
      "         finite_rows: 1890306\n",
      "                mean: 523942.5563862871\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 166.41964216695058\n",
      "                 p10: 50.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 101.50428163399192\n",
      "                 p80: 125.85865654948871\n",
      "                 p90: 214.822120682574\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 399,754\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 1490552\n",
      "                mean: 107.26843679968692\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 107.3475801458264\n",
      "                 p10: 100.0\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 105.00242818425396\n",
      "                 p90: 130.00058127891486\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Sales_Per_Share_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Sales_Per_Share_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     1,929,643\n",
      "Excludedby Frequency (E/L/R/U): 0\n",
      "Dropped by quality (Pct rules): 399,754\n",
      "Output rows (final):            1,529,889\n",
      "Check: excluded + dropped + output = 1,929,643\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This script takes an input DataFrame `mixed_encoded` (if present in the global scope)\n",
    "# that contains financial time-series data (per company, item, currency, fiscal period,\n",
    "# and PIT Date). It then:\n",
    "#\n",
    "# 1. Cleans and standardizes key columns (dates, numeric types, string IDs).\n",
    "# 2. Excludes rows with certain frequencies (E/L/R/U).\n",
    "# 3. Parses fiscal period strings into quarter/semester/trimester indicators (QNUM/SNUM/TNUM).\n",
    "# 4. Uses a custom, vectorized \"as-of\" join (`asof_numpy`) to attach the most recent\n",
    "#    annual, quarterly, semiannual, and trimester values for each (ID, HistCurrency,\n",
    "#    ItemCode, FiscalPeriod) up to each row’s PIT Date.\n",
    "# 5. Builds \"full-year\" candidate values from:\n",
    "#       - actual annuals (A),\n",
    "#       - sum of Q1..Q4 (Q4 proxy),\n",
    "#       - sum of S1..S2 (S2 proxy),\n",
    "#       - sum of T1..T3 (T3 proxy),\n",
    "#    and selects the best candidate based on priorities and relationship to the row’s\n",
    "#    fiscal period (same-year vs prior-year).\n",
    "# 6. Computes an annual PIT-based metric `AnnPITValue` and compares it to the “true”\n",
    "#    annual value (`TrueValue`) to derive a percentage `AnnPITValue_Pct` for QC.\n",
    "# 7. Performs quality checks:\n",
    "#       - Ensures no period-date is after the PIT Date.\n",
    "#       - Drops rows whose `AnnPITValue_Pct` is outside the range [50, 200] or infinite.\n",
    "# 8. Keeps a curated set of columns, drops helper columns, and saves:\n",
    "#       - a full output file\n",
    "#       - a subset file with key columns for quick inspection.\n",
    "# 9. Prints row-accounting stats and frees some memory.\n",
    "#\n",
    "# If `mixed_encoded` is not defined or is None, it simply prints a message and exits.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta            # <--- Added to fix NameError\n",
    "from scipy.stats.mstats import winsorize  # <--- Added to fix NameError\n",
    "\n",
    "# Enable pandas \"copy-on-write\" behavior to reduce unintended chained assignment effects\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Helper function to build a string key from multiple columns.\n",
    "    For each row, join the values of 'cols' with '||'.\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    This is a manual, NumPy-based implementation of an \"as-of merge\" grouped by `by_cols`.\n",
    "    It:\n",
    "    - filters out rows with missing required fields,\n",
    "    - normalizes PIT Date to daily precision,\n",
    "    - groups right_df by composite key of `by_cols`,\n",
    "    - for each left row, binary-searches into the matching right group by PIT Date,\n",
    "      picking the last date <= the left PIT Date,\n",
    "    - returns two aligned arrays: (values, dates) for the left_df rows.\n",
    "    \"\"\"\n",
    "    # Initialize outputs with NaNs and NaT for all left_df rows\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns on each side\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask: rows that have all required fields non-null\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If either side has no valid rows, return the default empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on filtered copies only (avoid side effects)\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime at day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build composite keys for group-level match (based on by_cols)\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and date to enable binary search per key\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract NumPy arrays for right side\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and first index for each key in the sorted right side\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Build a dict: key -> (dates array, values array) slice\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]  # start of this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end of this key\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Left side indices and arrays\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left side by key (stable mergesort to preserve original row order within key)\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # i iterates over the sorted left rows\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current composite key\n",
    "        j = i + 1\n",
    "        # Find contiguous block [i:j) for this key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Dates and original positions of this key’s left rows\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # If we have matching right-hand slices for this key, do the as-of search\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # searchsorted(..., 'right') - 1 gives index of last r_date <= block_date\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # positions where such a date exists\n",
    "            if np.any(valid):\n",
    "                # Fill outputs for left rows where we found a valid match\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        # Move to next key block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile calculation: returns quantile q of Series `s`,\n",
    "    or NaN if quantile fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Computes a summary of finite values of a series:\n",
    "      - count\n",
    "      - mean, median\n",
    "      - 1% winsorized mean\n",
    "      - selected deciles (p10, p20, ..., p90)\n",
    "    Infinite values are treated as NaN and removed.\n",
    "    If no finite values remain, returns an empty dict.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop all NaNs\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures the array is writable for winsorize\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Priority for full-year candidates ----------\n",
    "\n",
    "# Fixed priority mapping for full-year candidates:\n",
    "#   'A'  : actual annual value\n",
    "#   'Q4' : annual proxy from four quarters\n",
    "#   'T3' : annual proxy from three trimesters\n",
    "#   'S2' : annual proxy from two semesters\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # highest priority: actual annual\n",
    "    'Q4': 90,  # then Q1+Q2+Q3+Q4\n",
    "    'T3': 80,  # then T1+T2+T3\n",
    "    'S2': 70,  # then S1+S2\n",
    "}\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "# Only run the main logic if a global DataFrame `mixed_encoded` exists and is not None\n",
    "if 'mixed_encoded' in globals() and mixed_encoded is not None:\n",
    "    # Count initial input rows\n",
    "    input_rows = len(mixed_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy of the input dataset\n",
    "    working = mixed_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Exclude certain frequencies (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create mask of rows whose Frequency is one of E, L, R, U (case-insensitive)\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    # Count how many rows will be excluded\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    # Keep only rows that are NOT in the exclusion set\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Set dtypes and normalize important columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert PIT Date to datetime (coerce errors -> NaT), floor to day\n",
    "    working['PIT Date']     = pd.to_datetime(working['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    # FiscalPeriod: numeric (e.g., 2021, 2022, ...)\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    # Value: numeric (float)\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Convert key ID / code columns to string to ensure consistency\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse Q/S/T markers from Str_FiscalPrd (like 'Q1Y2023')\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter number Q1..Q4 from e.g. \"Q1Y2023\" into QNUM\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual number S1..S2 into SNUM\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester number T1..T3 into TNUM\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ensure period columns exist (Q1..Q4, S1..S2, T1..T3, A + their date cols)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create value columns for Q1..Q4, S1..S2, T1..T3, A if they are missing\n",
    "    for c in [*(f'Q{i}' for i in range(1, 5)),\n",
    "              *(f'S{i}' for i in range(1, 3)),\n",
    "              *(f'T{i}' for i in range(1, 4)),\n",
    "              'A']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Create corresponding *_Date columns if missing\n",
    "    for c in [*(f'Q{i}_Date' for i in range(1, 5)),\n",
    "              *(f'S{i}_Date' for i in range(1, 3)),\n",
    "              *(f'T{i}_Date' for i in range(1, 4)),\n",
    "              'A_Date']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Base key for many of the as-of mappings\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Derive TrueValue from annuals (A/B frequencies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Mask annual-like rows where Value is present\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    # annual_src: one row per (ID, FiscalPeriod, HistCurrency) with last PIT Date\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Left-join true annual value back onto working\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping (same FiscalPeriod) for A/Q/S/T\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----- Annual -----\n",
    "    # Source rows for annual frequencies A/B\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    # As-of join: for each working row, get most recent annual value by PIT Date\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period of annual value (same as row's FiscalPeriod when present)\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # ----- Quarterly -----\n",
    "    # Source rows for quarterly frequencies (Q/C) with valid QNUM\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Restrict to a specific quarter q\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        # As-of join for that quarter\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        # Origin FP column for that quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        # Fill origin FP only where quarter value is non-null and origin not yet set\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Semiannual -----\n",
    "    # Source rows for semiannual frequencies (S/F) with valid SNUM\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Trimester -----\n",
    "    # Source rows for trimester frequencies (T/K) with valid TNUM\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels & normalize dates (NO prev-year fill, NO forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort working data consistently for downstream calculations\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    # List of all period value columns\n",
    "    value_cols_all  = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                      [f'S{i}' for i in range(1, 3)] + \\\n",
    "                      [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    # Corresponding date columns\n",
    "    date_cols_all   = [f'{c}_Date' for c in value_cols_all]\n",
    "    # Corresponding origin FP columns\n",
    "    origin_cols_all = [f'{c}_OriginFP' for c in value_cols_all]\n",
    "\n",
    "    # Ensure that all date columns are proper datetimes (floor to day)\n",
    "    # Note: explicitly no groupby-forward-fill here – only asof-filled values remain\n",
    "    for c in date_cols_all:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Build full-year candidates from fixed sets (Q1–Q4, S1–S2, T1–T3)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def full_year_from_fixed(row, labels, pit, cutoff):\n",
    "        \"\"\"\n",
    "        Fixed full-year from given labels (e.g. Q1..Q4, S1..S2, T1..T3):\n",
    "\n",
    "          - Requires ALL labels to have:\n",
    "              value, date, origin_fp.\n",
    "          - All dates must be within [cutoff, pit].\n",
    "          - origin_fp = max(origin_fp_i)  (the 'newest year' across components).\n",
    "\n",
    "        Returns (dt, val, origin_fp) or (NaT, NaN, NaN) if any requirement fails.\n",
    "        \"\"\"\n",
    "        vals, dts, fps = [], [], []\n",
    "        for lbl in labels:\n",
    "            v = row.get(lbl, np.nan)\n",
    "            d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "            o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "            # Require non-null for value, date, origin FP\n",
    "            if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            # Normalize date and check it lies within [cutoff, pit]\n",
    "            d = pd.to_datetime(d, errors='coerce')\n",
    "            if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            vals.append(float(v))\n",
    "            dts.append(d)\n",
    "            fps.append(int(o))\n",
    "\n",
    "        # Sum all component values, pick latest date and max origin FP\n",
    "        total_val = float(np.nansum(vals))\n",
    "        latest_dt = max(dts)\n",
    "        origin_fp = max(fps)\n",
    "        return latest_dt, total_val, origin_fp\n",
    "\n",
    "    def pick_annpit_sum_with_origin(row):\n",
    "        \"\"\"\n",
    "        For each row, pick the best annual PIT-based value (AnnPITValue) by:\n",
    "\n",
    "        1) Building a time window [PIT - 365 days, PIT].\n",
    "        2) Generating candidate annual values:\n",
    "           - actual annual A (if within window and with origin FP),\n",
    "           - Q4 proxy from Q1..Q4,\n",
    "           - S2 proxy from S1..S2,\n",
    "           - T3 proxy from T1..T3,\n",
    "           each with associated priority and origin FP.\n",
    "        3) Ranking candidates according to:\n",
    "           - Same-year A (highest),\n",
    "           - Same-year proxies (Q4/T3/S2),\n",
    "           - Prior-year A,\n",
    "           - Prior-year proxies,\n",
    "           - Otherwise: any remaining candidate by (priority, latest date).\n",
    "        4) Returning the chosen candidate's value (keeping zeros, but filtering NaNs).\n",
    "        \"\"\"\n",
    "        pit = row['PIT Date']\n",
    "        if pd.isna(pit):\n",
    "            return np.nan\n",
    "        cutoff = pit - timedelta(days=365)  # Now works because timedelta is imported\n",
    "\n",
    "        # Current row's fiscal period, cast to int if possible\n",
    "        fp = row.get('FiscalPeriod', np.nan)\n",
    "        try:\n",
    "            fp_int = int(fp) if not pd.isna(fp) else None\n",
    "        except Exception:\n",
    "            fp_int = None\n",
    "\n",
    "        # Collect candidate tuples: (label, priority, date, value, origin_fp)\n",
    "        candidates = []\n",
    "\n",
    "        # --- Candidate A: actual annual (0 is allowed)\n",
    "        A_val = row.get('A', np.nan)\n",
    "        A_dt  = row.get('A_Date', pd.NaT)\n",
    "        A_ofp = row.get('A_OriginFP', np.nan)\n",
    "        if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "            A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "            if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "                candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "        # --- Candidate Q4: sum of Q1..Q4 (0 allowed)\n",
    "        q4_dt, q4_val, q4_fp = full_year_from_fixed(\n",
    "            row, [f'Q{i}' for i in range(1, 5)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "            candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "        # --- Candidate S2: sum of S1..S2 (0 allowed)\n",
    "        s2_dt, s2_val, s2_fp = full_year_from_fixed(\n",
    "            row, [f'S{i}' for i in range(1, 3)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(s2_val) and pd.notna(s2_dt) and not pd.isna(s2_fp):\n",
    "            candidates.append(('S2', _PERIOD_PRIORITY['S2'], s2_dt, float(s2_val), int(s2_fp)))\n",
    "\n",
    "        # --- Candidate T3: sum of T1..T3 (0 allowed)\n",
    "        t3_dt, t3_val, t3_fp = full_year_from_fixed(\n",
    "            row, [f'T{i}' for i in range(1, 4)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(t3_val) and pd.notna(t3_dt) and not pd.isna(t3_fp):\n",
    "            candidates.append(('T3', _PERIOD_PRIORITY['T3'], t3_dt, float(t3_val), int(t3_fp)))\n",
    "\n",
    "        # If no candidates, return NaN\n",
    "        if not candidates:\n",
    "            return np.nan\n",
    "\n",
    "        # Filter out NaN-valued candidates (keep 0-valued ones)\n",
    "        def valid(seq):\n",
    "            return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "        # Relationship between row's fiscal period and candidate's origin FP:\n",
    "        # same / prior / other / unknown\n",
    "        def rel(c):\n",
    "            _, _, _, _, ofp = c\n",
    "            if fp_int is None or ofp is None:\n",
    "                return 'unknown'\n",
    "            if ofp == fp_int:\n",
    "                return 'same'\n",
    "            if ofp == fp_int - 1:\n",
    "                return 'prior'\n",
    "            return 'other'\n",
    "\n",
    "        # 1) Same-year actual annual A (max by date)\n",
    "        same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "        if same_A:\n",
    "            best = max(same_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 2) Same-year proxies (Q4/T3/S2), max by (priority, date)\n",
    "        same_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'same'\n",
    "        )\n",
    "        if same_proxies:\n",
    "            best = max(same_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 3) Prior-year actual annual A\n",
    "        prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "        if prior_A:\n",
    "            best = max(prior_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 4) Prior-year proxies (Q4/S2/T3) if no prior A\n",
    "        prior_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'prior'\n",
    "        )\n",
    "        if prior_proxies:\n",
    "            best = max(prior_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "        others = valid(candidates)\n",
    "        if others:\n",
    "            best = max(others, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # Final fallback: 0.0 (should rarely be reached)\n",
    "        return 0.0\n",
    "\n",
    "    # Apply the selection function row-wise to produce AnnPITValue\n",
    "    working['AnnPITValue'] = working.apply(pick_annpit_sum_with_origin, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check + PRE-DROP stats\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Columns whose dates should not exceed PIT Date\n",
    "    date_cols = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Restrict to ones actually present\n",
    "    present = [c for c in date_cols if c in working.columns]\n",
    "\n",
    "    viol_counts = {}  # per-label violation counts\n",
    "    # Mask for rows with any future-dated period\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        # A violation is when period date > PIT Date (both need to be non-null)\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m  # accumulate violations across columns\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flagrows with at least one future-date error\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct + quality drop\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute AnnPITValue as % of TrueValue (only when TrueValue != 0)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Stats before dropping low-quality rows\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    # Build masks for dropping:\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)  # infinite percentages\n",
    "    is_finite = np.isfinite(pct)\n",
    "    # Out-of-range if % > 200 or % < 50 (but finite)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    # Rows to drop: infinite or out-of-range values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that passed the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Stats after dropping\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns & save\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Base descriptive columns to keep (if present)\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Build ordered list of period date/value columns\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Final set of columns to keep in output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Helper columns to drop before export\n",
    "    drop_cols = ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    # Also drop all *_OriginFP columns\n",
    "    drop_cols += [c for c in working.columns if c.endswith('_OriginFP')]\n",
    "    working.drop(columns=[c for c in drop_cols if c in working.columns],\n",
    "                 inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder and restrict columns to the final layout\n",
    "    mixed_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # Sanity checks: necessary globals must exist\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Build full output path and save pipe-delimited file\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    mixed_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Create a subset for lighter inspection\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "                   \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    # Only keep subset columns that actually exist\n",
    "    subset_cols_existing = [col for col in subset_cols if col in mixed_processed.columns]\n",
    "    subset_df = mixed_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP,\n",
    "                              f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    # Explicitly delete subset_df to free memory\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(mixed_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excludedby Frequency (E/L/R/U): {excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    # Sum up excluded + dropped + remaining and check against original count\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. \"\n",
    "              \"Investigate upstream filtering or unexpected drops.\")\n",
    "\n",
    "    # Trigger garbage collection as a final cleanup step\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # If the main input dataset is not available, skip all processing\n",
    "    print(\"mixed_encoded not found or None; skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3VTfZP_evHC"
   },
   "source": [
    "### Mixed 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLn4aq_-ln7l"
   },
   "source": [
    "#### Set Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1765651197914,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "GznZhTHGln7m",
    "outputId": "2d14fbe1-caf1-4a42-c196-8381bac59c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: Mixed_11  ->  ItemName: 'Selling_General__Administrative_Expenses'\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This code selects which Mixed_* dataset should be processed by choosing an index\n",
    "# (e.g., Mixed_1, Mixed_2, ...). It then:\n",
    "#\n",
    "#   1. Ensures that a dictionary `mixed_vars` exists, mapping keys like \"Mixed_1\"\n",
    "#      to item names.\n",
    "#   2. Builds the key corresponding to the selected index and retrieves the\n",
    "#      associated item name (`target_item_name`).\n",
    "#   3. Prints which Mixed_* item was selected.\n",
    "#   4. Constructs paths and filenames based on global variables and the selected item.\n",
    "#   5. Ensures that the output directory exists by creating it if necessary.\n",
    "#\n",
    "# The goal is to centralize selection of a single Mixed_* dataset and prepare paths\n",
    "# for downstream processing.\n",
    "\n",
    "\n",
    "# === Select which Mixed_* item to run ===\n",
    "mixed_index = 11  # Change this to process another dataset (e.g., 10)\n",
    "\n",
    "# Validate that the dictionary of mixed item names exists\n",
    "assert 'mixed_vars' in globals(), \"mixed_vars dict not found in globals().\"\n",
    "\n",
    "# Build the key (e.g., \"Mixed_1\") and retrieve the associated item name\n",
    "item_key = f\"Mixed_{mixed_index}\"\n",
    "target_item_name = mixed_vars.get(item_key)\n",
    "assert target_item_name, f\"{item_key} not found in mixed_vars.\"\n",
    "\n",
    "# Inform which item was selected\n",
    "print(f\"Selected: {item_key}  ->  ItemName: '{target_item_name}'\")\n",
    "\n",
    "# === Paths (reusing globals) ===\n",
    "assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "\n",
    "# Construct the name of the input file for the selected item\n",
    "file_name = f\"work_subset_{target_item_name}.txt\"\n",
    "file_path = os.path.join(Temp_file_path_DP, file_name)\n",
    "\n",
    "# Construct the base name for output files (will later be suffixed)\n",
    "base_output_filename = f\"Mixed_{target_item_name}_complete\"\n",
    "\n",
    "# Ensure the output directory exists; create it (including parent dirs) if needed\n",
    "Path(Temp_file_path_DP).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hmPmyusln7n"
   },
   "source": [
    "#### Import relevant data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "executionInfo": {
     "elapsed": 980,
     "status": "ok",
     "timestamp": 1765651198903,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "u6uPSYTQln7o",
    "outputId": "780d4c8c-e742-4045-d671-135a78e32321"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importing full dataset for Item: 'Selling_General__Administrative_Expenses' ...\n",
      "Full dataset loaded successfully: 3,078,865 rows total.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1101</td>\n",
       "      <td>171.617279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1101</td>\n",
       "      <td>189.549299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1101</td>\n",
       "      <td>129.874835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1101</td>\n",
       "      <td>137.582823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1101</td>\n",
       "      <td>98.561032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3         1992  December   \n",
       "1          Ars  1995-12-29         A          3         1993  December   \n",
       "2          Ars  1995-12-29         A          3         1994  December   \n",
       "3          Ars  1996-05-03         A          3         1995  December   \n",
       "4          Ars  1998-07-03         A          3         1996  December   \n",
       "\n",
       "  ItemCode       Value  \n",
       "0     1101  171.617279  \n",
       "1     1101  189.549299  \n",
       "2     1101  129.874835  \n",
       "3     1101  137.582823  \n",
       "4     1101   98.561032  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This block:\n",
    "#   1. Announces the import of a full dataset for the given `target_item_name`.\n",
    "#   2. Checks whether the file at `file_path` exists.\n",
    "#   3. If it exists, calls `import_file_to_dataframe(file_path)` to load the data\n",
    "#      into `mixed_raw`.\n",
    "#   4. If the loaded DataFrame is non-empty, prints a success message including\n",
    "#      the number of rows and shows the first few rows (via display or fallback\n",
    "#      to text printing).\n",
    "#   5. If the load fails or returns an empty DataFrame, prints a warning and\n",
    "#      creates an empty DataFrame.\n",
    "#   6. If the file does not exist, prints an error message and sets `mixed_raw`\n",
    "#      to an empty DataFrame.\n",
    "#   7. Finally, it runs `gc.collect()` to trigger garbage collection and free\n",
    "#      memory.\n",
    "#\n",
    "# Note: All previous emoji symbols in the print statements have been removed.\n",
    "\n",
    "# Inform the user that we are starting the import for this item\n",
    "print(f\"\\nImporting full dataset for Item: '{target_item_name}' ...\")\n",
    "\n",
    "# Check if the file path exists on disk\n",
    "if os.path.exists(file_path):\n",
    "    # If the file exists, attempt to import it into a DataFrame\n",
    "    mixed_raw = import_file_to_dataframe(file_path)\n",
    "\n",
    "    # If the DataFrame is successfully loaded and not empty\n",
    "    if mixed_raw is not None and not mixed_raw.empty:\n",
    "        # Print a success message including row count with thousands separator\n",
    "        print(f\"Full dataset loaded successfully: {len(mixed_raw):,} rows total.\")\n",
    "\n",
    "        try:\n",
    "            # Try to display the first few rows (Jupyter / IPython display)\n",
    "            display(mixed_raw.head())\n",
    "        except Exception:\n",
    "            # If display is not available, fall back to printing as plain text\n",
    "            print(mixed_raw.head().to_string(index=False))\n",
    "    else:\n",
    "        # If DataFrame is None or empty, warn and create an empty DataFrame\n",
    "        print(\"Dataset appears empty or could not be loaded.\")\n",
    "        mixed_raw = pd.DataFrame()\n",
    "else:\n",
    "    # If the file does not exist, report an error and set mixed_raw to empty\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    mixed_raw = pd.DataFrame()\n",
    "\n",
    "# Run garbage collection to free up memory after the load attempt\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDTixK2xln7o"
   },
   "source": [
    "#### Encode Frequency Code (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1765651199325,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "XRy54bSIln7p",
    "outputId": "d5ad0d61-4d3e-44a1-a17a-cbdcb0217277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding to Mixed dataset for 'Selling_General__Administrative_Expenses' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_581638/2687430345.py:85: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Q1Y07' 'Q2Y07' 'Q3Y07' ... 'Q3Y12' 'Q4Y12' 'Q1Y13']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ImplCountry</th>\n",
       "      <th>CurrentCurrency</th>\n",
       "      <th>HistCurrency</th>\n",
       "      <th>PIT Date</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>UpdateCode</th>\n",
       "      <th>FiscalPeriod</th>\n",
       "      <th>FYE Month</th>\n",
       "      <th>ItemCode</th>\n",
       "      <th>Value</th>\n",
       "      <th>Str_FiscalPrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1992</td>\n",
       "      <td>December</td>\n",
       "      <td>1101</td>\n",
       "      <td>171.617279</td>\n",
       "      <td>Y92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>December</td>\n",
       "      <td>1101</td>\n",
       "      <td>189.549299</td>\n",
       "      <td>Y93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1995-12-29</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>December</td>\n",
       "      <td>1101</td>\n",
       "      <td>129.874835</td>\n",
       "      <td>Y94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>December</td>\n",
       "      <td>1101</td>\n",
       "      <td>137.582823</td>\n",
       "      <td>Y95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02500770</td>\n",
       "      <td>PEUGEOT CITROEN ARGENTINA S.A.</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Ars</td>\n",
       "      <td>Ars</td>\n",
       "      <td>1998-07-03</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>1996</td>\n",
       "      <td>December</td>\n",
       "      <td>1101</td>\n",
       "      <td>98.561032</td>\n",
       "      <td>Y96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                     CompanyName ImplCountry CurrentCurrency  \\\n",
       "0  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "1  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "2  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "3  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "4  C02500770  PEUGEOT CITROEN ARGENTINA S.A.   Argentina             Ars   \n",
       "\n",
       "  HistCurrency    PIT Date Frequency UpdateCode  FiscalPeriod FYE Month  \\\n",
       "0          Ars  1995-12-29         A          3          1992  December   \n",
       "1          Ars  1995-12-29         A          3          1993  December   \n",
       "2          Ars  1995-12-29         A          3          1994  December   \n",
       "3          Ars  1996-05-03         A          3          1995  December   \n",
       "4          Ars  1998-07-03         A          3          1996  December   \n",
       "\n",
       "  ItemCode       Value Str_FiscalPrd  \n",
       "0     1101  171.617279           Y92  \n",
       "1     1101  189.549299           Y93  \n",
       "2     1101  129.874835           Y94  \n",
       "3     1101  137.582823           Y95  \n",
       "4     1101   98.561032           Y96  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This snippet provides:\n",
    "#\n",
    "# 1. A helper function `last2` that returns the last two digits of a number as a\n",
    "#    zero-padded string (for building YY strings).\n",
    "#\n",
    "# 2. A function `add_str_fiscalprd(df)` which:\n",
    "#    - Works on a copy of an input DataFrame containing financial periods.\n",
    "#    - Normalizes the 'Frequency' (upper-case, no missing).\n",
    "#    - Stores the original 'FiscalPeriod' and converts it to numeric.\n",
    "#    - Creates a string representation 'Str_FiscalPrd' depending on the frequency:\n",
    "#         - Q/C/E/R: quarter-based (\"QnYyy\")\n",
    "#         - A/B: annual (\"Yyy\")\n",
    "#         - F/S: semiannual (\"SnYyy\")\n",
    "#         - K/T/L/U: trimester-like (\"TnYyy\")\n",
    "#    - Derives an implied full-year integer 'ImplFiscPer_Calculated' from the\n",
    "#      two-digit year (80–99 => 19xx, else 20xx).\n",
    "#    - For annual rows (A/B), checks discrepancies between original\n",
    "#      'FiscalPeriod' and implied full-year; prints a small preview & total count.\n",
    "#    - Overwrites 'FiscalPeriod' with 'ImplFiscPer_Calculated' and drops helper\n",
    "#      columns.\n",
    "#\n",
    "# 3. A small driver block that:\n",
    "#    - Checks that `mixed_raw` exists and is non-empty.\n",
    "#    - Applies `add_str_fiscalprd` to produce `mixed_encoded`.\n",
    "#    - Displays a head preview or prints a message and sets `mixed_encoded = None`\n",
    "#      if input is missing/empty.\n",
    "\n",
    "def last2(n):\n",
    "    \"\"\"Return last two digits as zero-padded string, or None if NaN.\"\"\"\n",
    "    # If input is NaN (or pandas-style missing), return None (no digits)\n",
    "    if pd.isna(n):\n",
    "        return None\n",
    "    # Convert to int, format as 4-digit zero-padded string, take last 2 chars\n",
    "    # Example: n=2023 -> \"2023\"[-2:] -> \"23\"\n",
    "    return f\"{int(n):04d}\"[-2:]\n",
    "\n",
    "\n",
    "def add_str_fiscalprd(df):\n",
    "    \"\"\"\n",
    "    Creates 'Str_FiscalPrd' and overwrites 'FiscalPeriod' with an implied full year.\n",
    "    Also prints discrepancies for Annual (A,B) rows between original and calculated year.\n",
    "\n",
    "    The mapping logic:\n",
    "      - Quarterly (C, Q, E, R):   fp encodes a quarter index -> \"Q{1-4}Y{yy}\"\n",
    "      - Annual   (A, B):         fp is the year itself      -> \"Y{yy}\"\n",
    "      - Semiannual (F, S):       fp encodes half-year       -> \"S{1-2}Y{yy}\"\n",
    "      - Trimester-like (K, T,\n",
    "         L, U):                  fp encodes trimester       -> \"T{1-3}Y{yy}\"\n",
    "    Then we re-infer the full year from the YY part with 19xx/20xx rule.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the original input DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize frequency codes:\n",
    "    # - uppercase for consistency\n",
    "    # - fill missing values with empty string\n",
    "    df[\"Frequency\"] = df[\"Frequency\"].str.upper().fillna(\"\")\n",
    "\n",
    "    # Store original FiscalPeriod in a separate column for later comparison\n",
    "    df['Original_FiscalPeriod'] = df['FiscalPeriod']\n",
    "\n",
    "    # Convert FiscalPeriod to numeric, coerce errors to NaN\n",
    "    fp = pd.to_numeric(df[\"FiscalPeriod\"], errors=\"coerce\")\n",
    "\n",
    "    # Masks for different frequency groups\n",
    "    m_quarter = df[\"Frequency\"].isin([\"C\", \"Q\", \"E\", \"R\"])  # quarter-based\n",
    "    m_AB      = df[\"Frequency\"].isin([\"A\", \"B\"])            # annual\n",
    "    m_FS      = df[\"Frequency\"].isin([\"F\", \"S\"])            # semiannual\n",
    "    m_KTLU    = df[\"Frequency\"].isin([\"K\", \"T\", \"L\", \"U\"])  # trimester-like\n",
    "\n",
    "    # Initialize Str_FiscalPrd as NaN; we will fill per frequency group\n",
    "    df[\"Str_FiscalPrd\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Quarter-based encoding\n",
    "    # -------------------------\n",
    "    # Quarter number: (fp % 4) + 1 => yields 1..4 (if fp is integer-based encoding)\n",
    "    q_part = ((fp % 4) + 1).where(m_quarter)\n",
    "    # Year part for quarter: integer division by 4, then take last two digits\n",
    "    q_year = (fp // 4).where(m_quarter).apply(last2)\n",
    "    # Assign \"Q{quarter}Y{yy}\" for quarter frequencies\n",
    "    df.loc[m_quarter, \"Str_FiscalPrd\"] = (\n",
    "        \"Q\" +\n",
    "        q_part.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        q_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Annual encoding (A, B)\n",
    "    # -------------------------\n",
    "    # Year part is fp itself for A/B; take last two digits via last2\n",
    "    ab_year = fp.where(m_AB).apply(last2)\n",
    "    # Assign \"Y{yy}\" for annual frequencies\n",
    "    df.loc[m_AB, \"Str_FiscalPrd\"] = \"Y\" + ab_year.fillna('')\n",
    "\n",
    "    # -------------------------\n",
    "    # Semiannual encoding (F, S)\n",
    "    # -------------------------\n",
    "    # Semester number: (fp % 2) + 1 => 1 or 2\n",
    "    fs_sem  = ((fp % 2) + 1).where(m_FS)\n",
    "    # Year part: fp // 2, then last two digits\n",
    "    fs_year = (fp // 2).where(m_FS).apply(last2)\n",
    "    # Assign \"S{sem}Y{yy}\" for F/S\n",
    "    df.loc[m_FS, \"Str_FiscalPrd\"] = (\n",
    "        \"S\" +\n",
    "        fs_sem.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        fs_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Trimester-like encoding (K, T, L, U)\n",
    "    # -------------------------\n",
    "    # Term number: (fp % 3) + 1 => 1..3\n",
    "    t_term  = ((fp % 3) + 1).where(m_KTLU)\n",
    "    # Year part: fp // 3, then last two digits\n",
    "    t_year  = (fp // 3).where(m_KTLU).apply(last2)\n",
    "    # Assign \"T{term}Y{yy}\" for K/T/L/U frequencies\n",
    "    df.loc[m_KTLU, \"Str_FiscalPrd\"] = (\n",
    "        \"T\" +\n",
    "        t_term.astype(\"Int64\").astype(str) +\n",
    "        \"Y\" +\n",
    "        t_year.fillna('')\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Implied full year from Str_FiscalPrd (19xx / 20xx reconstruction)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Extract two-digit year from \"Yyy\" pattern, e.g., \"Q1Y23\" -> \"23\"\n",
    "    year_part = df['Str_FiscalPrd'].str.extract(r'Y(\\d{2})', expand=False)\n",
    "    # Convert that to numeric (NaN if not parseable)\n",
    "    year_numeric = pd.to_numeric(year_part, errors='coerce')\n",
    "\n",
    "    # Map two-digit year to full year:\n",
    "    #  - 80–99 => 19xx\n",
    "    #  - else  => 20xx\n",
    "    df['ImplFiscPer_Calculated'] = year_numeric.apply(\n",
    "        lambda x: int(f\"19{int(x):02d}\") if pd.notna(x) and x >= 80\n",
    "        else (int(f\"20{int(x):02d}\") if pd.notna(x) else np.nan)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Discrepancies check for Annual frequencies (A, B)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Work on a subset of annual frequencies only\n",
    "    annual_rows_for_check = df[m_AB].copy()\n",
    "\n",
    "    # Build boolean mask where implied full year does NOT match the original\n",
    "    discrepancy_mask_annual = ~(\n",
    "        # Either they match numerically...\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'] ==\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce'))\n",
    "        |\n",
    "        # ...or both are NaN\n",
    "        (annual_rows_for_check['ImplFiscPer_Calculated'].isna() &\n",
    "         pd.to_numeric(annual_rows_for_check['Original_FiscalPeriod'], errors='coerce').isna())\n",
    "    )\n",
    "\n",
    "    # Subset rows with discrepancies\n",
    "    discrepancy_rows = annual_rows_for_check[discrepancy_mask_annual].copy()\n",
    "\n",
    "    # If we have any, show a short sample and the total count\n",
    "    if not discrepancy_rows.empty:\n",
    "        print(\"\\nDiscrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies:\")\n",
    "        display(\n",
    "            discrepancy_rows[\n",
    "                ['ID', 'Frequency', 'Original_FiscalPeriod', 'Str_FiscalPrd', 'ImplFiscPer_Calculated']\n",
    "            ].head()\n",
    "        )\n",
    "        print(f\"Total discrepancies found for Annual frequencies: {len(discrepancy_rows)}\")\n",
    "    else:\n",
    "        print(\"\\nNo discrepancies found between original FiscalPeriod and calculated ImplFiscPer for Annual (A, B) frequencies.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Overwrite FiscalPeriod and drop temporary helper columns\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Replace 'FiscalPeriod' with the implied full-year value we just calculated\n",
    "    df['FiscalPeriod'] = df['ImplFiscPer_Calculated']\n",
    "    # Drop helper columns not needed downstream\n",
    "    df.drop(columns=['Original_FiscalPeriod', 'ImplFiscPer_Calculated'], inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: apply encoding to mixed_raw if present and non-empty\n",
    "# =============================================================================\n",
    "if 'mixed_raw' in globals() and mixed_raw is not None and not mixed_raw.empty:\n",
    "    # Inform which item we are encoding for\n",
    "    print(f\"Applying encoding to Mixed dataset for '{target_item_name}' ...\")\n",
    "    # Apply fiscal period encoding\n",
    "    mixed_encoded = add_str_fiscalprd(mixed_raw)\n",
    "    # Show a preview of the encoded data\n",
    "    display(mixed_encoded.head())\n",
    "else:\n",
    "    # If mixed_raw is missing or empty, we skip and mark mixed_encoded as None\n",
    "    print(\"mixed_raw not found or empty. Cannot perform encoding.\")\n",
    "    mixed_encoded = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6Ghh2-uln7p"
   },
   "source": [
    "#### Annualize data with most recent information (Check of output required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7869,
     "status": "ok",
     "timestamp": 1765651207196,
     "user": {
      "displayName": "Kilian Große-Wortmann",
      "userId": "02304931418978819695"
     },
     "user_tz": -60
    },
    "id": "74-uB7S1n1iH",
    "outputId": "fe3462d5-4e59-4a60-907e-71f4d4278fc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dataset contains 3,078,865 rows before processing.\n",
      "\n",
      "\n",
      "=== Future-date check (period dates > PIT Date) ===\n",
      "Per-label violations: {'A_Date': 0, 'Q1_Date': 0, 'Q2_Date': 0, 'Q3_Date': 0, 'Q4_Date': 0, 'S1_Date': 0, 'S2_Date': 0, 'T1_Date': 0, 'T2_Date': 0, 'T3_Date': 0}\n",
      "Rows with ANY future-dated period value: 0\n",
      "\n",
      "=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\n",
      "         finite_rows: 1929431\n",
      "                mean: 18990.9321407337\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.04908826721622\n",
      "                 p10: 99.30057821787052\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 101.93498320484426\n",
      "\n",
      "Rows to drop due to AnnPITValue_Pct (±inf or >200 or <50): 54,319\n",
      "\n",
      "=== AnnPITValue_Pct summary — AFTER quality drop ===\n",
      "         finite_rows: 1875112\n",
      "                mean: 100.34380370354451\n",
      "              median: 100.0\n",
      "winsorized_mean_1pct: 100.18822378291644\n",
      "                 p10: 99.94984695952232\n",
      "                 p20: 100.0\n",
      "                 p30: 100.0\n",
      "                 p40: 100.0\n",
      "                 p50: 100.0\n",
      "                 p60: 100.0\n",
      "                 p70: 100.0\n",
      "                 p80: 100.0\n",
      "                 p90: 101.4906996621759\n",
      "\n",
      "Saved full: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Selling_General__Administrative_Expenses_complete.txt\n",
      "Saved subset: /home/jovyan/work/hpool1/pseidel/test/Temp/TempDataPreparation/Mixed_Selling_General__Administrative_Expenses_complete_subset.txt\n",
      "\n",
      "=== Row Accounting ===\n",
      "Input rows:                     3,078,865\n",
      "Excludedby Frequency (E/L/R/U): 0\n",
      "Dropped by quality (Pct rules): 54,319\n",
      "Output rows (final):            3,024,546\n",
      "Check: excluded + dropped + output = 3,078,865\n",
      "Row counts reconcile exactly.\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# =====================================================================================\n",
    "# SUMMARY\n",
    "# =====================================================================================\n",
    "# This script takes an input DataFrame `mixed_encoded` (if present in the global scope)\n",
    "# that contains financial time-series data (per company, item, currency, fiscal period,\n",
    "# and PIT Date). It then:\n",
    "#\n",
    "# 1. Cleans and standardizes key columns (dates, numeric types, string IDs).\n",
    "# 2. Excludes rows with certain frequencies (E/L/R/U).\n",
    "# 3. Parses fiscal period strings into quarter/semester/trimester indicators (QNUM/SNUM/TNUM).\n",
    "# 4. Uses a custom, vectorized \"as-of\" join (`asof_numpy`) to attach the most recent\n",
    "#    annual, quarterly, semiannual, and trimester values for each (ID, HistCurrency,\n",
    "#    ItemCode, FiscalPeriod) up to each row’s PIT Date.\n",
    "# 5. Builds \"full-year\" candidate values from:\n",
    "#       - actual annuals (A),\n",
    "#       - sum of Q1..Q4 (Q4 proxy),\n",
    "#       - sum of S1..S2 (S2 proxy),\n",
    "#       - sum of T1..T3 (T3 proxy),\n",
    "#    and selects the best candidate based on priorities and relationship to the row’s\n",
    "#    fiscal period (same-year vs prior-year).\n",
    "# 6. Computes an annual PIT-based metric `AnnPITValue` and compares it to the “true”\n",
    "#    annual value (`TrueValue`) to derive a percentage `AnnPITValue_Pct` for QC.\n",
    "# 7. Performs quality checks:\n",
    "#       - Ensures no period-date is after the PIT Date.\n",
    "#       - Drops rows whose `AnnPITValue_Pct` is outside the range [50, 200] or infinite.\n",
    "# 8. Keeps a curated set of columns, drops helper columns, and saves:\n",
    "#       - a full output file\n",
    "#       - a subset file with key columns for quick inspection.\n",
    "# 9. Prints row-accounting stats and frees some memory.\n",
    "#\n",
    "# If `mixed_encoded` is not defined or is None, it simply prints a message and exits.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from datetime import timedelta            # <--- Added to fix NameError\n",
    "from scipy.stats.mstats import winsorize  # <--- Added to fix NameError\n",
    "\n",
    "# Enable pandas \"copy-on-write\" behavior to reduce unintended chained assignment effects\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# ---------- Helper: fast as-of (right.PIT <= left.PIT) ----------\n",
    "\n",
    "def _key(fr, cols):\n",
    "    \"\"\"\n",
    "    Helper function to build a string key from multiple columns.\n",
    "    For each row, join the values of 'cols' with '||'.\n",
    "    \"\"\"\n",
    "    return fr[cols].astype(str).agg('||'.join, axis=1)\n",
    "\n",
    "\n",
    "def asof_numpy(left_df: pd.DataFrame, right_df: pd.DataFrame, by_cols: list[str]):\n",
    "    \"\"\"\n",
    "    For each row in left_df, find the latest (as-of) Value from right_df\n",
    "    with matching by_cols and right_df['PIT Date'] <= left_df['PIT Date'].\n",
    "\n",
    "    This is a manual, NumPy-based implementation of an \"as-of merge\" grouped by `by_cols`.\n",
    "    It:\n",
    "    - filters out rows with missing required fields,\n",
    "    - normalizes PIT Date to daily precision,\n",
    "    - groups right_df by composite key of `by_cols`,\n",
    "    - for each left row, binary-searches into the matching right group by PIT Date,\n",
    "      picking the last date <= the left PIT Date,\n",
    "    - returns two aligned arrays: (values, dates) for the left_df rows.\n",
    "    \"\"\"\n",
    "    # Initialize outputs with NaNs and NaT for all left_df rows\n",
    "    out_vals  = np.full(len(left_df), np.nan, dtype='float64')\n",
    "    out_dates = np.full(len(left_df), 'NaT', dtype='datetime64[ns]')\n",
    "\n",
    "    # Required columns on each side\n",
    "    left_req  = by_cols + ['PIT Date']\n",
    "    right_req = by_cols + ['PIT Date', 'Value']\n",
    "\n",
    "    # Mask: rows that have all required fields non-null\n",
    "    lmask = left_df[left_req].notna().all(axis=1)\n",
    "    rmask = right_df[right_req].notna().all(axis=1)\n",
    "\n",
    "    # If either side has no valid rows, return the default empty outputs\n",
    "    if not lmask.any() or not rmask.any():\n",
    "        return out_vals, out_dates\n",
    "\n",
    "    # Work on filtered copies only (avoid side effects)\n",
    "    l = left_df.loc[lmask, left_req].copy()\n",
    "    r = right_df.loc[rmask, right_req].copy()\n",
    "\n",
    "    # Normalize PIT Date columns to datetime at day precision\n",
    "    l['PIT Date'] = pd.to_datetime(l['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    r['PIT Date'] = pd.to_datetime(r['PIT Date'], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # Build composite keys for group-level match (based on by_cols)\n",
    "    l['__k'] = _key(l, by_cols)\n",
    "    r['__k'] = _key(r, by_cols)\n",
    "\n",
    "    # Sort right side by key and date to enable binary search per key\n",
    "    r = r.sort_values(['__k', 'PIT Date']).reset_index(drop=True)\n",
    "\n",
    "    # Extract NumPy arrays for right side\n",
    "    rk   = r['__k'].to_numpy()\n",
    "    rdt  = r['PIT Date'].to_numpy()\n",
    "    rval = r['Value'].to_numpy()\n",
    "\n",
    "    # Find unique keys and first index for each key in the sorted right side\n",
    "    uniq, first = np.unique(rk, return_index=True)\n",
    "\n",
    "    # Build a dict: key -> (dates array, values array) slice\n",
    "    slices = {}\n",
    "    for i, k in enumerate(uniq):\n",
    "        s = first[i]  # start of this key\n",
    "        e = first[i + 1] if i + 1 < len(first) else len(r)  # end of this key\n",
    "        slices[k] = (rdt[s:e], rval[s:e])\n",
    "\n",
    "    # Left side indices and arrays\n",
    "    l_idx = l.index.to_numpy()\n",
    "    lk    = l['__k'].to_numpy()\n",
    "    ldt   = l['PIT Date'].to_numpy()\n",
    "\n",
    "    # Sort left side by key (stable mergesort to preserve original row order within key)\n",
    "    order = np.argsort(lk, kind='mergesort')\n",
    "    sk, sd, sp = lk[order], ldt[order], l_idx[order]\n",
    "\n",
    "    # i iterates over the sorted left rows\n",
    "    i = 0\n",
    "    n = len(sk)\n",
    "    while i < n:\n",
    "        k = sk[i]  # current composite key\n",
    "        j = i + 1\n",
    "        # Find contiguous block [i:j) for this key\n",
    "        while j < n and sk[j] == k:\n",
    "            j += 1\n",
    "\n",
    "        # Dates and original positions of this key’s left rows\n",
    "        block_dates = sd[i:j]\n",
    "        block_pos   = sp[i:j]\n",
    "\n",
    "        # If we have matching right-hand slices for this key, do the as-of search\n",
    "        if k in slices:\n",
    "            r_dates, r_vals = slices[k]\n",
    "            # searchsorted(..., 'right') - 1 gives index of last r_date <= block_date\n",
    "            pos   = np.searchsorted(r_dates, block_dates, side='right') - 1\n",
    "            valid = pos >= 0  # positions where such a date exists\n",
    "            if np.any(valid):\n",
    "                # Fill outputs for left rows where we found a valid match\n",
    "                out_vals[block_pos[valid]]  = r_vals[pos[valid]]\n",
    "                out_dates[block_pos[valid]] = r_dates[pos[valid]]\n",
    "        # Move to next key block\n",
    "        i = j\n",
    "\n",
    "    return out_vals, out_dates\n",
    "\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def pctile(s, q):\n",
    "    \"\"\"\n",
    "    Safe percentile calculation: returns quantile q of Series `s`,\n",
    "    or NaN if quantile fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.quantile(q, interpolation='linear')\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def summarize_pct(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Computes a summary of finite values of a series:\n",
    "      - count\n",
    "      - mean, median\n",
    "      - 1% winsorized mean\n",
    "      - selected deciles (p10, p20, ..., p90)\n",
    "    Infinite values are treated as NaN and removed.\n",
    "    If no finite values remain, returns an empty dict.\n",
    "    \"\"\"\n",
    "    # Replace +/-inf with NaN and drop all NaNs\n",
    "    s = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if s.empty:\n",
    "        return {}\n",
    "    \n",
    "    # FIX APPLIED HERE: .to_numpy().copy() ensures the array is writable for winsorize\n",
    "    w_mean = winsorize(s.to_numpy().copy(), limits=[0.01, 0.01]).mean()\n",
    "    \n",
    "    return {\n",
    "        \"finite_rows\": len(s),\n",
    "        \"mean\": s.mean(),\n",
    "        \"median\": s.median(),\n",
    "        \"winsorized_mean_1pct\": w_mean,\n",
    "        \"p10\": pctile(s, 0.10), \"p20\": pctile(s, 0.20), \"p30\": pctile(s, 0.30),\n",
    "        \"p40\": pctile(s, 0.40), \"p50\": pctile(s, 0.50), \"p60\": pctile(s, 0.60),\n",
    "        \"p70\": pctile(s, 0.70), \"p80\": pctile(s, 0.80), \"p90\": pctile(s, 0.90)\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Priority for full-year candidates ----------\n",
    "\n",
    "# Fixed priority mapping for full-year candidates:\n",
    "#   'A'  : actual annual value\n",
    "#   'Q4' : annual proxy from four quarters\n",
    "#   'T3' : annual proxy from three trimesters\n",
    "#   'S2' : annual proxy from two semesters\n",
    "_PERIOD_PRIORITY = {\n",
    "    'A': 100,  # highest priority: actual annual\n",
    "    'Q4': 90,  # then Q1+Q2+Q3+Q4\n",
    "    'T3': 80,  # then T1+T2+T3\n",
    "    'S2': 70,  # then S1+S2\n",
    "}\n",
    "\n",
    "# ============================ MAIN ============================\n",
    "\n",
    "# Only run the main logic if a global DataFrame `mixed_encoded` exists and is not None\n",
    "if 'mixed_encoded' in globals() and mixed_encoded is not None:\n",
    "    # Count initial input rows\n",
    "    input_rows = len(mixed_encoded)\n",
    "    print(f\"Input dataset contains {input_rows:,} rows before processing.\\n\")\n",
    "\n",
    "    # Work on a copy of the input dataset\n",
    "    working = mixed_encoded.copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Exclude certain frequencies (E/L/R/U)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create mask of rows whose Frequency is one of E, L, R, U (case-insensitive)\n",
    "    excl_mask = working['Frequency'].astype(str).str.upper().isin(['E', 'L', 'R', 'U'])\n",
    "    # Count how many rows will be excluded\n",
    "    excluded_rows = int(excl_mask.sum())\n",
    "    # Keep only rows that are NOT in the exclusion set\n",
    "    working = working.loc[~excl_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Set dtypes and normalize important columns\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert PIT Date to datetime (coerce errors -> NaT), floor to day\n",
    "    working['PIT Date']     = pd.to_datetime(working['PIT Date'], errors='coerce').dt.floor('D')\n",
    "    # FiscalPeriod: numeric (e.g., 2021, 2022, ...)\n",
    "    working['FiscalPeriod'] = pd.to_numeric(working['FiscalPeriod'], errors='coerce')\n",
    "    # Value: numeric (float)\n",
    "    working['Value']        = pd.to_numeric(working['Value'], errors='coerce')\n",
    "\n",
    "    # Convert key ID / code columns to string to ensure consistency\n",
    "    for c in ['ID', 'HistCurrency', 'ItemCode', 'Frequency', 'Str_FiscalPrd']:\n",
    "        if c in working.columns:\n",
    "            working[c] = working[c].astype(str)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Parse Q/S/T markers from Str_FiscalPrd (like 'Q1Y2023')\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract quarter number Q1..Q4 from e.g. \"Q1Y2023\" into QNUM\n",
    "    working['QNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^Q([1-4])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract semiannual number S1..S2 into SNUM\n",
    "    working['SNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^S([1-2])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    # Extract trimester number T1..T3 into TNUM\n",
    "    working['TNUM'] = pd.to_numeric(\n",
    "        working['Str_FiscalPrd'].str.extract(r'^T([1-3])Y', expand=False),\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ensure period columns exist (Q1..Q4, S1..S2, T1..T3, A + their date cols)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create value columns for Q1..Q4, S1..S2, T1..T3, A if they are missing\n",
    "    for c in [*(f'Q{i}' for i in range(1, 5)),\n",
    "              *(f'S{i}' for i in range(1, 3)),\n",
    "              *(f'T{i}' for i in range(1, 4)),\n",
    "              'A']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = np.nan\n",
    "\n",
    "    # Create corresponding *_Date columns if missing\n",
    "    for c in [*(f'Q{i}_Date' for i in range(1, 5)),\n",
    "              *(f'S{i}_Date' for i in range(1, 3)),\n",
    "              *(f'T{i}_Date' for i in range(1, 4)),\n",
    "              'A_Date']:\n",
    "        if c not in working.columns:\n",
    "            working[c] = pd.NaT\n",
    "\n",
    "    # Base key for many of the as-of mappings\n",
    "    base_keys = ['ID', 'HistCurrency', 'ItemCode', 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1) Derive TrueValue from annuals (A/B frequencies)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Mask annual-like rows where Value is present\n",
    "    mask_annual = working['Frequency'].isin(['A', 'B']) & working['Value'].notna()\n",
    "    # annual_src: one row per (ID, FiscalPeriod, HistCurrency) with last PIT Date\n",
    "    annual_src = (\n",
    "        working.loc[mask_annual,\n",
    "                    ['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date', 'Value']]\n",
    "        .sort_values(['ID', 'FiscalPeriod', 'HistCurrency', 'PIT Date'])\n",
    "        .drop_duplicates(['ID', 'FiscalPeriod', 'HistCurrency'], keep='last')\n",
    "        .rename(columns={'Value': 'TrueValue', 'PIT Date': 'TrueValue_Date'})\n",
    "    )\n",
    "    # Left-join true annual value back onto working\n",
    "    working = working.merge(\n",
    "        annual_src,\n",
    "        on=['ID', 'FiscalPeriod', 'HistCurrency'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2) As-of mapping (same FiscalPeriod) for A/Q/S/T\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----- Annual -----\n",
    "    # Source rows for annual frequencies A/B\n",
    "    src_A = working.loc[\n",
    "        working['Frequency'].isin(['A', 'B']) & working['Value'].notna(),\n",
    "        base_keys + ['PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    # As-of join: for each working row, get most recent annual value by PIT Date\n",
    "    vA, dA = asof_numpy(working, src_A, by_cols=base_keys)\n",
    "    working['A'], working['A_Date'] = vA, dA\n",
    "    # Origin fiscal period of annual value (same as row's FiscalPeriod when present)\n",
    "    working['A_OriginFP'] = np.where(\n",
    "        working['A'].notna(), working['FiscalPeriod'], np.nan\n",
    "    )\n",
    "\n",
    "    # ----- Quarterly -----\n",
    "    # Source rows for quarterly frequencies (Q/C) with valid QNUM\n",
    "    src_Q = working.loc[\n",
    "        working['Frequency'].isin(['Q', 'C']) & working['QNUM'].notna(),\n",
    "        base_keys + ['QNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for q in (1, 2, 3, 4):\n",
    "        # Restrict to a specific quarter q\n",
    "        rv = src_Q[src_Q['QNUM'] == q].drop(columns=['QNUM'])\n",
    "        # As-of join for that quarter\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'Q{q}', f'Q{q}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        # Origin FP column for that quarter\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        # Fill origin FP only where quarter value is non-null and origin not yet set\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Semiannual -----\n",
    "    # Source rows for semiannual frequencies (S/F) with valid SNUM\n",
    "    src_S = working.loc[\n",
    "        working['Frequency'].isin(['S', 'F']) & working['SNUM'].notna(),\n",
    "        base_keys + ['SNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for s in (1, 2):\n",
    "        rv = src_S[src_S['SNUM'] == s].drop(columns=['SNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'S{s}', f'S{s}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # ----- Trimester -----\n",
    "    # Source rows for trimester frequencies (T/K) with valid TNUM\n",
    "    src_T = working.loc[\n",
    "        working['Frequency'].isin(['T', 'K']) & working['TNUM'].notna(),\n",
    "        base_keys + ['TNUM', 'PIT Date', 'Value']\n",
    "    ].copy()\n",
    "    for t in (1, 2, 3):\n",
    "        rv = src_T[src_T['TNUM'] == t].drop(columns=['TNUM'])\n",
    "        v, d = asof_numpy(working, rv, by_cols=base_keys)\n",
    "        col, dcol = f'T{t}', f'T{t}_Date'\n",
    "        working[col], working[dcol] = v, d\n",
    "        ocol = f'{col}_OriginFP'\n",
    "        if ocol not in working.columns:\n",
    "            working[ocol] = np.nan\n",
    "        m = working[col].notna() & working[ocol].isna()\n",
    "        working.loc[m, ocol] = working.loc[m, 'FiscalPeriod']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3) Prepare labels & normalize dates (NO prev-year fill, NO forward-fill)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sort working data consistently for downstream calculations\n",
    "    working = working.sort_values(['ID', 'HistCurrency', 'FiscalPeriod', 'PIT Date'])\n",
    "\n",
    "    # List of all period value columns\n",
    "    value_cols_all  = [f'Q{i}' for i in range(1, 5)] + \\\n",
    "                      [f'S{i}' for i in range(1, 3)] + \\\n",
    "                      [f'T{i}' for i in range(1, 4)] + ['A']\n",
    "    # Corresponding date columns\n",
    "    date_cols_all   = [f'{c}_Date' for c in value_cols_all]\n",
    "    # Corresponding origin FP columns\n",
    "    origin_cols_all = [f'{c}_OriginFP' for c in value_cols_all]\n",
    "\n",
    "    # Ensure that all date columns are proper datetimes (floor to day)\n",
    "    # Note: explicitly no groupby-forward-fill here – only asof-filled values remain\n",
    "    for c in date_cols_all:\n",
    "        if c in working.columns:\n",
    "            working[c] = pd.to_datetime(working[c], errors='coerce').dt.floor('D')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4) Build full-year candidates from fixed sets (Q1–Q4, S1–S2, T1–T3)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def full_year_from_fixed(row, labels, pit, cutoff):\n",
    "        \"\"\"\n",
    "        Fixed full-year from given labels (e.g. Q1..Q4, S1..S2, T1..T3):\n",
    "\n",
    "          - Requires ALL labels to have:\n",
    "              value, date, origin_fp.\n",
    "          - All dates must be within [cutoff, pit].\n",
    "          - origin_fp = max(origin_fp_i)  (the 'newest year' across components).\n",
    "\n",
    "        Returns (dt, val, origin_fp) or (NaT, NaN, NaN) if any requirement fails.\n",
    "        \"\"\"\n",
    "        vals, dts, fps = [], [], []\n",
    "        for lbl in labels:\n",
    "            v = row.get(lbl, np.nan)\n",
    "            d = row.get(f'{lbl}_Date', pd.NaT)\n",
    "            o = row.get(f'{lbl}_OriginFP', np.nan)\n",
    "\n",
    "            # Require non-null for value, date, origin FP\n",
    "            if pd.isna(v) or pd.isna(d) or pd.isna(o):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            # Normalize date and check it lies within [cutoff, pit]\n",
    "            d = pd.to_datetime(d, errors='coerce')\n",
    "            if pd.isna(d) or not (cutoff <= d <= pit):\n",
    "                return pd.NaT, np.nan, np.nan\n",
    "\n",
    "            vals.append(float(v))\n",
    "            dts.append(d)\n",
    "            fps.append(int(o))\n",
    "\n",
    "        # Sum all component values, pick latest date and max origin FP\n",
    "        total_val = float(np.nansum(vals))\n",
    "        latest_dt = max(dts)\n",
    "        origin_fp = max(fps)\n",
    "        return latest_dt, total_val, origin_fp\n",
    "\n",
    "    def pick_annpit_sum_with_origin(row):\n",
    "        \"\"\"\n",
    "        For each row, pick the best annual PIT-based value (AnnPITValue) by:\n",
    "\n",
    "        1) Building a time window [PIT - 365 days, PIT].\n",
    "        2) Generating candidate annual values:\n",
    "           - actual annual A (if within window and with origin FP),\n",
    "           - Q4 proxy from Q1..Q4,\n",
    "           - S2 proxy from S1..S2,\n",
    "           - T3 proxy from T1..T3,\n",
    "           each with associated priority and origin FP.\n",
    "        3) Ranking candidates according to:\n",
    "           - Same-year A (highest),\n",
    "           - Same-year proxies (Q4/T3/S2),\n",
    "           - Prior-year A,\n",
    "           - Prior-year proxies,\n",
    "           - Otherwise: any remaining candidate by (priority, latest date).\n",
    "        4) Returning the chosen candidate's value (keeping zeros, but filtering NaNs).\n",
    "        \"\"\"\n",
    "        pit = row['PIT Date']\n",
    "        if pd.isna(pit):\n",
    "            return np.nan\n",
    "        cutoff = pit - timedelta(days=365)  # Now works because timedelta is imported\n",
    "\n",
    "        # Current row's fiscal period, cast to int if possible\n",
    "        fp = row.get('FiscalPeriod', np.nan)\n",
    "        try:\n",
    "            fp_int = int(fp) if not pd.isna(fp) else None\n",
    "        except Exception:\n",
    "            fp_int = None\n",
    "\n",
    "        # Collect candidate tuples: (label, priority, date, value, origin_fp)\n",
    "        candidates = []\n",
    "\n",
    "        # --- Candidate A: actual annual (0 is allowed)\n",
    "        A_val = row.get('A', np.nan)\n",
    "        A_dt  = row.get('A_Date', pd.NaT)\n",
    "        A_ofp = row.get('A_OriginFP', np.nan)\n",
    "        if pd.notna(A_val) and pd.notna(A_dt) and not pd.isna(A_ofp):\n",
    "            A_dt = pd.to_datetime(A_dt, errors='coerce')\n",
    "            if pd.notna(A_dt) and (cutoff <= A_dt <= pit):\n",
    "                candidates.append(('A', _PERIOD_PRIORITY['A'], A_dt, float(A_val), int(A_ofp)))\n",
    "\n",
    "        # --- Candidate Q4: sum of Q1..Q4 (0 allowed)\n",
    "        q4_dt, q4_val, q4_fp = full_year_from_fixed(\n",
    "            row, [f'Q{i}' for i in range(1, 5)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(q4_val) and pd.notna(q4_dt) and not pd.isna(q4_fp):\n",
    "            candidates.append(('Q4', _PERIOD_PRIORITY['Q4'], q4_dt, float(q4_val), int(q4_fp)))\n",
    "\n",
    "        # --- Candidate S2: sum of S1..S2 (0 allowed)\n",
    "        s2_dt, s2_val, s2_fp = full_year_from_fixed(\n",
    "            row, [f'S{i}' for i in range(1, 3)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(s2_val) and pd.notna(s2_dt) and not pd.isna(s2_fp):\n",
    "            candidates.append(('S2', _PERIOD_PRIORITY['S2'], s2_dt, float(s2_val), int(s2_fp)))\n",
    "\n",
    "        # --- Candidate T3: sum of T1..T3 (0 allowed)\n",
    "        t3_dt, t3_val, t3_fp = full_year_from_fixed(\n",
    "            row, [f'T{i}' for i in range(1, 4)], pit, cutoff\n",
    "        )\n",
    "        if pd.notna(t3_val) and pd.notna(t3_dt) and not pd.isna(t3_fp):\n",
    "            candidates.append(('T3', _PERIOD_PRIORITY['T3'], t3_dt, float(t3_val), int(t3_fp)))\n",
    "\n",
    "        # If no candidates, return NaN\n",
    "        if not candidates:\n",
    "            return np.nan\n",
    "\n",
    "        # Filter out NaN-valued candidates (keep 0-valued ones)\n",
    "        def valid(seq):\n",
    "            return [c for c in seq if not np.isnan(c[3])]\n",
    "\n",
    "        # Relationship between row's fiscal period and candidate's origin FP:\n",
    "        # same / prior / other / unknown\n",
    "        def rel(c):\n",
    "            _, _, _, _, ofp = c\n",
    "            if fp_int is None or ofp is None:\n",
    "                return 'unknown'\n",
    "            if ofp == fp_int:\n",
    "                return 'same'\n",
    "            if ofp == fp_int - 1:\n",
    "                return 'prior'\n",
    "            return 'other'\n",
    "\n",
    "        # 1) Same-year actual annual A (max by date)\n",
    "        same_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'same')\n",
    "        if same_A:\n",
    "            best = max(same_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 2) Same-year proxies (Q4/T3/S2), max by (priority, date)\n",
    "        same_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'same'\n",
    "        )\n",
    "        if same_proxies:\n",
    "            best = max(same_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 3) Prior-year actual annual A\n",
    "        prior_A = valid(c for c in candidates if c[0] == 'A' and rel(c) == 'prior')\n",
    "        if prior_A:\n",
    "            best = max(prior_A, key=lambda x: x[2])\n",
    "            return best[3]\n",
    "\n",
    "        # 4) Prior-year proxies (Q4/S2/T3) if no prior A\n",
    "        prior_proxies = valid(\n",
    "            c for c in candidates\n",
    "            if c[0] in ('Q4', 'S2', 'T3') and rel(c) == 'prior'\n",
    "        )\n",
    "        if prior_proxies:\n",
    "            best = max(prior_proxies, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # 5) Fallback: any candidate (other/unknown) by (priority, date)\n",
    "        others = valid(candidates)\n",
    "        if others:\n",
    "            best = max(others, key=lambda x: (x[1], x[2]))\n",
    "            return best[3]\n",
    "\n",
    "        # Final fallback: 0.0 (should rarely be reached)\n",
    "        return 0.0\n",
    "\n",
    "    # Apply the selection function row-wise to produce AnnPITValue\n",
    "    working['AnnPITValue'] = working.apply(pick_annpit_sum_with_origin, axis=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5) QC: Future-date check + PRE-DROP stats\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Columns whose dates should not exceed PIT Date\n",
    "    date_cols = [\n",
    "        'A_Date',\n",
    "        'Q1_Date', 'Q2_Date', 'Q3_Date', 'Q4_Date',\n",
    "        'S1_Date', 'S2_Date',\n",
    "        'T1_Date', 'T2_Date', 'T3_Date'\n",
    "    ]\n",
    "    # Restrict to ones actually present\n",
    "    present = [c for c in date_cols if c in working.columns]\n",
    "\n",
    "    viol_counts = {}  # per-label violation counts\n",
    "    # Mask for rows with any future-dated period\n",
    "    any_mask = pd.Series(False, index=working.index)\n",
    "\n",
    "    for c in present:\n",
    "        # A violation is when period date > PIT Date (both need to be non-null)\n",
    "        m = (\n",
    "            working[c].notna() &\n",
    "            working['PIT Date'].notna() &\n",
    "            (pd.to_datetime(working[c], errors='coerce') > working['PIT Date'])\n",
    "        )\n",
    "        viol_counts[c] = int(m.sum())\n",
    "        any_mask |= m  # accumulate violations across columns\n",
    "\n",
    "    total_future_viol = int(any_mask.sum())\n",
    "    print(\"\\n=== Future-date check (period dates > PIT Date) ===\")\n",
    "    print(\"Per-label violations:\", viol_counts)\n",
    "    print(f\"Rows with ANY future-dated period value: {total_future_viol}\")\n",
    "    # Flagrows with at least one future-date error\n",
    "    working['HasFutureDateError'] = any_mask\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6) AnnPITValue_Pct + quality drop\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute AnnPITValue as % of TrueValue (only when TrueValue != 0)\n",
    "    working['AnnPITValue_Pct'] = np.where(\n",
    "        working['AnnPITValue'].notna() &\n",
    "        working['TrueValue'].notna() &\n",
    "        (working['TrueValue'] != 0),\n",
    "        (working['AnnPITValue'] / working['TrueValue']) * 100,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Stats before dropping low-quality rows\n",
    "    pre_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary (finite only) — BEFORE quality drop ===\")\n",
    "    for k, v in pre_stats.items():\n",
    "        print(f\"{k:>20}: {v}\")\n",
    "\n",
    "    # Build masks for dropping:\n",
    "    pct = working['AnnPITValue_Pct']\n",
    "    is_inf = np.isinf(pct)  # infinite percentages\n",
    "    is_finite = np.isfinite(pct)\n",
    "    # Out-of-range if % > 200 or % < 50 (but finite)\n",
    "    out_of_range = is_finite & ((pct > 200) | (pct < 50))\n",
    "    # Rows to drop: infinite or out-of-range values\n",
    "    to_drop_quality = is_inf | out_of_range\n",
    "\n",
    "    dropped_quality_rows = int(to_drop_quality.sum())\n",
    "    print(f\"\\nRows to drop due to AnnPITValue_Pct (±inf or >200 or <50): {dropped_quality_rows:,}\")\n",
    "\n",
    "    # Keep only rows that passed the quality filter\n",
    "    working = working.loc[~to_drop_quality].copy()\n",
    "\n",
    "    # Stats after dropping\n",
    "    post_stats = summarize_pct(working['AnnPITValue_Pct'])\n",
    "    print(\"\\n=== AnnPITValue_Pct summary — AFTER quality drop ===\")\n",
    "    if post_stats:\n",
    "        for k, v in post_stats.items():\n",
    "            print(f\"{k:>20}: {v}\")\n",
    "    else:\n",
    "        print(\"No finite values remain after the quality drop.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7) Final columns & save\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Base descriptive columns to keep (if present)\n",
    "    base_cols = [\n",
    "        'ID', 'CompanyName', 'ImplCountry', 'CurrentCurrency', 'HistCurrency',\n",
    "        'PIT Date', 'Frequency', 'UpdateCode', 'FiscalPeriod', 'FYE Month',\n",
    "        'ItemCode', 'Value', 'Str_FiscalPrd'\n",
    "    ]\n",
    "\n",
    "    # Build ordered list of period date/value columns\n",
    "    freq_cols = []\n",
    "    for i in range(1, 5):\n",
    "        freq_cols += [f'Q{i}_Date', f'Q{i}']\n",
    "    for i in range(1, 3):\n",
    "        freq_cols += [f'S{i}_Date', f'S{i}']\n",
    "    for i in range(1, 4):\n",
    "        freq_cols += [f'T{i}_Date', f'T{i}']\n",
    "    freq_cols += ['A_Date', 'A']\n",
    "\n",
    "    # Final set of columns to keep in output\n",
    "    keep_cols = (\n",
    "        [c for c in base_cols if c in working.columns] +\n",
    "        ['TrueValue', 'AnnPITValue', 'AnnPITValue_Pct', 'HasFutureDateError'] +\n",
    "        [c for c in freq_cols if c in working.columns]\n",
    "    )\n",
    "\n",
    "    # Helper columns to drop before export\n",
    "    drop_cols = ['QNUM', 'SNUM', 'TNUM', 'TrueValue_Date']\n",
    "    # Also drop all *_OriginFP columns\n",
    "    drop_cols += [c for c in working.columns if c.endswith('_OriginFP')]\n",
    "    working.drop(columns=[c for c in drop_cols if c in working.columns],\n",
    "                 inplace=True, errors='ignore')\n",
    "\n",
    "    # Reorder and restrict columns to the final layout\n",
    "    mixed_processed = working.reindex(columns=keep_cols)\n",
    "\n",
    "    # Sanity checks: necessary globals must exist\n",
    "    assert 'Temp_file_path_DP' in globals(), \"Temp_file_path_DP not found.\"\n",
    "    assert 'base_output_filename' in globals(), \"base_output_filename not found (set in Cell 0).\"\n",
    "\n",
    "    # Build full output path and save pipe-delimited file\n",
    "    out_full = os.path.join(Temp_file_path_DP, f\"{base_output_filename}.txt\")\n",
    "    mixed_processed.to_csv(out_full, sep='|', index=False)\n",
    "    print(\"\\nSaved full:\", out_full)\n",
    "\n",
    "    # Create a subset for lighter inspection\n",
    "    subset_cols = [\"ID\", \"PIT Date\", \"CompanyName\", \"HistCurrency\",\n",
    "                   \"FiscalPeriod\", \"AnnPITValue\"]\n",
    "    # Only keep subset columns that actually exist\n",
    "    subset_cols_existing = [col for col in subset_cols if col in mixed_processed.columns]\n",
    "    subset_df = mixed_processed[subset_cols_existing].copy()\n",
    "    out_subset = os.path.join(Temp_file_path_DP,\n",
    "                              f\"{base_output_filename}_subset.txt\")\n",
    "    subset_df.to_csv(out_subset, sep='|', index=False)\n",
    "    print(\"Saved subset:\", out_subset)\n",
    "    # Explicitly delete subset_df to free memory\n",
    "    del subset_df\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8) Row-accounting overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_rows = len(mixed_processed)\n",
    "    print(\"\\n=== Row Accounting ===\")\n",
    "    print(f\"Input rows:                     {input_rows:,}\")\n",
    "    print(f\"Excludedby Frequency (E/L/R/U): {excluded_rows:,}\")\n",
    "    print(f\"Dropped by quality (Pct rules): {dropped_quality_rows:,}\")\n",
    "    print(f\"Output rows (final):            {output_rows:,}\")\n",
    "    # Sum up excluded + dropped + remaining and check against original count\n",
    "    check_total = excluded_rows + dropped_quality_rows + output_rows\n",
    "    print(f\"Check: excluded + dropped + output = {check_total:,}\")\n",
    "    if check_total == input_rows:\n",
    "        print(\"Row counts reconcile exactly.\")\n",
    "    else:\n",
    "        print(f\"Mismatch of {input_rows - check_total:+,} rows. \"\n",
    "              \"Investigate upstream filtering or unexpected drops.\")\n",
    "\n",
    "    # Trigger garbage collection as a final cleanup step\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    # If the main input dataset is not available, skip all processing\n",
    "    print(\"mixed_encoded not found or None; skipping.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN80RpXQFk6n+BCLQP5OgNG",
   "collapsed_sections": [
    "b06f6df4",
    "uQMCPR6XzurR",
    "9WkpqfMTNN5n",
    "Jiewmt0-enQr",
    "eOoCUcFp0NXg",
    "bc7c6fde",
    "0mt2zuCrWpqu",
    "b9d056d5",
    "YCYcPbuN7daJ",
    "tfymFmHKlaxp",
    "QvvZArvSlaxy",
    "FTgQiaZRlax0",
    "XxDnp4oTlax2",
    "jNeI0C596evi",
    "hdqMzznT6evk",
    "Qik10gqk6evl",
    "FUVVfe4m7irY",
    "kl2F2O9Z7irb",
    "Sy618EX27f-z",
    "Q1P0EPkJld6L",
    "LVPkkyBPld6M",
    "23tAphkild6N",
    "Yt2R_38_ld6O",
    "YezOgcNDemA3",
    "StJE6rLplfUL",
    "GhE_hKKNlfUO",
    "yP2kdbv8lfUP",
    "c6nPFyvKlfUQ",
    "bqwfFO1JenlH",
    "rYXZ1x5hlhAL",
    "BkFG1rgPlhAN",
    "Nb3kMzOtlhAO",
    "fk8C60tDlhAP",
    "T6c9PzcC6OOp",
    "VyjmiRpL6OO0",
    "ACFGXfeo6OO3",
    "POJ_JyKz6OO4",
    "hpv0bCD16OO6",
    "DJTHauUwmlXA",
    "wy8emrH0mlXO",
    "dkO_AqPWmlXU",
    "p-tZqZmZ74Kh",
    "tMZLiokn74Kk",
    "VCAiquVXeraA",
    "768TBlTPll2d",
    "9impe7mYll2e",
    "zxLTJ3rwll2f",
    "ZJHoFnUsll2g",
    "pmv5sQGjkadZ",
    "rwTT6Ghqkadb",
    "SYYmkDA0kadc",
    "w_PVN-U07-6h",
    "1GfGZvJL7-6k",
    "J3VTfZP_evHC",
    "dLn4aq_-ln7l",
    "-hmPmyusln7n",
    "MDTixK2xln7o",
    "D6Ghh2-uln7p"
   ],
   "history_visible": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
